{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports + general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms, datasets\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score, precision_recall_fscore_support, confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "from torchvision.models.resnet import conv3x3, _resnet, ResNet18_Weights\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageOps, Image\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# device = 'cpu' \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Paths to data\n",
    "data_folder = \"/mnt/d/Bachelor_work/data_for_model/kkanji2_known_unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreactBasicBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreactBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(PreactBasicBlock, self).__init__()\n",
    "\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv1(out)\n",
    "\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "\n",
    "        return out\n",
    "\n",
    "    # Check what is model params and conv3x3. Also normlayer is not used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cycle func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(model, threshold, loader, criterion, train=False, optimizer=None):\n",
    "    model.eval()\n",
    "    overall_loss = []\n",
    "    overall_accuracy = []\n",
    "    dataloader_iterator = iter(loader)\n",
    "\n",
    "    if train:\n",
    "        threshold.requires_grad = True\n",
    "    else:\n",
    "        threshold.requires_grad = False\n",
    "\n",
    "    for batch_id in tqdm(range(len(loader))):   \n",
    "        try:\n",
    "            X_batch, y_batch = next(dataloader_iterator)\n",
    "\n",
    "        except:\n",
    "            dataloader_iterator = iter(loader)\n",
    "            X_batch, y_batch = next(dataloader_iterator)\n",
    "    \n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            preds = model(X_batch) \n",
    "            confidences = torch.max(F.softmax(preds, dim=1), 1)[0]\n",
    "\n",
    "            binary_predictions = confidences > threshold\n",
    "            loss = criterion(binary_predictions, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                preds = model(X_batch) \n",
    "                confidences= torch.max(F.softmax(preds, dim=1), 1)[0]\n",
    "\n",
    "                binary_predictions = confidences > threshold\n",
    "                loss = criterion(binary_predictions, y_batch)\n",
    "\n",
    "        overall_loss.append(loss.item().mean())\n",
    "        accuracy = (binary_predictions == y_batch).float().mean().item()\n",
    "        overall_accuracy.append(accuracy)\n",
    "\n",
    "    mean_loss = sum(overall_loss) / len(loader)\n",
    "    mean_accuracy = sum(overall_accuracy) / len(loader)\n",
    "    return mean_loss.item(), mean_accuracy.item()\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset initialization\n",
    "def get_dataloaders(batch_size: int = 4096, train_test_indices_path: str = None):\n",
    "    # Define transformations\n",
    "    my_transform = transforms.Compose([\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.Resize(64),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize each channel to (-1, 1)  # Normalize to (-1, 1)\n",
    "            ])\n",
    "        \n",
    "    full_dataset = datasets.ImageFolder(root=data_folder, transform=my_transform)\n",
    "\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    # Split dataset into training and testing sets\n",
    "\n",
    "    if train_test_indices_path is not None and os.path.exists(train_test_indices_path):\n",
    "        with open(train_test_indices_path, \"rb\") as f:\n",
    "            indices = pickle.load(f)\n",
    "            train_indices = indices[\"train_indices\"]\n",
    "            test_indices = indices[\"test_indices\"]\n",
    "                \n",
    "    else:\n",
    "        train_indices, test_indices = train_test_split(\n",
    "            list(range(len(full_dataset))),\n",
    "            test_size=0.3,\n",
    "            stratify=[label for _, label in full_dataset.samples]\n",
    "        )\n",
    "        \n",
    "        # Save indices\n",
    "        with open(train_test_indices_path, \"wb\") as f:\n",
    "            pickle.dump({\"train_indices\": train_indices, \"test_indices\": test_indices}, f)\n",
    "\n",
    "    train_dataset = data.Subset(full_dataset, train_indices)\n",
    "    test_dataset = data.Subset(full_dataset, test_indices)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    trainloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    testloader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    return trainloader, testloader, full_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting stuff for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_stuff_for_training(pathes: dict = None, lr: float = 1e-3, scheduler_step_size: int = 10, scheduler_gamma: float = 0.1):    \n",
    "\n",
    "    stuff = {}\n",
    "    last_epoch = 0\n",
    "\n",
    "    if os.path.exists(pathes[\"last_epoch\"]):\n",
    "        last_epoch = np.load(pathes[\"last_epoch\"])\n",
    "    stuff[\"last_epoch\"] = last_epoch\n",
    "\n",
    "    model = _resnet(PreactBasicBlock, [2, 2, 2, 2], None, progress=False) # 'resnet18'\n",
    "    model.fc = nn.Linear(model.fc.in_features, 300)\n",
    "    \n",
    "    if os.path.exists(pathes[\"model\"]):\n",
    "        model.load_state_dict(torch.load(pathes[\"model\"]))\n",
    "    model = model.to(device)\n",
    "    stuff[\"model\"] = model\n",
    "\n",
    "    threshold = nn.Parameter(torch.tensor(0.5, requires_grad=True))\n",
    "    if os.path.exists(pathes[\"threshold\"]):\n",
    "        threshold = torch.load(pathes[\"threshold\"])\n",
    "    stuff[\"threshold\"] = threshold\n",
    "\n",
    "    optimizer = torch.optim.Adam([threshold], lr=lr) # , weight_decay=0.)\n",
    "    if os.path.exists(pathes[\"optimizer\"]):\n",
    "        optimizer.load_state_dict(torch.load(pathes[\"optimizer\"]))\n",
    "    stuff[\"optimizer\"] = optimizer\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step_size, gamma=scheduler_gamma)\n",
    "    if os.path.exists(pathes[\"scheduler\"]):\n",
    "        scheduler.load_state_dict(torch.load(pathes[\"scheduler\"]))\n",
    "    stuff[\"scheduler\"] = scheduler\n",
    "\n",
    "    train_loss_history = []\n",
    "    if os.path.exists(pathes[\"train_loss_history\"]):\n",
    "        train_loss_history = list(np.load(pathes[\"train_loss_history\"]))\n",
    "    stuff[\"train_loss_history\"] = train_loss_history\n",
    "\n",
    "    train_accuracy_history = []\n",
    "    if os.path.exists(pathes[\"train_accuracy_history\"]):\n",
    "        train_accuracy_history = list(np.load(pathes[\"train_accuracy_history\"]))\n",
    "    stuff[\"train_accuracy_history\"] = train_accuracy_history\n",
    "\n",
    "    val_loss_history = []\n",
    "    if os.path.exists(pathes[\"val_loss_history\"]):\n",
    "        val_loss_history = list(np.load(pathes[\"val_loss_history\"]))\n",
    "    stuff[\"val_loss_history\"] = val_loss_history\n",
    "\n",
    "    val_accuracy_history = []\n",
    "    if os.path.exists(pathes[\"val_accuracy_history\"]):\n",
    "        val_accuracy_history = list(np.load(pathes[\"val_accuracy_history\"]))\n",
    "    stuff[\"val_accuracy_history\"] = val_accuracy_history\n",
    "\n",
    "    max_val_accuracy = 0\n",
    "    if os.path.exists(pathes[\"max_val_accuracy\"]):\n",
    "        max_val_accuracy = np.load(pathes[\"max_val_accuracy\"])\n",
    "    stuff[\"max_val_accuracy\"] = max_val_accuracy\n",
    "\n",
    "    early_stopping = 5\n",
    "    if os.path.exists(pathes[\"early_stopping\"]):\n",
    "        early_stopping = np.load(pathes[\"early_stopping\"])\n",
    "    stuff[\"early_stopping\"] = early_stopping\n",
    "\n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_plot(train_data : list = None, val_data : list = None, title: str = None,  ylabel: str = None):\n",
    "    plt.plot(train_data, label=\"train\")\n",
    "    plt.plot(val_data, label=\"validation\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create pathes vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pathes_vocabulary(path : str = None):\n",
    "    pathes = {\"threshold\": f\"{path}/threshold.pth\",\"last_epoch\": f\"{path}/last_epoch.npy\", \"model\": f\"{path}/model.pth\", \"optimizer\": f\"{path}/optimizer.pth\", \n",
    "    \"scheduler\": f\"{path}/scheduler.pth\", \"train_loss_history\": f\"{path}/train_loss_history.npy\", \"train_accuracy_history\": f\"{path}/train_accuracy_history.npy\", \n",
    "    \"val_loss_history\": f\"{path}/val_loss_history.npy\", \"val_accuracy_history\": f\"{path}/val_accuracy_history.npy\", \n",
    "    \"max_val_accuracy\": f\"{path}/max_val_accuracy.npy\", \"early_stopping\": f\"{path}/early_stopping.npy\"}\n",
    "\n",
    "    return pathes\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(trainloader, testloader, stuff : dict = None, criterion=nn.CrossEntropyLoss(), save_path : str = None, epochs : int = 100, model_name : str = None):\n",
    "\n",
    "    last_epoch = stuff[\"last_epoch\"]\n",
    "    epochs += last_epoch \n",
    "\n",
    "    model = stuff[\"model\"]\n",
    "    threshold = stuff[\"threshold\"]\n",
    "    optimizer = stuff[\"optimizer\"]\n",
    "    scheduler = stuff[\"scheduler\"]\n",
    "\n",
    "    train_loss_history = stuff[\"train_loss_history\"]\n",
    "    train_accuracy_history = stuff[\"train_accuracy_history\"]\n",
    "\n",
    "    val_loss_history = stuff[\"val_loss_history\"]\n",
    "    val_accuracy_history = stuff[\"val_accuracy_history\"]\n",
    "\n",
    "    max_val_accuracy = stuff[\"max_val_accuracy\"]\n",
    "    early_stopping = stuff[\"early_stopping\"]\n",
    "\n",
    "    for epoch in tqdm(range(last_epoch, epochs)):\n",
    "\n",
    "        train_loss, train_accuracy = cycle(model, threshold, trainloader, criterion, train=True, optimizer=optimizer) #train(model, trainloader, criterion, optimizer, batch_size)\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_accuracy_history.append(train_accuracy)\n",
    "\n",
    "        val_loss, val_accuracy = cycle(model, threshold, testloader, criterion) #validate(model, testloader, criterion, batch_size)\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_accuracy_history.append(val_accuracy)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print('Epoch:', epoch+1)\n",
    "        print('Train: loss', train_loss, 'accuracy', train_accuracy)\n",
    "        print('Validation: loss', val_loss, 'accuracy', val_accuracy)\n",
    "\n",
    "        if val_accuracy > max_val_accuracy:\n",
    "            max_val_accuracy = val_accuracy\n",
    "            early_stopping = 5\n",
    "\n",
    "            # Save the model\n",
    "            torch.save(model.state_dict(), f'{save_path}/model.pth')\n",
    "\n",
    "            # Save the threshold\n",
    "            torch.save(threshold, f'{save_path}/threshold.pth')\n",
    "\n",
    "            # Save the optimizer\n",
    "            torch.save(optimizer.state_dict(), f'{save_path}/optimizer.pth')\n",
    "\n",
    "            # Save the scheduler\n",
    "            torch.save(scheduler.state_dict(), f'{save_path}/scheduler.pth')\n",
    "\n",
    "            # Save the loss history\n",
    "            np.save(f'{save_path}/train_loss_history.npy', train_loss_history)\n",
    "            np.save(f'{save_path}/val_loss_history.npy', val_loss_history)\n",
    "\n",
    "            # Save the accuracy history\n",
    "            np.save(f'{save_path}/train_accuracy_history.npy', train_accuracy_history)\n",
    "            np.save(f'{save_path}/val_accuracy_history.npy', val_accuracy_history)\n",
    "\n",
    "            # Save the last epoch\n",
    "            np.save(f'{save_path}/last_epoch.npy', epoch+1)\n",
    "\n",
    "            # Save max validation accuracy\n",
    "            np.save(f'{save_path}/max_val_accuracy.npy', max_val_accuracy)\n",
    "\n",
    "            # Save early stopping\n",
    "            np.save(f'{save_path}/early_stopping.npy', early_stopping)\n",
    "\n",
    "        else:\n",
    "            early_stopping -= 1\n",
    "\n",
    "        if early_stopping == 0:\n",
    "            break\n",
    "\n",
    "\n",
    "    print(f'Best validation accuracy for {model_name}:', max(val_accuracy_history), 'Epoch', val_accuracy_history.index(max(val_accuracy_history))+1)\n",
    "    return train_accuracy_history, val_accuracy_history, train_loss_history, val_loss_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
