{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_kkanji_dir = \"/home/beav3r/Bachelor_work/Bachelor_work/data/kkanji/kkanji2\"\n",
    "generative_kkanji_dir = \"/mnt/d/Bachelor_work/data_for_model/kkanji2_with_gen\"\n",
    "kkanji_for_GAN_dir = \"/mnt/d/Bachelor_work/data_for_model/kkanji_for_GAN\"\n",
    "kkanji_for_VAE_dir = \"/mnt/d/Bachelor_work/data_for_model/kkanji_for_VAE\"\n",
    "kkanji_for_aug2_dir = \"/mnt/d/Bachelor_work/data_for_model/kkanji_for_aug2\"\n",
    "kkanji_aug_only_dir = \"/mnt/d/Bachelor_work/data_for_model/kkanji_balanced_aug_only\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_inf = 9223372036854775807\n",
    "\n",
    "num_classes_GAN = 0\n",
    "for i in os.listdir(kkanji_for_GAN_dir):\n",
    "    num_classes_GAN += 1\n",
    "num_classes_GAN\n",
    "\n",
    "num_classes_VAE = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function for VAE\n",
    "def loss_function_VAE(reconstructed_img, original_img, mu, logvar):\n",
    "    # Reconstruction loss (using MSE or BCE)\n",
    "    recon_loss = F.mse_loss(reconstructed_img, original_img, reduction='sum')\n",
    "\n",
    "    # KL Divergence: regularizing the latent space\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    # Total loss: reconstruction + KL divergence\n",
    "    return recon_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.6313726..1.0].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f982328b010>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Image preprocessing for resnet 18\n",
    "\n",
    "# Preprocessing: Resize to 224x224 and stack the grayscale image into 3 channels\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),     # Resize to 224x224\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to 3-channel\n",
    "    transforms.ToTensor(),             # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalize\n",
    "])\n",
    "\n",
    "# Load and preprocess image\n",
    "image = Image.open('/home/beav3r/Bachelor_work/Bachelor_work/data/CASIA Online and Offline Chinese Handwriting Databases/My_processed_Gnt1.0Train/鹅/0.png')  # Open image \n",
    "\n",
    "\n",
    "# Save original image\n",
    "image.save('/mnt/d/Bachelor_work/data_for_model/test.png')\n",
    "\n",
    "# Show original image\n",
    "plt.imshow(image, cmap='gray')  # Show image\n",
    "plt.show()\n",
    "\n",
    "image_tensor_without_batch = preprocess(image)  # Preprocess image\n",
    "\n",
    "# Add batch dimension\n",
    "image_tensor = image_tensor_without_batch.unsqueeze(0)  # Add batch size\n",
    "\n",
    "# Show image\n",
    "plt.imshow(image_tensor[0].permute(1, 2, 0).numpy())  # Show image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-14: ['U+9808', 'U+6182', 'U+7169', 'U+7981', 'U+770B', 'U+5E97', 'U+98FD', 'U+8B02', 'U+56FA', 'U+5805', 'U+52FA', 'U+72D0', 'U+79AE', 'U+99C4', 'U+9591', 'U+6176', 'U+7C9F', 'U+7B39', 'U+9298', 'U+6271', 'U+57F7', 'U+65E7', 'U+697C', 'U+5DF2', 'U+8AFA', 'U+8A2D', 'U+5BE6', 'U+671F', 'U+6804', 'U+76DC', 'U+5951', 'U+8B49', 'U+5B78', 'U+6939', 'U+67AF', 'U+8CAF', 'U+53F7', 'U+536F', 'U+804A', 'U+86FE', 'U+99B3', 'U+7B0B', 'U+689D', 'U+5036', 'U+839E', 'U+5E0C', 'U+5B09', 'U+83F4', 'U+99C8', 'U+7BA1', 'U+53F2', 'U+5302', 'U+9D89', 'U+7384', 'U+96CC', 'U+6D32', 'U+9D2C', 'U+9685', 'U+52DE', 'U+91C8', 'U+5504', 'U+9C39', 'U+8F9E', 'U+7078', 'U+7ADF', 'U+9451', 'U+7434', 'U+6578', 'U+5C31', 'U+9B54', 'U+4FEF', 'U+82DE', 'U+6C60', 'U+523A', 'U+8429', 'U+99D5', 'U+6D74', 'U+6B6F', 'U+5BDD', 'U+5962', 'U+9818', 'U+7D9A', 'U+854E', 'U+6AFB', 'U+5215', 'U+8A8D', 'U+9B22', 'U+901F', 'U+96E2', 'U+6D78', 'U+4EF6', 'U+59D3', 'U+9244', 'U+602A', 'U+6F09', 'U+6276', 'U+8A82', 'U+548E', 'U+6190', 'U+6539', 'U+8B77', 'U+7C92', 'U+969B', 'U+9304', 'U+62DC', 'U+7544', 'U+5948', 'U+7FA4', 'U+4FF5', 'U+5897', 'U+7E21', 'U+67D0', 'U+8A98', 'U+9017', 'U+4F10', 'U+8B6C', 'U+7A1A', 'U+8ECD', 'U+5782', 'U+7261', 'U+6D5C', 'U+9D2B', 'U+63BB', 'U+62CD', 'U+755D', 'U+542B', 'U+81ED', 'U+8CA2', 'U+4F2F', 'U+85E5', 'U+975C', 'U+5E1D', 'U+6590', 'U+7C3E', 'U+675C', 'U+8B58', 'U+7CDE', 'U+65CF', 'U+5ED3', 'U+75B5', 'U+7AAE', 'U+5D07', 'U+8207', 'U+5875', 'U+8457', 'U+7FD2', 'U+5EAB', 'U+67F1', 'U+9119', 'U+58FA', 'U+733F', 'U+5468', 'U+6232', 'U+71D7', 'U+6562', 'U+6E20', 'U+592E', 'U+5FA9', 'U+58F7', 'U+8B7D', 'U+5FEB', 'U+72FC', 'U+7965', 'U+6155', 'U+5C01', 'U+9EDE', 'U+8CAA', 'U+673D', 'U+8A93', 'U+672D', 'U+62ED', 'U+6328', 'U+5DE3', 'U+79D1', 'U+5C61', 'U+834A', 'U+6FC1', 'U+6089', 'U+85FB', 'U+9707', 'U+6E2C', 'U+5FD9', 'U+914C', 'U+852D', 'U+7E2B', 'U+8FF0', 'U+8557', 'U+87BA', 'U+6715', 'U+6170', 'U+9189', 'U+76F2', 'U+6CAB', 'U+55A7', 'U+7DD2', 'U+95A8', 'U+5609', 'U+4EAB', 'U+84EC', 'U+6148', 'U+9192', 'U+9F9D', 'U+5B22', 'U+74E2', 'U+617E', 'U+51A0', 'U+9D08', 'U+62F6', 'U+6C72', 'U+5B9D', 'U+8CEA', 'U+9810', 'U+8F09', 'U+53AD', 'U+5712', 'U+6311', 'U+71ED', 'U+52D9', 'U+9AD4', 'U+8E0A', 'U+8463', 'U+7E70', 'U+75E9', 'U+8305', 'U+7A00', 'U+6842', 'U+6EC5', 'U+5098', 'U+6C5A', 'U+8569', 'U+5FDC', 'U+8A55', 'U+72B6', 'U+6DE1', 'U+7DB1', 'U+660C', 'U+50E5', 'U+58F2', 'U+7E2E', 'U+9769', 'U+96EB', 'U+5243', 'U+830E', 'U+62C2', 'U+85AE', 'U+4E82', 'U+5937', 'U+7624', 'U+6BBC', 'U+63A2', 'U+6241', 'U+66D9', 'U+6753', 'U+5C02', 'U+5320', 'U+6ADB', 'U+7A3D', 'U+628A', 'U+7C7E', 'U+5629', 'U+86AB', 'U+8131', 'U+66FE', 'U+9091', 'U+5AC1', 'U+9BE8', 'U+7551', 'U+7E69', 'U+9072', 'U+7BC7', 'U+5983', 'U+984D', 'U+9855', 'U+53CD', 'U+88E1', 'U+5016', 'U+820D', 'U+4E4E', 'U+88B4', 'U+5B66', 'U+9010', 'U+5EDF', 'U+63A8', 'U+8077', 'U+6DE8', 'U+5C07', 'U+6589', 'U+970A', 'U+5351', 'U+87A2', 'U+4EA5', 'U+7259', 'U+7E4B', 'U+5A5A', 'U+8D66', 'U+8A3B', 'U+9867', 'U+5C4A', 'U+7CE7', 'U+609F', 'U+5B5F', 'U+9C2D', 'U+5D29', 'U+5BDF', 'U+6E4A', 'U+5A01', 'U+52AA', 'U+68A2', 'U+58FD', 'U+817F', 'U+8F3F', 'U+596A', 'U+5BA5', 'U+6C7A', 'U+9B93', 'U+6714', 'U+4FAD', 'U+8218', 'U+68A8', 'U+6727', 'U+85AF', 'U+4E8E', 'U+5F83', 'U+5208', 'U+9041', 'U+6D3E', 'U+6EFF', 'U+59D1', 'U+51A5', 'U+79FB', 'U+57A2', 'U+9006', 'U+6E9C', 'U+61F7', 'U+8CD1', 'U+6BCB', 'U+5230', 'U+82B9', 'U+58DF', 'U+8E44', 'U+6697', 'U+5EFA', 'U+9322', 'U+9594', 'U+8A0E', 'U+8EB0', 'U+87C4', 'U+9A19', 'U+6A3D', 'U+67C4', 'U+50DE', 'U+4F76', 'U+927E', 'U+65BD', 'U+5353', 'U+6012', 'U+8CDC', 'U+6162', 'U+82A6', 'U+8CB8', 'U+6E3E', 'U+88DD', 'U+51CC', 'U+8178', 'U+535C', 'U+7947', 'U+75BE', 'U+4F38', 'U+7FA8', 'U+8766', 'U+57D2', 'U+81BD', 'U+6A58', 'U+50D5', 'U+9042', 'U+7A7F', 'U+8B83', 'U+59BE', 'U+82A5', 'U+68EE', 'U+652F', 'U+585A', 'U+6CC1', 'U+620C', 'U+74E6', 'U+5085', 'U+8102', 'U+99C5', 'U+7E7B', 'U+982C', 'U+9B1F', 'U+8475', 'U+65B7', 'U+4F3D', 'U+5BB4', 'U+708A', 'U+9DC4', 'U+80C6', 'U+7CFA', 'U+60A3', 'U+9069', 'U+72ED', 'U+6DF5', 'U+585E', 'U+9059', 'U+6F0F', 'U+8AB2', 'U+7560', 'U+8986', 'U+9E81', 'U+6551', 'U+50CD', 'U+7DF4', 'U+6D1B', 'U+51A8', 'U+80A9', 'U+72F8', 'U+62DD', 'U+8ACC', 'U+856A', 'U+6DAF', 'U+639F', 'U+73E0', 'U+7DEC', 'U+4EC7', 'U+983B', 'U+9A37', 'U+5186', 'U+4FA0', 'U+6CBC', 'U+9C2F', 'U+5BC5', 'U+4E11', 'U+60A6', 'U+7B1B', 'U+5996', 'U+8882', 'U+6A19', 'U+500D', 'U+88B7', 'U+932B', 'U+990C', 'U+674E', 'U+6DBC', 'U+9F4B', 'U+900F', 'U+971E', 'U+839A', 'U+54ED', 'U+7B95', 'U+5E37', 'U+6C57', 'U+9727', 'U+5E8F', 'U+6F54', 'U+5026', 'U+7CC0', 'U+62D9', 'U+7F77', 'U+633D', 'U+5E9C', 'U+674F', 'U+6652', 'U+8E8D', 'U+7E94', 'U+8CC3', 'U+7E1B', 'U+707D', 'U+9D5E', 'U+72E9', 'U+8613', 'U+7DCB', 'U+5CF0', 'U+7E45', 'U+6B96', 'U+8CBB', 'U+88F3', 'U+6F70', 'U+9AED', 'U+838A', 'U+9003', 'U+64CD', 'U+54B3', 'U+71C3', 'U+6020', 'U+55C5', 'U+95DC', 'U+5BE1', 'U+9234', 'U+512A', 'U+9B6F', 'U+91D8', 'U+8FF9', 'U+8B00', 'U+8155', 'U+9C57', 'U+90ED', 'U+6458', 'U+5112', 'U+8F15', 'U+8870', 'U+7A3F', 'U+55DC', 'U+82B8', 'U+96A8', 'U+6607', 'U+864E', 'U+8DEA', 'U+9BA8', 'U+849C', 'U+97FF', 'U+6CE8', 'U+83C5', 'U+60A9', 'U+6816', 'U+5DE8', 'U+67D1', 'U+51B6', 'U+507D', 'U+502D', 'U+8ADA', 'U+658E', 'U+7A62', 'U+7CBD', 'U+9BD6', 'U+7A81', 'U+698E', 'U+7363', 'U+64B2', 'U+72A2', 'U+683D', 'U+7D10', 'U+66C9', 'U+71E5', 'U+9C78', 'U+984C', 'U+9A12', 'U+9688', 'U+89C0', 'U+6666', 'U+7D17', 'U+9EBA', 'U+64F2', 'U+7656', 'U+6734', 'U+6850', 'U+86A4', 'U+904D', 'U+4E59', 'U+8521', 'U+64EC', 'U+85BA', 'U+541D', 'U+6075', 'U+6B0A', 'U+86DB', 'U+7126', 'U+72AC', 'U+5203', 'U+8996', 'U+68D2', 'U+887E', 'U+9802', 'U+9716', 'U+66B4', 'U+7DBE', 'U+5204', 'U+59D4', 'U+5078', 'U+5BDB', 'U+7FC5', 'U+64AB', 'U+8F49', 'U+84BF', 'U+65E8', 'U+566A', 'U+96DC', 'U+9EB9', 'U+6AC3', 'U+7AFA', 'U+6A80', 'U+7814', 'U+79E6', 'U+7AC8', 'U+8015', 'U+8997', 'U+5824', 'U+7AFF', 'U+6A59', 'U+8AAD', 'U+83B1', 'U+9599', 'U+8431', 'U+7109', 'U+83AB', 'U+8FE6', 'U+847A', 'U+98E9', 'U+4F98', 'U+970E', 'U+85A6', 'U+91AB', 'U+71F5', 'U+9759', 'U+5315', 'U+67CF', 'U+7B26', 'U+8549', 'U+89B3', 'U+6349', 'U+7E8C', 'U+8C48', 'U+666E', 'U+8B90', 'U+5F82', 'U+605F', 'U+797F', 'U+6B53', 'U+5835', 'U+8577', 'U+96F7', 'U+8F1D', 'U+87C7', 'U+6536', 'U+5893', 'U+58EC', 'U+4F9D', 'U+8499', 'U+7460', 'U+6DF7', 'U+59E8', 'U+6881', 'U+8FB0', 'U+63B4', 'U+9CF0', 'U+6F5C', 'U+9ED8', 'U+5E83', 'U+9C48', 'U+58F9', 'U+7B87', 'U+5D50', 'U+69AE', 'U+8C8C', 'U+4F73', 'U+7C83', 'U+91E1', 'U+5360', 'U+9BA0', 'U+8A1B', 'U+7AC3', 'U+8857', 'U+7D39', 'U+4F4E', 'U+81F4', 'U+9ECD', 'U+6C50', 'U+6591', 'U+9D5C', 'U+5C08', 'U+6CB8', 'U+6294', 'U+51C6', 'U+8201', 'U+6367', 'U+5384', 'U+51C4', 'U+62DB', 'U+82AD', 'U+98F4', 'U+72D7', 'U+80A1', 'U+5B8C', 'U+5DF4', 'U+6F64', 'U+665D', 'U+795F', 'U+7B56', 'U+7BC9', 'U+5F8B', 'U+594F', 'U+5E7D', 'U+6F5B', 'U+65EC', 'U+80DE', 'U+877F', 'U+52C7', 'U+85DD', 'U+7B97', 'U+623E', 'U+8D08', 'U+50CF', 'U+70DF', 'U+8706', 'U+907F', 'U+9F13', 'U+68D8', 'U+7B92', 'U+9BAB', 'U+5446', 'U+88DF', 'U+9945', 'U+9C0C', 'U+695A', 'U+5F27', 'U+614B', 'U+4FC2', 'U+6388', 'U+9E93', 'U+5C5E', 'U+4E1E', 'U+76E7', 'U+8B21', 'U+7D75', 'U+4FCA', 'U+4FC3', 'U+9B4F', 'U+557C', 'U+8CB3', 'U+96DB', 'U+614E', 'U+6775', 'U+7A3B', 'U+5F48', 'U+9798', 'U+99D2', 'U+727D', 'U+5EC9', 'U+873B', 'U+67D8', 'U+6854', 'U+9F61', 'U+6F45', 'U+85EA', 'U+8F14', 'U+85AB', 'U+638C', 'U+6841', 'U+5ECA', 'U+5BE9', 'U+8FB1', 'U+5BC2', 'U+6052', 'U+7DD1', 'U+9084', 'U+6DC0', 'U+8A66', 'U+8888', 'U+5C06', 'U+77E3', 'U+5A7F', 'U+7A57', 'U+5711', 'U+5D8C', 'U+871C', 'U+9B31', 'U+6173', 'U+8A89', 'U+5F93', 'U+885D', 'U+88FE', 'U+8CBC', 'U+4E4F', 'U+5C4D', 'U+9F52', 'U+5F14', 'U+555C', 'U+890C', 'U+5E3D', 'U+55DA', 'U+6CBD', 'U+9DAF', 'U+80AF', 'U+75F4', 'U+7164', 'U+55AB', 'U+6059', 'U+6982', 'U+8CC7', 'U+4ECD', 'U+8852', 'U+81E8', 'U+7DB8', 'U+898F', 'U+5854', 'U+6E8F', 'U+86F8', 'U+67B8', 'U+63FA', 'U+52A3', 'U+9D59', 'U+6897', 'U+8B17', 'U+6191', 'U+53F6', 'U+4E43', 'U+88F8', 'U+7621', 'U+7D3A', 'U+9320', 'U+86C9', 'U+656C', 'U+535A', 'U+641C', 'U+5AE9', 'U+8702', 'U+5323', 'U+55E3', 'U+623B', 'U+6F6E', 'U+8AF7', 'U+5DF3', 'U+8129', 'U+6258', 'U+5E45', 'U+7089', 'U+83F1', 'U+9F08', 'U+6B74', 'U+27752', 'U+96D6', 'U+533B', 'U+5F18', 'U+814E', 'U+50F9', 'U+5DEB', 'U+6681', 'U+6FA4', 'U+7DBA', 'U+7C8B', 'U+7AC4', 'U+6893', 'U+55B6', 'U+542E', 'U+9BF7', 'U+9673', 'U+7CEF', 'U+9C24', 'U+6777', 'U+52C5', 'U+8B1B', 'U+53B3', 'U+7344', 'U+682A', 'U+6D29', 'U+7701', 'U+8CD3', 'U+62D4', 'U+5642', 'U+6D1E', 'U+540A', 'U+50B7', 'U+8073', 'U+5FAE', 'U+9EE8', 'U+5217', 'U+8395', 'U+67F9', 'U+76DF', 'U+6C92', 'U+68A7', 'U+5BFF', 'U+543B', 'U+5475', 'U+6212', 'U+8568', 'U+7262', 'U+6A9C', 'U+56AE', 'U+7DE9', 'U+5294', 'U+56A2', 'U+7566', 'U+5378', 'U+8A34', 'U+68AD', 'U+8B93', 'U+71A8', 'U+6293', 'U+5F4C', 'U+660F', 'U+9285', 'U+66F3', 'U+8C61', 'U+5CE8', 'U+6919', 'U+978D', 'U+9675', 'U+7240', 'U+7573', 'U+6572', 'U+887F', 'U+5A92', 'U+6FEF', 'U+5DAE', 'U+7ACA', 'U+6101', 'U+6B61', 'U+53D4', 'U+525B', 'U+69CC', 'U+8096', 'U+68D7', 'U+7325', 'U+5BA3', 'U+8B19', 'U+6FE1', 'U+6021', 'U+83F0', 'U+5E7F', 'U+5BD3', 'U+7FF0', 'U+805F', 'U+8B72', 'U+633F', 'U+78A7', 'U+54BD', 'U+9CF3', 'U+8DE8', 'U+5C3F', 'U+811A', 'U+51DD', 'U+819A', 'U+4ED4', 'U+96DE', 'U+8956', 'U+6838', 'U+6E5B', 'U+7955', 'U+9B41', 'U+75AB', 'U+7766', 'U+758B', 'U+7F75', 'U+5C51', 'U+6CA1', 'U+6FDF', 'U+8A50', 'U+502B', 'U+9B45', 'U+81CD', 'U+6414', 'U+72F9', 'U+6DEB', 'U+83A7', 'U+5FDD', 'U+895F', 'U+7B65', 'U+6F06', 'U+9761', 'U+5815', 'U+86A2', 'U+51E0', 'U+89F8', 'U+613C', 'U+79E4', 'U+5109', 'U+8A54', 'U+751E', 'U+6B20', 'U+6A1F', 'U+7B6D', 'U+82B3', 'U+8B2B', 'U+77BF', 'U+4EDD', 'U+7D1B', 'U+8607', 'U+9DFA', 'U+69FF', 'U+8749', 'U+723E', 'U+9453', 'U+68DF', 'U+837B', 'U+62AB', 'U+50C5', 'U+7768', 'U+524A', 'U+61C7', 'U+4FAF', 'U+78C1', 'U+540F', 'U+68F9', 'U+5E16', 'U+9A57', 'U+936C', 'U+865E', 'U+68C4', 'U+5DBD', 'U+9C52', 'U+947F', 'U+4EC0', 'U+5076', 'U+993B', 'U+821C', 'U+670B', 'U+67D4', 'U+9663', 'U+8EF8', 'U+803B', 'U+87F6', 'U+725D', 'U+5872', 'U+5553', 'U+9A55', 'U+798D', 'U+6F02', 'U+619A', 'U+6D0B', 'U+53C9', 'U+693D', 'U+7210', 'U+76DE', 'U+9078', 'U+6249', 'U+6C93', 'U+7B75', 'U+9B91', 'U+7156', 'U+846D', 'U+61FA', 'U+8CC4', 'U+9909', 'U+8A6B', 'U+6BBB', 'U+6291', 'U+751C', 'U+4E89', 'U+82F1', 'U+9D51', 'U+6E38', 'U+9BCA', 'U+822C', 'U+8526', 'U+57FA', 'U+52C3', 'U+56C0', 'U+7E26', 'U+87FE', 'U+7985', 'U+5029', 'U+67A1', 'U+6773', 'U+60DF', 'U+9C0A', 'U+5A9A', 'U+65C1', 'U+8328', 'U+91C7', 'U+6E09', 'U+919C', 'U+9BA7', 'U+5A3C', 'U+7F3A', 'U+9F9C', 'U+5EF3', 'U+840D', 'U+6787', 'U+7B94', 'U+6627', 'U+8010', 'U+5608', 'U+8F03', 'U+813E', 'U+8074', 'U+68B5', 'U+900D', 'U+5925', 'U+844E', 'U+5589', 'U+53E1', 'U+6F15', 'U+7CDD', 'U+68E0', 'U+9913', 'U+88C2', 'U+889E', 'U+5A36', 'U+59E6', 'U+5B54', 'U+6D12', 'U+5638', 'U+5E87', 'U+8D16', 'U+602F', 'U+52B1', 'U+5955', 'U+9A28', 'U+9785', 'U+5C2D', 'U+4E18', 'U+546A', 'U+9C72', 'U+7BC4', 'U+7511', 'U+8AF1', 'U+935B', 'U+9264', 'U+6EF4', 'U+7669', 'U+6566', 'U+7C98', 'U+9177', 'U+673A', 'U+71BE', 'U+9742', 'U+5BD4', 'U+5E55', 'U+860B', 'U+6DF3', 'U+9C47', 'U+7CFB', 'U+5F81', 'U+4F3A', 'U+8782', 'U+80CE', 'U+5DBA', 'U+9817', 'U+5EB6', 'U+9739', 'U+61A4', 'U+7C3F', 'U+5371', 'U+8F2F', 'U+9047', 'U+53A8', 'U+8205', 'U+87BB', 'U+6717', 'U+53E2', 'U+6070', 'U+9928', 'U+79E9', 'U+8700', 'U+5492', 'U+8711', 'U+5E7B', 'U+64A5', 'U+7AA9', 'U+5366', 'U+57C3', 'U+88B1', 'U+8271', 'U+925B', 'U+6E9D', 'U+68B0', 'U+83B5', 'U+95D5', 'U+81D8', 'U+8CE6', 'U+7A37', 'U+907D', 'U+7329', 'U+6750', 'U+7FE0', 'U+9EBF', 'U+6D95', 'U+5A25', 'U+9D72', 'U+9B9F', 'U+8058', 'U+5A62', 'U+81A0', 'U+7E8F', 'U+7815', 'U+543C', 'U+70AE', 'U+7372', 'U+9DD7', 'U+92F8', 'U+5954', 'U+56E3', 'U+9682', 'U+7CCA', 'U+8D14', 'U+6A29', 'U+620A', 'U+67B6', 'U+6BBD', 'U+5C53', 'U+5C0E', 'U+60B6', 'U+7893', 'U+788E', 'U+8679', 'U+80E4', 'U+76E5', 'U+6A47', 'U+9AFB', 'U+8292', 'U+9C06', 'U+86CE', 'U+9C67', 'U+4EC2', 'U+7E7C', 'U+7B6C', 'U+5FFF', 'U+7D68', 'U+7761', 'U+8086', 'U+7FAE', 'U+7C43', 'U+83D6', 'U+8EBE', 'U+79E1', 'U+7DFB', 'U+5C60', 'U+6478', 'U+76B7', 'U+9EB5', 'U+863F', 'U+7D35', 'U+6894', 'U+752B', 'U+76BA', 'U+731C', 'U+6E0B', 'U+57DF', 'U+5984', 'U+95A3', 'U+52AB', 'U+7AD2', 'U+5145', 'U+70C8', 'U+5E0B', 'U+86EE', 'U+643A', 'U+56B4', 'U+8A17', 'U+5265', 'U+664B', 'U+9BD4', 'U+9396', 'U+6319', 'U+5C55', 'U+6230', 'U+816B', 'U+5ABD', 'U+9EE0', 'U+5023', 'U+991D', 'U+7A1C', 'U+7F78', 'U+6280', 'U+7F70', 'U+96F6', 'U+7E37', 'U+5E1B', 'U+82DF', 'U+693F', 'U+5358', 'U+598A', 'U+545F', 'U+67B4', 'U+6ED1', 'U+86F9', 'U+8D99', 'U+723C', 'U+65E0', 'U+5761', 'U+5EFC', 'U+6B3A', 'U+504F', 'U+6EBA', 'U+6483', 'U+64E7', 'U+9730', 'U+9CF6', 'U+6F14', 'U+63EE', 'U+731B', 'U+9F3E', 'U+53CC', 'U+87F7', 'U+7FB9', 'U+8CFD', 'U+5A23', 'U+8006', 'U+6CF0', 'U+6BEC', 'U+65D7', 'U+9700', 'U+6C40', 'U+9BF0', 'U+745E', 'U+6FEB', 'U+7248', 'U+7D5E', 'U+79A6', 'U+6B1D', 'U+675E', 'U+5978', 'U+8500', 'U+5BF5', 'U+91CB', 'U+5F3E', 'U+7D71', 'U+7F94', 'U+9EAA', 'U+7009', 'U+5D6F', 'U+605A', 'U+8165', 'U+792B', 'U+86C4', 'U+5544', 'U+75B1', 'U+68FA', 'U+5263', 'U+7BE0', 'U+82D1', 'U+7D99', 'U+5DE1', 'U+4FAE', 'U+9178', 'U+920D', 'U+966A', 'U+6F2B', 'U+7881', 'U+5339', 'U+7C97', 'U+9C36', 'U+9038', 'U+5E63', 'U+8F1B', 'U+84FC', 'U+5065', 'U+80C3', 'U+6467', 'U+88F9', 'U+9266', 'U+7D02', 'U+8133', 'U+5C16', 'U+6BC6', 'U+665E', 'U+8B1D', 'U+5FB9', 'U+5557', 'U+9D86', 'U+6392', 'U+865F', 'U+6208', 'U+9768', 'U+85CD', 'U+8466', 'U+629B', 'U+7E3E', 'U+8A8C', 'U+6015', 'U+58FC', 'U+805A', 'U+8AED', 'U+5960', 'U+5E9A', 'U+5199', 'U+91A2', 'U+582F', 'U+60F1', 'U+4E4D', 'U+76D2', 'U+5BB0', 'U+88C5', 'U+5DE2', 'U+69CD', 'U+69C3', 'U+9470', 'U+531C', 'U+7C11', 'U+9BF5', 'U+8F5F', 'U+5507', 'U+8202', 'U+54A8', 'U+8420', 'U+6127', 'U+6DBF', 'U+6BDF', 'U+50C9', 'U+5919', 'U+9B58', 'U+8821', 'U+8CA9', 'U+97A0', 'U+86DF', 'U+7960', 'U+63E1', 'U+56C3', 'U+8258', 'U+5B40', 'U+601C', 'U+5B8B', 'U+5F59', 'U+976D', 'U+6A8E', 'U+74B0', 'U+964B', 'U+8E74', 'U+509A', 'U+88DC', 'U+4FCE', 'U+5F70', 'U+58C7', 'U+70B3', 'U+8862', 'U+87E0', 'U+5EE2', 'U+51CB', 'U+8881', 'U+5FA8', 'U+6668', 'U+7E5E', 'U+7AC5', 'U+761F', 'U+61F2', 'U+8B9A', 'U+9EA9', 'U+5211', 'U+8805', 'U+9940', 'U+6063', 'U+803D', 'U+8602', 'U+6AC2', 'U+7DA2', 'U+708E', 'U+84BC', 'U+82E3', 'U+91AF', 'U+7A17', 'U+7825', 'U+6092', 'U+5B51', 'U+8A23', 'U+5640', 'U+97EE', 'U+56C2', 'U+6248', 'U+8944', 'U+8562', 'U+241C6', 'U+8331', 'U+722D', 'U+74A7', 'U+626E', 'U+8D05', 'U+608C', 'U+87C6', 'U+7578', 'U+9949', 'U+6FD5', 'U+9C23', 'U+7AF6', 'U+80DD', 'U+7114', 'U+7409', 'U+8540', 'U+584A', 'U+FA5C', 'U+9065', 'U+9F2B', 'U+5289', 'U+98DC', 'U+7BAD', 'U+55AA', 'U+8E30', 'U+78D0', 'U+6E07', 'U+690B', 'U+9665', 'U+7E0A', 'U+6874', 'U+66F9', 'U+5403', 'U+86EF', 'U+95A4', 'U+8347', 'U+8168', 'U+5C04', 'U+7CB3', 'U+98AF', 'U+614C', 'U+99D0', 'U+9C29', 'U+867B', 'U+9174', 'U+8925', 'U+75B2', 'U+74E0', 'U+5480', 'U+79EC', 'U+9942', 'U+9C4F', 'U+7CD5', 'U+8C5A', 'U+4F75', 'U+86AF', 'U+74CF', 'U+5BA6', 'U+58AE', 'U+777E', 'U+7684', 'U+9A13', 'U+5EC3', 'U+91AA', 'U+90C1', 'U+526A', 'U+9F0E', 'U+756B', 'U+9677', 'U+55A9', 'U+8CA8', 'U+7378', 'U+6CF3', 'U+809A', 'U+6398', 'U+99ED', 'U+7957', 'U+5840', 'U+81C9', 'U+8438', 'U+96FB', 'U+5018', 'U+9786', 'U+6CE1', 'U+7ABB', 'U+95CD', 'U+7E1E', 'U+9295', 'U+9077', 'U+773A', 'U+5121', 'U+84A1', 'U+7E3A', 'U+983D', 'U+8B92', 'U+54F2', 'U+5287', 'U+881C', 'U+7642', 'U+901D', 'U+96F9', 'U+4E19', 'U+9DEF', 'U+63A0', 'U+689F', 'U+918D', 'U+9921', 'U+72AF', 'U+7984', 'U+53A0', 'U+87D2', 'U+526F', 'U+4FDF', 'U+985B', 'U+66A6', 'U+67FB', 'U+8D6B', 'U+8910', 'U+9475', 'U+74BD', 'U+67FE', 'U+9BD1', 'U+814B', 'U+5A9B', 'U+849F', 'U+851E', 'U+7F88', 'U+633E', 'U+5F69', 'U+95E2', 'U+79BD', 'U+62CC', 'U+5080', 'U+5E06', 'U+6389', 'U+6A12', 'U+7483', 'U+73B2', 'U+4FE4', 'U+7D45', 'U+9190', 'U+93B0', 'U+7119', 'U+67E9', 'U+6D85', 'U+8E81', 'U+76D6', 'U+8166', 'U+6DF9', 'U+6FB3', 'U+7953', 'U+5A2F', 'U+6E13', 'U+64C2', 'U+9AB8', 'U+7E46', 'U+8B4F', 'U+9C10', 'U+5B64', 'U+8B5A', 'U+6905', 'U+6EC4', 'U+8B8A', 'U+61B6', 'U+69BE', 'U+505C', 'U+8A95', 'U+6A90', 'U+62F3', 'U+5178', 'U+93AE', 'U+874B', 'U+7C0D', 'U+8A87', 'U+72A0', 'U+50F5', 'U+9013', 'U+6C83', 'U+9D50', 'U+7A20', 'U+67CA', 'U+6676', 'U+9C3B', 'U+9354', 'U+69B4', 'U+5147', 'U+9676', 'U+68AF', 'U+8ACF', 'U+978B', 'U+7267', 'U+FA1D', 'U+80EF', 'U+883B', 'U+7239', 'U+8318', 'U+66DD', 'U+75B3', 'U+5C40', 'U+60A7', 'U+8317', 'U+5F90', 'U+798A', 'U+958F', 'U+593E', 'U+88C1', 'U+7E92', 'U+6416', 'U+24FA3', 'U+9AD9', 'U+50FB', 'U+FA38', 'U+83EB', 'U+8AB9', 'U+5275', 'U+518C', 'U+6F23', 'U+66E0', 'U+7554', 'U+9BAA', 'U+7328', 'U+75D8', 'U+7C4D', 'U+54FD', 'U+6096', 'U+7375', 'U+99DF', 'U+816E', 'U+696B', 'U+8DCB', 'U+5A35', 'U+9214', 'U+8FEB', 'U+4FF8', 'U+8671', 'U+7D21', 'U+93CA', 'U+7DC7', 'U+8FA8', 'U+7D2F', 'U+840C', 'U+51CD', 'U+570D', 'U+677C', 'U+8703', 'U+9C08', 'U+662D', 'U+5617', 'U+84BB', 'U+649E', 'U+9C6A', 'U+8597', 'U+5F91', 'U+9CEB', 'U+5C6F', 'U+5F11', 'U+9DE6', 'U+83D8', 'U+9237', 'U+828D', 'U+89DC', 'U+8E4A', 'U+63A9', 'U+62D2', 'U+82E1', 'U+93AD', 'U+5146', 'U+5D69', 'U+4F0D', 'U+68BC', 'U+62D7', 'U+6C08', 'U+83D4', 'U+9952', 'U+6A61', 'U+6867', 'U+5D11', 'U+9BD2', 'U+70D9', 'U+6226', 'U+8543', 'U+68F2', 'U+572D', 'U+6F3F', 'U+70F9', 'U+8693', 'U+7CAE', 'U+7D6E', 'U+6953', 'U+88D9', 'U+7912', 'U+9958', 'U+5AD7', 'U+6548', 'U+9EDB', 'U+54B8', 'U+7AB6', 'U+5056', 'U+9F07', 'U+68B3', 'U+53DB', 'U+8334', 'U+8514', 'U+55FD', 'U+6A97', 'U+771B', 'U+8303', 'U+7433', 'U+9B87', 'U+9C15', 'U+6995', 'U+6DB8', 'U+81C8', 'U+6853', 'U+6762', 'U+6634', 'U+7058', 'U+7A95', 'U+60FA', 'U+5011', 'U+9E8C', 'U+51B4', 'U+55D4', 'U+67E4', 'U+62CA', 'U+9830', 'U+8E87', 'U+83DF', 'U+7F9E', 'U+7459', 'U+9871', 'U+60C6', 'U+69D6', 'U+87DB', 'U+5B5C', 'U+7149', 'U+7682', 'U+8E47', 'U+68CD', 'U+7F54', 'U+793A', 'U+81C6', 'U+5598', 'U+833C', 'U+9686', 'U+6CD4', 'U+8796', 'U+78E4', 'U+977C', 'U+9870', 'U+5A11', 'U+9AC4', 'U+99DD', 'U+61C8', 'U+9EB4', 'U+606D', 'U+9A2B', 'U+620E', 'U+75E2', 'U+63CF', 'U+795A', 'U+6165', 'U+95CC', 'U+9B2A', 'U+6537', 'U+7737', 'U+5AB3', 'U+9005', 'U+8012', 'U+68C9', 'U+5DCC', 'U+8DCF', 'U+6B3D', 'U+8396', 'U+9AF7', 'U+6726', 'U+74EE', 'U+7403', 'U+8EAC', 'U+6961', 'U+66B9', 'U+8DDF', 'U+9D7A', 'U+91F5', 'U+52F8', 'U+5516', 'U+903C', 'U+9910', 'U+6BD8', 'U+9DAB', 'U+9E88', 'U+81D1', 'U+69EA', 'U+62BD', 'U+676D', 'U+89E6', 'U+6CC4', 'U+9E75', 'U+557B', 'U+7B67', 'U+640F', 'U+7A3E', 'U+5D14', 'U+6F66', 'U+6167', 'U+85A8', 'U+6602', 'U+86DE', 'U+8563', 'U+5F09', 'U+5319', 'U+7950', 'U+8D67', 'U+5AF0', 'U+6803', 'U+77BC', 'U+7CC4', 'U+9B66', 'U+8339', 'U+80B1', 'U+87C0', 'U+906D', 'U+8CDB', 'U+9156', 'U+8A85', 'U+9DF2', 'U+29780', 'U+876E', 'U+5354', 'U+8630', 'U+9B1B', 'U+7C1F', 'U+8734', 'U+9903', 'U+5EF1', 'U+8062', 'U+5B41', 'U+67B2', 'U+9837', 'U+514B', 'U+9C12', 'U+9BCE', 'U+980C', 'U+8A1D', 'U+7515', 'U+566C', 'U+8755', 'U+77E9', 'U+9ED0', 'U+559F', 'U+5A1F', 'U+8098', 'U+9C7A', 'U+5F98', 'U+8299', 'U+4FB2', 'U+9C14', 'U+71E7', 'U+89E7', 'U+63B2', 'U+98FE', 'U+8C7A', 'U+717B', 'U+8A02', 'U+6108', 'U+8C49', 'U+77AC', 'U+75B9', 'U+6EA2', 'U+6F32', 'U+9724', 'U+7D46', 'U+9AEF', 'U+857A', 'U+5C41', 'U+7E4A', 'U+65A7', 'U+90AF', 'U+9089', 'U+6BC0', 'U+695C', 'U+73CA', 'U+7334', 'U+604D', 'U+6556', 'U+62C5', 'U+88D8', 'U+7DAD', 'U+62C7', 'U+7B19', 'U+932C', 'U+936E', 'U+811B', 'U+85FF', 'U+5B0B', 'U+9BA6', 'U+8EAF', 'U+6060', 'U+5436', 'U+90DB', 'U+6B2C', 'U+839F', 'U+976A', 'U+9BF2', 'U+62EC', 'U+4E99', 'U+882E', 'U+65AC', 'U+4093', 'U+7A4F', 'U+7B72', 'U+698A', 'U+87DF', 'U+5F6B', 'U+6ED3', 'U+9D12', 'U+621F', 'U+6759', 'U+9D5D', 'U+5C69', 'U+6E23', 'U+97FB', 'U+8D0F', 'U+81C2', 'U+8515', 'U+25DA1', 'U+5AB1', 'U+6518', 'U+8708', 'U+8F9F', 'U+7279', 'U+58EF', 'U+838E', 'U+8CFA', 'U+621D', 'U+6A8B', 'U+79B1', 'U+7738', 'U+5048', 'U+6056', 'U+5DD6', 'U+79B0', 'U+56C9', 'U+90A6', 'U+9859', 'U+5239', 'U+63E9', 'U+7962', 'U+8D04', 'U+9B44', 'U+FA4A', 'U+6492', 'U+6BEB', 'U+76B9', 'U+68D0', 'U+62D0', 'U+678B', 'U+786B', 'U+937E', 'U+69D9', 'U+894C', 'U+863C', 'U+83EA', 'U+6168', 'U+4EA8', 'U+7552', 'U+64D2', 'U+6E7E', 'U+9756', 'U+8EC6', 'U+7E4D', 'U+82F5', 'U+79E7', 'U+562F', 'U+4572', 'U+7F5F', 'U+6FF5', 'U+75DE', 'U+6822', 'U+4E98', 'U+7B06', 'U+8F4D', 'U+8941', 'U+632B', 'U+634F', 'U+9C6E', 'U+8594', 'U+9B51', 'U+4C61', 'U+7A88', 'U+8759', 'U+9C25', 'U+5B30', 'U+64AE', 'U+7C12', 'U+59DA', 'U+9149', 'U+6951', 'U+91C4', 'U+7252', 'U+8413', 'U+77DA', 'U+5B32', 'U+9811', 'U+6959', 'U+737A', 'U+97ED', 'U+6801', 'U+6EF8', 'U+91B4', 'U+7B4F', 'U+64DA', 'U+7627', 'U+63AC', 'U+6F84', 'U+633A', 'U+7E11', 'U+659C', 'U+655D', 'U+72CE', 'U+6ABB', 'U+6AD3', 'U+745A', 'U+96D5', 'U+84C9', 'U+7DE1', 'U+8B20', 'U+7F79', 'U+8872', 'U+734F', 'U+610D', 'U+775A', 'U+6196', 'U+5830', 'U+7832', 'U+93E4', 'U+8123', 'U+96D9', 'U+9132', 'U+696F', 'U+8D85', 'U+6F78', 'U+9410', 'U+6FC2', 'U+524B', 'U+6255', 'U+595A', 'U+7D22', 'U+62D3', 'U+9B12', 'U+6684', 'U+6821', 'U+85F7', 'U+6F3B', 'U+6E67', 'U+5751', 'U+937C', 'U+8CB6', 'U+8618', 'U+5747', 'U+8822', 'U+5BB8', 'U+7C46', 'U+86C1', 'U+75FF', 'U+83BE', 'U+83A8', 'U+59A8', 'U+8778', 'U+901E', 'U+5B36', 'U+9E9F', 'U+77DB', 'U+6F6F', 'U+6960', 'U+8764', 'U+6279', 'U+942B', 'U+87F2', 'U+992C', 'U+6574', 'U+7C5F', 'U+4543', 'U+558B', 'U+7F9A', 'U+5FA0', 'U+8C4C', 'U+51F9', 'U+6760', 'U+FA45', 'U+96BB', 'U+5B70', 'U+6A14', 'U+8404', 'U+9C3E', 'U+5C50', 'U+85B7', 'U+58CC', 'U+6772', 'U+7630', 'U+67A9', 'U+637B', 'U+60B5', 'U+91FF', 'U+6C7E', 'U+83C1', 'U+95BB', 'U+5EB8', 'U+6AE8', 'U+7A6A', 'U+7A9F', 'U+84CD', 'U+93E5', 'U+967A', 'U+8901', 'U+9821', 'U+59EA', 'U+7678', 'U+559D', 'U+7208', 'U+8FE9', 'U+7B8F', 'U+69F9', 'U+9805', 'U+7030', 'U+755A', 'U+5583', 'U+66FC', 'U+85D0', 'U+7A0D', 'U+7FE1', 'U+80FC', 'U+9E9D', 'U+8E06', 'U+9C5A', 'U+5632', 'U+7165', 'U+5AE1', 'U+7B50', 'U+8621', 'U+7BFC', 'U+83CC', 'U+5F73', 'U+7387', 'U+4C99', 'U+9C45', 'U+831C', 'U+6187', 'U+8E48', 'U+630C', 'U+52DF', 'U+63F4', 'U+60F0', 'U+4EAE', 'U+9BAC', 'U+5141', 'U+9834', 'U+4E17', 'U+8912', 'U+5E62', 'U+840A', 'U+7280', 'U+9F6C', 'U+8513', 'U+7762', 'U+746A', 'U+6ADA', 'U+6460', 'U+62FF', 'U+8A91', 'U+9C3A', 'U+723D', 'U+7C82', 'U+68E7', 'U+5BE7', 'U+8CED', 'U+82EB', 'U+69BF', 'U+8E1D', 'U+631F', 'U+5B99', 'U+7977', 'U+968B', 'U+7CF6', 'U+9C41', 'U+97C3', 'U+9F34', 'U+87CB', 'U+9D3D', 'U+4F47', 'U+888D', 'U+7E35', 'U+939A', 'U+92E4', 'U+5EC8', 'U+7724', 'U+9082', 'U+514C', 'U+9E95', 'U+731F', 'U+83DD', 'U+7E61', 'U+6974', 'U+8C3F', 'U+9435', 'U+736C', 'U+7C54', 'U+55C7', 'U+916C', 'U+8E64', 'U+8018', 'U+5C09', 'U+7C69', 'U+845C', 'U+9932', 'U+681D', 'U+737B', 'U+53DF', 'U+8757', 'U+8725', 'U+7CDC', 'U+7F60', 'U+5C28', 'U+83FD', 'U+672E', 'U+5F17', 'U+71FF', 'U+8E1E', 'U+6A0B', 'U+9EFD', 'U+6159', 'U+9283', 'U+90CA', 'U+5E5F', 'U+72C4', 'U+86BA', 'U+9C46', 'U+8A5B', 'U+9169', 'U+8913', 'U+8972', 'U+5BE8', 'U+6FA1', 'U+9951', 'U+86A3', 'U+716C', 'U+6E24', 'U+559E', 'U+69C1', 'U+7E55', 'U+7246', 'U+5104', 'U+887D', 'U+7A76', 'U+7A31', 'U+7F4C', 'U+6A66', 'U+75A3', 'U+98AD', 'U+810A', 'U+62B5', 'U+7A97', 'U+7C6C', 'U+9BA5', 'U+7CB2', 'U+9394', 'U+85B9', 'U+8DD6', 'U+9DC2', 'U+6236', 'U+58EE', 'U+9A45', 'U+7BF7', 'U+514A', 'U+91BF', 'U+5D16', 'U+699B', 'U+522A', 'U+59CA', 'U+8C55', 'U+86CD', 'U+54E2', 'U+4F69', 'U+5269', 'U+5256', 'U+64E1', 'U+8CBF', 'U+9F2C', 'U+7396', 'U+9F4A', 'U+7AEA', 'U+964C', 'U+621B', 'U+72DB', 'U+70AC', 'U+9DC9', 'U+9C4A', 'U+822A', 'U+8803', 'U+596E', 'U+69C7', 'U+8182', 'U+77BB', 'U+52E6', 'U+5CE0', 'U+85FA', 'U+9ECE', 'U+7BD7', 'U+29DDA', 'U+7919', 'U+6A13', 'U+5EF7', 'U+59A3', 'U+87AB', 'U+9A62', 'U+7235', 'U+7A3C', 'U+659B', 'U+95D6', 'U+7422', 'U+8004', 'U+5D19', 'U+4EED', 'U+67B7', 'U+7B25', 'U+608B', 'U+984B', 'U+99C6', 'U+973D', 'U+55DF', 'U+29E75', 'U+84F4', 'U+71DF', 'U+5CA8', 'U+96C7', 'U+9B5F', 'U+691C', 'U+9F3A', 'U+652C', 'U+8F9C', 'U+501A', 'U+58F3', 'U+8B5C', 'U+9B0C', 'U+9B1A', 'U+837C', 'U+63DB', 'U+9E7D', 'U+9310', 'U+8710', 'U+4E42', 'U+8AA6', 'U+6F5F', 'U+5B95', 'U+874E', 'U+7E3D', 'U+5852', 'U+8E2A', 'U+5E25', 'U+57F5', 'U+5C05', 'U+7E3B', 'U+9306', 'U+86B8', 'U+858F', 'U+7500', 'U+724C', 'U+8DF3', 'U+516A', 'U+73ED', 'U+52C9', 'U+9BEF', 'U+53D9', 'U+99A8', 'U+7891', 'U+9E8B', 'U+5C4E', 'U+7BE6', 'U+8629', 'U+7652', 'U+79E3', 'U+9A6B', 'U+61F6', 'U+9B29', 'U+8EA1', 'U+9698', 'U+63BE', 'U+54E1', 'U+7B18', 'U+9F5F', 'U+62D8', 'U+827E', 'U+826A', 'U+8AEB', 'U+9B73', 'U+74CA', 'U+5398', 'U+7763', 'U+5BC7', 'U+8729', 'U+9C76', 'U+7BAA', 'U+8FC2', 'U+69AA', 'U+859C', 'U+8461', 'U+7CB9', 'U+8831', 'U+6B27', 'U+88D4', 'U+9E0A', 'U+6B9E', 'U+53CE', 'U+7187', 'U+9E1E', 'U+812F', 'U+8760', 'U+6B47', 'U+8789', 'U+8E8A', 'U+8787', 'U+5DF7', 'U+574F', 'U+7B4D', 'U+9B3B', 'U+858A', 'U+8591', 'U+515C', 'U+9BB2', 'U+75F0', 'U+60FB', 'U+8753', 'U+54AB', 'U+84D9', 'U+6D2A', 'U+5CFD', 'U+69CE', 'U+659F', 'U+6DB2', 'U+7D2C', 'U+974D', 'U+9AE9', 'U+775C', 'U+8B0E', 'U+58DC', 'U+615D', 'U+52FE', 'U+8DFC', 'U+6BAF', 'U+6583', 'U+57D3', 'U+54A5', 'U+8476', 'U+870D', 'U+82D3', 'U+86B6', 'U+50B5', 'U+8216', 'U+722C', 'U+8581', 'U+6620', 'U+8CC8', 'U+7C17', 'U+821B', 'U+8AA8', 'U+9B18', 'U+5EE9', 'U+9C63', 'U+85DC', 'U+9D1E', 'U+8236', 'U+5BE4', 'U+68B6', 'U+7BC6', 'U+9444', 'U+8A25', 'U+920E', 'U+6991', 'U+9403', 'U+878D', 'U+7C4F', 'U+873E', 'U+8ADB', 'U+9DBA', 'U+8D74', 'U+8774', 'U+95BC', 'U+520A', 'U+6840', 'U+58D2', 'U+9B97', 'U+7FD4', 'U+5271', 'U+5F8A', 'U+9DC3', 'U+5CB3', 'U+9B85', 'U+8061', 'U+9F62', 'U+9B2E', 'U+82EA', 'U+82E7', 'U+783A', 'U+7D43', 'U+7827', 'U+5EB7', 'U+8385', 'U+58BE', 'U+721B', 'U+57DC', 'U+8DCC', 'U+870A', 'U+78FD', 'U+66DC'] (2556) AVG: 4.046948356807512\n",
      "15-29: ['U+86C7', 'U+582A', 'U+5BA4', 'U+96A0', 'U+622F', 'U+8A6E', 'U+501F', 'U+8CDE', 'U+7D76', 'U+969C', 'U+6355', 'U+52E4', 'U+8FB2', 'U+5967', 'U+7D0B', 'U+4F9B', 'U+576A', 'U+671B', 'U+6A21', 'U+5169', 'U+5947', 'U+559C', 'U+5B58', 'U+7027', 'U+5074', 'U+5C0A', 'U+4E92', 'U+8A69', 'U+5CA1', 'U+76E1', 'U+6DFA', 'U+84B2', 'U+5DFE', 'U+4E08', 'U+7DB2', 'U+6247', 'U+59DC', 'U+8868', 'U+5800', 'U+820E', 'U+963F', 'U+6B32', 'U+5E78', 'U+9054', 'U+6C42', 'U+5DE5', 'U+8B70', 'U+4F1A', 'U+6C5D', 'U+7232', 'U+7A40', 'U+76C6', 'U+647A', 'U+829D', 'U+59EB', 'U+6CE5', 'U+4FDD', 'U+751A', 'U+508D', 'U+5531', 'U+71B1', 'U+7D79', 'U+75DB', 'U+7834', 'U+6E90', 'U+908A', 'U+5FFD', 'U+5834', 'U+6CCA', 'U+868A', 'U+7E6A', 'U+719F', 'U+8A13', 'U+5152', 'U+9694', 'U+5451', 'U+604B', 'U+62B1', 'U+5E2F', 'U+6674', 'U+76D7', 'U+9D8F', 'U+9DF9', 'U+661F', 'U+4E32', 'U+60E1', 'U+7A93', 'U+53D7', 'U+56F0', 'U+79BF', 'U+4EA6', 'U+7070', 'U+9806', 'U+6613', 'U+9BAD', 'U+958B', 'U+96C1', 'U+9332', 'U+5E84', 'U+690D', 'U+4ECF', 'U+67F4', 'U+6043', 'U+544A', 'U+8CE2', 'U+611A', 'U+6DCB', 'U+611F', 'U+6606', 'U+808C', 'U+65BC', 'U+78E8', 'U+5E96', 'U+821E', 'U+7368', 'U+59B9', 'U+5E2D', 'U+8981', 'U+7CA5', 'U+81BE', 'U+91CF', 'U+6094', 'U+7709', 'U+88AB', 'U+5E76', 'U+77ED', 'U+697D', 'U+5BF6', 'U+4E0E', 'U+6691', 'U+57A3', 'U+96D1', 'U+5C13', 'U+8776', 'U+6F38', 'U+64B0', 'U+7D20', 'U+5EB5', 'U+8302', 'U+71D2', 'U+5047', 'U+6A5F', 'U+5831', 'U+98EE', 'U+7591', 'U+5E02', 'U+6575', 'U+76E4', 'U+9CE9', 'U+711A', 'U+9E97', 'U+5B97', 'U+61D0', 'U+54C0', 'U+519D', 'U+754C', 'U+5B9B', 'U+8A3C', 'U+8A73', 'U+9014', 'U+7E04', 'U+8FBB', 'U+63A5', 'U+5FE0', 'U+7159', 'U+8FDA', 'U+7A32', 'U+5F92', 'U+5100', 'U+5F31', 'U+5B98', 'U+52FF', 'U+5A46', 'U+685C', 'U+8CA0', 'U+683C', 'U+722A', 'U+80B2', 'U+8840', 'U+78EF', 'U+4E7E', 'U+6068', 'U+5F25', 'U+9664', 'U+5F13', 'U+56F2', 'U+4FF3', 'U+87F9', 'U+819D', 'U+718A', 'U+6BBA', 'U+6417', 'U+54B2', 'U+820C', 'U+51F6', 'U+FA55', 'U+9813', 'U+71D5', 'U+6687', 'U+81FC', 'U+59AC', 'U+4E80', 'U+6E08', 'U+7092', 'U+71C8', 'U+5ACC', 'U+71AC', 'U+73FE', 'U+5F0F', 'U+6756', 'U+83EF', 'U+6703', 'U+518A', 'U+63C9', 'U+640D', 'U+66AB', 'U+7167', 'U+632F', 'U+8276', 'U+5993', 'U+6D99', 'U+FA11', 'U+4EA1', 'U+8304', 'U+629C', 'U+7BE4', 'U+5C1A', 'U+9748', 'U+6731', 'U+60E0', 'U+541F', 'U+9DB4', 'U+6069', 'U+842C', 'U+6A39', 'U+5370', 'U+5272', 'U+786F', 'U+5716', 'U+5C3C', 'U+63A1', 'U+96C5', 'U+96C9', 'U+5DEE', 'U+9B42', 'U+8B80', 'U+83E9', 'U+5E33', 'U+5442', 'U+91DC', 'U+5750', 'U+5F01', 'U+667A', 'U+5FCC', 'U+82D7', 'U+7ADC', 'U+5BB3', 'U+76CA', 'U+5C0D', 'U+5CB8', 'U+58C1', 'U+676F', 'U+8FF7', 'U+98E2', 'U+7948', 'U+552F', 'U+50BE', 'U+95C7', 'U+5012', 'U+85C1', 'U+758A', 'U+74F6', 'U+5374', 'U+755C', 'U+583A', 'U+6295', 'U+6234', 'U+5E61', 'U+4E14', 'U+82D4', 'U+4E3C', 'U+5348', 'U+90E1', 'U+9154', 'U+7345', 'U+5BFE', 'U+79C0', 'U+8FEF', 'U+500B', 'U+9019', 'U+8CE4', 'U+8655', 'U+8CB0', 'U+938C', 'U+5857', 'U+9632', 'U+60D1', 'U+52D8', 'U+904B', 'U+8CA1', 'U+5224', 'U+79AA', 'U+7C21', 'U+6C70', 'U+5CEF', 'U+767B', 'U+6669', 'U+4EFB', 'U+7FFC', 'U+5C48', 'U+865A', 'U+8377', 'U+9418', 'U+7E23', 'U+9A5A', 'U+6628', 'U+9EA5', 'U+9B8E', 'U+52D5', 'U+5F79', 'U+66FD', 'U+57CB', 'U+820A', 'U+8C9E', 'U+8CCE', 'U+809D', 'U+6563', 'U+8056', 'U+5EF6', 'U+8494', 'U+7562', 'U+53EB', 'U+6A02', 'U+5E7C', 'U+5404', 'U+5F66', 'U+758E', 'U+7567', 'U+5352', 'U+5345', 'U+60DA', 'U+6442', 'U+5410', 'U+9326', 'U+4EEE', 'U+4E58', 'U+90AA', 'U+77E2', 'U+9957', 'U+93E1', 'U+7C00', 'U+6843', 'U+6028', 'U+9000', 'U+8A2A', 'U+767A', 'U+9640', 'U+8AE7', 'U+8AA4', 'U+653F', 'U+9F8D', 'U+7E41', 'U+846C', 'U+6E56', 'U+753B', 'U+4E73', 'U+5C65', 'U+6C88', 'U+4F34', 'U+540E', 'U+60F3', 'U+7DDA', 'U+52E7', 'U+50AC', 'U+4EF2', 'U+5A6C', 'U+5118', 'U+7AE0', 'U+67FF', 'U+86D9', 'U+9699', 'U+6D44', 'U+5618', 'U+7C95', 'U+767C', 'U+6016', 'U+91DD', 'U+7956', 'U+7B48', 'U+7B52', 'U+694A', 'U+6284', 'U+514D', 'U+86E4', 'U+7F8A', 'U+56DE', 'U+702C', 'U+5883', 'U+8FFA', 'U+9589', 'U+62E0', 'U+7A42', 'U+4FC4', 'U+5CF6', 'U+98F2', 'U+6C96', 'U+85AA', 'U+5DE7', 'U+5713', 'U+80F4', 'U+4EE4', 'U+6E7F', 'U+51C9', 'U+89A7', 'U+58A8', 'U+7CA7', 'U+7CDF', 'U+8718', 'U+7FEB', 'U+9781', 'U+65A4', 'U+4FEE', 'U+7099', 'U+537F', 'U+616E', 'U+9045', 'U+5C4F', 'U+9BAE', 'U+5F9E', 'U+5AC9', 'U+69CB', 'U+666F', 'U+5C90', 'U+7BE9', 'U+53C3', 'U+6F01', 'U+5B6B', 'U+7CE0', 'U+5BF3', 'U+862D', 'U+8853'] (441) AVG: 20.97732426303855\n",
      "30-99: ['U+6597', 'U+4E45', 'U+76AE', 'U+5BD2', 'U+5B9A', 'U+6309', 'U+75C5', 'U+6253', 'U+7570', 'U+5438', 'U+4E21', 'U+9EC4', 'U+5E03', 'U+5F85', 'U+6CB3', 'U+4FD7', 'U+539F', 'U+590F', 'U+8CC0', 'U+843D', 'U+9162', 'U+59BB', 'U+982D', 'U+725B', 'U+9BDB', 'U+8AF8', 'U+56F3', 'U+9262', 'U+53EA', 'U+9055', 'U+6570', 'U+9B3C', 'U+6307', 'U+6DF1', 'U+5C0B', 'U+904A', 'U+76C3', 'U+5B88', 'U+6749', 'U+8AAC', 'U+5BFA', 'U+9803', 'U+6E05', 'U+696D', 'U+672B', 'U+7230', 'U+5439', 'U+4F4F', 'U+4E38', 'U+8089', 'U+5317', 'U+7E01', 'U+9678', 'U+5A66', 'U+66AE', 'U+4ED6', 'U+5BE2', 'U+8C9D', 'U+6A4B', 'U+9032', 'U+5510', 'U+54E5', 'U+6B73', 'U+58F1', 'U+7F8E', 'U+5922', 'U+8DEF', 'U+6BD4', 'U+8072', 'U+845B', 'U+58F0', 'U+7BB1', 'U+4ED9', 'U+6FC3', 'U+5177', 'U+738B', 'U+771F', 'U+8896', 'U+5668', 'U+5E30', 'U+58EB', 'U+51AC', 'U+8A70', 'U+7D50', 'U+82E6', 'U+9918', 'U+8A08', 'U+7D44', 'U+5915', 'U+66F4', 'U+51E6', 'U+96C4', 'U+5143', 'U+7BC0', 'U+65AF', 'U+4F5B', 'U+611B', 'U+7565', 'U+8FBC', 'U+723A', 'U+732A', 'U+8A18', 'U+83D3', 'U+5DF1', 'U+9E7F', 'U+866B', 'U+6795', 'U+597D', 'U+9732', 'U+98FC', 'U+6BD2', 'U+96C6', 'U+6C11', 'U+65E9', 'U+80A5', 'U+80E1', 'U+59CB', 'U+934B', 'U+8AB0', 'U+85A9', 'U+7686', 'U+96E3', 'U+5C24', 'U+50E7', 'U+7FC1', 'U+51B7', 'U+8A60', 'U+8CB4', 'U+7688', 'U+5B89', 'U+9670', 'U+6CE2', 'U+5D0E', 'U+62BC', 'U+5229', 'U+8FC4', 'U+679C', 'U+7D05', 'U+54C9', 'U+5BC4', 'U+9CF4', 'U+9023', 'U+7DE8', 'U+52A9', 'U+7523', 'U+9EA6', 'U+6027', 'U+6C0F', 'U+9996', 'U+679D', 'U+663C', 'U+6E21', 'U+53EF', 'U+9D28', 'U+74DC', 'U+6B4C', 'U+5584', 'U+7D2B', 'U+81F3', 'U+5949', 'U+8CAB', 'U+5200', 'U+5F1F', 'U+5B9C', 'U+6200', 'U+6848', 'U+4E39', 'U+828B', 'U+7DBF', 'U+6EDD', 'U+4E95', 'U+821F', 'U+5BA2', 'U+7E54', 'U+9280', 'U+677F', 'U+6469', 'U+7CBE', 'U+5316', 'U+554F', 'U+5A18', 'U+5FD7', 'U+4FBF', 'U+5FF5', 'U+60B2', 'U+6CEA', 'U+8A71', 'U+773C', 'U+7247', 'U+5728', 'U+70BA', 'U+65E6', 'U+6CA2', 'U+6900', 'U+8EE2', 'U+7802', 'U+6D17', 'U+543E', 'U+80F8', 'U+6912', 'U+7576', 'U+8033', 'U+7336', 'U+8FD4', 'U+6B5F', 'U+6614', 'U+5BAE', 'U+5FB3', 'U+8AA0', 'U+50B3', 'U+5974', 'U+967D', 'U+8C50', 'U+4F4D', 'U+9AEA', 'U+9AEE', 'U+53C2', 'U+5F53', 'U+6BBF', 'U+7B51', 'U+84CB', 'U+67D3', 'U+79D8', 'U+6D6E', 'U+56E0', 'U+9BC9', 'U+8C37', 'U+800C', 'U+7CD6', 'U+5449', 'U+5B85', 'U+90A3', 'U+732E', 'U+7A7A', 'U+59C9', 'U+5099', 'U+8CE3', 'U+67DA', 'U+5EFF', 'U+5EFB', 'U+5C3A', 'U+91A4', 'U+53EC', 'U+6B78', 'U+92AD', 'U+6876', 'U+5168', 'U+8A33', 'U+4E01', 'U+7FCC', 'U+6CE3', 'U+539A', 'U+523B', 'U+53F0', 'U+4E86', 'U+65C5', 'U+6025', 'U+9C60', 'U+98DB', 'U+968E', 'U+8239', 'U+85AC', 'U+5BCC', 'U+62F5', 'U+5484', 'U+59FF', 'U+8AC7', 'U+8EFD', 'U+8A63', 'U+8170', 'U+9F20', 'U+606F', 'U+6C38', 'U+99FF', 'U+9858', 'U+6B8B', 'U+89E3', 'U+7E6D', 'U+6BCE', 'U+9022', 'U+975E', 'U+5EAD', 'U+9644', 'U+96B1', 'U+84EE', 'U+9662', 'U+9F3B', 'U+983C', 'U+5FC5', 'U+7A74', 'U+63DA', 'U+7B46', 'U+899A', 'U+57CE', 'U+8352', 'U+90F7', 'U+6B4E', 'U+5E8A', 'U+6BDB', 'U+7D00', 'U+907A', 'U+9AA8', 'U+6D88', 'U+793C', 'U+6B69', 'U+968F', 'U+826F', 'U+51E1', 'U+795D', 'U+5149', 'U+885B', 'U+7A4D', 'U+7F6A', 'U+8CB7', 'U+6B98', 'U+5FD8', 'U+8836', 'U+5BD0', 'U+5F10', 'U+7AEF', 'U+7531', 'U+5F71', 'U+8DA3', 'U+6559', 'U+516C', 'U+670D', 'U+6050', 'U+89BA', 'U+7BB8', 'U+6700', 'U+6E1B', 'U+84B8', 'U+4F1D', 'U+653E', 'U+7C60', 'U+8C4A', 'U+672A', 'U+5247', 'U+529B', 'U+6F22', 'U+5E36', 'U+517C', 'U+5C3D', 'U+5144', 'U+9EB8', 'U+971C', 'U+4F11', 'U+4F59', 'U+4F7F', 'U+6B8A', 'U+984F', 'U+679A', 'U+9001', 'U+6975', 'U+7D72', 'U+6696', 'U+7532', 'U+62FE', 'U+4F8B', 'U+706F', 'U+8208', 'U+5C3B', 'U+771E', 'U+9B92', 'U+7D04', 'U+6B62', 'U+4EC1', 'U+9EBB', 'U+7B54', 'U+7897', 'U+82BD', 'U+4F0F', 'U+4E9B', 'U+60DC', 'U+6751', 'U+68DA', 'U+4453', 'U+8ECA', 'U+8AD6', 'U+7D0D', 'U+67F3', 'U+4EAD', 'U+53E5', 'U+8D70', 'U+639B', 'U+9020', 'U+7687', 'U+7D93', 'U+8ACB', 'U+7720', 'U+5236', 'U+793E', 'U+8003', 'U+99B4', 'U+7683', 'U+52B4', 'U+5BB5', 'U+5B87', 'U+5426', 'U+4E5E', 'U+5B63', 'U+81E3', 'U+5E95', 'U+7559', 'U+88CF', 'U+5CA9', 'U+64AD', 'U+574A', 'U+675F', 'U+756A', 'U+6CC9', 'U+83CA', 'U+6E29', 'U+8F29', 'U+85CF', 'U+618E', 'U+5EE3', 'U+7B4B', 'U+6D6A', 'U+80CC', 'U+7BED', 'U+624D', 'U+8CCA', 'U+8535', 'U+5E7E', 'U+5B5D', 'U+96C0', 'U+73CD', 'U+91E3', 'U+929A', 'U+4EA4', 'U+6C37', 'U+6D45', 'U+5301', 'U+9854', 'U+66F2', 'U+4EF0', 'U+8CAC', 'U+5718', 'U+70CF', 'U+4F53', 'U+4E26', 'U+5802', 'U+5373', 'U+8ABF', 'U+53CB', 'U+70AD', 'U+7518', 'U+690E', 'U+8846', 'U+60E3', 'U+7F85', 'U+964D', 'U+5377', 'U+72C2', 'U+8150', 'U+7AE5', 'U+8FCE', 'U+9650', 'U+4E57', 'U+5009', 'U+6065', 'U+6D3B', 'U+70B9', 'U+6E80', 'U+5909', 'U+567A', 'U+65E2', 'U+6DFB', 'U+8CA7', 'U+6817', 'U+888B', 'U+8A31', 'U+6A2A', 'U+85E4', 'U+8E0F', 'U+5931', 'U+914D', 'U+796D', 'U+5305', 'U+63D0', 'U+66F0', 'U+636E', 'U+81E5', 'U+61C9', 'U+4E71', 'U+5FCD', 'U+56D8', 'U+518D', 'U+4E88', 'U+6383', 'U+7B20', 'U+8ED2', 'U+5BB9', 'U+96A3', 'U+7D4C', 'U+6761', 'U+6797', 'U+53F8', 'U+8F2A', 'U+8FFD', 'U+88FD', 'U+5546', 'U+798F', 'U+6FF1', 'U+72EC', 'U+529F', 'U+8278', 'U+65AD', 'U+91AC', 'U+63C3', 'U+627F', 'U+8471', 'U+514E', 'U+732B', 'U+8107', 'U+8F9B', 'U+5BC6', 'U+6D66', 'U+6C99', 'U+5999'] (521) AVG: 53.73704414587332\n",
      "100-299: ['U+6851', 'U+4ED8', 'U+9B5A', 'U+9053', 'U+9593', 'U+5C4B', 'U+5916', 'U+98DF', 'U+5929', 'U+91D1', 'U+7D19', 'U+884C', 'U+7528', 'U+91CE', 'U+4F86', 'U+516B', 'U+53CA', 'U+4E03', 'U+672C', 'U+795E', 'U+5DE6', 'U+570B', 'U+9577', 'U+6210', 'U+7A0B', 'U+716E', 'U+7A2E', 'U+6211', 'U+6765', 'U+7537', 'U+571F', 'U+7406', 'U+4E0D', 'U+66F8', 'U+5869', 'U+8005', 'U+4EAC', 'U+6C17', 'U+592A', 'U+79C1', 'U+90E8', 'U+5E73', 'U+53E4', 'U+713C', 'U+901A', 'U+8349', 'U+90CE', 'U+4F5C', 'U+7B2C', 'U+5C11', 'U+8A9E', 'U+7389', 'U+706B', 'U+660E', 'U+6216', 'U+7530', 'U+5375', 'U+5DDD', 'U+714E', 'U+5F37', 'U+591A', 'U+6839', 'U+8C46', 'U+9580', 'U+61F8', 'U+7DCF', 'U+5730', 'U+8D8A', 'U+6E6F', 'U+6C23', 'U+81EA', 'U+677E', 'U+80FD', 'U+6F2C', 'U+6D77', 'U+547C', 'U+76BF', 'U+6CD5', 'U+8FD1', 'U+5F80', 'U+5F62', 'U+5EA6', 'U+77E5', 'U+4E5D', 'U+6545', 'U+5148', 'U+95A2', 'U+52A0', 'U+4F8D', 'U+5171', 'U+5965', 'U+8A5E', 'U+60AA', 'U+9AD8', 'U+9063', 'U+5175', 'U+7AF9', 'U+985E', 'U+8FBA', 'U+5EA7', 'U+521D', 'U+6A23', 'U+5E38', 'U+5F97', 'U+7C89', 'U+5982', 'U+6587', 'U+8DB3', 'U+9752', 'U+623F', 'U+91CC', 'U+5F7C', 'U+7FA9', 'U+4EE5', 'U+83DC', 'U+7B11', 'U+5DFB', 'U+6B66', 'U+5E2B', 'U+8584', 'U+96E8', 'U+8001', 'U+6268', 'U+5BF8', 'U+4ECB', 'U+564C', 'U+90FD', 'U+7C73', 'U+6CB9', 'U+5B9F', 'U+5225', 'U+5357', 'U+89AA', 'U+4EE3', 'U+6577', 'U+6599', 'U+76F8', 'U+5C3E', 'U+671D', 'U+7136', 'U+52DD', 'U+4F46', 'U+82E5', 'U+5BBF', 'U+9060', 'U+6B21', 'U+60C5', 'U+990A', 'U+4E07', 'U+904E', 'U+6368', 'U+4FE1', 'U+753A', 'U+99AC', 'U+9905', 'U+4E3B', 'U+97F3', 'U+6B63', 'U+8D77', 'U+8179', 'U+4F0A', 'U+7740', 'U+52E2', 'U+757F', 'U+5343', 'U+8863', 'U+98EF', 'U+7236', 'U+96EA', 'U+7121', 'U+9762', 'U+4F50', 'U+7B49', 'U+6D25', 'U+89D2', 'U+79CB', 'U+767E', 'U+5347', 'U+79F0', 'U+76DB', 'U+6885', 'U+66FF', 'U+81B3', 'U+7D30', 'U+5150', 'U+53BB', 'U+96F2', 'U+4F3C', 'U+6BCD', 'U+8D64', 'U+9ED2', 'U+5F35', 'U+541B', 'U+5411', 'U+8338', 'U+5E72', 'U+8DE1', 'U+77F3', 'U+9999', 'U+91CD', 'U+55B0', 'U+7CF8', 'U+610F', 'U+547D', 'U+885E', 'U+6301', 'U+6298', 'U+5742', 'U+7FBD', 'U+534A', 'U+5D8B', 'U+6BB5', 'U+6D41', 'U+76F4', 'U+54C1', 'U+65B0', 'U+7D42', 'U+6B7B', 'U+81FA', 'U+80B4', 'U+5B57'] (221) AVG: 161.68778280542986\n",
      "300+: ['U+4E00', 'U+4E91', 'U+4E8B', 'U+4EBA', 'U+53C8', 'U+5165', 'U+7269', 'U+51FA', 'U+898B', 'U+5927', 'U+5B50', 'U+6B64', 'U+5176', 'U+4E5F', 'U+65E5', 'U+5C0F', 'U+65B9', 'U+4E0A', 'U+662F', 'U+4E09', 'U+4E2D', 'U+4F55', 'U+5019', 'U+5973', 'U+56FD', 'U+4E8C', 'U+6642', 'U+4ECA', 'U+5FA1', 'U+6709', 'U+6240', 'U+5FC3', 'U+524D', 'U+5408', 'U+6C34', 'U+8EAB', 'U+4E94', 'U+5C71', 'U+5341', 'U+540C', 'U+6C41', 'U+7D66', 'U+624B', 'U+7ACB', 'U+8695', 'U+69D8', 'U+4E4B', 'U+4E0B', 'U+5409', 'U+4E16', 'U+8A00', 'U+7533', 'U+4ED5', 'U+6708', 'U+56DB', 'U+9152', 'U+540D', 'U+751F', 'U+6771', 'U+5DDE', 'U+5F8C', 'U+53E3', 'U+516D', 'U+6C5F', 'U+6728', 'U+98A8', 'U+5BB6', 'U+82B1', 'U+591C', 'U+601D', 'U+5C45', 'U+6625', 'U+53F3', 'U+6238', 'U+767D', 'U+53D6', 'U+8272', 'U+5206', 'U+5185', 'U+9CE5', 'U+805E', 'U+5E74', 'U+592B', 'U+76EE', 'U+5473', 'U+897F', 'U+5F15', 'U+8449', 'U+7F6E', 'U+548C', 'U+5207', 'U+6CBB', 'U+8336'] (93) AVG: 613.9677419354839\n"
     ]
    }
   ],
   "source": [
    "kkanji_images_classes_by_amount = defaultdict(list)\n",
    "kkanji_images_classes_by_amount['1-14'] = []\n",
    "kkanji_images_classes_by_amount['15-29'] = []\n",
    "kkanji_images_classes_by_amount['30-99'] = []\n",
    "kkanji_images_classes_by_amount['100-299'] = []\n",
    "kkanji_images_classes_by_amount['300+'] = []\n",
    "kkanji_images_accumulator = defaultdict(int)\n",
    "kkanji_images_accumulator['1-14'] = 0\n",
    "kkanji_images_accumulator['15-29'] = 0\n",
    "kkanji_images_accumulator['30-99'] = 0\n",
    "kkanji_images_accumulator['100-299'] = 0\n",
    "kkanji_images_accumulator['300+'] = 0\n",
    "\n",
    "# Function to classify the value into the appropriate category\n",
    "def classify_value(value):\n",
    "    if 1 <= value <= 14:\n",
    "        return '1-14'\n",
    "    elif 15 <= value <= 29:\n",
    "        return '15-29'\n",
    "    elif 30 <= value <= 99:\n",
    "        return '30-99'\n",
    "    elif 100 <= value <= 299:\n",
    "        return '100-299'\n",
    "    else:\n",
    "        return '300+'\n",
    "\n",
    "# Read and process the file\n",
    "with open('/home/beav3r/Bachelor_work/Bachelor_work/kkanji_classes_distribution_sorted.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into Unicode character and value\n",
    "        unicode_char, value_str = line.strip().split()\n",
    "        value = int(value_str)\n",
    "        \n",
    "        # Classify the value and add to the appropriate category\n",
    "        category = classify_value(value)\n",
    "        kkanji_images_classes_by_amount[category].append(unicode_char)\n",
    "\n",
    "        # Accumulate the value\n",
    "        kkanji_images_accumulator[category] += value\n",
    "\n",
    "# Print the dictionary to verify the result\n",
    "for category, unicode_chars in kkanji_images_classes_by_amount.items():\n",
    "    print(f\"{category}: {unicode_chars} ({len(unicode_chars)}) AVG: {kkanji_images_accumulator[category] / len(unicode_chars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the augmentation pipeline\n",
    "\n",
    "augmentation_pipeline = transforms.Compose([\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.RandomResizedCrop(64, scale=(0.95, 1.05)),\n",
    "    transforms.RandomAffine(degrees=0, shear=0.05),\n",
    "    # transforms.ToTensor(),  # Convert PIL image to tensor\n",
    "    # transforms.Lambda(lambda img: img + torch.randn_like(img) * 0.01),  # Add random noise\n",
    "    # transforms.ToPILImage()  # Convert tensor back to PIL image\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each directory in group '1-14' enable image augmentation to reach 15 images\n",
    "\n",
    "for unicode_char in kkanji_images_classes_by_amount['1-14']:\n",
    "    # Define the directory\n",
    "    char_dir = os.path.join(main_kkanji_dir, unicode_char)\n",
    "    \n",
    "    # List all files in the directory\n",
    "    files = os.listdir(char_dir)\n",
    "    \n",
    "    # Check if there are less than 15 images\n",
    "    if len(files) < 15:\n",
    "        # Apply augmentation pipeline\n",
    "        for i in range(15 - len(files)):\n",
    "            # Load the random image\n",
    "            rand_number = np.random.randint(0, len(files))\n",
    "            image = Image.open(os.path.join(char_dir, files[rand_number]))\n",
    "\n",
    "            augmented_image = augmentation_pipeline(image)\n",
    "\n",
    "            save_dir = os.path.join(generative_kkanji_dir, unicode_char)\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            \n",
    "            # Save the augmented image\n",
    "            augmented_image.save(os.path.join(save_dir, f\"gen_aug1_{i}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kanji for training GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy all ''1-14' and '15-29' images from kkanji2_with_gen directory to the kkanji_for_GAN directory \n",
    "\n",
    "for unicode_char in kkanji_images_classes_by_amount['1-14'] + kkanji_images_classes_by_amount['15-29']:\n",
    "    # Define the directory\n",
    "    char_dir = os.path.join(generative_kkanji_dir, unicode_char)\n",
    "\n",
    "    # Create the directory in kkanji_for_GAN\n",
    "    os.makedirs(os.path.join(kkanji_for_GAN_dir, unicode_char), exist_ok=True)\n",
    "    \n",
    "    # List all files in the directory\n",
    "    files = os.listdir(char_dir)\n",
    "    \n",
    "    # Copy all files to the kkanji_for_GAN directory\n",
    "    for file in files:\n",
    "        os.system(f\"cp {os.path.join(char_dir, file)} {os.path.join(kkanji_for_GAN_dir, unicode_char)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big CGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generator for Conditional GAN with Embeddings\n",
    "class Generator_big(nn.Module):\n",
    "    def __init__(self, latent_dim, img_shape, num_classes, embedding_dim=100):\n",
    "        super(Generator_big, self).__init__()\n",
    "        self.img_shape = img_shape\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.label_emb = nn.Embedding(num_classes, embedding_dim)  # Embedding layer\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + embedding_dim, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, int(torch.prod(torch.tensor(img_shape)))),\n",
    "            nn.Tanh()  # Output normalized to (-1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        # Embedding class labels\n",
    "        label_embedding = self.label_emb(labels)\n",
    "        gen_input = torch.cat((z, label_embedding), -1)  # Concatenate noise and label embeddings\n",
    "        img = self.model(gen_input)\n",
    "        img = img.view(img.size(0), *self.img_shape)  # Reshape to image format\n",
    "        return img\n",
    "\n",
    "# Define the Discriminator for Conditional GAN with Embeddings\n",
    "class Discriminator_big(nn.Module):\n",
    "    def __init__(self, img_shape, num_classes, embedding_dim=100):\n",
    "        super(Discriminator_big, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.label_emb = nn.Embedding(num_classes, embedding_dim)  # Embedding layer\n",
    "\n",
    "        # Calculate the flattened image size\n",
    "        img_flattened_size = int(torch.prod(torch.tensor(img_shape)))\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(img_flattened_size + embedding_dim, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()  # Output as probability\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        img_flat = img.view(img.size(0), -1)  # Flatten the image\n",
    "        label_embedding = self.label_emb(labels)\n",
    "        d_in = torch.cat((img_flat, label_embedding), -1)  # Concatenate image and label embeddings\n",
    "        \n",
    "        # print(\"D_in: \", d_in.shape, d_in)\n",
    "        # print(\"Label embedding: \", label_embedding.shape, label_embedding)\n",
    "        # print(\"Img_flat: \", img_flat.shape, img_flat)\n",
    "        \n",
    "        validity = self.model(d_in)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv CGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generator with convolutional layers for GAN Conditional GAN with Embeddings\n",
    "class Generator_conv(nn.Module):\n",
    "    def __init__(self, latent_dim, embedding_dim, num_classes):\n",
    "        super(Generator_conv, self).__init__()\n",
    "        \n",
    "        # Define the input embedding layer\n",
    "        self.embedding = nn.Embedding(num_classes, embedding_dim)\n",
    "        \n",
    "        # Define the linear layer to map the latent vector to the convolutional layer\n",
    "        self.linear = nn.Linear(latent_dim + embedding_dim, 128 * 8 * 8)\n",
    "        \n",
    "        # Define the convolutional layers\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            # nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "            # nn.BatchNorm2d(16),\n",
    "            # nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, z, labels):\n",
    "        # Get the embeddings\n",
    "        embeddings = self.embedding(labels)\n",
    "        \n",
    "        # Concatenate the embeddings with the latent vector\n",
    "        input = torch.cat((z, embeddings), dim=1)\n",
    "        \n",
    "        # Map the input to the convolutional layer\n",
    "        x = self.linear(input)\n",
    "        x = x.view(-1, 128, 8, 8)\n",
    "        \n",
    "        # Forward pass through the convolutional layers\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# Define the Discriminator with convolutional layers for Conditional GAN with Embeddings\n",
    "class Discriminator_conv(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_classes):\n",
    "        super(Discriminator_conv, self).__init__()\n",
    "        \n",
    "        # Define the input embedding layer\n",
    "        self.embedding = nn.Embedding(num_classes, embedding_dim)\n",
    "        \n",
    "        # Define the convolutional layers\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        # Define the linear layer to map the convolutional layer to the output\n",
    "        self.linear = nn.Linear(256 * 8 * 8 + embedding_dim, 1)\n",
    "\n",
    "        # Define the sigmoid layer\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, labels):\n",
    "        # Forward pass through the convolutional layers\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        # Flatten the output\n",
    "        x = x.view(-1, 256 * 8 * 8)\n",
    "        \n",
    "        # Get the embeddings\n",
    "        embeddings = self.embedding(labels)\n",
    "        \n",
    "        # Concatenate the embeddings with the flattened output\n",
    "        input = torch.cat((x, embeddings), dim=1)\n",
    "        \n",
    "        # Forward pass through the linear layer\n",
    "        x = self.linear(input)\n",
    "\n",
    "        # Forward pass through the sigmoid layer\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generator for Conditional GAN with Embeddings\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, img_shape, num_classes, embedding_dim=100):\n",
    "        super(Generator, self).__init__()\n",
    "        self.img_shape = img_shape\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.label_emb = nn.Embedding(num_classes, embedding_dim)  # Embedding layer\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + embedding_dim, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, int(torch.prod(torch.tensor(img_shape)))),\n",
    "            nn.Tanh()  # Output normalized to (-1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        # Embedding class labels\n",
    "        label_embedding = self.label_emb(labels)\n",
    "        gen_input = torch.cat((z, label_embedding), -1)  # Concatenate noise and label embeddings\n",
    "        img = self.model(gen_input)\n",
    "        img = img.view(img.size(0), *self.img_shape)  # Reshape to image format\n",
    "        return img\n",
    "\n",
    "# Define the Discriminator for Conditional GAN with Embeddings\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_shape, num_classes, embedding_dim=100):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.label_emb = nn.Embedding(num_classes, embedding_dim)  # Embedding layer\n",
    "\n",
    "        # Calculate the flattened image size\n",
    "        img_flattened_size = int(torch.prod(torch.tensor(img_shape)))\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(img_flattened_size + embedding_dim, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()  # Output as probability\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        img_flat = img.view(img.size(0), -1)  # Flatten the image\n",
    "        label_embedding = self.label_emb(labels)\n",
    "        d_in = torch.cat((img_flat, label_embedding), -1)  # Concatenate image and label embeddings\n",
    "        \n",
    "        # print(\"D_in: \", d_in.shape, d_in)\n",
    "        # print(\"Label embedding: \", label_embedding.shape, label_embedding)\n",
    "        # print(\"Img_flat: \", img_flat.shape, img_flat)\n",
    "        \n",
    "        validity = self.model(d_in)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader CGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "# Data loader for real images (replace with your dataset)\n",
    "dataloader_GAN = DataLoader(\n",
    "    datasets.ImageFolder(\n",
    "        kkanji_for_GAN_dir,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.Resize(64),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])  # Normalize to (-1, 1)\n",
    "        ])\n",
    "    ),\n",
    "    batch_size=128, shuffle=True, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original CGAN Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_367754/4046080017.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  generator.load_state_dict(torch.load(f\"/mnt/d/Bachelor_work/data_for_model/GAN_train_generated/generator_epoch_{start_epoch}.pth\"))\n",
      "/tmp/ipykernel_367754/4046080017.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  discriminator.load_state_dict(torch.load(f\"/mnt/d/Bachelor_work/data_for_model/GAN_train_generated/discriminator_epoch_{start_epoch}.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded generator and discriminator from epoch 430\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataloader_GAN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, epochs):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (imgs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mdataloader_GAN\u001b[49m):\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;66;03m# Transfer real images and labels to GPU\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m         \u001b[38;5;66;03m# real_imgs = imgs.cuda()\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;66;03m# labels = labels.cuda()  # Class labels for conditional GAN\u001b[39;00m\n\u001b[1;32m     39\u001b[0m         real_imgs \u001b[38;5;241m=\u001b[39m imgs\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;66;03m# print(\"Real_imgs: \", real_imgs.shape)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m         \u001b[38;5;66;03m# Labels for real and fake images (real: 1, fake: 0)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \n\u001b[1;32m     45\u001b[0m         \u001b[38;5;66;03m# real_labels = torch.ones((imgs.size(0), 1)).cuda()\u001b[39;00m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;66;03m# fake_labels = torch.zeros((imgs.size(0), 1)).cuda()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader_GAN' is not defined"
     ]
    }
   ],
   "source": [
    "# Ortiginal training \n",
    "\n",
    "epochs = 601  # Number of training epochs\n",
    "\n",
    "start_epoch = 430\n",
    "\n",
    "# Initialize the Generator and Discriminator for cGAN\n",
    "latent_dim = 100  # Dimensionality of the latent noise vector\n",
    "img_shape = (1, 64, 64)  # Grayscale images (1 channel, 64x64)\n",
    "\n",
    "# generator = Generator(latent_dim, img_shape, num_classes).cuda()\n",
    "# discriminator = Discriminator(img_shape, num_classes).cuda()\n",
    "\n",
    "generator = Generator(latent_dim, img_shape, num_classes_GAN)\n",
    "discriminator = Discriminator(img_shape, num_classes_GAN)\n",
    "\n",
    "# Check if the model is already trained\n",
    "if os.path.exists(f\"/mnt/d/Bachelor_work/data_for_model/GAN_train_generated/generator_epoch_{start_epoch}.pth\"):\n",
    "    generator.load_state_dict(torch.load(f\"/mnt/d/Bachelor_work/data_for_model/GAN_train_generated/generator_epoch_{start_epoch}.pth\"))\n",
    "    discriminator.load_state_dict(torch.load(f\"/mnt/d/Bachelor_work/data_for_model/GAN_train_generated/discriminator_epoch_{start_epoch}.pth\"))\n",
    "\n",
    "    print(f\"Loaded generator and discriminator from epoch {start_epoch}\")\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    for i, (imgs, labels) in enumerate(dataloader_GAN):\n",
    "        # Transfer real images and labels to GPU\n",
    "\n",
    "        # real_imgs = imgs.cuda()\n",
    "        # labels = labels.cuda()  # Class labels for conditional GAN\n",
    "\n",
    "        real_imgs = imgs\n",
    "\n",
    "        # print(\"Real_imgs: \", real_imgs.shape)\n",
    "\n",
    "        # Labels for real and fake images (real: 1, fake: 0)\n",
    "\n",
    "        # real_labels = torch.ones((imgs.size(0), 1)).cuda()\n",
    "        # fake_labels = torch.zeros((imgs.size(0), 1)).cuda()\n",
    "\n",
    "        real_labels = torch.ones((imgs.size(0), 1))\n",
    "        fake_labels = torch.zeros((imgs.size(0), 1))\n",
    "\n",
    "        # ======================\n",
    "        # Train Discriminator\n",
    "        # ======================\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Sample random noise and generate class labels\n",
    "        \n",
    "        # z = torch.randn(imgs.size(0), latent_dim).cuda()\n",
    "\n",
    "        z = torch.randn(imgs.size(0), latent_dim)\n",
    "\n",
    "        # Generate fake images based on noise and class labels\n",
    "        fake_imgs = generator(z, labels)\n",
    "\n",
    "        # Discriminator loss for real and fake images\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs, labels), real_labels)\n",
    "        fake_loss = adversarial_loss(discriminator(fake_imgs.detach(), labels), fake_labels)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # ======================\n",
    "        # Train Generator\n",
    "        # ======================\n",
    "        for _ in range(5):\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Generate fake images based on noise and class labels\n",
    "            z = torch.randn(imgs.size(0), latent_dim) \n",
    "            fake_imgs = generator(z, labels)\n",
    "\n",
    "            # Try to fool the discriminator with generated images\n",
    "            g_loss = adversarial_loss(discriminator(fake_imgs, labels), real_labels)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch}/{epochs}] [Batch {i}/{len(dataloader_GAN)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
    "\n",
    "        # Save generated images at regular intervals\n",
    "        if i % 300 == 0:\n",
    "            save_image(fake_imgs.data[:25], f\"/mnt/d/Bachelor_work/data_for_model/GAN_train_generated/{epoch}_{i}.png\", nrow=5, normalize=True)\n",
    "        \n",
    "    # Save the model checkpoints\n",
    "    if epoch % 10 == 0 and epoch != start_epoch:\n",
    "        torch.save(generator.state_dict(), f\"/mnt/d/Bachelor_work/data_for_model/GAN_train_generated/generator_epoch_{epoch}.pth\")\n",
    "        torch.save(discriminator.state_dict(), f\"/mnt/d/Bachelor_work/data_for_model/GAN_train_generated/discriminator_epoch_{epoch}.pth\")\n",
    "\n",
    "        # Save last epoch number\n",
    "        with open(\"/mnt/d/Bachelor_work/data_for_model/GAN_train_generated/last_epoch.txt\", \"w\") as file:\n",
    "            file.write(str(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv CGAN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded generator and discriminator from epoch 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_491931/3547472807.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  generator.load_state_dict(torch.load(f\"/mnt/d/Bachelor_work/data_for_model/conv_GAN_train_generated/generator_conv_epoch_{start_epoch}.pth\"))\n",
      "/tmp/ipykernel_491931/3547472807.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  discriminator.load_state_dict(torch.load(f\"/mnt/d/Bachelor_work/data_for_model/conv_GAN_train_generated/discriminator_conv_epoch_{start_epoch}.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 50/1001] [Batch 0/372] [D loss: 0.6935524940490723] [G loss: 0.4570632874965668]\n",
      "[Epoch 50/1001] [Batch 1/372] [D loss: 0.7295355796813965] [G loss: 0.6982858777046204]\n",
      "[Epoch 50/1001] [Batch 2/372] [D loss: 0.6935823559761047] [G loss: 0.842128574848175]\n",
      "[Epoch 50/1001] [Batch 3/372] [D loss: 0.7034446001052856] [G loss: 0.7357797622680664]\n",
      "[Epoch 50/1001] [Batch 4/372] [D loss: 0.6944735646247864] [G loss: 0.6293002367019653]\n",
      "[Epoch 50/1001] [Batch 5/372] [D loss: 0.6949197053909302] [G loss: 0.6508606672286987]\n",
      "[Epoch 50/1001] [Batch 6/372] [D loss: 0.6941013932228088] [G loss: 0.714974582195282]\n",
      "[Epoch 50/1001] [Batch 7/372] [D loss: 0.6934418678283691] [G loss: 0.712973952293396]\n",
      "[Epoch 50/1001] [Batch 8/372] [D loss: 0.6940141916275024] [G loss: 0.6773192286491394]\n",
      "[Epoch 50/1001] [Batch 9/372] [D loss: 0.6932759881019592] [G loss: 0.6788725852966309]\n",
      "[Epoch 50/1001] [Batch 10/372] [D loss: 0.6923555135726929] [G loss: 0.6976972818374634]\n",
      "[Epoch 50/1001] [Batch 11/372] [D loss: 0.6928239464759827] [G loss: 0.693577229976654]\n",
      "[Epoch 50/1001] [Batch 12/372] [D loss: 0.6923060417175293] [G loss: 0.6844396591186523]\n",
      "[Epoch 50/1001] [Batch 13/372] [D loss: 0.6932995319366455] [G loss: 0.6921018958091736]\n",
      "[Epoch 50/1001] [Batch 14/372] [D loss: 0.6931159496307373] [G loss: 0.6914767026901245]\n",
      "[Epoch 50/1001] [Batch 15/372] [D loss: 0.6926570534706116] [G loss: 0.689880907535553]\n",
      "[Epoch 50/1001] [Batch 16/372] [D loss: 0.6927380561828613] [G loss: 0.691694974899292]\n",
      "[Epoch 50/1001] [Batch 17/372] [D loss: 0.6929164528846741] [G loss: 0.6883921027183533]\n",
      "[Epoch 50/1001] [Batch 18/372] [D loss: 0.6923271417617798] [G loss: 0.6885023713111877]\n",
      "[Epoch 50/1001] [Batch 19/372] [D loss: 0.6932067275047302] [G loss: 0.6936463713645935]\n",
      "[Epoch 50/1001] [Batch 20/372] [D loss: 0.6936906576156616] [G loss: 0.6868046522140503]\n",
      "[Epoch 50/1001] [Batch 21/372] [D loss: 0.6937174201011658] [G loss: 0.6883796453475952]\n",
      "[Epoch 50/1001] [Batch 22/372] [D loss: 0.6932830810546875] [G loss: 0.6916248798370361]\n",
      "[Epoch 50/1001] [Batch 23/372] [D loss: 0.6930423974990845] [G loss: 0.688739538192749]\n",
      "[Epoch 50/1001] [Batch 24/372] [D loss: 0.6931372880935669] [G loss: 0.6908738017082214]\n",
      "[Epoch 50/1001] [Batch 25/372] [D loss: 0.6923055648803711] [G loss: 0.6900668144226074]\n",
      "[Epoch 50/1001] [Batch 26/372] [D loss: 0.6925272345542908] [G loss: 0.68535977602005]\n",
      "[Epoch 50/1001] [Batch 27/372] [D loss: 0.6918284893035889] [G loss: 0.6963252425193787]\n",
      "[Epoch 50/1001] [Batch 28/372] [D loss: 0.6920560598373413] [G loss: 0.6892101764678955]\n",
      "[Epoch 50/1001] [Batch 29/372] [D loss: 0.6930071115493774] [G loss: 0.6858437657356262]\n",
      "[Epoch 50/1001] [Batch 30/372] [D loss: 0.6939336061477661] [G loss: 0.6925049424171448]\n",
      "[Epoch 50/1001] [Batch 31/372] [D loss: 0.6930410861968994] [G loss: 0.686019778251648]\n",
      "[Epoch 50/1001] [Batch 32/372] [D loss: 0.693639874458313] [G loss: 0.6909144520759583]\n",
      "[Epoch 50/1001] [Batch 33/372] [D loss: 0.6932517290115356] [G loss: 0.6908377408981323]\n",
      "[Epoch 50/1001] [Batch 34/372] [D loss: 0.6933038234710693] [G loss: 0.6844457387924194]\n",
      "[Epoch 50/1001] [Batch 35/372] [D loss: 0.6928883790969849] [G loss: 0.7004662156105042]\n",
      "[Epoch 50/1001] [Batch 36/372] [D loss: 0.6932945251464844] [G loss: 0.6724272966384888]\n",
      "[Epoch 50/1001] [Batch 37/372] [D loss: 0.6931661367416382] [G loss: 0.7130494117736816]\n",
      "[Epoch 50/1001] [Batch 38/372] [D loss: 0.6926941871643066] [G loss: 0.6622028946876526]\n",
      "[Epoch 50/1001] [Batch 39/372] [D loss: 0.6931368112564087] [G loss: 0.7288632392883301]\n",
      "[Epoch 50/1001] [Batch 40/372] [D loss: 0.6931638717651367] [G loss: 0.6357688903808594]\n",
      "[Epoch 50/1001] [Batch 41/372] [D loss: 0.6942576169967651] [G loss: 0.7670769691467285]\n",
      "[Epoch 50/1001] [Batch 42/372] [D loss: 0.6956917643547058] [G loss: 0.5940302014350891]\n",
      "[Epoch 50/1001] [Batch 43/372] [D loss: 0.6988508105278015] [G loss: 0.8220134377479553]\n",
      "[Epoch 50/1001] [Batch 44/372] [D loss: 0.7012067437171936] [G loss: 0.5686296820640564]\n",
      "[Epoch 50/1001] [Batch 45/372] [D loss: 0.7019093036651611] [G loss: 0.7992513179779053]\n",
      "[Epoch 50/1001] [Batch 46/372] [D loss: 0.6984193921089172] [G loss: 0.6318753957748413]\n",
      "[Epoch 50/1001] [Batch 47/372] [D loss: 0.6948132514953613] [G loss: 0.7060225605964661]\n",
      "[Epoch 50/1001] [Batch 48/372] [D loss: 0.6933186650276184] [G loss: 0.701858401298523]\n",
      "[Epoch 50/1001] [Batch 49/372] [D loss: 0.693156361579895] [G loss: 0.6708351969718933]\n",
      "[Epoch 50/1001] [Batch 50/372] [D loss: 0.6926534175872803] [G loss: 0.7069610953330994]\n",
      "[Epoch 50/1001] [Batch 51/372] [D loss: 0.6933802366256714] [G loss: 0.6780779361724854]\n",
      "[Epoch 50/1001] [Batch 52/372] [D loss: 0.6936459541320801] [G loss: 0.6958945989608765]\n",
      "[Epoch 50/1001] [Batch 53/372] [D loss: 0.693388819694519] [G loss: 0.6886427998542786]\n",
      "[Epoch 50/1001] [Batch 54/372] [D loss: 0.6930537223815918] [G loss: 0.6843637228012085]\n",
      "[Epoch 50/1001] [Batch 55/372] [D loss: 0.6933507919311523] [G loss: 0.6941643953323364]\n",
      "[Epoch 50/1001] [Batch 56/372] [D loss: 0.6937495470046997] [G loss: 0.6875768899917603]\n",
      "[Epoch 50/1001] [Batch 57/372] [D loss: 0.6933022141456604] [G loss: 0.6888636350631714]\n",
      "[Epoch 50/1001] [Batch 58/372] [D loss: 0.6928881406784058] [G loss: 0.6878358125686646]\n",
      "[Epoch 50/1001] [Batch 59/372] [D loss: 0.6929612755775452] [G loss: 0.692953884601593]\n",
      "[Epoch 50/1001] [Batch 60/372] [D loss: 0.6935432553291321] [G loss: 0.6848753690719604]\n",
      "[Epoch 50/1001] [Batch 61/372] [D loss: 0.6925672292709351] [G loss: 0.6962102651596069]\n",
      "[Epoch 50/1001] [Batch 62/372] [D loss: 0.6925555467605591] [G loss: 0.6823397874832153]\n",
      "[Epoch 50/1001] [Batch 63/372] [D loss: 0.6925933361053467] [G loss: 0.6958447098731995]\n",
      "[Epoch 50/1001] [Batch 64/372] [D loss: 0.693864107131958] [G loss: 0.6794323921203613]\n",
      "[Epoch 50/1001] [Batch 65/372] [D loss: 0.6935420632362366] [G loss: 0.700837254524231]\n",
      "[Epoch 50/1001] [Batch 66/372] [D loss: 0.6931332349777222] [G loss: 0.6757783889770508]\n",
      "[Epoch 50/1001] [Batch 67/372] [D loss: 0.6939131617546082] [G loss: 0.7080605030059814]\n",
      "[Epoch 50/1001] [Batch 68/372] [D loss: 0.6931005716323853] [G loss: 0.6632194519042969]\n",
      "[Epoch 50/1001] [Batch 69/372] [D loss: 0.6934190988540649] [G loss: 0.7249122858047485]\n",
      "[Epoch 50/1001] [Batch 70/372] [D loss: 0.6932460069656372] [G loss: 0.6442851424217224]\n",
      "[Epoch 50/1001] [Batch 71/372] [D loss: 0.6943026185035706] [G loss: 0.7550302743911743]\n",
      "[Epoch 50/1001] [Batch 72/372] [D loss: 0.6938931345939636] [G loss: 0.6065537929534912]\n",
      "[Epoch 50/1001] [Batch 73/372] [D loss: 0.6962800621986389] [G loss: 0.8156076669692993]\n",
      "[Epoch 50/1001] [Batch 74/372] [D loss: 0.6998133659362793] [G loss: 0.552103579044342]\n",
      "[Epoch 50/1001] [Batch 75/372] [D loss: 0.7046189308166504] [G loss: 0.8588320016860962]\n",
      "[Epoch 50/1001] [Batch 76/372] [D loss: 0.7058202028274536] [G loss: 0.5671619772911072]\n",
      "[Epoch 50/1001] [Batch 77/372] [D loss: 0.702330470085144] [G loss: 0.7695893049240112]\n",
      "[Epoch 50/1001] [Batch 78/372] [D loss: 0.6958805322647095] [G loss: 0.6663299798965454]\n",
      "[Epoch 50/1001] [Batch 79/372] [D loss: 0.6931948065757751] [G loss: 0.679096519947052]\n",
      "[Epoch 50/1001] [Batch 80/372] [D loss: 0.6930607557296753] [G loss: 0.7129577398300171]\n",
      "[Epoch 50/1001] [Batch 81/372] [D loss: 0.6929459571838379] [G loss: 0.669247031211853]\n",
      "[Epoch 50/1001] [Batch 82/372] [D loss: 0.6929105520248413] [G loss: 0.7045984268188477]\n",
      "[Epoch 50/1001] [Batch 83/372] [D loss: 0.6935802698135376] [G loss: 0.679826557636261]\n",
      "[Epoch 50/1001] [Batch 84/372] [D loss: 0.692950427532196] [G loss: 0.6946807503700256]\n",
      "[Epoch 50/1001] [Batch 85/372] [D loss: 0.6934971809387207] [G loss: 0.692910373210907]\n",
      "[Epoch 50/1001] [Batch 86/372] [D loss: 0.6940388679504395] [G loss: 0.6845972537994385]\n",
      "[Epoch 50/1001] [Batch 87/372] [D loss: 0.6932093501091003] [G loss: 0.6965987682342529]\n",
      "[Epoch 50/1001] [Batch 88/372] [D loss: 0.693646252155304] [G loss: 0.6829304099082947]\n",
      "[Epoch 50/1001] [Batch 89/372] [D loss: 0.6924774646759033] [G loss: 0.6949198842048645]\n",
      "[Epoch 50/1001] [Batch 90/372] [D loss: 0.6928948163986206] [G loss: 0.6884019374847412]\n",
      "[Epoch 50/1001] [Batch 91/372] [D loss: 0.6932803392410278] [G loss: 0.6932093501091003]\n",
      "[Epoch 50/1001] [Batch 92/372] [D loss: 0.6927746534347534] [G loss: 0.6836199760437012]\n",
      "[Epoch 50/1001] [Batch 93/372] [D loss: 0.6926671266555786] [G loss: 0.7002250552177429]\n",
      "[Epoch 50/1001] [Batch 94/372] [D loss: 0.6930022239685059] [G loss: 0.6775736808776855]\n",
      "[Epoch 50/1001] [Batch 95/372] [D loss: 0.6922644972801208] [G loss: 0.6990897059440613]\n",
      "[Epoch 50/1001] [Batch 96/372] [D loss: 0.6929159164428711] [G loss: 0.6815623044967651]\n",
      "[Epoch 50/1001] [Batch 97/372] [D loss: 0.6924538612365723] [G loss: 0.6962263584136963]\n",
      "[Epoch 50/1001] [Batch 98/372] [D loss: 0.6933543682098389] [G loss: 0.6870438456535339]\n",
      "[Epoch 50/1001] [Batch 99/372] [D loss: 0.6929441690444946] [G loss: 0.6866085529327393]\n",
      "[Epoch 50/1001] [Batch 100/372] [D loss: 0.6928064823150635] [G loss: 0.6969218850135803]\n",
      "[Epoch 50/1001] [Batch 101/372] [D loss: 0.6929963827133179] [G loss: 0.6733707189559937]\n",
      "[Epoch 50/1001] [Batch 102/372] [D loss: 0.6935712099075317] [G loss: 0.7141287326812744]\n",
      "[Epoch 50/1001] [Batch 103/372] [D loss: 0.693564772605896] [G loss: 0.6601020693778992]\n",
      "[Epoch 50/1001] [Batch 104/372] [D loss: 0.6939527988433838] [G loss: 0.7269519567489624]\n",
      "[Epoch 50/1001] [Batch 105/372] [D loss: 0.6934816241264343] [G loss: 0.643618643283844]\n",
      "[Epoch 50/1001] [Batch 106/372] [D loss: 0.6937639713287354] [G loss: 0.7543307542800903]\n",
      "[Epoch 50/1001] [Batch 107/372] [D loss: 0.6947509050369263] [G loss: 0.6120380163192749]\n",
      "[Epoch 50/1001] [Batch 108/372] [D loss: 0.695730447769165] [G loss: 0.8003842830657959]\n",
      "[Epoch 50/1001] [Batch 109/372] [D loss: 0.6981587409973145] [G loss: 0.573546826839447]\n",
      "[Epoch 50/1001] [Batch 110/372] [D loss: 0.7005429863929749] [G loss: 0.8276553153991699]\n",
      "[Epoch 50/1001] [Batch 111/372] [D loss: 0.7013977766036987] [G loss: 0.5764583349227905]\n",
      "[Epoch 50/1001] [Batch 112/372] [D loss: 0.7000457048416138] [G loss: 0.7889540195465088]\n",
      "[Epoch 50/1001] [Batch 113/372] [D loss: 0.6976030468940735] [G loss: 0.6283923387527466]\n",
      "[Epoch 50/1001] [Batch 114/372] [D loss: 0.6947917342185974] [G loss: 0.7227370738983154]\n",
      "[Epoch 50/1001] [Batch 115/372] [D loss: 0.693539023399353] [G loss: 0.6777645349502563]\n",
      "[Epoch 50/1001] [Batch 116/372] [D loss: 0.6927769184112549] [G loss: 0.6878870129585266]\n",
      "[Epoch 50/1001] [Batch 117/372] [D loss: 0.6934541463851929] [G loss: 0.7006378769874573]\n",
      "[Epoch 50/1001] [Batch 118/372] [D loss: 0.6932195425033569] [G loss: 0.6754194498062134]\n",
      "[Epoch 50/1001] [Batch 119/372] [D loss: 0.6934595108032227] [G loss: 0.7052170634269714]\n",
      "[Epoch 50/1001] [Batch 120/372] [D loss: 0.6926968693733215] [G loss: 0.6786012649536133]\n",
      "[Epoch 50/1001] [Batch 121/372] [D loss: 0.693060040473938] [G loss: 0.6957413554191589]\n",
      "[Epoch 50/1001] [Batch 122/372] [D loss: 0.6930971145629883] [G loss: 0.6846320629119873]\n",
      "[Epoch 50/1001] [Batch 123/372] [D loss: 0.6937476396560669] [G loss: 0.6925519108772278]\n",
      "[Epoch 50/1001] [Batch 124/372] [D loss: 0.6928259134292603] [G loss: 0.6875298619270325]\n",
      "[Epoch 50/1001] [Batch 125/372] [D loss: 0.6938076019287109] [G loss: 0.6958496570587158]\n",
      "[Epoch 50/1001] [Batch 126/372] [D loss: 0.6921391487121582] [G loss: 0.6817930936813354]\n",
      "[Epoch 50/1001] [Batch 127/372] [D loss: 0.6927708983421326] [G loss: 0.6976289749145508]\n",
      "[Epoch 50/1001] [Batch 128/372] [D loss: 0.6927318572998047] [G loss: 0.689011812210083]\n",
      "[Epoch 50/1001] [Batch 129/372] [D loss: 0.6934385895729065] [G loss: 0.6865502595901489]\n",
      "[Epoch 50/1001] [Batch 130/372] [D loss: 0.692467451095581] [G loss: 0.6949690580368042]\n",
      "[Epoch 50/1001] [Batch 131/372] [D loss: 0.6926591396331787] [G loss: 0.6796479225158691]\n",
      "[Epoch 50/1001] [Batch 132/372] [D loss: 0.6938900351524353] [G loss: 0.7024489641189575]\n",
      "[Epoch 50/1001] [Batch 133/372] [D loss: 0.6926164627075195] [G loss: 0.673445463180542]\n",
      "[Epoch 50/1001] [Batch 134/372] [D loss: 0.6932114362716675] [G loss: 0.7093601226806641]\n",
      "[Epoch 50/1001] [Batch 135/372] [D loss: 0.6940925121307373] [G loss: 0.6638197302818298]\n",
      "[Epoch 50/1001] [Batch 136/372] [D loss: 0.6940065622329712] [G loss: 0.7186790108680725]\n",
      "[Epoch 50/1001] [Batch 137/372] [D loss: 0.6939300298690796] [G loss: 0.6570026278495789]\n",
      "[Epoch 50/1001] [Batch 138/372] [D loss: 0.6940515041351318] [G loss: 0.7276432514190674]\n",
      "[Epoch 50/1001] [Batch 139/372] [D loss: 0.6946812868118286] [G loss: 0.6498588919639587]\n",
      "[Epoch 50/1001] [Batch 140/372] [D loss: 0.694206714630127] [G loss: 0.7334145307540894]\n",
      "[Epoch 50/1001] [Batch 141/372] [D loss: 0.6941461563110352] [G loss: 0.6432293653488159]\n",
      "[Epoch 50/1001] [Batch 142/372] [D loss: 0.6935049891471863] [G loss: 0.7475466728210449]\n",
      "[Epoch 50/1001] [Batch 143/372] [D loss: 0.6939257383346558] [G loss: 0.6249417066574097]\n",
      "[Epoch 50/1001] [Batch 144/372] [D loss: 0.6944873929023743] [G loss: 0.7715200185775757]\n",
      "[Epoch 50/1001] [Batch 145/372] [D loss: 0.6961255669593811] [G loss: 0.6023659110069275]\n",
      "[Epoch 50/1001] [Batch 146/372] [D loss: 0.6975656747817993] [G loss: 0.7968175411224365]\n",
      "[Epoch 50/1001] [Batch 147/372] [D loss: 0.6984976530075073] [G loss: 0.5853773355484009]\n",
      "[Epoch 50/1001] [Batch 148/372] [D loss: 0.698825478553772] [G loss: 0.8028560280799866]\n",
      "[Epoch 50/1001] [Batch 149/372] [D loss: 0.6989902257919312] [G loss: 0.5951722860336304]\n",
      "[Epoch 50/1001] [Batch 150/372] [D loss: 0.6973626613616943] [G loss: 0.778027355670929]\n",
      "[Epoch 50/1001] [Batch 151/372] [D loss: 0.6961592435836792] [G loss: 0.6276259422302246]\n",
      "[Epoch 50/1001] [Batch 152/372] [D loss: 0.6949493885040283] [G loss: 0.7360055446624756]\n",
      "[Epoch 50/1001] [Batch 153/372] [D loss: 0.6943533420562744] [G loss: 0.6597121953964233]\n",
      "[Epoch 50/1001] [Batch 154/372] [D loss: 0.6928881406784058] [G loss: 0.70838463306427]\n",
      "[Epoch 50/1001] [Batch 155/372] [D loss: 0.6928660869598389] [G loss: 0.6801223158836365]\n",
      "[Epoch 50/1001] [Batch 156/372] [D loss: 0.6928584575653076] [G loss: 0.6899467706680298]\n",
      "[Epoch 50/1001] [Batch 157/372] [D loss: 0.6923619508743286] [G loss: 0.6903249025344849]\n",
      "[Epoch 50/1001] [Batch 158/372] [D loss: 0.6931148767471313] [G loss: 0.687016487121582]\n",
      "[Epoch 50/1001] [Batch 159/372] [D loss: 0.6943438053131104] [G loss: 0.7005550861358643]\n",
      "[Epoch 50/1001] [Batch 160/372] [D loss: 0.6931524276733398] [G loss: 0.6724510192871094]\n",
      "[Epoch 50/1001] [Batch 161/372] [D loss: 0.6929405927658081] [G loss: 0.7082678079605103]\n",
      "[Epoch 50/1001] [Batch 162/372] [D loss: 0.6938173770904541] [G loss: 0.6635267734527588]\n",
      "[Epoch 50/1001] [Batch 163/372] [D loss: 0.6927335262298584] [G loss: 0.7208505868911743]\n",
      "[Epoch 50/1001] [Batch 164/372] [D loss: 0.6934138536453247] [G loss: 0.6544264554977417]\n",
      "[Epoch 50/1001] [Batch 165/372] [D loss: 0.6935259103775024] [G loss: 0.7222704887390137]\n",
      "[Epoch 50/1001] [Batch 166/372] [D loss: 0.6940474510192871] [G loss: 0.6546037793159485]\n",
      "[Epoch 50/1001] [Batch 167/372] [D loss: 0.6941525936126709] [G loss: 0.7284443378448486]\n",
      "[Epoch 50/1001] [Batch 168/372] [D loss: 0.6940152049064636] [G loss: 0.6462694406509399]\n",
      "[Epoch 50/1001] [Batch 169/372] [D loss: 0.6944019198417664] [G loss: 0.7382702231407166]\n",
      "[Epoch 50/1001] [Batch 170/372] [D loss: 0.6936665177345276] [G loss: 0.6316015720367432]\n",
      "[Epoch 50/1001] [Batch 171/372] [D loss: 0.6938237547874451] [G loss: 0.7621856927871704]\n",
      "[Epoch 50/1001] [Batch 172/372] [D loss: 0.6956143975257874] [G loss: 0.6087228655815125]\n",
      "[Epoch 50/1001] [Batch 173/372] [D loss: 0.6963108777999878] [G loss: 0.7957116961479187]\n",
      "[Epoch 50/1001] [Batch 174/372] [D loss: 0.6985808610916138] [G loss: 0.5821990966796875]\n",
      "[Epoch 50/1001] [Batch 175/372] [D loss: 0.6989595890045166] [G loss: 0.8100430369377136]\n",
      "[Epoch 50/1001] [Batch 176/372] [D loss: 0.6992931962013245] [G loss: 0.5875344276428223]\n",
      "[Epoch 50/1001] [Batch 177/372] [D loss: 0.6991389393806458] [G loss: 0.7893999814987183]\n",
      "[Epoch 50/1001] [Batch 178/372] [D loss: 0.6973146200180054] [G loss: 0.6158580183982849]\n",
      "[Epoch 50/1001] [Batch 179/372] [D loss: 0.6951996088027954] [G loss: 0.7440611720085144]\n",
      "[Epoch 50/1001] [Batch 180/372] [D loss: 0.6948528289794922] [G loss: 0.6586047410964966]\n",
      "[Epoch 50/1001] [Batch 181/372] [D loss: 0.6936856508255005] [G loss: 0.705377459526062]\n",
      "[Epoch 50/1001] [Batch 182/372] [D loss: 0.6936690807342529] [G loss: 0.683720052242279]\n",
      "[Epoch 50/1001] [Batch 183/372] [D loss: 0.6933020949363708] [G loss: 0.6858292818069458]\n",
      "[Epoch 50/1001] [Batch 184/372] [D loss: 0.692688524723053] [G loss: 0.6952241659164429]\n",
      "[Epoch 50/1001] [Batch 185/372] [D loss: 0.6933043003082275] [G loss: 0.683430552482605]\n",
      "[Epoch 50/1001] [Batch 186/372] [D loss: 0.6937199234962463] [G loss: 0.6956457495689392]\n",
      "[Epoch 50/1001] [Batch 187/372] [D loss: 0.6924021244049072] [G loss: 0.6870743036270142]\n",
      "[Epoch 50/1001] [Batch 188/372] [D loss: 0.6926337480545044] [G loss: 0.6894885897636414]\n",
      "[Epoch 50/1001] [Batch 189/372] [D loss: 0.6925172805786133] [G loss: 0.6903480887413025]\n",
      "[Epoch 50/1001] [Batch 190/372] [D loss: 0.692866325378418] [G loss: 0.686016857624054]\n",
      "[Epoch 50/1001] [Batch 191/372] [D loss: 0.6934739947319031] [G loss: 0.6949084401130676]\n",
      "[Epoch 50/1001] [Batch 192/372] [D loss: 0.6923811435699463] [G loss: 0.6847496628761292]\n",
      "[Epoch 50/1001] [Batch 193/372] [D loss: 0.6932982802391052] [G loss: 0.6983118057250977]\n",
      "[Epoch 50/1001] [Batch 194/372] [D loss: 0.6933935880661011] [G loss: 0.6782837510108948]\n",
      "[Epoch 50/1001] [Batch 195/372] [D loss: 0.6930544972419739] [G loss: 0.7011903524398804]\n",
      "[Epoch 50/1001] [Batch 196/372] [D loss: 0.6925594210624695] [G loss: 0.6758136749267578]\n",
      "[Epoch 50/1001] [Batch 197/372] [D loss: 0.6933672428131104] [G loss: 0.7084921598434448]\n",
      "[Epoch 50/1001] [Batch 198/372] [D loss: 0.6926103830337524] [G loss: 0.6647388339042664]\n",
      "[Epoch 50/1001] [Batch 199/372] [D loss: 0.6932505369186401] [G loss: 0.7191058397293091]\n",
      "[Epoch 50/1001] [Batch 200/372] [D loss: 0.693866491317749] [G loss: 0.6568511724472046]\n",
      "[Epoch 50/1001] [Batch 201/372] [D loss: 0.6932511925697327] [G loss: 0.7289640307426453]\n",
      "[Epoch 50/1001] [Batch 202/372] [D loss: 0.6941406726837158] [G loss: 0.6438140273094177]\n",
      "[Epoch 50/1001] [Batch 203/372] [D loss: 0.6932827234268188] [G loss: 0.7445082068443298]\n",
      "[Epoch 50/1001] [Batch 204/372] [D loss: 0.6945772171020508] [G loss: 0.6319822072982788]\n",
      "[Epoch 50/1001] [Batch 205/372] [D loss: 0.695202112197876] [G loss: 0.7556425333023071]\n",
      "[Epoch 50/1001] [Batch 206/372] [D loss: 0.6955348253250122] [G loss: 0.6213147640228271]\n",
      "[Epoch 50/1001] [Batch 207/372] [D loss: 0.6959642767906189] [G loss: 0.7676870226860046]\n",
      "[Epoch 50/1001] [Batch 208/372] [D loss: 0.6962922215461731] [G loss: 0.6108320355415344]\n",
      "[Epoch 50/1001] [Batch 209/372] [D loss: 0.6967114210128784] [G loss: 0.7791410088539124]\n",
      "[Epoch 50/1001] [Batch 210/372] [D loss: 0.6969904899597168] [G loss: 0.6069859862327576]\n",
      "[Epoch 50/1001] [Batch 211/372] [D loss: 0.696832537651062] [G loss: 0.7794895172119141]\n",
      "[Epoch 50/1001] [Batch 212/372] [D loss: 0.6962592005729675] [G loss: 0.608930230140686]\n",
      "[Epoch 50/1001] [Batch 213/372] [D loss: 0.6963576078414917] [G loss: 0.775319516658783]\n",
      "[Epoch 50/1001] [Batch 214/372] [D loss: 0.6956056356430054] [G loss: 0.6162571310997009]\n",
      "[Epoch 50/1001] [Batch 215/372] [D loss: 0.6954241991043091] [G loss: 0.7564780712127686]\n",
      "[Epoch 50/1001] [Batch 216/372] [D loss: 0.6951436996459961] [G loss: 0.6405477523803711]\n",
      "[Epoch 50/1001] [Batch 217/372] [D loss: 0.6938821077346802] [G loss: 0.7307950258255005]\n",
      "[Epoch 50/1001] [Batch 218/372] [D loss: 0.6932692527770996] [G loss: 0.6548839807510376]\n",
      "[Epoch 50/1001] [Batch 219/372] [D loss: 0.6931262612342834] [G loss: 0.7180579900741577]\n",
      "[Epoch 50/1001] [Batch 220/372] [D loss: 0.6930935382843018] [G loss: 0.6656144857406616]\n",
      "[Epoch 50/1001] [Batch 221/372] [D loss: 0.6924489736557007] [G loss: 0.7135895490646362]\n",
      "[Epoch 50/1001] [Batch 222/372] [D loss: 0.6926604509353638] [G loss: 0.6638951897621155]\n",
      "[Epoch 50/1001] [Batch 223/372] [D loss: 0.6934210062026978] [G loss: 0.7144290208816528]\n",
      "[Epoch 50/1001] [Batch 224/372] [D loss: 0.6936627626419067] [G loss: 0.6657554507255554]\n",
      "[Epoch 50/1001] [Batch 225/372] [D loss: 0.6929951906204224] [G loss: 0.71141517162323]\n",
      "[Epoch 50/1001] [Batch 226/372] [D loss: 0.693845808506012] [G loss: 0.6661971211433411]\n",
      "[Epoch 50/1001] [Batch 227/372] [D loss: 0.6942816972732544] [G loss: 0.7046814560890198]\n",
      "[Epoch 50/1001] [Batch 228/372] [D loss: 0.6928360462188721] [G loss: 0.6759330034255981]\n",
      "[Epoch 50/1001] [Batch 229/372] [D loss: 0.6936882734298706] [G loss: 0.7007943987846375]\n",
      "[Epoch 50/1001] [Batch 230/372] [D loss: 0.6929798126220703] [G loss: 0.6726363897323608]\n",
      "[Epoch 50/1001] [Batch 231/372] [D loss: 0.6917369365692139] [G loss: 0.7095217108726501]\n",
      "[Epoch 50/1001] [Batch 232/372] [D loss: 0.6915274262428284] [G loss: 0.6624124050140381]\n",
      "[Epoch 50/1001] [Batch 233/372] [D loss: 0.6931737661361694] [G loss: 0.7236955761909485]\n",
      "[Epoch 50/1001] [Batch 234/372] [D loss: 0.6935303211212158] [G loss: 0.6470186710357666]\n",
      "[Epoch 50/1001] [Batch 235/372] [D loss: 0.6946190595626831] [G loss: 0.7402158975601196]\n",
      "[Epoch 50/1001] [Batch 236/372] [D loss: 0.6938485503196716] [G loss: 0.6326697468757629]\n",
      "[Epoch 50/1001] [Batch 237/372] [D loss: 0.6941691637039185] [G loss: 0.7621467113494873]\n",
      "[Epoch 50/1001] [Batch 238/372] [D loss: 0.6960344314575195] [G loss: 0.6035497188568115]\n",
      "[Epoch 50/1001] [Batch 239/372] [D loss: 0.6959887146949768] [G loss: 0.8000959157943726]\n",
      "[Epoch 50/1001] [Batch 240/372] [D loss: 0.6977263689041138] [G loss: 0.5771870017051697]\n",
      "[Epoch 50/1001] [Batch 241/372] [D loss: 0.700584352016449] [G loss: 0.8322992324829102]\n",
      "[Epoch 50/1001] [Batch 242/372] [D loss: 0.7021622061729431] [G loss: 0.5620146989822388]\n",
      "[Epoch 50/1001] [Batch 243/372] [D loss: 0.7019048929214478] [G loss: 0.8159271478652954]\n",
      "[Epoch 50/1001] [Batch 244/372] [D loss: 0.7005642652511597] [G loss: 0.6031282544136047]\n",
      "[Epoch 50/1001] [Batch 245/372] [D loss: 0.6964056491851807] [G loss: 0.7468612790107727]\n",
      "[Epoch 50/1001] [Batch 246/372] [D loss: 0.6944589614868164] [G loss: 0.6628312468528748]\n",
      "[Epoch 50/1001] [Batch 247/372] [D loss: 0.6927634477615356] [G loss: 0.694084107875824]\n",
      "[Epoch 50/1001] [Batch 248/372] [D loss: 0.69350266456604] [G loss: 0.6981279850006104]\n",
      "[Epoch 50/1001] [Batch 249/372] [D loss: 0.6927832365036011] [G loss: 0.6779566407203674]\n",
      "[Epoch 50/1001] [Batch 250/372] [D loss: 0.6928340792655945] [G loss: 0.6989395618438721]\n",
      "[Epoch 50/1001] [Batch 251/372] [D loss: 0.6934067010879517] [G loss: 0.681285560131073]\n",
      "[Epoch 50/1001] [Batch 252/372] [D loss: 0.6933823823928833] [G loss: 0.6972839832305908]\n",
      "[Epoch 50/1001] [Batch 253/372] [D loss: 0.6924886107444763] [G loss: 0.6816538572311401]\n",
      "[Epoch 50/1001] [Batch 254/372] [D loss: 0.6931411027908325] [G loss: 0.6984325647354126]\n",
      "[Epoch 50/1001] [Batch 255/372] [D loss: 0.6916754245758057] [G loss: 0.6838695406913757]\n",
      "[Epoch 50/1001] [Batch 256/372] [D loss: 0.6927158832550049] [G loss: 0.6857900023460388]\n",
      "[Epoch 50/1001] [Batch 257/372] [D loss: 0.6936664581298828] [G loss: 0.6954387426376343]\n",
      "[Epoch 50/1001] [Batch 258/372] [D loss: 0.6927027106285095] [G loss: 0.6837325692176819]\n",
      "[Epoch 50/1001] [Batch 259/372] [D loss: 0.6937429904937744] [G loss: 0.6900847554206848]\n",
      "[Epoch 50/1001] [Batch 260/372] [D loss: 0.6928932666778564] [G loss: 0.6904600858688354]\n",
      "[Epoch 50/1001] [Batch 261/372] [D loss: 0.6917829513549805] [G loss: 0.685715913772583]\n",
      "[Epoch 50/1001] [Batch 262/372] [D loss: 0.6936622858047485] [G loss: 0.6960098743438721]\n",
      "[Epoch 50/1001] [Batch 263/372] [D loss: 0.6936208009719849] [G loss: 0.6791460514068604]\n",
      "[Epoch 50/1001] [Batch 264/372] [D loss: 0.6932200789451599] [G loss: 0.7038125991821289]\n",
      "[Epoch 50/1001] [Batch 265/372] [D loss: 0.6933618783950806] [G loss: 0.6663122773170471]\n",
      "[Epoch 50/1001] [Batch 266/372] [D loss: 0.6933071613311768] [G loss: 0.7169513702392578]\n",
      "[Epoch 50/1001] [Batch 267/372] [D loss: 0.6938276290893555] [G loss: 0.6588233113288879]\n",
      "[Epoch 50/1001] [Batch 268/372] [D loss: 0.6927334666252136] [G loss: 0.7204366326332092]\n",
      "[Epoch 50/1001] [Batch 269/372] [D loss: 0.6938074827194214] [G loss: 0.6586042642593384]\n",
      "[Epoch 50/1001] [Batch 270/372] [D loss: 0.6935068964958191] [G loss: 0.7205424308776855]\n",
      "[Epoch 50/1001] [Batch 271/372] [D loss: 0.6932248473167419] [G loss: 0.6599158644676208]\n",
      "[Epoch 50/1001] [Batch 272/372] [D loss: 0.6933602690696716] [G loss: 0.7219605445861816]\n",
      "[Epoch 50/1001] [Batch 273/372] [D loss: 0.6932246685028076] [G loss: 0.6559317111968994]\n",
      "[Epoch 50/1001] [Batch 274/372] [D loss: 0.6926409006118774] [G loss: 0.7283414006233215]\n",
      "[Epoch 50/1001] [Batch 275/372] [D loss: 0.6935155391693115] [G loss: 0.6488137245178223]\n",
      "[Epoch 50/1001] [Batch 276/372] [D loss: 0.6949582099914551] [G loss: 0.7386465668678284]\n",
      "[Epoch 50/1001] [Batch 277/372] [D loss: 0.6945106983184814] [G loss: 0.6332029104232788]\n",
      "[Epoch 50/1001] [Batch 278/372] [D loss: 0.6942739486694336] [G loss: 0.7562500238418579]\n",
      "[Epoch 50/1001] [Batch 279/372] [D loss: 0.6951792240142822] [G loss: 0.6180247664451599]\n",
      "[Epoch 50/1001] [Batch 280/372] [D loss: 0.6952465772628784] [G loss: 0.7727560997009277]\n",
      "[Epoch 50/1001] [Batch 281/372] [D loss: 0.6961820125579834] [G loss: 0.6015751361846924]\n",
      "[Epoch 50/1001] [Batch 282/372] [D loss: 0.6971169710159302] [G loss: 0.803708553314209]\n",
      "[Epoch 50/1001] [Batch 283/372] [D loss: 0.6988279819488525] [G loss: 0.5767076015472412]\n",
      "[Epoch 50/1001] [Batch 284/372] [D loss: 0.6993284821510315] [G loss: 0.8144425749778748]\n",
      "[Epoch 50/1001] [Batch 285/372] [D loss: 0.7000696659088135] [G loss: 0.5849471688270569]\n",
      "[Epoch 50/1001] [Batch 286/372] [D loss: 0.6992255449295044] [G loss: 0.7918727397918701]\n",
      "[Epoch 50/1001] [Batch 287/372] [D loss: 0.6971725225448608] [G loss: 0.6141778230667114]\n",
      "[Epoch 50/1001] [Batch 288/372] [D loss: 0.6964473724365234] [G loss: 0.7548425197601318]\n",
      "[Epoch 50/1001] [Batch 289/372] [D loss: 0.6943383812904358] [G loss: 0.6433072090148926]\n",
      "[Epoch 50/1001] [Batch 290/372] [D loss: 0.6938552856445312] [G loss: 0.7275354862213135]\n",
      "[Epoch 50/1001] [Batch 291/372] [D loss: 0.6928590536117554] [G loss: 0.6632643342018127]\n",
      "[Epoch 50/1001] [Batch 292/372] [D loss: 0.6934330463409424] [G loss: 0.7076898217201233]\n",
      "[Epoch 50/1001] [Batch 293/372] [D loss: 0.6940929293632507] [G loss: 0.6710304021835327]\n",
      "[Epoch 50/1001] [Batch 294/372] [D loss: 0.6939574480056763] [G loss: 0.7091110348701477]\n",
      "[Epoch 50/1001] [Batch 295/372] [D loss: 0.6933125853538513] [G loss: 0.6731248497962952]\n",
      "[Epoch 50/1001] [Batch 296/372] [D loss: 0.6932711601257324] [G loss: 0.7006773948669434]\n",
      "[Epoch 50/1001] [Batch 297/372] [D loss: 0.6930524706840515] [G loss: 0.6824072003364563]\n",
      "[Epoch 50/1001] [Batch 298/372] [D loss: 0.6927904486656189] [G loss: 0.6957497596740723]\n",
      "[Epoch 50/1001] [Batch 299/372] [D loss: 0.6929104328155518] [G loss: 0.6813711524009705]\n",
      "[Epoch 50/1001] [Batch 300/372] [D loss: 0.693055272102356] [G loss: 0.6953622698783875]\n",
      "[Epoch 50/1001] [Batch 301/372] [D loss: 0.6932226419448853] [G loss: 0.6795812845230103]\n",
      "[Epoch 50/1001] [Batch 302/372] [D loss: 0.6931804418563843] [G loss: 0.7007267475128174]\n",
      "[Epoch 50/1001] [Batch 303/372] [D loss: 0.6932815909385681] [G loss: 0.6840759515762329]\n",
      "[Epoch 50/1001] [Batch 304/372] [D loss: 0.6926143169403076] [G loss: 0.6897099614143372]\n",
      "[Epoch 50/1001] [Batch 305/372] [D loss: 0.6932157278060913] [G loss: 0.6878007650375366]\n",
      "[Epoch 50/1001] [Batch 306/372] [D loss: 0.6930270195007324] [G loss: 0.6890491843223572]\n",
      "[Epoch 50/1001] [Batch 307/372] [D loss: 0.6931687593460083] [G loss: 0.6862470507621765]\n",
      "[Epoch 50/1001] [Batch 308/372] [D loss: 0.6939088702201843] [G loss: 0.6940758228302002]\n",
      "[Epoch 50/1001] [Batch 309/372] [D loss: 0.6917940974235535] [G loss: 0.6836687326431274]\n",
      "[Epoch 50/1001] [Batch 310/372] [D loss: 0.6933366656303406] [G loss: 0.6935055255889893]\n",
      "[Epoch 50/1001] [Batch 311/372] [D loss: 0.6932246685028076] [G loss: 0.6836852431297302]\n",
      "[Epoch 50/1001] [Batch 312/372] [D loss: 0.6938207149505615] [G loss: 0.6965237259864807]\n",
      "[Epoch 50/1001] [Batch 313/372] [D loss: 0.6927377581596375] [G loss: 0.6800718903541565]\n",
      "[Epoch 50/1001] [Batch 314/372] [D loss: 0.6934489011764526] [G loss: 0.6980329155921936]\n",
      "[Epoch 50/1001] [Batch 315/372] [D loss: 0.6931062936782837] [G loss: 0.6791883707046509]\n",
      "[Epoch 50/1001] [Batch 316/372] [D loss: 0.6941993236541748] [G loss: 0.698697566986084]\n",
      "[Epoch 50/1001] [Batch 317/372] [D loss: 0.6923106908798218] [G loss: 0.6824607849121094]\n",
      "[Epoch 50/1001] [Batch 318/372] [D loss: 0.6921683549880981] [G loss: 0.6948382258415222]\n",
      "[Epoch 50/1001] [Batch 319/372] [D loss: 0.6925687789916992] [G loss: 0.6790283918380737]\n",
      "[Epoch 50/1001] [Batch 320/372] [D loss: 0.6927652359008789] [G loss: 0.7036469578742981]\n",
      "[Epoch 50/1001] [Batch 321/372] [D loss: 0.6939157843589783] [G loss: 0.6698217988014221]\n",
      "[Epoch 50/1001] [Batch 322/372] [D loss: 0.6927781701087952] [G loss: 0.7142447233200073]\n",
      "[Epoch 50/1001] [Batch 323/372] [D loss: 0.694035530090332] [G loss: 0.6563552618026733]\n",
      "[Epoch 50/1001] [Batch 324/372] [D loss: 0.6927618980407715] [G loss: 0.7321369051933289]\n",
      "[Epoch 50/1001] [Batch 325/372] [D loss: 0.6940346360206604] [G loss: 0.6380711793899536]\n",
      "[Epoch 50/1001] [Batch 326/372] [D loss: 0.6949700117111206] [G loss: 0.7544575929641724]\n",
      "[Epoch 50/1001] [Batch 327/372] [D loss: 0.6958428621292114] [G loss: 0.612022876739502]\n",
      "[Epoch 50/1001] [Batch 328/372] [D loss: 0.6958558559417725] [G loss: 0.7890458703041077]\n",
      "[Epoch 50/1001] [Batch 329/372] [D loss: 0.6983066201210022] [G loss: 0.583134651184082]\n",
      "[Epoch 50/1001] [Batch 330/372] [D loss: 0.699845552444458] [G loss: 0.8300971984863281]\n",
      "[Epoch 50/1001] [Batch 331/372] [D loss: 0.7023775577545166] [G loss: 0.5540835857391357]\n",
      "[Epoch 50/1001] [Batch 332/372] [D loss: 0.7036398649215698] [G loss: 0.8467402458190918]\n",
      "[Epoch 50/1001] [Batch 333/372] [D loss: 0.7044029235839844] [G loss: 0.5666201114654541]\n",
      "[Epoch 50/1001] [Batch 334/372] [D loss: 0.7016610503196716] [G loss: 0.7998376488685608]\n",
      "[Epoch 50/1001] [Batch 335/372] [D loss: 0.6982430815696716] [G loss: 0.620478093624115]\n",
      "[Epoch 50/1001] [Batch 336/372] [D loss: 0.6965067386627197] [G loss: 0.7329039573669434]\n",
      "[Epoch 50/1001] [Batch 337/372] [D loss: 0.693604052066803] [G loss: 0.6692636609077454]\n",
      "[Epoch 50/1001] [Batch 338/372] [D loss: 0.6931362748146057] [G loss: 0.6936253309249878]\n",
      "[Epoch 50/1001] [Batch 339/372] [D loss: 0.6925491094589233] [G loss: 0.692205548286438]\n",
      "[Epoch 50/1001] [Batch 340/372] [D loss: 0.692835807800293] [G loss: 0.6861788630485535]\n",
      "[Epoch 50/1001] [Batch 341/372] [D loss: 0.6927261352539062] [G loss: 0.691651463508606]\n",
      "[Epoch 50/1001] [Batch 342/372] [D loss: 0.6931334137916565] [G loss: 0.6895648241043091]\n",
      "[Epoch 50/1001] [Batch 343/372] [D loss: 0.6933913230895996] [G loss: 0.688153862953186]\n",
      "[Epoch 50/1001] [Batch 344/372] [D loss: 0.6924459934234619] [G loss: 0.6898244619369507]\n",
      "[Epoch 50/1001] [Batch 345/372] [D loss: 0.6932554244995117] [G loss: 0.6951821446418762]\n",
      "[Epoch 50/1001] [Batch 346/372] [D loss: 0.6925556063652039] [G loss: 0.6814326047897339]\n",
      "[Epoch 50/1001] [Batch 347/372] [D loss: 0.6923571825027466] [G loss: 0.6993824243545532]\n",
      "[Epoch 50/1001] [Batch 348/372] [D loss: 0.6930780410766602] [G loss: 0.6802869439125061]\n",
      "[Epoch 50/1001] [Batch 349/372] [D loss: 0.692785382270813] [G loss: 0.6987560987472534]\n",
      "[Epoch 50/1001] [Batch 350/372] [D loss: 0.6933901906013489] [G loss: 0.6768427491188049]\n",
      "[Epoch 50/1001] [Batch 351/372] [D loss: 0.6931272745132446] [G loss: 0.7031645774841309]\n",
      "[Epoch 50/1001] [Batch 352/372] [D loss: 0.6930323243141174] [G loss: 0.6736961007118225]\n",
      "[Epoch 50/1001] [Batch 353/372] [D loss: 0.6924616098403931] [G loss: 0.7029047608375549]\n",
      "[Epoch 50/1001] [Batch 354/372] [D loss: 0.6921493411064148] [G loss: 0.6724158525466919]\n",
      "[Epoch 50/1001] [Batch 355/372] [D loss: 0.6925176382064819] [G loss: 0.7168999910354614]\n",
      "[Epoch 50/1001] [Batch 356/372] [D loss: 0.6922845840454102] [G loss: 0.6542384624481201]\n",
      "[Epoch 50/1001] [Batch 357/372] [D loss: 0.6932865381240845] [G loss: 0.7324848175048828]\n",
      "[Epoch 50/1001] [Batch 358/372] [D loss: 0.6935032606124878] [G loss: 0.6434152722358704]\n",
      "[Epoch 50/1001] [Batch 359/372] [D loss: 0.6954296827316284] [G loss: 0.7426575422286987]\n",
      "[Epoch 50/1001] [Batch 360/372] [D loss: 0.6946452856063843] [G loss: 0.6311784982681274]\n",
      "[Epoch 50/1001] [Batch 361/372] [D loss: 0.6951440572738647] [G loss: 0.7526249885559082]\n",
      "[Epoch 50/1001] [Batch 362/372] [D loss: 0.6970981359481812] [G loss: 0.6222385764122009]\n",
      "[Epoch 50/1001] [Batch 363/372] [D loss: 0.6964536905288696] [G loss: 0.7651044726371765]\n",
      "[Epoch 50/1001] [Batch 364/372] [D loss: 0.6961383819580078] [G loss: 0.6145883202552795]\n",
      "[Epoch 50/1001] [Batch 365/372] [D loss: 0.6951370239257812] [G loss: 0.7700101733207703]\n",
      "[Epoch 50/1001] [Batch 366/372] [D loss: 0.6960099935531616] [G loss: 0.6139707565307617]\n",
      "[Epoch 50/1001] [Batch 367/372] [D loss: 0.6961702108383179] [G loss: 0.7680886387825012]\n",
      "[Epoch 50/1001] [Batch 368/372] [D loss: 0.6956530809402466] [G loss: 0.6208618879318237]\n",
      "[Epoch 50/1001] [Batch 369/372] [D loss: 0.6961307525634766] [G loss: 0.7559245824813843]\n",
      "[Epoch 50/1001] [Batch 370/372] [D loss: 0.6951173543930054] [G loss: 0.6301965117454529]\n",
      "[Epoch 50/1001] [Batch 371/372] [D loss: 0.6949198246002197] [G loss: 0.742655336856842]\n",
      "[Epoch 51/1001] [Batch 0/372] [D loss: 0.6936054229736328] [G loss: 0.6414506435394287]\n",
      "[Epoch 51/1001] [Batch 1/372] [D loss: 0.6938439607620239] [G loss: 0.7367194294929504]\n",
      "[Epoch 51/1001] [Batch 2/372] [D loss: 0.6926333904266357] [G loss: 0.6518552303314209]\n",
      "[Epoch 51/1001] [Batch 3/372] [D loss: 0.6948132514953613] [G loss: 0.7249904870986938]\n",
      "[Epoch 51/1001] [Batch 4/372] [D loss: 0.6936997771263123] [G loss: 0.6557201147079468]\n",
      "[Epoch 51/1001] [Batch 5/372] [D loss: 0.6937665343284607] [G loss: 0.7245170474052429]\n",
      "[Epoch 51/1001] [Batch 6/372] [D loss: 0.6941585540771484] [G loss: 0.6560882329940796]\n",
      "[Epoch 51/1001] [Batch 7/372] [D loss: 0.6934348344802856] [G loss: 0.7224230170249939]\n",
      "[Epoch 51/1001] [Batch 8/372] [D loss: 0.6931720972061157] [G loss: 0.662430465221405]\n",
      "[Epoch 51/1001] [Batch 9/372] [D loss: 0.6928969621658325] [G loss: 0.7113272547721863]\n",
      "[Epoch 51/1001] [Batch 10/372] [D loss: 0.6929806470870972] [G loss: 0.6760436296463013]\n",
      "[Epoch 51/1001] [Batch 11/372] [D loss: 0.6925570368766785] [G loss: 0.7015085220336914]\n",
      "[Epoch 51/1001] [Batch 12/372] [D loss: 0.6944431066513062] [G loss: 0.677274227142334]\n",
      "[Epoch 51/1001] [Batch 13/372] [D loss: 0.6930369138717651] [G loss: 0.6948472857475281]\n",
      "[Epoch 51/1001] [Batch 14/372] [D loss: 0.691473126411438] [G loss: 0.6888890862464905]\n",
      "[Epoch 51/1001] [Batch 15/372] [D loss: 0.6918150186538696] [G loss: 0.6881667375564575]\n",
      "[Epoch 51/1001] [Batch 16/372] [D loss: 0.6925311088562012] [G loss: 0.6951165795326233]\n",
      "[Epoch 51/1001] [Batch 17/372] [D loss: 0.6921483278274536] [G loss: 0.6806606650352478]\n",
      "[Epoch 51/1001] [Batch 18/372] [D loss: 0.6926581263542175] [G loss: 0.6968477368354797]\n",
      "[Epoch 51/1001] [Batch 19/372] [D loss: 0.6931205987930298] [G loss: 0.6760449409484863]\n",
      "[Epoch 51/1001] [Batch 20/372] [D loss: 0.6919537782669067] [G loss: 0.7019284963607788]\n",
      "[Epoch 51/1001] [Batch 21/372] [D loss: 0.6920265555381775] [G loss: 0.6720156073570251]\n",
      "[Epoch 51/1001] [Batch 22/372] [D loss: 0.6934216022491455] [G loss: 0.7014647126197815]\n",
      "[Epoch 51/1001] [Batch 23/372] [D loss: 0.6936826705932617] [G loss: 0.6723458170890808]\n",
      "[Epoch 51/1001] [Batch 24/372] [D loss: 0.6939883232116699] [G loss: 0.7051940560340881]\n",
      "[Epoch 51/1001] [Batch 25/372] [D loss: 0.6928250193595886] [G loss: 0.6709051728248596]\n",
      "[Epoch 51/1001] [Batch 26/372] [D loss: 0.6922704577445984] [G loss: 0.7071254253387451]\n",
      "[Epoch 51/1001] [Batch 27/372] [D loss: 0.6932941675186157] [G loss: 0.66839998960495]\n",
      "[Epoch 51/1001] [Batch 28/372] [D loss: 0.6932731866836548] [G loss: 0.7124760150909424]\n",
      "[Epoch 51/1001] [Batch 29/372] [D loss: 0.693909764289856] [G loss: 0.6585999131202698]\n",
      "[Epoch 51/1001] [Batch 30/372] [D loss: 0.6935347318649292] [G loss: 0.7322618365287781]\n",
      "[Epoch 51/1001] [Batch 31/372] [D loss: 0.69413161277771] [G loss: 0.6325234770774841]\n",
      "[Epoch 51/1001] [Batch 32/372] [D loss: 0.6940145492553711] [G loss: 0.7661241292953491]\n",
      "[Epoch 51/1001] [Batch 33/372] [D loss: 0.6951743364334106] [G loss: 0.6028298735618591]\n",
      "[Epoch 51/1001] [Batch 34/372] [D loss: 0.6968472599983215] [G loss: 0.7990226745605469]\n",
      "[Epoch 51/1001] [Batch 35/372] [D loss: 0.6981868743896484] [G loss: 0.5732722878456116]\n",
      "[Epoch 51/1001] [Batch 36/372] [D loss: 0.7008985280990601] [G loss: 0.8344692587852478]\n",
      "[Epoch 51/1001] [Batch 37/372] [D loss: 0.7020750045776367] [G loss: 0.5553476810455322]\n",
      "[Epoch 51/1001] [Batch 38/372] [D loss: 0.7023341655731201] [G loss: 0.8331910371780396]\n",
      "[Epoch 51/1001] [Batch 39/372] [D loss: 0.7013183832168579] [G loss: 0.5792555809020996]\n",
      "[Epoch 51/1001] [Batch 40/372] [D loss: 0.6987558603286743] [G loss: 0.7830445766448975]\n",
      "[Epoch 51/1001] [Batch 41/372] [D loss: 0.6963503360748291] [G loss: 0.6283621191978455]\n",
      "[Epoch 51/1001] [Batch 42/372] [D loss: 0.695187509059906] [G loss: 0.7337058186531067]\n",
      "[Epoch 51/1001] [Batch 43/372] [D loss: 0.6932809352874756] [G loss: 0.6618785262107849]\n",
      "[Epoch 51/1001] [Batch 44/372] [D loss: 0.693321943283081] [G loss: 0.7077616453170776]\n",
      "[Epoch 51/1001] [Batch 45/372] [D loss: 0.6937845349311829] [G loss: 0.6749225854873657]\n",
      "[Epoch 51/1001] [Batch 46/372] [D loss: 0.6938350200653076] [G loss: 0.702591598033905]\n",
      "[Epoch 51/1001] [Batch 47/372] [D loss: 0.6934014558792114] [G loss: 0.6760870814323425]\n",
      "[Epoch 51/1001] [Batch 48/372] [D loss: 0.6932010054588318] [G loss: 0.7011388540267944]\n",
      "[Epoch 51/1001] [Batch 49/372] [D loss: 0.6926240921020508] [G loss: 0.6787793040275574]\n",
      "[Epoch 51/1001] [Batch 50/372] [D loss: 0.6926270723342896] [G loss: 0.6979353427886963]\n",
      "[Epoch 51/1001] [Batch 51/372] [D loss: 0.6915665864944458] [G loss: 0.6819671392440796]\n",
      "[Epoch 51/1001] [Batch 52/372] [D loss: 0.6926360130310059] [G loss: 0.6945717334747314]\n",
      "[Epoch 51/1001] [Batch 53/372] [D loss: 0.6933484077453613] [G loss: 0.687484860420227]\n",
      "[Epoch 51/1001] [Batch 54/372] [D loss: 0.6930654048919678] [G loss: 0.6878706812858582]\n",
      "[Epoch 51/1001] [Batch 55/372] [D loss: 0.6927846670150757] [G loss: 0.6927909851074219]\n",
      "[Epoch 51/1001] [Batch 56/372] [D loss: 0.6938738822937012] [G loss: 0.6828785538673401]\n",
      "[Epoch 51/1001] [Batch 57/372] [D loss: 0.692706823348999] [G loss: 0.692994236946106]\n",
      "[Epoch 51/1001] [Batch 58/372] [D loss: 0.6925936937332153] [G loss: 0.6820460557937622]\n",
      "[Epoch 51/1001] [Batch 59/372] [D loss: 0.6933344602584839] [G loss: 0.6933075189590454]\n",
      "[Epoch 51/1001] [Batch 60/372] [D loss: 0.6925241947174072] [G loss: 0.6786710619926453]\n",
      "[Epoch 51/1001] [Batch 61/372] [D loss: 0.6936669945716858] [G loss: 0.7082475423812866]\n",
      "[Epoch 51/1001] [Batch 62/372] [D loss: 0.6917101144790649] [G loss: 0.6569444537162781]\n",
      "[Epoch 51/1001] [Batch 63/372] [D loss: 0.6928869485855103] [G loss: 0.731203556060791]\n",
      "[Epoch 51/1001] [Batch 64/372] [D loss: 0.6934153437614441] [G loss: 0.6461367607116699]\n",
      "[Epoch 51/1001] [Batch 65/372] [D loss: 0.6943724155426025] [G loss: 0.7371175289154053]\n",
      "[Epoch 51/1001] [Batch 66/372] [D loss: 0.6931823492050171] [G loss: 0.6402502059936523]\n",
      "[Epoch 51/1001] [Batch 67/372] [D loss: 0.6942850351333618] [G loss: 0.74483722448349]\n",
      "[Epoch 51/1001] [Batch 68/372] [D loss: 0.6937830448150635] [G loss: 0.6362002491950989]\n",
      "[Epoch 51/1001] [Batch 69/372] [D loss: 0.693946123123169] [G loss: 0.7470581531524658]\n",
      "[Epoch 51/1001] [Batch 70/372] [D loss: 0.6942815184593201] [G loss: 0.629213809967041]\n",
      "[Epoch 51/1001] [Batch 71/372] [D loss: 0.6947795152664185] [G loss: 0.760631799697876]\n",
      "[Epoch 51/1001] [Batch 72/372] [D loss: 0.6951056122779846] [G loss: 0.6196944117546082]\n",
      "[Epoch 51/1001] [Batch 73/372] [D loss: 0.6962149143218994] [G loss: 0.7650962471961975]\n",
      "[Epoch 51/1001] [Batch 74/372] [D loss: 0.6954987645149231] [G loss: 0.6150447726249695]\n",
      "[Epoch 51/1001] [Batch 75/372] [D loss: 0.6950030326843262] [G loss: 0.7712084650993347]\n",
      "[Epoch 51/1001] [Batch 76/372] [D loss: 0.6955494284629822] [G loss: 0.6155497431755066]\n",
      "[Epoch 51/1001] [Batch 77/372] [D loss: 0.6954347491264343] [G loss: 0.7658064365386963]\n",
      "[Epoch 51/1001] [Batch 78/372] [D loss: 0.6958210468292236] [G loss: 0.6167731285095215]\n",
      "[Epoch 51/1001] [Batch 79/372] [D loss: 0.6955162882804871] [G loss: 0.7677097916603088]\n",
      "[Epoch 51/1001] [Batch 80/372] [D loss: 0.694536566734314] [G loss: 0.6198757886886597]\n",
      "[Epoch 51/1001] [Batch 81/372] [D loss: 0.6962049603462219] [G loss: 0.7615875601768494]\n",
      "[Epoch 51/1001] [Batch 82/372] [D loss: 0.6951055526733398] [G loss: 0.6260358095169067]\n",
      "[Epoch 51/1001] [Batch 83/372] [D loss: 0.6942344307899475] [G loss: 0.7435222864151001]\n",
      "[Epoch 51/1001] [Batch 84/372] [D loss: 0.6937412023544312] [G loss: 0.6424187421798706]\n",
      "[Epoch 51/1001] [Batch 85/372] [D loss: 0.6954727172851562] [G loss: 0.7373422384262085]\n",
      "[Epoch 51/1001] [Batch 86/372] [D loss: 0.6935659646987915] [G loss: 0.6445769667625427]\n",
      "[Epoch 51/1001] [Batch 87/372] [D loss: 0.6944277286529541] [G loss: 0.7275556325912476]\n",
      "[Epoch 51/1001] [Batch 88/372] [D loss: 0.6935406923294067] [G loss: 0.6578730940818787]\n",
      "[Epoch 51/1001] [Batch 89/372] [D loss: 0.6919193267822266] [G loss: 0.7149364948272705]\n",
      "[Epoch 51/1001] [Batch 90/372] [D loss: 0.6932828426361084] [G loss: 0.6713200211524963]\n",
      "[Epoch 51/1001] [Batch 91/372] [D loss: 0.6928488612174988] [G loss: 0.7061136364936829]\n",
      "[Epoch 51/1001] [Batch 92/372] [D loss: 0.6933307647705078] [G loss: 0.6693517565727234]\n",
      "[Epoch 51/1001] [Batch 93/372] [D loss: 0.6934690475463867] [G loss: 0.7074751257896423]\n",
      "[Epoch 51/1001] [Batch 94/372] [D loss: 0.6925134062767029] [G loss: 0.670818030834198]\n",
      "[Epoch 51/1001] [Batch 95/372] [D loss: 0.6917855739593506] [G loss: 0.706998348236084]\n",
      "[Epoch 51/1001] [Batch 96/372] [D loss: 0.6928427219390869] [G loss: 0.6713292598724365]\n",
      "[Epoch 51/1001] [Batch 97/372] [D loss: 0.6926669478416443] [G loss: 0.7112607955932617]\n",
      "[Epoch 51/1001] [Batch 98/372] [D loss: 0.6936554908752441] [G loss: 0.6602824926376343]\n",
      "[Epoch 51/1001] [Batch 99/372] [D loss: 0.6935228705406189] [G loss: 0.7217776775360107]\n",
      "[Epoch 51/1001] [Batch 100/372] [D loss: 0.6930780410766602] [G loss: 0.6547524929046631]\n",
      "[Epoch 51/1001] [Batch 101/372] [D loss: 0.6927808523178101] [G loss: 0.7239111661911011]\n",
      "[Epoch 51/1001] [Batch 102/372] [D loss: 0.6945144534111023] [G loss: 0.652370810508728]\n",
      "[Epoch 51/1001] [Batch 103/372] [D loss: 0.6923282146453857] [G loss: 0.7258140444755554]\n",
      "[Epoch 51/1001] [Batch 104/372] [D loss: 0.6932079792022705] [G loss: 0.6471241116523743]\n",
      "[Epoch 51/1001] [Batch 105/372] [D loss: 0.6936478614807129] [G loss: 0.7351728677749634]\n",
      "[Epoch 51/1001] [Batch 106/372] [D loss: 0.6947683095932007] [G loss: 0.6429493427276611]\n",
      "[Epoch 51/1001] [Batch 107/372] [D loss: 0.6938810348510742] [G loss: 0.7391055226325989]\n",
      "[Epoch 51/1001] [Batch 108/372] [D loss: 0.6936607360839844] [G loss: 0.6364559531211853]\n",
      "[Epoch 51/1001] [Batch 109/372] [D loss: 0.6957610249519348] [G loss: 0.7454177737236023]\n",
      "[Epoch 51/1001] [Batch 110/372] [D loss: 0.6948482990264893] [G loss: 0.6203354597091675]\n",
      "[Epoch 51/1001] [Batch 111/372] [D loss: 0.6969424486160278] [G loss: 0.7825106382369995]\n",
      "[Epoch 51/1001] [Batch 112/372] [D loss: 0.6971514225006104] [G loss: 0.5833768248558044]\n",
      "[Epoch 51/1001] [Batch 113/372] [D loss: 0.6998202800750732] [G loss: 0.8240229487419128]\n",
      "[Epoch 51/1001] [Batch 114/372] [D loss: 0.7013136744499207] [G loss: 0.5599372386932373]\n",
      "[Epoch 51/1001] [Batch 115/372] [D loss: 0.7032356262207031] [G loss: 0.8392513990402222]\n",
      "[Epoch 51/1001] [Batch 116/372] [D loss: 0.7020350098609924] [G loss: 0.5750182271003723]\n",
      "[Epoch 51/1001] [Batch 117/372] [D loss: 0.7008274793624878] [G loss: 0.7876023650169373]\n",
      "[Epoch 51/1001] [Batch 118/372] [D loss: 0.6974338293075562] [G loss: 0.6236824989318848]\n",
      "[Epoch 51/1001] [Batch 119/372] [D loss: 0.6952551007270813] [G loss: 0.7346614003181458]\n",
      "[Epoch 51/1001] [Batch 120/372] [D loss: 0.6932807564735413] [G loss: 0.6645189523696899]\n",
      "[Epoch 51/1001] [Batch 121/372] [D loss: 0.6933271884918213] [G loss: 0.7011681795120239]\n",
      "[Epoch 51/1001] [Batch 122/372] [D loss: 0.6931780576705933] [G loss: 0.6915119886398315]\n",
      "[Epoch 51/1001] [Batch 123/372] [D loss: 0.6931976079940796] [G loss: 0.6811000108718872]\n",
      "[Epoch 51/1001] [Batch 124/372] [D loss: 0.6926577091217041] [G loss: 0.702974259853363]\n",
      "[Epoch 51/1001] [Batch 125/372] [D loss: 0.6925891637802124] [G loss: 0.669556736946106]\n",
      "[Epoch 51/1001] [Batch 126/372] [D loss: 0.694202184677124] [G loss: 0.7131752967834473]\n",
      "[Epoch 51/1001] [Batch 127/372] [D loss: 0.6935570240020752] [G loss: 0.6678330302238464]\n",
      "[Epoch 51/1001] [Batch 128/372] [D loss: 0.6930153369903564] [G loss: 0.700722336769104]\n",
      "[Epoch 51/1001] [Batch 129/372] [D loss: 0.693415105342865] [G loss: 0.6815311908721924]\n",
      "[Epoch 51/1001] [Batch 130/372] [D loss: 0.692175030708313] [G loss: 0.6965142488479614]\n",
      "[Epoch 51/1001] [Batch 131/372] [D loss: 0.6933422088623047] [G loss: 0.6812275052070618]\n",
      "[Epoch 51/1001] [Batch 132/372] [D loss: 0.6924623250961304] [G loss: 0.6953012943267822]\n",
      "[Epoch 51/1001] [Batch 133/372] [D loss: 0.6926411390304565] [G loss: 0.6792360544204712]\n",
      "[Epoch 51/1001] [Batch 134/372] [D loss: 0.6932351589202881] [G loss: 0.6961777210235596]\n",
      "[Epoch 51/1001] [Batch 135/372] [D loss: 0.6938151717185974] [G loss: 0.6958383321762085]\n",
      "[Epoch 51/1001] [Batch 136/372] [D loss: 0.6919927597045898] [G loss: 0.6743104457855225]\n",
      "[Epoch 51/1001] [Batch 137/372] [D loss: 0.6931890249252319] [G loss: 0.7106242775917053]\n",
      "[Epoch 51/1001] [Batch 138/372] [D loss: 0.6930975914001465] [G loss: 0.6573606729507446]\n",
      "[Epoch 51/1001] [Batch 139/372] [D loss: 0.6926091909408569] [G loss: 0.7261354923248291]\n",
      "[Epoch 51/1001] [Batch 140/372] [D loss: 0.6943960189819336] [G loss: 0.6538229584693909]\n",
      "[Epoch 51/1001] [Batch 141/372] [D loss: 0.6932054758071899] [G loss: 0.7257493734359741]\n",
      "[Epoch 51/1001] [Batch 142/372] [D loss: 0.6942318677902222] [G loss: 0.656169056892395]\n",
      "[Epoch 51/1001] [Batch 143/372] [D loss: 0.6924743056297302] [G loss: 0.7199298739433289]\n",
      "[Epoch 51/1001] [Batch 144/372] [D loss: 0.6938892602920532] [G loss: 0.6623536348342896]\n",
      "[Epoch 51/1001] [Batch 145/372] [D loss: 0.693048894405365] [G loss: 0.7111635208129883]\n",
      "[Epoch 51/1001] [Batch 146/372] [D loss: 0.6933057308197021] [G loss: 0.666869580745697]\n",
      "[Epoch 51/1001] [Batch 147/372] [D loss: 0.6922847032546997] [G loss: 0.7140922546386719]\n",
      "[Epoch 51/1001] [Batch 148/372] [D loss: 0.6925461292266846] [G loss: 0.6631374955177307]\n",
      "[Epoch 51/1001] [Batch 149/372] [D loss: 0.6926912069320679] [G loss: 0.7179603576660156]\n",
      "[Epoch 51/1001] [Batch 150/372] [D loss: 0.6940152645111084] [G loss: 0.654037356376648]\n",
      "[Epoch 51/1001] [Batch 151/372] [D loss: 0.6946457624435425] [G loss: 0.7279274463653564]\n",
      "[Epoch 51/1001] [Batch 152/372] [D loss: 0.6938318014144897] [G loss: 0.6486855149269104]\n",
      "[Epoch 51/1001] [Batch 153/372] [D loss: 0.6933700442314148] [G loss: 0.7302098274230957]\n",
      "[Epoch 51/1001] [Batch 154/372] [D loss: 0.6934874653816223] [G loss: 0.6402509808540344]\n",
      "[Epoch 51/1001] [Batch 155/372] [D loss: 0.6942031383514404] [G loss: 0.7451915144920349]\n",
      "[Epoch 51/1001] [Batch 156/372] [D loss: 0.6943256855010986] [G loss: 0.624619722366333]\n",
      "[Epoch 51/1001] [Batch 157/372] [D loss: 0.6968492865562439] [G loss: 0.7679158449172974]\n",
      "[Epoch 51/1001] [Batch 158/372] [D loss: 0.6961327791213989] [G loss: 0.6083828806877136]\n",
      "[Epoch 51/1001] [Batch 159/372] [D loss: 0.696129560470581] [G loss: 0.780501127243042]\n",
      "[Epoch 51/1001] [Batch 160/372] [D loss: 0.6964131593704224] [G loss: 0.6077934503555298]\n",
      "[Epoch 51/1001] [Batch 161/372] [D loss: 0.6959426999092102] [G loss: 0.7702967524528503]\n",
      "[Epoch 51/1001] [Batch 162/372] [D loss: 0.6954494714736938] [G loss: 0.6182647347450256]\n",
      "[Epoch 51/1001] [Batch 163/372] [D loss: 0.6960983276367188] [G loss: 0.7602956295013428]\n",
      "[Epoch 51/1001] [Batch 164/372] [D loss: 0.6950323581695557] [G loss: 0.6237340569496155]\n",
      "[Epoch 51/1001] [Batch 165/372] [D loss: 0.6954835653305054] [G loss: 0.7540239691734314]\n",
      "[Epoch 51/1001] [Batch 166/372] [D loss: 0.6953177452087402] [G loss: 0.6315101981163025]\n",
      "[Epoch 51/1001] [Batch 167/372] [D loss: 0.6923688054084778] [G loss: 0.7461134791374207]\n",
      "[Epoch 51/1001] [Batch 168/372] [D loss: 0.6941146850585938] [G loss: 0.6365451812744141]\n",
      "[Epoch 51/1001] [Batch 169/372] [D loss: 0.6948506832122803] [G loss: 0.7388267517089844]\n",
      "[Epoch 51/1001] [Batch 170/372] [D loss: 0.6916704177856445] [G loss: 0.643161416053772]\n",
      "[Epoch 51/1001] [Batch 171/372] [D loss: 0.6948360800743103] [G loss: 0.7334027886390686]\n",
      "[Epoch 51/1001] [Batch 172/372] [D loss: 0.6932032108306885] [G loss: 0.6463596820831299]\n",
      "[Epoch 51/1001] [Batch 173/372] [D loss: 0.6959165334701538] [G loss: 0.7285584211349487]\n",
      "[Epoch 51/1001] [Batch 174/372] [D loss: 0.6952945590019226] [G loss: 0.6403459906578064]\n",
      "[Epoch 51/1001] [Batch 175/372] [D loss: 0.6938287019729614] [G loss: 0.740838885307312]\n",
      "[Epoch 51/1001] [Batch 176/372] [D loss: 0.6929588317871094] [G loss: 0.6338211894035339]\n",
      "[Epoch 51/1001] [Batch 177/372] [D loss: 0.6945841312408447] [G loss: 0.7486791610717773]\n",
      "[Epoch 51/1001] [Batch 178/372] [D loss: 0.695597767829895] [G loss: 0.6300325989723206]\n",
      "[Epoch 51/1001] [Batch 179/372] [D loss: 0.6951088905334473] [G loss: 0.748091995716095]\n",
      "[Epoch 51/1001] [Batch 180/372] [D loss: 0.6941267251968384] [G loss: 0.6348421573638916]\n",
      "[Epoch 51/1001] [Batch 181/372] [D loss: 0.6950796246528625] [G loss: 0.7425884008407593]\n",
      "[Epoch 51/1001] [Batch 182/372] [D loss: 0.6941301226615906] [G loss: 0.6416957378387451]\n",
      "[Epoch 51/1001] [Batch 183/372] [D loss: 0.6935848593711853] [G loss: 0.7362678647041321]\n",
      "[Epoch 51/1001] [Batch 184/372] [D loss: 0.6931352615356445] [G loss: 0.6442282199859619]\n",
      "[Epoch 51/1001] [Batch 185/372] [D loss: 0.6939537525177002] [G loss: 0.7317377924919128]\n",
      "[Epoch 51/1001] [Batch 186/372] [D loss: 0.6924059391021729] [G loss: 0.6490345597267151]\n",
      "[Epoch 51/1001] [Batch 187/372] [D loss: 0.6929184198379517] [G loss: 0.7271362543106079]\n",
      "[Epoch 51/1001] [Batch 188/372] [D loss: 0.6934342980384827] [G loss: 0.6497204899787903]\n",
      "[Epoch 51/1001] [Batch 189/372] [D loss: 0.692716658115387] [G loss: 0.7313060760498047]\n",
      "[Epoch 51/1001] [Batch 190/372] [D loss: 0.6938346028327942] [G loss: 0.6427773237228394]\n",
      "[Epoch 51/1001] [Batch 191/372] [D loss: 0.6937849521636963] [G loss: 0.737896203994751]\n",
      "[Epoch 51/1001] [Batch 192/372] [D loss: 0.6943799257278442] [G loss: 0.6337693333625793]\n",
      "[Epoch 51/1001] [Batch 193/372] [D loss: 0.6942458152770996] [G loss: 0.7530325651168823]\n",
      "[Epoch 51/1001] [Batch 194/372] [D loss: 0.6949754953384399] [G loss: 0.618523359298706]\n",
      "[Epoch 51/1001] [Batch 195/372] [D loss: 0.6950326561927795] [G loss: 0.7795867323875427]\n",
      "[Epoch 51/1001] [Batch 196/372] [D loss: 0.6967831254005432] [G loss: 0.5959353446960449]\n",
      "[Epoch 51/1001] [Batch 197/372] [D loss: 0.6971802115440369] [G loss: 0.801878809928894]\n",
      "[Epoch 51/1001] [Batch 198/372] [D loss: 0.6978230476379395] [G loss: 0.5869759321212769]\n",
      "[Epoch 51/1001] [Batch 199/372] [D loss: 0.6981790065765381] [G loss: 0.7956362962722778]\n",
      "[Epoch 51/1001] [Batch 200/372] [D loss: 0.6981359720230103] [G loss: 0.6016290783882141]\n",
      "[Epoch 51/1001] [Batch 201/372] [D loss: 0.6968085765838623] [G loss: 0.7746512293815613]\n",
      "[Epoch 51/1001] [Batch 202/372] [D loss: 0.6965134739875793] [G loss: 0.6211376786231995]\n",
      "[Epoch 51/1001] [Batch 203/372] [D loss: 0.695820152759552] [G loss: 0.7431070804595947]\n",
      "[Epoch 51/1001] [Batch 204/372] [D loss: 0.6938793659210205] [G loss: 0.6527208685874939]\n",
      "[Epoch 51/1001] [Batch 205/372] [D loss: 0.6937004327774048] [G loss: 0.7144596576690674]\n",
      "[Epoch 51/1001] [Batch 206/372] [D loss: 0.6935932040214539] [G loss: 0.6741220355033875]\n",
      "[Epoch 51/1001] [Batch 207/372] [D loss: 0.6923818588256836] [G loss: 0.6934918165206909]\n",
      "[Epoch 51/1001] [Batch 208/372] [D loss: 0.692302942276001] [G loss: 0.6908496618270874]\n",
      "[Epoch 51/1001] [Batch 209/372] [D loss: 0.6926695108413696] [G loss: 0.677941083908081]\n",
      "[Epoch 51/1001] [Batch 210/372] [D loss: 0.693571925163269] [G loss: 0.698846697807312]\n",
      "[Epoch 51/1001] [Batch 211/372] [D loss: 0.6938604712486267] [G loss: 0.676933765411377]\n",
      "[Epoch 51/1001] [Batch 212/372] [D loss: 0.6933772563934326] [G loss: 0.6984580159187317]\n",
      "[Epoch 51/1001] [Batch 213/372] [D loss: 0.6921228170394897] [G loss: 0.6828656196594238]\n",
      "[Epoch 51/1001] [Batch 214/372] [D loss: 0.6921783685684204] [G loss: 0.6905456781387329]\n",
      "[Epoch 51/1001] [Batch 215/372] [D loss: 0.69334876537323] [G loss: 0.690593957901001]\n",
      "[Epoch 51/1001] [Batch 216/372] [D loss: 0.6914044618606567] [G loss: 0.6849613785743713]\n",
      "[Epoch 51/1001] [Batch 217/372] [D loss: 0.6922909021377563] [G loss: 0.6947232484817505]\n",
      "[Epoch 51/1001] [Batch 218/372] [D loss: 0.6910390853881836] [G loss: 0.6796612739562988]\n",
      "[Epoch 51/1001] [Batch 219/372] [D loss: 0.6943325996398926] [G loss: 0.7025665640830994]\n",
      "[Epoch 51/1001] [Batch 220/372] [D loss: 0.6924439668655396] [G loss: 0.667336106300354]\n",
      "[Epoch 51/1001] [Batch 221/372] [D loss: 0.6935418844223022] [G loss: 0.7105110883712769]\n",
      "[Epoch 51/1001] [Batch 222/372] [D loss: 0.6934093236923218] [G loss: 0.6633903980255127]\n",
      "[Epoch 51/1001] [Batch 223/372] [D loss: 0.6936778426170349] [G loss: 0.7146329879760742]\n",
      "[Epoch 51/1001] [Batch 224/372] [D loss: 0.694068968296051] [G loss: 0.663608729839325]\n",
      "[Epoch 51/1001] [Batch 225/372] [D loss: 0.6914712190628052] [G loss: 0.7058814167976379]\n",
      "[Epoch 51/1001] [Batch 226/372] [D loss: 0.6934925317764282] [G loss: 0.6702715158462524]\n",
      "[Epoch 51/1001] [Batch 227/372] [D loss: 0.6948637366294861] [G loss: 0.7153429388999939]\n",
      "[Epoch 51/1001] [Batch 228/372] [D loss: 0.6938958168029785] [G loss: 0.6589730381965637]\n",
      "[Epoch 51/1001] [Batch 229/372] [D loss: 0.6932046413421631] [G loss: 0.7196133136749268]\n",
      "[Epoch 51/1001] [Batch 230/372] [D loss: 0.6943227052688599] [G loss: 0.6532975435256958]\n",
      "[Epoch 51/1001] [Batch 231/372] [D loss: 0.6937446594238281] [G loss: 0.7340966463088989]\n",
      "[Epoch 51/1001] [Batch 232/372] [D loss: 0.6938816905021667] [G loss: 0.6379740238189697]\n",
      "[Epoch 51/1001] [Batch 233/372] [D loss: 0.694670557975769] [G loss: 0.7501847743988037]\n",
      "[Epoch 51/1001] [Batch 234/372] [D loss: 0.6948732137680054] [G loss: 0.624954342842102]\n",
      "[Epoch 51/1001] [Batch 235/372] [D loss: 0.6945522427558899] [G loss: 0.7642691135406494]\n",
      "[Epoch 51/1001] [Batch 236/372] [D loss: 0.694770097732544] [G loss: 0.6117185354232788]\n",
      "[Epoch 51/1001] [Batch 237/372] [D loss: 0.6947935223579407] [G loss: 0.7777143120765686]\n",
      "[Epoch 51/1001] [Batch 238/372] [D loss: 0.695439875125885] [G loss: 0.6034500598907471]\n",
      "[Epoch 51/1001] [Batch 239/372] [D loss: 0.6981834173202515] [G loss: 0.7937460541725159]\n",
      "[Epoch 51/1001] [Batch 240/372] [D loss: 0.6990140676498413] [G loss: 0.5895341634750366]\n",
      "[Epoch 51/1001] [Batch 241/372] [D loss: 0.6975657939910889] [G loss: 0.795198917388916]\n",
      "[Epoch 51/1001] [Batch 242/372] [D loss: 0.6970111131668091] [G loss: 0.5987682938575745]\n",
      "[Epoch 51/1001] [Batch 243/372] [D loss: 0.6978579759597778] [G loss: 0.7772562503814697]\n",
      "[Epoch 51/1001] [Batch 244/372] [D loss: 0.6983882188796997] [G loss: 0.6176304817199707]\n",
      "[Epoch 51/1001] [Batch 245/372] [D loss: 0.6953397989273071] [G loss: 0.7508312463760376]\n",
      "[Epoch 51/1001] [Batch 246/372] [D loss: 0.6950327157974243] [G loss: 0.6422879099845886]\n",
      "[Epoch 51/1001] [Batch 247/372] [D loss: 0.6945779919624329] [G loss: 0.724128246307373]\n",
      "[Epoch 51/1001] [Batch 248/372] [D loss: 0.6934504508972168] [G loss: 0.6598035097122192]\n",
      "[Epoch 51/1001] [Batch 249/372] [D loss: 0.6936490535736084] [G loss: 0.7106475234031677]\n",
      "[Epoch 51/1001] [Batch 250/372] [D loss: 0.6914774179458618] [G loss: 0.6684740781784058]\n",
      "[Epoch 51/1001] [Batch 251/372] [D loss: 0.6934299468994141] [G loss: 0.711233377456665]\n",
      "[Epoch 51/1001] [Batch 252/372] [D loss: 0.6941460967063904] [G loss: 0.6682936549186707]\n",
      "[Epoch 51/1001] [Batch 253/372] [D loss: 0.6933490037918091] [G loss: 0.7050966024398804]\n",
      "[Epoch 51/1001] [Batch 254/372] [D loss: 0.6934747695922852] [G loss: 0.6738401651382446]\n",
      "[Epoch 51/1001] [Batch 255/372] [D loss: 0.6931713819503784] [G loss: 0.706132173538208]\n",
      "[Epoch 51/1001] [Batch 256/372] [D loss: 0.692884087562561] [G loss: 0.6654961109161377]\n",
      "[Epoch 51/1001] [Batch 257/372] [D loss: 0.6925334930419922] [G loss: 0.7128008604049683]\n",
      "[Epoch 51/1001] [Batch 258/372] [D loss: 0.6930134892463684] [G loss: 0.6702194809913635]\n",
      "[Epoch 51/1001] [Batch 259/372] [D loss: 0.693854570388794] [G loss: 0.700707197189331]\n",
      "[Epoch 51/1001] [Batch 260/372] [D loss: 0.6922997236251831] [G loss: 0.6848901510238647]\n",
      "[Epoch 51/1001] [Batch 261/372] [D loss: 0.692639946937561] [G loss: 0.686495304107666]\n",
      "[Epoch 51/1001] [Batch 262/372] [D loss: 0.6919094920158386] [G loss: 0.6948355436325073]\n",
      "[Epoch 51/1001] [Batch 263/372] [D loss: 0.693128228187561] [G loss: 0.6793855428695679]\n",
      "[Epoch 51/1001] [Batch 264/372] [D loss: 0.6927515268325806] [G loss: 0.6953073740005493]\n",
      "[Epoch 51/1001] [Batch 265/372] [D loss: 0.6933196187019348] [G loss: 0.6864711046218872]\n",
      "[Epoch 51/1001] [Batch 266/372] [D loss: 0.6917331218719482] [G loss: 0.6893945336341858]\n",
      "[Epoch 51/1001] [Batch 267/372] [D loss: 0.6927667856216431] [G loss: 0.6875667572021484]\n",
      "[Epoch 51/1001] [Batch 268/372] [D loss: 0.6937965154647827] [G loss: 0.6872271299362183]\n",
      "[Epoch 51/1001] [Batch 269/372] [D loss: 0.6932048797607422] [G loss: 0.6909071207046509]\n",
      "[Epoch 51/1001] [Batch 270/372] [D loss: 0.6920349597930908] [G loss: 0.6842725276947021]\n",
      "[Epoch 51/1001] [Batch 271/372] [D loss: 0.6935908794403076] [G loss: 0.6946872472763062]\n",
      "[Epoch 51/1001] [Batch 272/372] [D loss: 0.6923781633377075] [G loss: 0.6787434816360474]\n",
      "[Epoch 51/1001] [Batch 273/372] [D loss: 0.6932944655418396] [G loss: 0.6996990442276001]\n",
      "[Epoch 51/1001] [Batch 274/372] [D loss: 0.692372739315033] [G loss: 0.6768262386322021]\n",
      "[Epoch 51/1001] [Batch 275/372] [D loss: 0.6922526359558105] [G loss: 0.7088893055915833]\n",
      "[Epoch 51/1001] [Batch 276/372] [D loss: 0.6941587924957275] [G loss: 0.662414014339447]\n",
      "[Epoch 51/1001] [Batch 277/372] [D loss: 0.6934317350387573] [G loss: 0.7163704633712769]\n",
      "[Epoch 51/1001] [Batch 278/372] [D loss: 0.6927492022514343] [G loss: 0.6573731303215027]\n",
      "[Epoch 51/1001] [Batch 279/372] [D loss: 0.6942629814147949] [G loss: 0.7258985042572021]\n",
      "[Epoch 51/1001] [Batch 280/372] [D loss: 0.6946426630020142] [G loss: 0.6478041410446167]\n",
      "[Epoch 51/1001] [Batch 281/372] [D loss: 0.6938003301620483] [G loss: 0.7349253296852112]\n",
      "[Epoch 51/1001] [Batch 282/372] [D loss: 0.6937320232391357] [G loss: 0.6381051540374756]\n",
      "[Epoch 51/1001] [Batch 283/372] [D loss: 0.6940003633499146] [G loss: 0.7525919675827026]\n",
      "[Epoch 51/1001] [Batch 284/372] [D loss: 0.6951693296432495] [G loss: 0.6159842014312744]\n",
      "[Epoch 51/1001] [Batch 285/372] [D loss: 0.6953434348106384] [G loss: 0.7841438055038452]\n",
      "[Epoch 51/1001] [Batch 286/372] [D loss: 0.6968704462051392] [G loss: 0.5863441228866577]\n",
      "[Epoch 51/1001] [Batch 287/372] [D loss: 0.6989098191261292] [G loss: 0.8236756324768066]\n",
      "[Epoch 51/1001] [Batch 288/372] [D loss: 0.7009199857711792] [G loss: 0.5620821118354797]\n",
      "[Epoch 51/1001] [Batch 289/372] [D loss: 0.7025855183601379] [G loss: 0.840835452079773]\n",
      "[Epoch 51/1001] [Batch 290/372] [D loss: 0.7027937769889832] [G loss: 0.5622290372848511]\n",
      "[Epoch 51/1001] [Batch 291/372] [D loss: 0.7019141912460327] [G loss: 0.8213602304458618]\n",
      "[Epoch 51/1001] [Batch 292/372] [D loss: 0.6998254060745239] [G loss: 0.5910415649414062]\n",
      "[Epoch 51/1001] [Batch 293/372] [D loss: 0.6980038285255432] [G loss: 0.774052619934082]\n",
      "[Epoch 51/1001] [Batch 294/372] [D loss: 0.6961817741394043] [G loss: 0.6329469680786133]\n",
      "[Epoch 51/1001] [Batch 295/372] [D loss: 0.6942142248153687] [G loss: 0.7238657474517822]\n",
      "[Epoch 51/1001] [Batch 296/372] [D loss: 0.6926429271697998] [G loss: 0.669594407081604]\n",
      "[Epoch 51/1001] [Batch 297/372] [D loss: 0.6922425627708435] [G loss: 0.702266275882721]\n",
      "[Epoch 51/1001] [Batch 298/372] [D loss: 0.6937010288238525] [G loss: 0.6791645884513855]\n",
      "[Epoch 51/1001] [Batch 299/372] [D loss: 0.6931189298629761] [G loss: 0.6950674057006836]\n",
      "[Epoch 51/1001] [Batch 300/372] [D loss: 0.6923563480377197] [G loss: 0.6867156028747559]\n",
      "[Epoch 51/1001] [Batch 301/372] [D loss: 0.6932729482650757] [G loss: 0.6884854435920715]\n",
      "[Epoch 51/1001] [Batch 302/372] [D loss: 0.6927651166915894] [G loss: 0.6872557997703552]\n",
      "[Epoch 51/1001] [Batch 303/372] [D loss: 0.692628026008606] [G loss: 0.6936140656471252]\n",
      "[Epoch 51/1001] [Batch 304/372] [D loss: 0.6935513615608215] [G loss: 0.682911217212677]\n",
      "[Epoch 51/1001] [Batch 305/372] [D loss: 0.6941221952438354] [G loss: 0.6969386339187622]\n",
      "[Epoch 51/1001] [Batch 306/372] [D loss: 0.6932939887046814] [G loss: 0.6737741231918335]\n",
      "[Epoch 51/1001] [Batch 307/372] [D loss: 0.6931288242340088] [G loss: 0.7052786946296692]\n",
      "[Epoch 51/1001] [Batch 308/372] [D loss: 0.6922638416290283] [G loss: 0.667334794998169]\n",
      "[Epoch 51/1001] [Batch 309/372] [D loss: 0.6928554773330688] [G loss: 0.7165282368659973]\n",
      "[Epoch 51/1001] [Batch 310/372] [D loss: 0.6912246942520142] [G loss: 0.6604761481285095]\n",
      "[Epoch 51/1001] [Batch 311/372] [D loss: 0.6913003921508789] [G loss: 0.7163122892379761]\n",
      "[Epoch 51/1001] [Batch 312/372] [D loss: 0.6943950057029724] [G loss: 0.653270959854126]\n",
      "[Epoch 51/1001] [Batch 313/372] [D loss: 0.6947847604751587] [G loss: 0.7288141846656799]\n",
      "[Epoch 51/1001] [Batch 314/372] [D loss: 0.6937082409858704] [G loss: 0.6439412832260132]\n",
      "[Epoch 51/1001] [Batch 315/372] [D loss: 0.6947310566902161] [G loss: 0.7367900609970093]\n",
      "[Epoch 51/1001] [Batch 316/372] [D loss: 0.6930109262466431] [G loss: 0.6416485905647278]\n",
      "[Epoch 51/1001] [Batch 317/372] [D loss: 0.694832444190979] [G loss: 0.7357171773910522]\n",
      "[Epoch 51/1001] [Batch 318/372] [D loss: 0.6943610906600952] [G loss: 0.6390822529792786]\n",
      "[Epoch 51/1001] [Batch 319/372] [D loss: 0.6940115094184875] [G loss: 0.7484866380691528]\n",
      "[Epoch 51/1001] [Batch 320/372] [D loss: 0.6937841773033142] [G loss: 0.6278824806213379]\n",
      "[Epoch 51/1001] [Batch 321/372] [D loss: 0.6953861713409424] [G loss: 0.754772961139679]\n",
      "[Epoch 51/1001] [Batch 322/372] [D loss: 0.6960757970809937] [G loss: 0.6187449097633362]\n",
      "[Epoch 51/1001] [Batch 323/372] [D loss: 0.6954735517501831] [G loss: 0.7603708505630493]\n",
      "[Epoch 51/1001] [Batch 324/372] [D loss: 0.6943528652191162] [G loss: 0.6145169734954834]\n",
      "[Epoch 51/1001] [Batch 325/372] [D loss: 0.695611834526062] [G loss: 0.7736594080924988]\n",
      "[Epoch 51/1001] [Batch 326/372] [D loss: 0.6961197257041931] [G loss: 0.6051594614982605]\n",
      "[Epoch 51/1001] [Batch 327/372] [D loss: 0.6955817937850952] [G loss: 0.779723048210144]\n",
      "[Epoch 51/1001] [Batch 328/372] [D loss: 0.697231650352478] [G loss: 0.6049716472625732]\n",
      "[Epoch 51/1001] [Batch 329/372] [D loss: 0.6972228288650513] [G loss: 0.7747341394424438]\n",
      "[Epoch 51/1001] [Batch 330/372] [D loss: 0.6972732543945312] [G loss: 0.6165874004364014]\n",
      "[Epoch 51/1001] [Batch 331/372] [D loss: 0.6963095664978027] [G loss: 0.7560212016105652]\n",
      "[Epoch 51/1001] [Batch 332/372] [D loss: 0.6957681179046631] [G loss: 0.6363387107849121]\n",
      "[Epoch 51/1001] [Batch 333/372] [D loss: 0.6936875581741333] [G loss: 0.7288020849227905]\n",
      "[Epoch 51/1001] [Batch 334/372] [D loss: 0.6929293870925903] [G loss: 0.6628672480583191]\n",
      "[Epoch 51/1001] [Batch 335/372] [D loss: 0.6936067342758179] [G loss: 0.7098889946937561]\n",
      "[Epoch 51/1001] [Batch 336/372] [D loss: 0.6931450963020325] [G loss: 0.6674835681915283]\n",
      "[Epoch 51/1001] [Batch 337/372] [D loss: 0.694336473941803] [G loss: 0.716022789478302]\n",
      "[Epoch 51/1001] [Batch 338/372] [D loss: 0.6932120323181152] [G loss: 0.6583461761474609]\n",
      "[Epoch 51/1001] [Batch 339/372] [D loss: 0.6930978894233704] [G loss: 0.7224122285842896]\n",
      "[Epoch 51/1001] [Batch 340/372] [D loss: 0.6927139759063721] [G loss: 0.6569503545761108]\n",
      "[Epoch 51/1001] [Batch 341/372] [D loss: 0.6934360265731812] [G loss: 0.7209514379501343]\n",
      "[Epoch 51/1001] [Batch 342/372] [D loss: 0.6933507919311523] [G loss: 0.6570987105369568]\n",
      "[Epoch 51/1001] [Batch 343/372] [D loss: 0.6934228539466858] [G loss: 0.7211875319480896]\n",
      "[Epoch 51/1001] [Batch 344/372] [D loss: 0.693779706954956] [G loss: 0.658378005027771]\n",
      "[Epoch 51/1001] [Batch 345/372] [D loss: 0.6936179995536804] [G loss: 0.7195459604263306]\n",
      "[Epoch 51/1001] [Batch 346/372] [D loss: 0.693290114402771] [G loss: 0.6599924564361572]\n",
      "[Epoch 51/1001] [Batch 347/372] [D loss: 0.6943951845169067] [G loss: 0.7208129167556763]\n",
      "[Epoch 51/1001] [Batch 348/372] [D loss: 0.692612886428833] [G loss: 0.6593307256698608]\n",
      "[Epoch 51/1001] [Batch 349/372] [D loss: 0.6935359239578247] [G loss: 0.7175391912460327]\n",
      "[Epoch 51/1001] [Batch 350/372] [D loss: 0.6930187940597534] [G loss: 0.6620146036148071]\n",
      "[Epoch 51/1001] [Batch 351/372] [D loss: 0.6940712928771973] [G loss: 0.7161543965339661]\n",
      "[Epoch 51/1001] [Batch 352/372] [D loss: 0.6917319297790527] [G loss: 0.652755618095398]\n",
      "[Epoch 51/1001] [Batch 353/372] [D loss: 0.6930263638496399] [G loss: 0.7315393090248108]\n",
      "[Epoch 51/1001] [Batch 354/372] [D loss: 0.690608024597168] [G loss: 0.6314228177070618]\n",
      "[Epoch 51/1001] [Batch 355/372] [D loss: 0.696152925491333] [G loss: 0.7749975323677063]\n",
      "[Epoch 51/1001] [Batch 356/372] [D loss: 0.6983250379562378] [G loss: 0.5893629789352417]\n",
      "[Epoch 51/1001] [Batch 357/372] [D loss: 0.6998215913772583] [G loss: 0.8097755908966064]\n",
      "[Epoch 51/1001] [Batch 358/372] [D loss: 0.6991696953773499] [G loss: 0.5754768252372742]\n",
      "[Epoch 51/1001] [Batch 359/372] [D loss: 0.7001826763153076] [G loss: 0.8149327635765076]\n",
      "[Epoch 51/1001] [Batch 360/372] [D loss: 0.7000253200531006] [G loss: 0.5896821022033691]\n",
      "[Epoch 51/1001] [Batch 361/372] [D loss: 0.6970696449279785] [G loss: 0.7766306400299072]\n",
      "[Epoch 51/1001] [Batch 362/372] [D loss: 0.6959555745124817] [G loss: 0.6255472898483276]\n",
      "[Epoch 51/1001] [Batch 363/372] [D loss: 0.6940735578536987] [G loss: 0.740513801574707]\n",
      "[Epoch 51/1001] [Batch 364/372] [D loss: 0.6939579844474792] [G loss: 0.6465879678726196]\n",
      "[Epoch 51/1001] [Batch 365/372] [D loss: 0.6933677196502686] [G loss: 0.7193918824195862]\n",
      "[Epoch 51/1001] [Batch 366/372] [D loss: 0.6930480003356934] [G loss: 0.6686499118804932]\n",
      "[Epoch 51/1001] [Batch 367/372] [D loss: 0.6939170956611633] [G loss: 0.7065442204475403]\n",
      "[Epoch 51/1001] [Batch 368/372] [D loss: 0.6931372880935669] [G loss: 0.6718363165855408]\n",
      "[Epoch 51/1001] [Batch 369/372] [D loss: 0.6931379437446594] [G loss: 0.7022557258605957]\n",
      "[Epoch 51/1001] [Batch 370/372] [D loss: 0.692834734916687] [G loss: 0.6760794520378113]\n",
      "[Epoch 51/1001] [Batch 371/372] [D loss: 0.6946247220039368] [G loss: 0.6950626373291016]\n",
      "[Epoch 52/1001] [Batch 0/372] [D loss: 0.6930694580078125] [G loss: 0.6847442388534546]\n",
      "[Epoch 52/1001] [Batch 1/372] [D loss: 0.6930938959121704] [G loss: 0.6873522400856018]\n",
      "[Epoch 52/1001] [Batch 2/372] [D loss: 0.6923139095306396] [G loss: 0.6906743049621582]\n",
      "[Epoch 52/1001] [Batch 3/372] [D loss: 0.6916667222976685] [G loss: 0.6854496002197266]\n",
      "[Epoch 52/1001] [Batch 4/372] [D loss: 0.6918967962265015] [G loss: 0.6899875998497009]\n",
      "[Epoch 52/1001] [Batch 5/372] [D loss: 0.6924424767494202] [G loss: 0.6895971298217773]\n",
      "[Epoch 52/1001] [Batch 6/372] [D loss: 0.6937344074249268] [G loss: 0.6888552904129028]\n",
      "[Epoch 52/1001] [Batch 7/372] [D loss: 0.6921073198318481] [G loss: 0.6839995384216309]\n",
      "[Epoch 52/1001] [Batch 8/372] [D loss: 0.692562460899353] [G loss: 0.6987295150756836]\n",
      "[Epoch 52/1001] [Batch 9/372] [D loss: 0.6919024586677551] [G loss: 0.6714581847190857]\n",
      "[Epoch 52/1001] [Batch 10/372] [D loss: 0.6914966106414795] [G loss: 0.7103539109230042]\n",
      "[Epoch 52/1001] [Batch 11/372] [D loss: 0.6924525499343872] [G loss: 0.659275233745575]\n",
      "[Epoch 52/1001] [Batch 12/372] [D loss: 0.6939126253128052] [G loss: 0.7295299768447876]\n",
      "[Epoch 52/1001] [Batch 13/372] [D loss: 0.694974422454834] [G loss: 0.6357463598251343]\n",
      "[Epoch 52/1001] [Batch 14/372] [D loss: 0.6934905052185059] [G loss: 0.7542458176612854]\n",
      "[Epoch 52/1001] [Batch 15/372] [D loss: 0.6945458650588989] [G loss: 0.6194183826446533]\n",
      "[Epoch 52/1001] [Batch 16/372] [D loss: 0.6947958469390869] [G loss: 0.770531415939331]\n",
      "[Epoch 52/1001] [Batch 17/372] [D loss: 0.6957190632820129] [G loss: 0.6085910797119141]\n",
      "[Epoch 52/1001] [Batch 18/372] [D loss: 0.6953002214431763] [G loss: 0.7761482000350952]\n",
      "[Epoch 52/1001] [Batch 19/372] [D loss: 0.6947793364524841] [G loss: 0.6091951727867126]\n",
      "[Epoch 52/1001] [Batch 20/372] [D loss: 0.6957635879516602] [G loss: 0.7729355096817017]\n",
      "[Epoch 52/1001] [Batch 21/372] [D loss: 0.695814847946167] [G loss: 0.6107579469680786]\n",
      "[Epoch 52/1001] [Batch 22/372] [D loss: 0.6953046321868896] [G loss: 0.7728596925735474]\n",
      "[Epoch 52/1001] [Batch 23/372] [D loss: 0.6957904696464539] [G loss: 0.6110846400260925]\n",
      "[Epoch 52/1001] [Batch 24/372] [D loss: 0.6956795454025269] [G loss: 0.768466591835022]\n",
      "[Epoch 52/1001] [Batch 25/372] [D loss: 0.694516122341156] [G loss: 0.6145074367523193]\n",
      "[Epoch 52/1001] [Batch 26/372] [D loss: 0.6945951581001282] [G loss: 0.7662174701690674]\n",
      "[Epoch 52/1001] [Batch 27/372] [D loss: 0.6943941712379456] [G loss: 0.6215175986289978]\n",
      "[Epoch 52/1001] [Batch 28/372] [D loss: 0.6960833668708801] [G loss: 0.7521291375160217]\n",
      "[Epoch 52/1001] [Batch 29/372] [D loss: 0.696707546710968] [G loss: 0.6308472156524658]\n",
      "[Epoch 52/1001] [Batch 30/372] [D loss: 0.695025622844696] [G loss: 0.740357518196106]\n",
      "[Epoch 52/1001] [Batch 31/372] [D loss: 0.6945162415504456] [G loss: 0.6477874517440796]\n",
      "[Epoch 52/1001] [Batch 32/372] [D loss: 0.6927777528762817] [G loss: 0.7215288877487183]\n",
      "[Epoch 52/1001] [Batch 33/372] [D loss: 0.693279504776001] [G loss: 0.6646586656570435]\n",
      "[Epoch 52/1001] [Batch 34/372] [D loss: 0.69328773021698] [G loss: 0.7064775228500366]\n",
      "[Epoch 52/1001] [Batch 35/372] [D loss: 0.6933675408363342] [G loss: 0.6752128005027771]\n",
      "[Epoch 52/1001] [Batch 36/372] [D loss: 0.6912773847579956] [G loss: 0.6999106407165527]\n",
      "[Epoch 52/1001] [Batch 37/372] [D loss: 0.692000150680542] [G loss: 0.6736046671867371]\n",
      "[Epoch 52/1001] [Batch 38/372] [D loss: 0.6929616928100586] [G loss: 0.6997320652008057]\n",
      "[Epoch 52/1001] [Batch 39/372] [D loss: 0.6922429800033569] [G loss: 0.6763937473297119]\n",
      "[Epoch 52/1001] [Batch 40/372] [D loss: 0.6933788061141968] [G loss: 0.7037035226821899]\n",
      "[Epoch 52/1001] [Batch 41/372] [D loss: 0.6939696669578552] [G loss: 0.664361298084259]\n",
      "[Epoch 52/1001] [Batch 42/372] [D loss: 0.6925610899925232] [G loss: 0.7189403176307678]\n",
      "[Epoch 52/1001] [Batch 43/372] [D loss: 0.6926475167274475] [G loss: 0.6503183245658875]\n",
      "[Epoch 52/1001] [Batch 44/372] [D loss: 0.6935697197914124] [G loss: 0.7436023354530334]\n",
      "[Epoch 52/1001] [Batch 45/372] [D loss: 0.693792462348938] [G loss: 0.6265692114830017]\n",
      "[Epoch 52/1001] [Batch 46/372] [D loss: 0.6939911842346191] [G loss: 0.7581178545951843]\n",
      "[Epoch 52/1001] [Batch 47/372] [D loss: 0.6946575045585632] [G loss: 0.6216875910758972]\n",
      "[Epoch 52/1001] [Batch 48/372] [D loss: 0.6947519183158875] [G loss: 0.7572340965270996]\n",
      "[Epoch 52/1001] [Batch 49/372] [D loss: 0.6942698955535889] [G loss: 0.6250186562538147]\n",
      "[Epoch 52/1001] [Batch 50/372] [D loss: 0.6958272457122803] [G loss: 0.7570076584815979]\n",
      "[Epoch 52/1001] [Batch 51/372] [D loss: 0.6954512000083923] [G loss: 0.6234936714172363]\n",
      "[Epoch 52/1001] [Batch 52/372] [D loss: 0.694847822189331] [G loss: 0.752145528793335]\n",
      "[Epoch 52/1001] [Batch 53/372] [D loss: 0.6927787661552429] [G loss: 0.6360262632369995]\n",
      "[Epoch 52/1001] [Batch 54/372] [D loss: 0.6933783888816833] [G loss: 0.7403901815414429]\n",
      "[Epoch 52/1001] [Batch 55/372] [D loss: 0.6939697265625] [G loss: 0.6462855339050293]\n",
      "[Epoch 52/1001] [Batch 56/372] [D loss: 0.6920994520187378] [G loss: 0.7319057583808899]\n",
      "[Epoch 52/1001] [Batch 57/372] [D loss: 0.6937427520751953] [G loss: 0.6429126858711243]\n",
      "[Epoch 52/1001] [Batch 58/372] [D loss: 0.693118691444397] [G loss: 0.7368618845939636]\n",
      "[Epoch 52/1001] [Batch 59/372] [D loss: 0.6936406493186951] [G loss: 0.6340492963790894]\n",
      "[Epoch 52/1001] [Batch 60/372] [D loss: 0.6945682764053345] [G loss: 0.7552304863929749]\n",
      "[Epoch 52/1001] [Batch 61/372] [D loss: 0.6949876546859741] [G loss: 0.6122828722000122]\n",
      "[Epoch 52/1001] [Batch 62/372] [D loss: 0.696953535079956] [G loss: 0.7706711292266846]\n",
      "[Epoch 52/1001] [Batch 63/372] [D loss: 0.696123480796814] [G loss: 0.6104509830474854]\n",
      "[Epoch 52/1001] [Batch 64/372] [D loss: 0.6965717077255249] [G loss: 0.768641471862793]\n",
      "[Epoch 52/1001] [Batch 65/372] [D loss: 0.6944259405136108] [G loss: 0.6190159320831299]\n",
      "[Epoch 52/1001] [Batch 66/372] [D loss: 0.695822536945343] [G loss: 0.7544224858283997]\n",
      "[Epoch 52/1001] [Batch 67/372] [D loss: 0.694640040397644] [G loss: 0.6231021881103516]\n",
      "[Epoch 52/1001] [Batch 68/372] [D loss: 0.6947567462921143] [G loss: 0.7520272731781006]\n",
      "[Epoch 52/1001] [Batch 69/372] [D loss: 0.6944283246994019] [G loss: 0.6276006102561951]\n",
      "[Epoch 52/1001] [Batch 70/372] [D loss: 0.6942287683486938] [G loss: 0.7543107271194458]\n",
      "[Epoch 52/1001] [Batch 71/372] [D loss: 0.6955446004867554] [G loss: 0.6262978315353394]\n",
      "[Epoch 52/1001] [Batch 72/372] [D loss: 0.6942880153656006] [G loss: 0.7446736097335815]\n",
      "[Epoch 52/1001] [Batch 73/372] [D loss: 0.6927263736724854] [G loss: 0.6378980278968811]\n",
      "[Epoch 52/1001] [Batch 74/372] [D loss: 0.6925379037857056] [G loss: 0.7372773289680481]\n",
      "[Epoch 52/1001] [Batch 75/372] [D loss: 0.6937185525894165] [G loss: 0.6330951452255249]\n",
      "[Epoch 52/1001] [Batch 76/372] [D loss: 0.6943466067314148] [G loss: 0.7615514993667603]\n",
      "[Epoch 52/1001] [Batch 77/372] [D loss: 0.695093035697937] [G loss: 0.6104844212532043]\n",
      "[Epoch 52/1001] [Batch 78/372] [D loss: 0.6958855390548706] [G loss: 0.7735609412193298]\n",
      "[Epoch 52/1001] [Batch 79/372] [D loss: 0.6951706409454346] [G loss: 0.6124953031539917]\n",
      "[Epoch 52/1001] [Batch 80/372] [D loss: 0.6956169009208679] [G loss: 0.7639440298080444]\n",
      "[Epoch 52/1001] [Batch 81/372] [D loss: 0.6945534944534302] [G loss: 0.6202982664108276]\n",
      "[Epoch 52/1001] [Batch 82/372] [D loss: 0.6952094435691833] [G loss: 0.7569829821586609]\n",
      "[Epoch 52/1001] [Batch 83/372] [D loss: 0.6941823959350586] [G loss: 0.6179174780845642]\n",
      "[Epoch 52/1001] [Batch 84/372] [D loss: 0.6935927867889404] [G loss: 0.7588989734649658]\n",
      "[Epoch 52/1001] [Batch 85/372] [D loss: 0.6962040662765503] [G loss: 0.6159840822219849]\n",
      "[Epoch 52/1001] [Batch 86/372] [D loss: 0.6948824524879456] [G loss: 0.7725698947906494]\n",
      "[Epoch 52/1001] [Batch 87/372] [D loss: 0.6958655714988708] [G loss: 0.6043910384178162]\n",
      "[Epoch 52/1001] [Batch 88/372] [D loss: 0.6961462497711182] [G loss: 0.7741072773933411]\n",
      "[Epoch 52/1001] [Batch 89/372] [D loss: 0.6956229209899902] [G loss: 0.6205177903175354]\n",
      "[Epoch 52/1001] [Batch 90/372] [D loss: 0.6952118873596191] [G loss: 0.7460080981254578]\n",
      "[Epoch 52/1001] [Batch 91/372] [D loss: 0.69400954246521] [G loss: 0.6448725461959839]\n",
      "[Epoch 52/1001] [Batch 92/372] [D loss: 0.6946043968200684] [G loss: 0.7281369566917419]\n",
      "[Epoch 52/1001] [Batch 93/372] [D loss: 0.6935340166091919] [G loss: 0.6588648557662964]\n",
      "[Epoch 52/1001] [Batch 94/372] [D loss: 0.692033052444458] [G loss: 0.7086406350135803]\n",
      "[Epoch 52/1001] [Batch 95/372] [D loss: 0.691219687461853] [G loss: 0.6756963729858398]\n",
      "[Epoch 52/1001] [Batch 96/372] [D loss: 0.6915068626403809] [G loss: 0.691851019859314]\n",
      "[Epoch 52/1001] [Batch 97/372] [D loss: 0.6917530298233032] [G loss: 0.6884778738021851]\n",
      "[Epoch 52/1001] [Batch 98/372] [D loss: 0.6921486258506775] [G loss: 0.6825454235076904]\n",
      "[Epoch 52/1001] [Batch 99/372] [D loss: 0.69189453125] [G loss: 0.6882451772689819]\n",
      "[Epoch 52/1001] [Batch 100/372] [D loss: 0.6937048435211182] [G loss: 0.6857942938804626]\n",
      "[Epoch 52/1001] [Batch 101/372] [D loss: 0.6924079656600952] [G loss: 0.6810242533683777]\n",
      "[Epoch 52/1001] [Batch 102/372] [D loss: 0.6918057203292847] [G loss: 0.6961321234703064]\n",
      "[Epoch 52/1001] [Batch 103/372] [D loss: 0.6916466355323792] [G loss: 0.6639446020126343]\n",
      "[Epoch 52/1001] [Batch 104/372] [D loss: 0.6917526721954346] [G loss: 0.7274585366249084]\n",
      "[Epoch 52/1001] [Batch 105/372] [D loss: 0.694111704826355] [G loss: 0.6344132423400879]\n",
      "[Epoch 52/1001] [Batch 106/372] [D loss: 0.6940507888793945] [G loss: 0.7556004524230957]\n",
      "[Epoch 52/1001] [Batch 107/372] [D loss: 0.695209264755249] [G loss: 0.6102293133735657]\n",
      "[Epoch 52/1001] [Batch 108/372] [D loss: 0.6970269680023193] [G loss: 0.7850872278213501]\n",
      "[Epoch 52/1001] [Batch 109/372] [D loss: 0.6971755027770996] [G loss: 0.5891150236129761]\n",
      "[Epoch 52/1001] [Batch 110/372] [D loss: 0.6964012384414673] [G loss: 0.8036105036735535]\n",
      "[Epoch 52/1001] [Batch 111/372] [D loss: 0.6980360150337219] [G loss: 0.5821300148963928]\n",
      "[Epoch 52/1001] [Batch 112/372] [D loss: 0.6975311040878296] [G loss: 0.7997193932533264]\n",
      "[Epoch 52/1001] [Batch 113/372] [D loss: 0.6977120637893677] [G loss: 0.6002247333526611]\n",
      "[Epoch 52/1001] [Batch 114/372] [D loss: 0.6960488557815552] [G loss: 0.7708218097686768]\n",
      "[Epoch 52/1001] [Batch 115/372] [D loss: 0.6966650485992432] [G loss: 0.6230183839797974]\n",
      "[Epoch 52/1001] [Batch 116/372] [D loss: 0.6946682929992676] [G loss: 0.7455328702926636]\n",
      "[Epoch 52/1001] [Batch 117/372] [D loss: 0.694676399230957] [G loss: 0.6482087969779968]\n",
      "[Epoch 52/1001] [Batch 118/372] [D loss: 0.6937311887741089] [G loss: 0.7160679697990417]\n",
      "[Epoch 52/1001] [Batch 119/372] [D loss: 0.6926219463348389] [G loss: 0.6697486639022827]\n",
      "[Epoch 52/1001] [Batch 120/372] [D loss: 0.6929887533187866] [G loss: 0.7038797736167908]\n",
      "[Epoch 52/1001] [Batch 121/372] [D loss: 0.6921379566192627] [G loss: 0.6687933206558228]\n",
      "[Epoch 52/1001] [Batch 122/372] [D loss: 0.6918057203292847] [G loss: 0.7093385457992554]\n",
      "[Epoch 52/1001] [Batch 123/372] [D loss: 0.6926764249801636] [G loss: 0.668972909450531]\n",
      "[Epoch 52/1001] [Batch 124/372] [D loss: 0.691624641418457] [G loss: 0.7050837278366089]\n",
      "[Epoch 52/1001] [Batch 125/372] [D loss: 0.6923823356628418] [G loss: 0.6725782155990601]\n",
      "[Epoch 52/1001] [Batch 126/372] [D loss: 0.6930327415466309] [G loss: 0.7020890712738037]\n",
      "[Epoch 52/1001] [Batch 127/372] [D loss: 0.6928491592407227] [G loss: 0.6739025115966797]\n",
      "[Epoch 52/1001] [Batch 128/372] [D loss: 0.6928882598876953] [G loss: 0.7007138729095459]\n",
      "[Epoch 52/1001] [Batch 129/372] [D loss: 0.6920182108879089] [G loss: 0.6821225881576538]\n",
      "[Epoch 52/1001] [Batch 130/372] [D loss: 0.6921623945236206] [G loss: 0.6882989406585693]\n",
      "[Epoch 52/1001] [Batch 131/372] [D loss: 0.6924252510070801] [G loss: 0.6895934343338013]\n",
      "[Epoch 52/1001] [Batch 132/372] [D loss: 0.691746711730957] [G loss: 0.6808674931526184]\n",
      "[Epoch 52/1001] [Batch 133/372] [D loss: 0.6916168928146362] [G loss: 0.691523551940918]\n",
      "[Epoch 52/1001] [Batch 134/372] [D loss: 0.693873405456543] [G loss: 0.6785716414451599]\n",
      "[Epoch 52/1001] [Batch 135/372] [D loss: 0.6923561096191406] [G loss: 0.7045758366584778]\n",
      "[Epoch 52/1001] [Batch 136/372] [D loss: 0.6934360265731812] [G loss: 0.6520695686340332]\n",
      "[Epoch 52/1001] [Batch 137/372] [D loss: 0.6916224360466003] [G loss: 0.734816312789917]\n",
      "[Epoch 52/1001] [Batch 138/372] [D loss: 0.692760705947876] [G loss: 0.6337806582450867]\n",
      "[Epoch 52/1001] [Batch 139/372] [D loss: 0.6938880085945129] [G loss: 0.7653409838676453]\n",
      "[Epoch 52/1001] [Batch 140/372] [D loss: 0.6950676441192627] [G loss: 0.5990840196609497]\n",
      "[Epoch 52/1001] [Batch 141/372] [D loss: 0.697040319442749] [G loss: 0.7974086999893188]\n",
      "[Epoch 52/1001] [Batch 142/372] [D loss: 0.6988035440444946] [G loss: 0.580727756023407]\n",
      "[Epoch 52/1001] [Batch 143/372] [D loss: 0.6985424757003784] [G loss: 0.8127858638763428]\n",
      "[Epoch 52/1001] [Batch 144/372] [D loss: 0.6985589265823364] [G loss: 0.5836928486824036]\n",
      "[Epoch 52/1001] [Batch 145/372] [D loss: 0.6986827850341797] [G loss: 0.7887768149375916]\n",
      "[Epoch 52/1001] [Batch 146/372] [D loss: 0.697502851486206] [G loss: 0.6125748753547668]\n",
      "[Epoch 52/1001] [Batch 147/372] [D loss: 0.6946280002593994] [G loss: 0.7430330514907837]\n",
      "[Epoch 52/1001] [Batch 148/372] [D loss: 0.6941860914230347] [G loss: 0.6525383591651917]\n",
      "[Epoch 52/1001] [Batch 149/372] [D loss: 0.6934571266174316] [G loss: 0.7065137624740601]\n",
      "[Epoch 52/1001] [Batch 150/372] [D loss: 0.6926125884056091] [G loss: 0.6797724366188049]\n",
      "[Epoch 52/1001] [Batch 151/372] [D loss: 0.692540168762207] [G loss: 0.6867456436157227]\n",
      "[Epoch 52/1001] [Batch 152/372] [D loss: 0.6915009617805481] [G loss: 0.6910900473594666]\n",
      "[Epoch 52/1001] [Batch 153/372] [D loss: 0.6928310394287109] [G loss: 0.6883777976036072]\n",
      "[Epoch 52/1001] [Batch 154/372] [D loss: 0.6916863918304443] [G loss: 0.6887267231941223]\n",
      "[Epoch 52/1001] [Batch 155/372] [D loss: 0.6937968134880066] [G loss: 0.6895509362220764]\n",
      "[Epoch 52/1001] [Batch 156/372] [D loss: 0.6937718391418457] [G loss: 0.6853426098823547]\n",
      "[Epoch 52/1001] [Batch 157/372] [D loss: 0.6929934024810791] [G loss: 0.6894930005073547]\n",
      "[Epoch 52/1001] [Batch 158/372] [D loss: 0.6935411691665649] [G loss: 0.6876226663589478]\n",
      "[Epoch 52/1001] [Batch 159/372] [D loss: 0.6930394768714905] [G loss: 0.6856896281242371]\n",
      "[Epoch 52/1001] [Batch 160/372] [D loss: 0.6916333436965942] [G loss: 0.6838446259498596]\n",
      "[Epoch 52/1001] [Batch 161/372] [D loss: 0.693964421749115] [G loss: 0.6945162415504456]\n",
      "[Epoch 52/1001] [Batch 162/372] [D loss: 0.6929360628128052] [G loss: 0.6727691292762756]\n",
      "[Epoch 52/1001] [Batch 163/372] [D loss: 0.6917361617088318] [G loss: 0.7071141004562378]\n",
      "[Epoch 52/1001] [Batch 164/372] [D loss: 0.6931772232055664] [G loss: 0.6551190614700317]\n",
      "[Epoch 52/1001] [Batch 165/372] [D loss: 0.6936938762664795] [G loss: 0.7371513843536377]\n",
      "[Epoch 52/1001] [Batch 166/372] [D loss: 0.6923043131828308] [G loss: 0.6171571016311646]\n",
      "[Epoch 52/1001] [Batch 167/372] [D loss: 0.6942731738090515] [G loss: 0.7818849086761475]\n",
      "[Epoch 52/1001] [Batch 168/372] [D loss: 0.6957393884658813] [G loss: 0.591695249080658]\n",
      "[Epoch 52/1001] [Batch 169/372] [D loss: 0.6982054114341736] [G loss: 0.8109906315803528]\n",
      "[Epoch 52/1001] [Batch 170/372] [D loss: 0.6993504762649536] [G loss: 0.5658663511276245]\n",
      "[Epoch 52/1001] [Batch 171/372] [D loss: 0.7011282444000244] [G loss: 0.8347589373588562]\n",
      "[Epoch 52/1001] [Batch 172/372] [D loss: 0.7018876075744629] [G loss: 0.5655114054679871]\n",
      "[Epoch 52/1001] [Batch 173/372] [D loss: 0.7017768621444702] [G loss: 0.8082674741744995]\n",
      "[Epoch 52/1001] [Batch 174/372] [D loss: 0.6988142728805542] [G loss: 0.6029897332191467]\n",
      "[Epoch 52/1001] [Batch 175/372] [D loss: 0.697208046913147] [G loss: 0.7478340864181519]\n",
      "[Epoch 52/1001] [Batch 176/372] [D loss: 0.6949476003646851] [G loss: 0.6485209465026855]\n",
      "[Epoch 52/1001] [Batch 177/372] [D loss: 0.6935592293739319] [G loss: 0.7193602323532104]\n",
      "[Epoch 52/1001] [Batch 178/372] [D loss: 0.6939584016799927] [G loss: 0.6656153798103333]\n",
      "[Epoch 52/1001] [Batch 179/372] [D loss: 0.6923950910568237] [G loss: 0.7045257091522217]\n",
      "[Epoch 52/1001] [Batch 180/372] [D loss: 0.6929685473442078] [G loss: 0.6735543608665466]\n",
      "[Epoch 52/1001] [Batch 181/372] [D loss: 0.6920866966247559] [G loss: 0.6994442343711853]\n",
      "[Epoch 52/1001] [Batch 182/372] [D loss: 0.691461443901062] [G loss: 0.6806823015213013]\n",
      "[Epoch 52/1001] [Batch 183/372] [D loss: 0.6919891238212585] [G loss: 0.6980814933776855]\n",
      "[Epoch 52/1001] [Batch 184/372] [D loss: 0.6930499076843262] [G loss: 0.67506343126297]\n",
      "[Epoch 52/1001] [Batch 185/372] [D loss: 0.6925595998764038] [G loss: 0.6935717463493347]\n",
      "[Epoch 52/1001] [Batch 186/372] [D loss: 0.6929172277450562] [G loss: 0.6832060217857361]\n",
      "[Epoch 52/1001] [Batch 187/372] [D loss: 0.6931345462799072] [G loss: 0.6976293921470642]\n",
      "[Epoch 52/1001] [Batch 188/372] [D loss: 0.6915680170059204] [G loss: 0.6763910055160522]\n",
      "[Epoch 52/1001] [Batch 189/372] [D loss: 0.6921306252479553] [G loss: 0.7060267329216003]\n",
      "[Epoch 52/1001] [Batch 190/372] [D loss: 0.6925531625747681] [G loss: 0.6647934913635254]\n",
      "[Epoch 52/1001] [Batch 191/372] [D loss: 0.692103922367096] [G loss: 0.7139096260070801]\n",
      "[Epoch 52/1001] [Batch 192/372] [D loss: 0.6932482719421387] [G loss: 0.6639708280563354]\n",
      "[Epoch 52/1001] [Batch 193/372] [D loss: 0.6936867833137512] [G loss: 0.7057399153709412]\n",
      "[Epoch 52/1001] [Batch 194/372] [D loss: 0.6915661096572876] [G loss: 0.6733749508857727]\n",
      "[Epoch 52/1001] [Batch 195/372] [D loss: 0.6932912468910217] [G loss: 0.6965464949607849]\n",
      "[Epoch 52/1001] [Batch 196/372] [D loss: 0.6928364038467407] [G loss: 0.6816425323486328]\n",
      "[Epoch 52/1001] [Batch 197/372] [D loss: 0.691077709197998] [G loss: 0.686118483543396]\n",
      "[Epoch 52/1001] [Batch 198/372] [D loss: 0.6929869651794434] [G loss: 0.6871829032897949]\n",
      "[Epoch 52/1001] [Batch 199/372] [D loss: 0.6917573809623718] [G loss: 0.6851514577865601]\n",
      "[Epoch 52/1001] [Batch 200/372] [D loss: 0.6934479475021362] [G loss: 0.6989381909370422]\n",
      "[Epoch 52/1001] [Batch 201/372] [D loss: 0.6917076110839844] [G loss: 0.6737390756607056]\n",
      "[Epoch 52/1001] [Batch 202/372] [D loss: 0.6934396028518677] [G loss: 0.7027286291122437]\n",
      "[Epoch 52/1001] [Batch 203/372] [D loss: 0.6934877038002014] [G loss: 0.6685755252838135]\n",
      "[Epoch 52/1001] [Batch 204/372] [D loss: 0.6929426789283752] [G loss: 0.7132956981658936]\n",
      "[Epoch 52/1001] [Batch 205/372] [D loss: 0.6923102736473083] [G loss: 0.6631346344947815]\n",
      "[Epoch 52/1001] [Batch 206/372] [D loss: 0.6918690800666809] [G loss: 0.7130141854286194]\n",
      "[Epoch 52/1001] [Batch 207/372] [D loss: 0.693840742111206] [G loss: 0.6637628674507141]\n",
      "[Epoch 52/1001] [Batch 208/372] [D loss: 0.6932258605957031] [G loss: 0.7115485668182373]\n",
      "[Epoch 52/1001] [Batch 209/372] [D loss: 0.6917295455932617] [G loss: 0.657735288143158]\n",
      "[Epoch 52/1001] [Batch 210/372] [D loss: 0.6918218731880188] [G loss: 0.7213917970657349]\n",
      "[Epoch 52/1001] [Batch 211/372] [D loss: 0.6926367282867432] [G loss: 0.6414862871170044]\n",
      "[Epoch 52/1001] [Batch 212/372] [D loss: 0.6933497190475464] [G loss: 0.7561831474304199]\n",
      "[Epoch 52/1001] [Batch 213/372] [D loss: 0.6943368911743164] [G loss: 0.5977542400360107]\n",
      "[Epoch 52/1001] [Batch 214/372] [D loss: 0.6969406604766846] [G loss: 0.8385385274887085]\n",
      "[Epoch 52/1001] [Batch 215/372] [D loss: 0.7014293074607849] [G loss: 0.5226010084152222]\n",
      "[Epoch 52/1001] [Batch 216/372] [D loss: 0.7085146903991699] [G loss: 0.8966640830039978]\n",
      "[Epoch 52/1001] [Batch 217/372] [D loss: 0.7086588144302368] [G loss: 0.5227211117744446]\n",
      "[Epoch 52/1001] [Batch 218/372] [D loss: 0.709080159664154] [G loss: 0.8619686365127563]\n",
      "[Epoch 52/1001] [Batch 219/372] [D loss: 0.7059668302536011] [G loss: 0.571273922920227]\n",
      "[Epoch 52/1001] [Batch 220/372] [D loss: 0.701194167137146] [G loss: 0.7690029144287109]\n",
      "[Epoch 52/1001] [Batch 221/372] [D loss: 0.6963027119636536] [G loss: 0.6489676237106323]\n",
      "[Epoch 52/1001] [Batch 222/372] [D loss: 0.6929285526275635] [G loss: 0.6973931789398193]\n",
      "[Epoch 52/1001] [Batch 223/372] [D loss: 0.6932663917541504] [G loss: 0.6915618181228638]\n",
      "[Epoch 52/1001] [Batch 224/372] [D loss: 0.694146990776062] [G loss: 0.681185245513916]\n",
      "[Epoch 52/1001] [Batch 225/372] [D loss: 0.6926937103271484] [G loss: 0.6910907030105591]\n",
      "[Epoch 52/1001] [Batch 226/372] [D loss: 0.6930563449859619] [G loss: 0.6850792765617371]\n",
      "[Epoch 52/1001] [Batch 227/372] [D loss: 0.6917799711227417] [G loss: 0.6822059154510498]\n",
      "[Epoch 52/1001] [Batch 228/372] [D loss: 0.6937507390975952] [G loss: 0.7032970786094666]\n",
      "[Epoch 52/1001] [Batch 229/372] [D loss: 0.6904304623603821] [G loss: 0.6623438000679016]\n",
      "[Epoch 52/1001] [Batch 230/372] [D loss: 0.6909036636352539] [G loss: 0.712573766708374]\n",
      "[Epoch 52/1001] [Batch 231/372] [D loss: 0.6906898021697998] [G loss: 0.6615315675735474]\n",
      "[Epoch 52/1001] [Batch 232/372] [D loss: 0.695427417755127] [G loss: 0.7069280743598938]\n",
      "[Epoch 52/1001] [Batch 233/372] [D loss: 0.6935287714004517] [G loss: 0.6645102500915527]\n",
      "[Epoch 52/1001] [Batch 234/372] [D loss: 0.6923805475234985] [G loss: 0.7072490453720093]\n",
      "[Epoch 52/1001] [Batch 235/372] [D loss: 0.69219970703125] [G loss: 0.6666756272315979]\n",
      "[Epoch 52/1001] [Batch 236/372] [D loss: 0.6944131851196289] [G loss: 0.7058236002922058]\n",
      "[Epoch 52/1001] [Batch 237/372] [D loss: 0.6922880411148071] [G loss: 0.6664828062057495]\n",
      "[Epoch 52/1001] [Batch 238/372] [D loss: 0.692989706993103] [G loss: 0.7101284265518188]\n",
      "[Epoch 52/1001] [Batch 239/372] [D loss: 0.6927194595336914] [G loss: 0.661123514175415]\n",
      "[Epoch 52/1001] [Batch 240/372] [D loss: 0.6933014392852783] [G loss: 0.7107844948768616]\n",
      "[Epoch 52/1001] [Batch 241/372] [D loss: 0.6932932138442993] [G loss: 0.665979266166687]\n",
      "[Epoch 52/1001] [Batch 242/372] [D loss: 0.6930536031723022] [G loss: 0.7088274955749512]\n",
      "[Epoch 52/1001] [Batch 243/372] [D loss: 0.6913699507713318] [G loss: 0.6686141490936279]\n",
      "[Epoch 52/1001] [Batch 244/372] [D loss: 0.6921502947807312] [G loss: 0.7021158933639526]\n",
      "[Epoch 52/1001] [Batch 245/372] [D loss: 0.6928855776786804] [G loss: 0.681281328201294]\n",
      "[Epoch 52/1001] [Batch 246/372] [D loss: 0.6934541463851929] [G loss: 0.6831679940223694]\n",
      "[Epoch 52/1001] [Batch 247/372] [D loss: 0.692584753036499] [G loss: 0.6964136958122253]\n",
      "[Epoch 52/1001] [Batch 248/372] [D loss: 0.6927856206893921] [G loss: 0.6741762161254883]\n",
      "[Epoch 52/1001] [Batch 249/372] [D loss: 0.6912255883216858] [G loss: 0.6997286677360535]\n",
      "[Epoch 52/1001] [Batch 250/372] [D loss: 0.6918283104896545] [G loss: 0.6800221800804138]\n",
      "[Epoch 52/1001] [Batch 251/372] [D loss: 0.6938314437866211] [G loss: 0.6952649354934692]\n",
      "[Epoch 52/1001] [Batch 252/372] [D loss: 0.6926987767219543] [G loss: 0.6733168363571167]\n",
      "[Epoch 52/1001] [Batch 253/372] [D loss: 0.6919784545898438] [G loss: 0.7084326148033142]\n",
      "[Epoch 52/1001] [Batch 254/372] [D loss: 0.6918284893035889] [G loss: 0.6637724041938782]\n",
      "[Epoch 52/1001] [Batch 255/372] [D loss: 0.692471981048584] [G loss: 0.7169252038002014]\n",
      "[Epoch 52/1001] [Batch 256/372] [D loss: 0.6924291849136353] [G loss: 0.6570139527320862]\n",
      "[Epoch 52/1001] [Batch 257/372] [D loss: 0.6927895545959473] [G loss: 0.7157570123672485]\n",
      "[Epoch 52/1001] [Batch 258/372] [D loss: 0.6924268007278442] [G loss: 0.6647346019744873]\n",
      "[Epoch 52/1001] [Batch 259/372] [D loss: 0.6940411329269409] [G loss: 0.7100602388381958]\n",
      "[Epoch 52/1001] [Batch 260/372] [D loss: 0.6941553354263306] [G loss: 0.6677691340446472]\n",
      "[Epoch 52/1001] [Batch 261/372] [D loss: 0.6930773258209229] [G loss: 0.7077315449714661]\n",
      "[Epoch 52/1001] [Batch 262/372] [D loss: 0.6931418180465698] [G loss: 0.6656307578086853]\n",
      "[Epoch 52/1001] [Batch 263/372] [D loss: 0.6932684183120728] [G loss: 0.7131983041763306]\n",
      "[Epoch 52/1001] [Batch 264/372] [D loss: 0.6911425590515137] [G loss: 0.6538785696029663]\n",
      "[Epoch 52/1001] [Batch 265/372] [D loss: 0.6938864588737488] [G loss: 0.7339490056037903]\n",
      "[Epoch 52/1001] [Batch 266/372] [D loss: 0.6942213773727417] [G loss: 0.6325401067733765]\n",
      "[Epoch 52/1001] [Batch 267/372] [D loss: 0.6955998539924622] [G loss: 0.7543414831161499]\n",
      "[Epoch 52/1001] [Batch 268/372] [D loss: 0.694068431854248] [G loss: 0.6091360449790955]\n",
      "[Epoch 52/1001] [Batch 269/372] [D loss: 0.695319652557373] [G loss: 0.7772666215896606]\n",
      "[Epoch 52/1001] [Batch 270/372] [D loss: 0.696346640586853] [G loss: 0.5865474939346313]\n",
      "[Epoch 52/1001] [Batch 271/372] [D loss: 0.697059154510498] [G loss: 0.8274757266044617]\n",
      "[Epoch 52/1001] [Batch 272/372] [D loss: 0.7026433944702148] [G loss: 0.5438886284828186]\n",
      "[Epoch 52/1001] [Batch 273/372] [D loss: 0.7051339745521545] [G loss: 0.8721342086791992]\n",
      "[Epoch 52/1001] [Batch 274/372] [D loss: 0.7069123983383179] [G loss: 0.5262610912322998]\n",
      "[Epoch 52/1001] [Batch 275/372] [D loss: 0.7083743810653687] [G loss: 0.8613593578338623]\n",
      "[Epoch 52/1001] [Batch 276/372] [D loss: 0.7052339315414429] [G loss: 0.5735901594161987]\n",
      "[Epoch 52/1001] [Batch 277/372] [D loss: 0.7003207802772522] [G loss: 0.7706428170204163]\n",
      "[Epoch 52/1001] [Batch 278/372] [D loss: 0.6960066556930542] [G loss: 0.6495876908302307]\n",
      "[Epoch 52/1001] [Batch 279/372] [D loss: 0.6935132145881653] [G loss: 0.6979612708091736]\n",
      "[Epoch 52/1001] [Batch 280/372] [D loss: 0.6930713653564453] [G loss: 0.6945364475250244]\n",
      "[Epoch 52/1001] [Batch 281/372] [D loss: 0.6918693780899048] [G loss: 0.6780044436454773]\n",
      "[Epoch 52/1001] [Batch 282/372] [D loss: 0.691723108291626] [G loss: 0.7010947465896606]\n",
      "[Epoch 52/1001] [Batch 283/372] [D loss: 0.6920974254608154] [G loss: 0.6785754561424255]\n",
      "[Epoch 52/1001] [Batch 284/372] [D loss: 0.6925868988037109] [G loss: 0.6938266754150391]\n",
      "[Epoch 52/1001] [Batch 285/372] [D loss: 0.6921781301498413] [G loss: 0.6803329586982727]\n",
      "[Epoch 52/1001] [Batch 286/372] [D loss: 0.6926594972610474] [G loss: 0.6932209730148315]\n",
      "[Epoch 52/1001] [Batch 287/372] [D loss: 0.6925358772277832] [G loss: 0.6840935349464417]\n",
      "[Epoch 52/1001] [Batch 288/372] [D loss: 0.6913817524909973] [G loss: 0.6864762902259827]\n",
      "[Epoch 52/1001] [Batch 289/372] [D loss: 0.6932839155197144] [G loss: 0.6977497935295105]\n",
      "[Epoch 52/1001] [Batch 290/372] [D loss: 0.6906974911689758] [G loss: 0.6732587814331055]\n",
      "[Epoch 52/1001] [Batch 291/372] [D loss: 0.6933954954147339] [G loss: 0.7137991786003113]\n",
      "[Epoch 52/1001] [Batch 292/372] [D loss: 0.6922272443771362] [G loss: 0.651272177696228]\n",
      "[Epoch 52/1001] [Batch 293/372] [D loss: 0.6936484575271606] [G loss: 0.7283715009689331]\n",
      "[Epoch 52/1001] [Batch 294/372] [D loss: 0.6924540996551514] [G loss: 0.6532022356987]\n",
      "[Epoch 52/1001] [Batch 295/372] [D loss: 0.6927273273468018] [G loss: 0.7148841619491577]\n",
      "[Epoch 52/1001] [Batch 296/372] [D loss: 0.6922849416732788] [G loss: 0.6693896651268005]\n",
      "[Epoch 52/1001] [Batch 297/372] [D loss: 0.6928318738937378] [G loss: 0.6942856311798096]\n",
      "[Epoch 52/1001] [Batch 298/372] [D loss: 0.6932216286659241] [G loss: 0.6813728213310242]\n",
      "[Epoch 52/1001] [Batch 299/372] [D loss: 0.6908659934997559] [G loss: 0.689153254032135]\n",
      "[Epoch 52/1001] [Batch 300/372] [D loss: 0.6930972933769226] [G loss: 0.7018010020256042]\n",
      "[Epoch 52/1001] [Batch 301/372] [D loss: 0.6925870180130005] [G loss: 0.6621043086051941]\n",
      "[Epoch 52/1001] [Batch 302/372] [D loss: 0.6928901672363281] [G loss: 0.7129944562911987]\n",
      "[Epoch 52/1001] [Batch 303/372] [D loss: 0.6934731006622314] [G loss: 0.6630176901817322]\n",
      "[Epoch 52/1001] [Batch 304/372] [D loss: 0.692747950553894] [G loss: 0.7095638513565063]\n",
      "[Epoch 52/1001] [Batch 305/372] [D loss: 0.6932591795921326] [G loss: 0.6659282445907593]\n",
      "[Epoch 52/1001] [Batch 306/372] [D loss: 0.6914435625076294] [G loss: 0.711464524269104]\n",
      "[Epoch 52/1001] [Batch 307/372] [D loss: 0.6917106509208679] [G loss: 0.6593588590621948]\n",
      "[Epoch 52/1001] [Batch 308/372] [D loss: 0.6929485201835632] [G loss: 0.7206791639328003]\n",
      "[Epoch 52/1001] [Batch 309/372] [D loss: 0.6924586892127991] [G loss: 0.6516240239143372]\n",
      "[Epoch 52/1001] [Batch 310/372] [D loss: 0.6934707164764404] [G loss: 0.7329673767089844]\n",
      "[Epoch 52/1001] [Batch 311/372] [D loss: 0.6945391297340393] [G loss: 0.6351267099380493]\n",
      "[Epoch 52/1001] [Batch 312/372] [D loss: 0.693293571472168] [G loss: 0.748155951499939]\n",
      "[Epoch 52/1001] [Batch 313/372] [D loss: 0.6940075159072876] [G loss: 0.6237743496894836]\n",
      "[Epoch 52/1001] [Batch 314/372] [D loss: 0.6953649520874023] [G loss: 0.7628819942474365]\n",
      "[Epoch 52/1001] [Batch 315/372] [D loss: 0.6948205232620239] [G loss: 0.6092087030410767]\n",
      "[Epoch 52/1001] [Batch 316/372] [D loss: 0.6948980093002319] [G loss: 0.7844758629798889]\n",
      "[Epoch 52/1001] [Batch 317/372] [D loss: 0.6970033645629883] [G loss: 0.594392716884613]\n",
      "[Epoch 52/1001] [Batch 318/372] [D loss: 0.6976036429405212] [G loss: 0.7894554138183594]\n",
      "[Epoch 52/1001] [Batch 319/372] [D loss: 0.6970043182373047] [G loss: 0.6021600365638733]\n",
      "[Epoch 52/1001] [Batch 320/372] [D loss: 0.6977635622024536] [G loss: 0.7696349024772644]\n",
      "[Epoch 52/1001] [Batch 321/372] [D loss: 0.6950005292892456] [G loss: 0.6268365383148193]\n",
      "[Epoch 52/1001] [Batch 322/372] [D loss: 0.693757951259613] [G loss: 0.7324234843254089]\n",
      "[Epoch 52/1001] [Batch 323/372] [D loss: 0.6936511993408203] [G loss: 0.6494218707084656]\n",
      "[Epoch 52/1001] [Batch 324/372] [D loss: 0.6933673620223999] [G loss: 0.7216060757637024]\n",
      "[Epoch 52/1001] [Batch 325/372] [D loss: 0.6918313503265381] [G loss: 0.6589188575744629]\n",
      "[Epoch 52/1001] [Batch 326/372] [D loss: 0.6927580833435059] [G loss: 0.7136314511299133]\n",
      "[Epoch 52/1001] [Batch 327/372] [D loss: 0.6930580139160156] [G loss: 0.6594008803367615]\n",
      "[Epoch 52/1001] [Batch 328/372] [D loss: 0.6918132901191711] [G loss: 0.7084054350852966]\n",
      "[Epoch 52/1001] [Batch 329/372] [D loss: 0.69263756275177] [G loss: 0.6711249351501465]\n",
      "[Epoch 52/1001] [Batch 330/372] [D loss: 0.6922749280929565] [G loss: 0.6995760202407837]\n",
      "[Epoch 52/1001] [Batch 331/372] [D loss: 0.6910958290100098] [G loss: 0.6826422214508057]\n",
      "[Epoch 52/1001] [Batch 332/372] [D loss: 0.691292405128479] [G loss: 0.6834015250205994]\n",
      "[Epoch 52/1001] [Batch 333/372] [D loss: 0.6921564340591431] [G loss: 0.6936500668525696]\n",
      "[Epoch 52/1001] [Batch 334/372] [D loss: 0.6893723607063293] [G loss: 0.6769427061080933]\n",
      "[Epoch 52/1001] [Batch 335/372] [D loss: 0.6909429430961609] [G loss: 0.7031726837158203]\n",
      "[Epoch 52/1001] [Batch 336/372] [D loss: 0.6917597055435181] [G loss: 0.6627374291419983]\n",
      "[Epoch 52/1001] [Batch 337/372] [D loss: 0.6927007436752319] [G loss: 0.7140370607376099]\n",
      "[Epoch 52/1001] [Batch 338/372] [D loss: 0.6929221153259277] [G loss: 0.6570985317230225]\n",
      "[Epoch 52/1001] [Batch 339/372] [D loss: 0.6927040815353394] [G loss: 0.7197837233543396]\n",
      "[Epoch 52/1001] [Batch 340/372] [D loss: 0.6929178237915039] [G loss: 0.654828667640686]\n",
      "[Epoch 52/1001] [Batch 341/372] [D loss: 0.6955791711807251] [G loss: 0.7238580584526062]\n",
      "[Epoch 52/1001] [Batch 342/372] [D loss: 0.6941640973091125] [G loss: 0.6462990045547485]\n",
      "[Epoch 52/1001] [Batch 343/372] [D loss: 0.6943346261978149] [G loss: 0.7272835373878479]\n",
      "[Epoch 52/1001] [Batch 344/372] [D loss: 0.6942100524902344] [G loss: 0.6434680819511414]\n",
      "[Epoch 52/1001] [Batch 345/372] [D loss: 0.6929613351821899] [G loss: 0.7313395738601685]\n",
      "[Epoch 52/1001] [Batch 346/372] [D loss: 0.6935486793518066] [G loss: 0.6366127133369446]\n",
      "[Epoch 52/1001] [Batch 347/372] [D loss: 0.6931732892990112] [G loss: 0.7457118630409241]\n",
      "[Epoch 52/1001] [Batch 348/372] [D loss: 0.693103015422821] [G loss: 0.627667248249054]\n",
      "[Epoch 52/1001] [Batch 349/372] [D loss: 0.6938385963439941] [G loss: 0.7543356418609619]\n",
      "[Epoch 52/1001] [Batch 350/372] [D loss: 0.6933315992355347] [G loss: 0.6179323196411133]\n",
      "[Epoch 52/1001] [Batch 351/372] [D loss: 0.6958343982696533] [G loss: 0.7732434272766113]\n",
      "[Epoch 52/1001] [Batch 352/372] [D loss: 0.6963623762130737] [G loss: 0.5989052653312683]\n",
      "[Epoch 52/1001] [Batch 353/372] [D loss: 0.6958444118499756] [G loss: 0.7877602577209473]\n",
      "[Epoch 52/1001] [Batch 354/372] [D loss: 0.6966273784637451] [G loss: 0.5895575881004333]\n",
      "[Epoch 52/1001] [Batch 355/372] [D loss: 0.6957480311393738] [G loss: 0.7929214239120483]\n",
      "[Epoch 52/1001] [Batch 356/372] [D loss: 0.6981298923492432] [G loss: 0.5962473154067993]\n",
      "[Epoch 52/1001] [Batch 357/372] [D loss: 0.6982890367507935] [G loss: 0.7762173414230347]\n",
      "[Epoch 52/1001] [Batch 358/372] [D loss: 0.6959148645401001] [G loss: 0.615915834903717]\n",
      "[Epoch 52/1001] [Batch 359/372] [D loss: 0.6965312957763672] [G loss: 0.7521777749061584]\n",
      "[Epoch 52/1001] [Batch 360/372] [D loss: 0.6944818496704102] [G loss: 0.6383209824562073]\n",
      "[Epoch 52/1001] [Batch 361/372] [D loss: 0.6944778561592102] [G loss: 0.7245683670043945]\n",
      "[Epoch 52/1001] [Batch 362/372] [D loss: 0.692054271697998] [G loss: 0.6574422121047974]\n",
      "[Epoch 52/1001] [Batch 363/372] [D loss: 0.6950662732124329] [G loss: 0.7099706530570984]\n",
      "[Epoch 52/1001] [Batch 364/372] [D loss: 0.6931165456771851] [G loss: 0.6665245890617371]\n",
      "[Epoch 52/1001] [Batch 365/372] [D loss: 0.692948579788208] [G loss: 0.6967761516571045]\n",
      "[Epoch 52/1001] [Batch 366/372] [D loss: 0.6928794384002686] [G loss: 0.6839492917060852]\n",
      "[Epoch 52/1001] [Batch 367/372] [D loss: 0.6922211050987244] [G loss: 0.6802829504013062]\n",
      "[Epoch 52/1001] [Batch 368/372] [D loss: 0.6929848194122314] [G loss: 0.7023986577987671]\n",
      "[Epoch 52/1001] [Batch 369/372] [D loss: 0.6910616755485535] [G loss: 0.6628884077072144]\n",
      "[Epoch 52/1001] [Batch 370/372] [D loss: 0.6929960250854492] [G loss: 0.7160400748252869]\n",
      "[Epoch 52/1001] [Batch 371/372] [D loss: 0.6919553279876709] [G loss: 0.6475107073783875]\n",
      "[Epoch 53/1001] [Batch 0/372] [D loss: 0.6932271718978882] [G loss: 0.7495115399360657]\n",
      "[Epoch 53/1001] [Batch 1/372] [D loss: 0.694446325302124] [G loss: 0.6179705858230591]\n",
      "[Epoch 53/1001] [Batch 2/372] [D loss: 0.6938529014587402] [G loss: 0.7672878503799438]\n",
      "[Epoch 53/1001] [Batch 3/372] [D loss: 0.6948341131210327] [G loss: 0.6148918271064758]\n",
      "[Epoch 53/1001] [Batch 4/372] [D loss: 0.6955789923667908] [G loss: 0.7662865519523621]\n",
      "[Epoch 53/1001] [Batch 5/372] [D loss: 0.6957122087478638] [G loss: 0.617724597454071]\n",
      "[Epoch 53/1001] [Batch 6/372] [D loss: 0.6958093643188477] [G loss: 0.7609647512435913]\n",
      "[Epoch 53/1001] [Batch 7/372] [D loss: 0.694486141204834] [G loss: 0.6153729557991028]\n",
      "[Epoch 53/1001] [Batch 8/372] [D loss: 0.694549560546875] [G loss: 0.7593050003051758]\n",
      "[Epoch 53/1001] [Batch 9/372] [D loss: 0.6952025890350342] [G loss: 0.6185604333877563]\n",
      "[Epoch 53/1001] [Batch 10/372] [D loss: 0.6959798336029053] [G loss: 0.7562685012817383]\n",
      "[Epoch 53/1001] [Batch 11/372] [D loss: 0.6945538520812988] [G loss: 0.6256213188171387]\n",
      "[Epoch 53/1001] [Batch 12/372] [D loss: 0.6932054162025452] [G loss: 0.7438295483589172]\n",
      "[Epoch 53/1001] [Batch 13/372] [D loss: 0.6942276358604431] [G loss: 0.6407988667488098]\n",
      "[Epoch 53/1001] [Batch 14/372] [D loss: 0.6921268701553345] [G loss: 0.7353326678276062]\n",
      "[Epoch 53/1001] [Batch 15/372] [D loss: 0.6920485496520996] [G loss: 0.6484598517417908]\n",
      "[Epoch 53/1001] [Batch 16/372] [D loss: 0.6932814121246338] [G loss: 0.7212265729904175]\n",
      "[Epoch 53/1001] [Batch 17/372] [D loss: 0.6920067071914673] [G loss: 0.6690847873687744]\n",
      "[Epoch 53/1001] [Batch 18/372] [D loss: 0.6923089623451233] [G loss: 0.6942001581192017]\n",
      "[Epoch 53/1001] [Batch 19/372] [D loss: 0.6920443773269653] [G loss: 0.6779991388320923]\n",
      "[Epoch 53/1001] [Batch 20/372] [D loss: 0.6928110122680664] [G loss: 0.6899148225784302]\n",
      "[Epoch 53/1001] [Batch 21/372] [D loss: 0.6929814219474792] [G loss: 0.6840572357177734]\n",
      "[Epoch 53/1001] [Batch 22/372] [D loss: 0.6908023953437805] [G loss: 0.6886508464813232]\n",
      "[Epoch 53/1001] [Batch 23/372] [D loss: 0.6931486129760742] [G loss: 0.6757434606552124]\n",
      "[Epoch 53/1001] [Batch 24/372] [D loss: 0.6925928592681885] [G loss: 0.6938779354095459]\n",
      "[Epoch 53/1001] [Batch 25/372] [D loss: 0.6933602690696716] [G loss: 0.6776940822601318]\n",
      "[Epoch 53/1001] [Batch 26/372] [D loss: 0.6927425265312195] [G loss: 0.6924067735671997]\n",
      "[Epoch 53/1001] [Batch 27/372] [D loss: 0.6917479634284973] [G loss: 0.6800969243049622]\n",
      "[Epoch 53/1001] [Batch 28/372] [D loss: 0.6920691728591919] [G loss: 0.6917279362678528]\n",
      "[Epoch 53/1001] [Batch 29/372] [D loss: 0.6929539442062378] [G loss: 0.6823869943618774]\n",
      "[Epoch 53/1001] [Batch 30/372] [D loss: 0.6931620836257935] [G loss: 0.6829026937484741]\n",
      "[Epoch 53/1001] [Batch 31/372] [D loss: 0.6917591094970703] [G loss: 0.7049087285995483]\n",
      "[Epoch 53/1001] [Batch 32/372] [D loss: 0.6929196715354919] [G loss: 0.6590268611907959]\n",
      "[Epoch 53/1001] [Batch 33/372] [D loss: 0.691745936870575] [G loss: 0.7212897539138794]\n",
      "[Epoch 53/1001] [Batch 34/372] [D loss: 0.6929387450218201] [G loss: 0.6331465840339661]\n",
      "[Epoch 53/1001] [Batch 35/372] [D loss: 0.6957272291183472] [G loss: 0.7639645934104919]\n",
      "[Epoch 53/1001] [Batch 36/372] [D loss: 0.6944917440414429] [G loss: 0.5985983610153198]\n",
      "[Epoch 53/1001] [Batch 37/372] [D loss: 0.6981487274169922] [G loss: 0.796565055847168]\n",
      "[Epoch 53/1001] [Batch 38/372] [D loss: 0.6956419944763184] [G loss: 0.5789991021156311]\n",
      "[Epoch 53/1001] [Batch 39/372] [D loss: 0.6991802453994751] [G loss: 0.823205828666687]\n",
      "[Epoch 53/1001] [Batch 40/372] [D loss: 0.7005329132080078] [G loss: 0.565758466720581]\n",
      "[Epoch 53/1001] [Batch 41/372] [D loss: 0.7006686925888062] [G loss: 0.8142181038856506]\n",
      "[Epoch 53/1001] [Batch 42/372] [D loss: 0.6997981071472168] [G loss: 0.5855546593666077]\n",
      "[Epoch 53/1001] [Batch 43/372] [D loss: 0.6985705494880676] [G loss: 0.7909708023071289]\n",
      "[Epoch 53/1001] [Batch 44/372] [D loss: 0.6953409314155579] [G loss: 0.6091535687446594]\n",
      "[Epoch 53/1001] [Batch 45/372] [D loss: 0.6946722865104675] [G loss: 0.7488024234771729]\n",
      "[Epoch 53/1001] [Batch 46/372] [D loss: 0.6942108273506165] [G loss: 0.6502803564071655]\n",
      "[Epoch 53/1001] [Batch 47/372] [D loss: 0.691795825958252] [G loss: 0.7064099907875061]\n",
      "[Epoch 53/1001] [Batch 48/372] [D loss: 0.6909027099609375] [G loss: 0.6812472343444824]\n",
      "[Epoch 53/1001] [Batch 49/372] [D loss: 0.6912809014320374] [G loss: 0.6888819932937622]\n",
      "[Epoch 53/1001] [Batch 50/372] [D loss: 0.6904505491256714] [G loss: 0.6884239912033081]\n",
      "[Epoch 53/1001] [Batch 51/372] [D loss: 0.6917288303375244] [G loss: 0.6808956861495972]\n",
      "[Epoch 53/1001] [Batch 52/372] [D loss: 0.6920738220214844] [G loss: 0.6836678981781006]\n",
      "[Epoch 53/1001] [Batch 53/372] [D loss: 0.6906810998916626] [G loss: 0.6960171461105347]\n",
      "[Epoch 53/1001] [Batch 54/372] [D loss: 0.6930538415908813] [G loss: 0.683198094367981]\n",
      "[Epoch 53/1001] [Batch 55/372] [D loss: 0.6926548480987549] [G loss: 0.6890974640846252]\n",
      "[Epoch 53/1001] [Batch 56/372] [D loss: 0.6939243674278259] [G loss: 0.6840441823005676]\n",
      "[Epoch 53/1001] [Batch 57/372] [D loss: 0.6934534907341003] [G loss: 0.6880365610122681]\n",
      "[Epoch 53/1001] [Batch 58/372] [D loss: 0.6921731233596802] [G loss: 0.6786514520645142]\n",
      "[Epoch 53/1001] [Batch 59/372] [D loss: 0.690786600112915] [G loss: 0.6960123777389526]\n",
      "[Epoch 53/1001] [Batch 60/372] [D loss: 0.6919705271720886] [G loss: 0.6861558556556702]\n",
      "[Epoch 53/1001] [Batch 61/372] [D loss: 0.6947294473648071] [G loss: 0.6848177313804626]\n",
      "[Epoch 53/1001] [Batch 62/372] [D loss: 0.6917858719825745] [G loss: 0.682876706123352]\n",
      "[Epoch 53/1001] [Batch 63/372] [D loss: 0.6922670602798462] [G loss: 0.6892639398574829]\n",
      "[Epoch 53/1001] [Batch 64/372] [D loss: 0.6912426948547363] [G loss: 0.6880623698234558]\n",
      "[Epoch 53/1001] [Batch 65/372] [D loss: 0.6935815811157227] [G loss: 0.6792365312576294]\n",
      "[Epoch 53/1001] [Batch 66/372] [D loss: 0.6922289133071899] [G loss: 0.6877605319023132]\n",
      "[Epoch 53/1001] [Batch 67/372] [D loss: 0.692084789276123] [G loss: 0.6867752075195312]\n",
      "[Epoch 53/1001] [Batch 68/372] [D loss: 0.6944986581802368] [G loss: 0.6883119344711304]\n",
      "[Epoch 53/1001] [Batch 69/372] [D loss: 0.6931689977645874] [G loss: 0.6785901188850403]\n",
      "[Epoch 53/1001] [Batch 70/372] [D loss: 0.6929926872253418] [G loss: 0.6985213160514832]\n",
      "[Epoch 53/1001] [Batch 71/372] [D loss: 0.6923505067825317] [G loss: 0.670703649520874]\n",
      "[Epoch 53/1001] [Batch 72/372] [D loss: 0.6928852200508118] [G loss: 0.7065982222557068]\n",
      "[Epoch 53/1001] [Batch 73/372] [D loss: 0.6925073862075806] [G loss: 0.6657562255859375]\n",
      "[Epoch 53/1001] [Batch 74/372] [D loss: 0.6948378086090088] [G loss: 0.7042670249938965]\n",
      "[Epoch 53/1001] [Batch 75/372] [D loss: 0.6920859813690186] [G loss: 0.6661525368690491]\n",
      "[Epoch 53/1001] [Batch 76/372] [D loss: 0.6915884017944336] [G loss: 0.7122437953948975]\n",
      "[Epoch 53/1001] [Batch 77/372] [D loss: 0.691831111907959] [G loss: 0.6467674374580383]\n",
      "[Epoch 53/1001] [Batch 78/372] [D loss: 0.6939734220504761] [G loss: 0.7406958937644958]\n",
      "[Epoch 53/1001] [Batch 79/372] [D loss: 0.6946510076522827] [G loss: 0.6189811825752258]\n",
      "[Epoch 53/1001] [Batch 80/372] [D loss: 0.6964213848114014] [G loss: 0.7819299101829529]\n",
      "[Epoch 53/1001] [Batch 81/372] [D loss: 0.6961256265640259] [G loss: 0.5837599635124207]\n",
      "[Epoch 53/1001] [Batch 82/372] [D loss: 0.6971771717071533] [G loss: 0.8085740208625793]\n",
      "[Epoch 53/1001] [Batch 83/372] [D loss: 0.6986593008041382] [G loss: 0.5731731057167053]\n",
      "[Epoch 53/1001] [Batch 84/372] [D loss: 0.7024815678596497] [G loss: 0.8226208686828613]\n",
      "[Epoch 53/1001] [Batch 85/372] [D loss: 0.7001329660415649] [G loss: 0.5679611563682556]\n",
      "[Epoch 53/1001] [Batch 86/372] [D loss: 0.7000080943107605] [G loss: 0.8081804513931274]\n",
      "[Epoch 53/1001] [Batch 87/372] [D loss: 0.6978784799575806] [G loss: 0.5962557792663574]\n",
      "[Epoch 53/1001] [Batch 88/372] [D loss: 0.6974429488182068] [G loss: 0.7709230780601501]\n",
      "[Epoch 53/1001] [Batch 89/372] [D loss: 0.6959775686264038] [G loss: 0.6274264454841614]\n",
      "[Epoch 53/1001] [Batch 90/372] [D loss: 0.6938663721084595] [G loss: 0.7339364886283875]\n",
      "[Epoch 53/1001] [Batch 91/372] [D loss: 0.6930245161056519] [G loss: 0.6566370725631714]\n",
      "[Epoch 53/1001] [Batch 92/372] [D loss: 0.6921876072883606] [G loss: 0.7043115496635437]\n",
      "[Epoch 53/1001] [Batch 93/372] [D loss: 0.6920522451400757] [G loss: 0.6791414022445679]\n",
      "[Epoch 53/1001] [Batch 94/372] [D loss: 0.6921162605285645] [G loss: 0.6881299018859863]\n",
      "[Epoch 53/1001] [Batch 95/372] [D loss: 0.6925052404403687] [G loss: 0.6835302710533142]\n",
      "[Epoch 53/1001] [Batch 96/372] [D loss: 0.691220223903656] [G loss: 0.6952897310256958]\n",
      "[Epoch 53/1001] [Batch 97/372] [D loss: 0.6903328895568848] [G loss: 0.6612831354141235]\n",
      "[Epoch 53/1001] [Batch 98/372] [D loss: 0.6914012432098389] [G loss: 0.7328844666481018]\n",
      "[Epoch 53/1001] [Batch 99/372] [D loss: 0.6902236342430115] [G loss: 0.6364749670028687]\n",
      "[Epoch 53/1001] [Batch 100/372] [D loss: 0.6948550343513489] [G loss: 0.7441115975379944]\n",
      "[Epoch 53/1001] [Batch 101/372] [D loss: 0.6951571106910706] [G loss: 0.6247496008872986]\n",
      "[Epoch 53/1001] [Batch 102/372] [D loss: 0.6945656538009644] [G loss: 0.7521541714668274]\n",
      "[Epoch 53/1001] [Batch 103/372] [D loss: 0.6948747634887695] [G loss: 0.632910966873169]\n",
      "[Epoch 53/1001] [Batch 104/372] [D loss: 0.6950808167457581] [G loss: 0.7373778820037842]\n",
      "[Epoch 53/1001] [Batch 105/372] [D loss: 0.6930588483810425] [G loss: 0.6451529860496521]\n",
      "[Epoch 53/1001] [Batch 106/372] [D loss: 0.6923801898956299] [G loss: 0.7324573397636414]\n",
      "[Epoch 53/1001] [Batch 107/372] [D loss: 0.6930220723152161] [G loss: 0.6390091776847839]\n",
      "[Epoch 53/1001] [Batch 108/372] [D loss: 0.6934043169021606] [G loss: 0.7285632491111755]\n",
      "[Epoch 53/1001] [Batch 109/372] [D loss: 0.6928366422653198] [G loss: 0.6436988115310669]\n",
      "[Epoch 53/1001] [Batch 110/372] [D loss: 0.6929311752319336] [G loss: 0.7238717079162598]\n",
      "[Epoch 53/1001] [Batch 111/372] [D loss: 0.6916398406028748] [G loss: 0.6524462103843689]\n",
      "[Epoch 53/1001] [Batch 112/372] [D loss: 0.6931431293487549] [G loss: 0.7283108830451965]\n",
      "[Epoch 53/1001] [Batch 113/372] [D loss: 0.6926290988922119] [G loss: 0.6448243856430054]\n",
      "[Epoch 53/1001] [Batch 114/372] [D loss: 0.6926426887512207] [G loss: 0.7282658815383911]\n",
      "[Epoch 53/1001] [Batch 115/372] [D loss: 0.693921685218811] [G loss: 0.6442323923110962]\n",
      "[Epoch 53/1001] [Batch 116/372] [D loss: 0.6925928592681885] [G loss: 0.7329422235488892]\n",
      "[Epoch 53/1001] [Batch 117/372] [D loss: 0.693915605545044] [G loss: 0.6408236026763916]\n",
      "[Epoch 53/1001] [Batch 118/372] [D loss: 0.6953750848770142] [G loss: 0.7386680245399475]\n",
      "[Epoch 53/1001] [Batch 119/372] [D loss: 0.6948455572128296] [G loss: 0.6359421014785767]\n",
      "[Epoch 53/1001] [Batch 120/372] [D loss: 0.6940551996231079] [G loss: 0.7342433929443359]\n",
      "[Epoch 53/1001] [Batch 121/372] [D loss: 0.6935631036758423] [G loss: 0.6396194696426392]\n",
      "[Epoch 53/1001] [Batch 122/372] [D loss: 0.6934137940406799] [G loss: 0.734488308429718]\n",
      "[Epoch 53/1001] [Batch 123/372] [D loss: 0.6938369274139404] [G loss: 0.6404774188995361]\n",
      "[Epoch 53/1001] [Batch 124/372] [D loss: 0.6940016150474548] [G loss: 0.7434918284416199]\n",
      "[Epoch 53/1001] [Batch 125/372] [D loss: 0.6935691833496094] [G loss: 0.6245043277740479]\n",
      "[Epoch 53/1001] [Batch 126/372] [D loss: 0.6930739879608154] [G loss: 0.7705920934677124]\n",
      "[Epoch 53/1001] [Batch 127/372] [D loss: 0.6957554817199707] [G loss: 0.603190004825592]\n",
      "[Epoch 53/1001] [Batch 128/372] [D loss: 0.6971790790557861] [G loss: 0.7781728506088257]\n",
      "[Epoch 53/1001] [Batch 129/372] [D loss: 0.6947243213653564] [G loss: 0.6060611009597778]\n",
      "[Epoch 53/1001] [Batch 130/372] [D loss: 0.6950178146362305] [G loss: 0.7721473574638367]\n",
      "[Epoch 53/1001] [Batch 131/372] [D loss: 0.6962405443191528] [G loss: 0.6043215990066528]\n",
      "[Epoch 53/1001] [Batch 132/372] [D loss: 0.6967048645019531] [G loss: 0.7878674268722534]\n",
      "[Epoch 53/1001] [Batch 133/372] [D loss: 0.6962155103683472] [G loss: 0.586348295211792]\n",
      "[Epoch 53/1001] [Batch 134/372] [D loss: 0.6973345875740051] [G loss: 0.8047143816947937]\n",
      "[Epoch 53/1001] [Batch 135/372] [D loss: 0.698884904384613] [G loss: 0.5763943195343018]\n",
      "[Epoch 53/1001] [Batch 136/372] [D loss: 0.6967953443527222] [G loss: 0.8036342263221741]\n",
      "[Epoch 53/1001] [Batch 137/372] [D loss: 0.6976616382598877] [G loss: 0.5973509550094604]\n",
      "[Epoch 53/1001] [Batch 138/372] [D loss: 0.696740448474884] [G loss: 0.7698177695274353]\n",
      "[Epoch 53/1001] [Batch 139/372] [D loss: 0.692980170249939] [G loss: 0.617329478263855]\n",
      "[Epoch 53/1001] [Batch 140/372] [D loss: 0.695531964302063] [G loss: 0.7545725703239441]\n",
      "[Epoch 53/1001] [Batch 141/372] [D loss: 0.6956424117088318] [G loss: 0.6280617713928223]\n",
      "[Epoch 53/1001] [Batch 142/372] [D loss: 0.6931861639022827] [G loss: 0.7452985644340515]\n",
      "[Epoch 53/1001] [Batch 143/372] [D loss: 0.6944352388381958] [G loss: 0.6352895498275757]\n",
      "[Epoch 53/1001] [Batch 144/372] [D loss: 0.6924768686294556] [G loss: 0.7324579358100891]\n",
      "[Epoch 53/1001] [Batch 145/372] [D loss: 0.6935029029846191] [G loss: 0.6498006582260132]\n",
      "[Epoch 53/1001] [Batch 146/372] [D loss: 0.6921155452728271] [G loss: 0.7172881960868835]\n",
      "[Epoch 53/1001] [Batch 147/372] [D loss: 0.690263569355011] [G loss: 0.6577402949333191]\n",
      "[Epoch 53/1001] [Batch 148/372] [D loss: 0.6932142972946167] [G loss: 0.7172791957855225]\n",
      "[Epoch 53/1001] [Batch 149/372] [D loss: 0.6929863691329956] [G loss: 0.6496532559394836]\n",
      "[Epoch 53/1001] [Batch 150/372] [D loss: 0.6931613683700562] [G loss: 0.7222527861595154]\n",
      "[Epoch 53/1001] [Batch 151/372] [D loss: 0.6929778456687927] [G loss: 0.6490582823753357]\n",
      "[Epoch 53/1001] [Batch 152/372] [D loss: 0.6927134990692139] [G loss: 0.7233037948608398]\n",
      "[Epoch 53/1001] [Batch 153/372] [D loss: 0.6946285963058472] [G loss: 0.65692538022995]\n",
      "[Epoch 53/1001] [Batch 154/372] [D loss: 0.6935476660728455] [G loss: 0.7086018919944763]\n",
      "[Epoch 53/1001] [Batch 155/372] [D loss: 0.6929178237915039] [G loss: 0.6675208210945129]\n",
      "[Epoch 53/1001] [Batch 156/372] [D loss: 0.6905593276023865] [G loss: 0.7047746777534485]\n",
      "[Epoch 53/1001] [Batch 157/372] [D loss: 0.6901633739471436] [G loss: 0.671994686126709]\n",
      "[Epoch 53/1001] [Batch 158/372] [D loss: 0.6936808228492737] [G loss: 0.6989027261734009]\n",
      "[Epoch 53/1001] [Batch 159/372] [D loss: 0.6909937858581543] [G loss: 0.6730448007583618]\n",
      "[Epoch 53/1001] [Batch 160/372] [D loss: 0.6914281845092773] [G loss: 0.70745849609375]\n",
      "[Epoch 53/1001] [Batch 161/372] [D loss: 0.6916944980621338] [G loss: 0.6569634079933167]\n",
      "[Epoch 53/1001] [Batch 162/372] [D loss: 0.6926252841949463] [G loss: 0.7258815765380859]\n",
      "[Epoch 53/1001] [Batch 163/372] [D loss: 0.6937716007232666] [G loss: 0.635906994342804]\n",
      "[Epoch 53/1001] [Batch 164/372] [D loss: 0.6930519938468933] [G loss: 0.7512458562850952]\n",
      "[Epoch 53/1001] [Batch 165/372] [D loss: 0.6936346292495728] [G loss: 0.6202420592308044]\n",
      "[Epoch 53/1001] [Batch 166/372] [D loss: 0.6950147151947021] [G loss: 0.7633990049362183]\n",
      "[Epoch 53/1001] [Batch 167/372] [D loss: 0.6943216323852539] [G loss: 0.6079928278923035]\n",
      "[Epoch 53/1001] [Batch 168/372] [D loss: 0.6941161155700684] [G loss: 0.7794196009635925]\n",
      "[Epoch 53/1001] [Batch 169/372] [D loss: 0.6948909163475037] [G loss: 0.5960977077484131]\n",
      "[Epoch 53/1001] [Batch 170/372] [D loss: 0.6973819732666016] [G loss: 0.794416606426239]\n",
      "[Epoch 53/1001] [Batch 171/372] [D loss: 0.6971957683563232] [G loss: 0.5784325003623962]\n",
      "[Epoch 53/1001] [Batch 172/372] [D loss: 0.6996028423309326] [G loss: 0.8226594924926758]\n",
      "[Epoch 53/1001] [Batch 173/372] [D loss: 0.7003029584884644] [G loss: 0.5675556659698486]\n",
      "[Epoch 53/1001] [Batch 174/372] [D loss: 0.7022231221199036] [G loss: 0.8145078420639038]\n",
      "[Epoch 53/1001] [Batch 175/372] [D loss: 0.6976734399795532] [G loss: 0.5874819755554199]\n",
      "[Epoch 53/1001] [Batch 176/372] [D loss: 0.6971869468688965] [G loss: 0.781714916229248]\n",
      "[Epoch 53/1001] [Batch 177/372] [D loss: 0.6953176259994507] [G loss: 0.6158901453018188]\n",
      "[Epoch 53/1001] [Batch 178/372] [D loss: 0.6950365304946899] [G loss: 0.7425569891929626]\n",
      "[Epoch 53/1001] [Batch 179/372] [D loss: 0.692305326461792] [G loss: 0.6473122239112854]\n",
      "[Epoch 53/1001] [Batch 180/372] [D loss: 0.6926687359809875] [G loss: 0.7140672206878662]\n",
      "[Epoch 53/1001] [Batch 181/372] [D loss: 0.6935476660728455] [G loss: 0.6660158634185791]\n",
      "[Epoch 53/1001] [Batch 182/372] [D loss: 0.6925076246261597] [G loss: 0.7046514749526978]\n",
      "[Epoch 53/1001] [Batch 183/372] [D loss: 0.6903573274612427] [G loss: 0.6664150953292847]\n",
      "[Epoch 53/1001] [Batch 184/372] [D loss: 0.6916154623031616] [G loss: 0.7109877467155457]\n",
      "[Epoch 53/1001] [Batch 185/372] [D loss: 0.6932190656661987] [G loss: 0.6538383960723877]\n",
      "[Epoch 53/1001] [Batch 186/372] [D loss: 0.6902583241462708] [G loss: 0.7212206125259399]\n",
      "[Epoch 53/1001] [Batch 187/372] [D loss: 0.69297194480896] [G loss: 0.6545725464820862]\n",
      "[Epoch 53/1001] [Batch 188/372] [D loss: 0.693524956703186] [G loss: 0.7214679718017578]\n",
      "[Epoch 53/1001] [Batch 189/372] [D loss: 0.6936548352241516] [G loss: 0.6408444046974182]\n",
      "[Epoch 53/1001] [Batch 190/372] [D loss: 0.6941715478897095] [G loss: 0.7296271324157715]\n",
      "[Epoch 53/1001] [Batch 191/372] [D loss: 0.693162739276886] [G loss: 0.6397574543952942]\n",
      "[Epoch 53/1001] [Batch 192/372] [D loss: 0.6934994459152222] [G loss: 0.744827389717102]\n",
      "[Epoch 53/1001] [Batch 193/372] [D loss: 0.6942167282104492] [G loss: 0.6220076680183411]\n",
      "[Epoch 53/1001] [Batch 194/372] [D loss: 0.6946678161621094] [G loss: 0.7615721225738525]\n",
      "[Epoch 53/1001] [Batch 195/372] [D loss: 0.6927987337112427] [G loss: 0.6045844554901123]\n",
      "[Epoch 53/1001] [Batch 196/372] [D loss: 0.69719398021698] [G loss: 0.7835195064544678]\n",
      "[Epoch 53/1001] [Batch 197/372] [D loss: 0.697433590888977] [G loss: 0.5872929096221924]\n",
      "[Epoch 53/1001] [Batch 198/372] [D loss: 0.6975083947181702] [G loss: 0.8069118857383728]\n",
      "[Epoch 53/1001] [Batch 199/372] [D loss: 0.7002041339874268] [G loss: 0.5759425163269043]\n",
      "[Epoch 53/1001] [Batch 200/372] [D loss: 0.7009615898132324] [G loss: 0.8010799288749695]\n",
      "[Epoch 53/1001] [Batch 201/372] [D loss: 0.6987853050231934] [G loss: 0.5985733270645142]\n",
      "[Epoch 53/1001] [Batch 202/372] [D loss: 0.6978269219398499] [G loss: 0.756555438041687]\n",
      "[Epoch 53/1001] [Batch 203/372] [D loss: 0.6941564083099365] [G loss: 0.6513581871986389]\n",
      "[Epoch 53/1001] [Batch 204/372] [D loss: 0.6938517093658447] [G loss: 0.6970227956771851]\n",
      "[Epoch 53/1001] [Batch 205/372] [D loss: 0.6928175687789917] [G loss: 0.6916115283966064]\n",
      "[Epoch 53/1001] [Batch 206/372] [D loss: 0.6915349364280701] [G loss: 0.6753814220428467]\n",
      "[Epoch 53/1001] [Batch 207/372] [D loss: 0.6908667087554932] [G loss: 0.6923744678497314]\n",
      "[Epoch 53/1001] [Batch 208/372] [D loss: 0.6893560886383057] [G loss: 0.6836744546890259]\n",
      "[Epoch 53/1001] [Batch 209/372] [D loss: 0.6939553022384644] [G loss: 0.69544517993927]\n",
      "[Epoch 53/1001] [Batch 210/372] [D loss: 0.6915591955184937] [G loss: 0.67619788646698]\n",
      "[Epoch 53/1001] [Batch 211/372] [D loss: 0.692227303981781] [G loss: 0.6934128999710083]\n",
      "[Epoch 53/1001] [Batch 212/372] [D loss: 0.691669225692749] [G loss: 0.6830034852027893]\n",
      "[Epoch 53/1001] [Batch 213/372] [D loss: 0.6929681301116943] [G loss: 0.6812752485275269]\n",
      "[Epoch 53/1001] [Batch 214/372] [D loss: 0.6927354335784912] [G loss: 0.6867327690124512]\n",
      "[Epoch 53/1001] [Batch 215/372] [D loss: 0.6918845176696777] [G loss: 0.6859928965568542]\n",
      "[Epoch 53/1001] [Batch 216/372] [D loss: 0.6921947598457336] [G loss: 0.6766049861907959]\n",
      "[Epoch 53/1001] [Batch 217/372] [D loss: 0.6928848028182983] [G loss: 0.6993119716644287]\n",
      "[Epoch 53/1001] [Batch 218/372] [D loss: 0.6929270029067993] [G loss: 0.6805472373962402]\n",
      "[Epoch 53/1001] [Batch 219/372] [D loss: 0.693768322467804] [G loss: 0.682469367980957]\n",
      "[Epoch 53/1001] [Batch 220/372] [D loss: 0.6921827793121338] [G loss: 0.68928062915802]\n",
      "[Epoch 53/1001] [Batch 221/372] [D loss: 0.6917024850845337] [G loss: 0.6784533858299255]\n",
      "[Epoch 53/1001] [Batch 222/372] [D loss: 0.6928320527076721] [G loss: 0.6968961954116821]\n",
      "[Epoch 53/1001] [Batch 223/372] [D loss: 0.690778374671936] [G loss: 0.6698441505432129]\n",
      "[Epoch 53/1001] [Batch 224/372] [D loss: 0.6938600540161133] [G loss: 0.7000938057899475]\n",
      "[Epoch 53/1001] [Batch 225/372] [D loss: 0.6913163661956787] [G loss: 0.6733458638191223]\n",
      "[Epoch 53/1001] [Batch 226/372] [D loss: 0.6940479874610901] [G loss: 0.6942520141601562]\n",
      "[Epoch 53/1001] [Batch 227/372] [D loss: 0.6946049928665161] [G loss: 0.6738849878311157]\n",
      "[Epoch 53/1001] [Batch 228/372] [D loss: 0.6911311149597168] [G loss: 0.7082228660583496]\n",
      "[Epoch 53/1001] [Batch 229/372] [D loss: 0.6934158802032471] [G loss: 0.6591756939888]\n",
      "[Epoch 53/1001] [Batch 230/372] [D loss: 0.6937733292579651] [G loss: 0.7222133874893188]\n",
      "[Epoch 53/1001] [Batch 231/372] [D loss: 0.6933236718177795] [G loss: 0.6389834880828857]\n",
      "[Epoch 53/1001] [Batch 232/372] [D loss: 0.6933696269989014] [G loss: 0.7575215101242065]\n",
      "[Epoch 53/1001] [Batch 233/372] [D loss: 0.6954959630966187] [G loss: 0.6084534525871277]\n",
      "[Epoch 53/1001] [Batch 234/372] [D loss: 0.6948258876800537] [G loss: 0.7859863638877869]\n",
      "[Epoch 53/1001] [Batch 235/372] [D loss: 0.6969687938690186] [G loss: 0.5874530673027039]\n",
      "[Epoch 53/1001] [Batch 236/372] [D loss: 0.6989060640335083] [G loss: 0.806118369102478]\n",
      "[Epoch 53/1001] [Batch 237/372] [D loss: 0.6988442540168762] [G loss: 0.5819019079208374]\n",
      "[Epoch 53/1001] [Batch 238/372] [D loss: 0.697367787361145] [G loss: 0.8020117282867432]\n",
      "[Epoch 53/1001] [Batch 239/372] [D loss: 0.69776850938797] [G loss: 0.5894205570220947]\n",
      "[Epoch 53/1001] [Batch 240/372] [D loss: 0.6972786784172058] [G loss: 0.7753417491912842]\n",
      "[Epoch 53/1001] [Batch 241/372] [D loss: 0.6960210800170898] [G loss: 0.6253689527511597]\n",
      "[Epoch 53/1001] [Batch 242/372] [D loss: 0.6947041749954224] [G loss: 0.7240476012229919]\n",
      "[Epoch 53/1001] [Batch 243/372] [D loss: 0.6932117938995361] [G loss: 0.6666576862335205]\n",
      "[Epoch 53/1001] [Batch 244/372] [D loss: 0.6928447484970093] [G loss: 0.6869271397590637]\n",
      "[Epoch 53/1001] [Batch 245/372] [D loss: 0.6928156614303589] [G loss: 0.6944905519485474]\n",
      "[Epoch 53/1001] [Batch 246/372] [D loss: 0.6904940009117126] [G loss: 0.6671135425567627]\n",
      "[Epoch 53/1001] [Batch 247/372] [D loss: 0.6913143992424011] [G loss: 0.7190494537353516]\n",
      "[Epoch 53/1001] [Batch 248/372] [D loss: 0.6931025385856628] [G loss: 0.6504186391830444]\n",
      "[Epoch 53/1001] [Batch 249/372] [D loss: 0.6926860809326172] [G loss: 0.7241597175598145]\n",
      "[Epoch 53/1001] [Batch 250/372] [D loss: 0.6921058297157288] [G loss: 0.6502149701118469]\n",
      "[Epoch 53/1001] [Batch 251/372] [D loss: 0.6950494050979614] [G loss: 0.7236201763153076]\n",
      "[Epoch 53/1001] [Batch 252/372] [D loss: 0.6942988038063049] [G loss: 0.6581538319587708]\n",
      "[Epoch 53/1001] [Batch 253/372] [D loss: 0.6942921876907349] [G loss: 0.7025293111801147]\n",
      "[Epoch 53/1001] [Batch 254/372] [D loss: 0.6924290060997009] [G loss: 0.6736926436424255]\n",
      "[Epoch 53/1001] [Batch 255/372] [D loss: 0.6911813020706177] [G loss: 0.697685718536377]\n",
      "[Epoch 53/1001] [Batch 256/372] [D loss: 0.6907854080200195] [G loss: 0.6672664880752563]\n",
      "[Epoch 53/1001] [Batch 257/372] [D loss: 0.6933070421218872] [G loss: 0.7077284455299377]\n",
      "[Epoch 53/1001] [Batch 258/372] [D loss: 0.6932218074798584] [G loss: 0.6741917133331299]\n",
      "[Epoch 53/1001] [Batch 259/372] [D loss: 0.6909670829772949] [G loss: 0.6930631399154663]\n",
      "[Epoch 53/1001] [Batch 260/372] [D loss: 0.6943204998970032] [G loss: 0.6817943453788757]\n",
      "[Epoch 53/1001] [Batch 261/372] [D loss: 0.6945210099220276] [G loss: 0.6854955554008484]\n",
      "[Epoch 53/1001] [Batch 262/372] [D loss: 0.692162036895752] [G loss: 0.6897825002670288]\n",
      "[Epoch 53/1001] [Batch 263/372] [D loss: 0.6932755708694458] [G loss: 0.680604875087738]\n",
      "[Epoch 53/1001] [Batch 264/372] [D loss: 0.6911196708679199] [G loss: 0.6922902464866638]\n",
      "[Epoch 53/1001] [Batch 265/372] [D loss: 0.6915598511695862] [G loss: 0.6770757436752319]\n",
      "[Epoch 53/1001] [Batch 266/372] [D loss: 0.6931656002998352] [G loss: 0.6972587704658508]\n",
      "[Epoch 53/1001] [Batch 267/372] [D loss: 0.6930307149887085] [G loss: 0.6627265810966492]\n",
      "[Epoch 53/1001] [Batch 268/372] [D loss: 0.6933091282844543] [G loss: 0.7123371362686157]\n",
      "[Epoch 53/1001] [Batch 269/372] [D loss: 0.6913368701934814] [G loss: 0.661393404006958]\n",
      "[Epoch 53/1001] [Batch 270/372] [D loss: 0.6904513835906982] [G loss: 0.7119848132133484]\n",
      "[Epoch 53/1001] [Batch 271/372] [D loss: 0.6918425559997559] [G loss: 0.654453456401825]\n",
      "[Epoch 53/1001] [Batch 272/372] [D loss: 0.6928940415382385] [G loss: 0.7395205497741699]\n",
      "[Epoch 53/1001] [Batch 273/372] [D loss: 0.6941388845443726] [G loss: 0.6210498809814453]\n",
      "[Epoch 53/1001] [Batch 274/372] [D loss: 0.695784866809845] [G loss: 0.7655348777770996]\n",
      "[Epoch 53/1001] [Batch 275/372] [D loss: 0.6943472623825073] [G loss: 0.6043831706047058]\n",
      "[Epoch 53/1001] [Batch 276/372] [D loss: 0.6961339712142944] [G loss: 0.793228268623352]\n",
      "[Epoch 53/1001] [Batch 277/372] [D loss: 0.6968741416931152] [G loss: 0.5776489973068237]\n",
      "[Epoch 53/1001] [Batch 278/372] [D loss: 0.6969155073165894] [G loss: 0.8157516121864319]\n",
      "[Epoch 53/1001] [Batch 279/372] [D loss: 0.699759840965271] [G loss: 0.5686756372451782]\n",
      "[Epoch 53/1001] [Batch 280/372] [D loss: 0.699691116809845] [G loss: 0.8217478394508362]\n",
      "[Epoch 53/1001] [Batch 281/372] [D loss: 0.6998276710510254] [G loss: 0.5690826177597046]\n",
      "[Epoch 53/1001] [Batch 282/372] [D loss: 0.6984810829162598] [G loss: 0.8220150470733643]\n",
      "[Epoch 53/1001] [Batch 283/372] [D loss: 0.700874388217926] [G loss: 0.5805436968803406]\n",
      "[Epoch 53/1001] [Batch 284/372] [D loss: 0.697317898273468] [G loss: 0.7861624956130981]\n",
      "[Epoch 53/1001] [Batch 285/372] [D loss: 0.6969932317733765] [G loss: 0.6160726547241211]\n",
      "[Epoch 53/1001] [Batch 286/372] [D loss: 0.6944476366043091] [G loss: 0.7494598627090454]\n",
      "[Epoch 53/1001] [Batch 287/372] [D loss: 0.6934710741043091] [G loss: 0.6456301808357239]\n",
      "[Epoch 53/1001] [Batch 288/372] [D loss: 0.6926052570343018] [G loss: 0.7148200869560242]\n",
      "[Epoch 53/1001] [Batch 289/372] [D loss: 0.6939011812210083] [G loss: 0.665919303894043]\n",
      "[Epoch 53/1001] [Batch 290/372] [D loss: 0.6929202079772949] [G loss: 0.6949999332427979]\n",
      "[Epoch 53/1001] [Batch 291/372] [D loss: 0.6927257180213928] [G loss: 0.6895634531974792]\n",
      "[Epoch 53/1001] [Batch 292/372] [D loss: 0.6924828290939331] [G loss: 0.676997721195221]\n",
      "[Epoch 53/1001] [Batch 293/372] [D loss: 0.6907005906105042] [G loss: 0.6971453428268433]\n",
      "[Epoch 53/1001] [Batch 294/372] [D loss: 0.6942725777626038] [G loss: 0.6770467758178711]\n",
      "[Epoch 53/1001] [Batch 295/372] [D loss: 0.6927754282951355] [G loss: 0.6924186944961548]\n",
      "[Epoch 53/1001] [Batch 296/372] [D loss: 0.6924815773963928] [G loss: 0.6778896450996399]\n",
      "[Epoch 53/1001] [Batch 297/372] [D loss: 0.6921320557594299] [G loss: 0.6960569620132446]\n",
      "[Epoch 53/1001] [Batch 298/372] [D loss: 0.691190242767334] [G loss: 0.6761716604232788]\n",
      "[Epoch 53/1001] [Batch 299/372] [D loss: 0.6927438378334045] [G loss: 0.7092079520225525]\n",
      "[Epoch 53/1001] [Batch 300/372] [D loss: 0.6911923885345459] [G loss: 0.6609615087509155]\n",
      "[Epoch 53/1001] [Batch 301/372] [D loss: 0.6913950443267822] [G loss: 0.7104873061180115]\n",
      "[Epoch 53/1001] [Batch 302/372] [D loss: 0.6932671070098877] [G loss: 0.6684578061103821]\n",
      "[Epoch 53/1001] [Batch 303/372] [D loss: 0.6914203763008118] [G loss: 0.701238751411438]\n",
      "[Epoch 53/1001] [Batch 304/372] [D loss: 0.6919588446617126] [G loss: 0.6730679273605347]\n",
      "[Epoch 53/1001] [Batch 305/372] [D loss: 0.6910176873207092] [G loss: 0.69334876537323]\n",
      "[Epoch 53/1001] [Batch 306/372] [D loss: 0.6913326382637024] [G loss: 0.6874765157699585]\n",
      "[Epoch 53/1001] [Batch 307/372] [D loss: 0.6918500661849976] [G loss: 0.6762957572937012]\n",
      "[Epoch 53/1001] [Batch 308/372] [D loss: 0.6922701597213745] [G loss: 0.7058310508728027]\n",
      "[Epoch 53/1001] [Batch 309/372] [D loss: 0.6904712319374084] [G loss: 0.6596832871437073]\n",
      "[Epoch 53/1001] [Batch 310/372] [D loss: 0.6926429271697998] [G loss: 0.7160060405731201]\n",
      "[Epoch 53/1001] [Batch 311/372] [D loss: 0.6924824714660645] [G loss: 0.6507376432418823]\n",
      "[Epoch 53/1001] [Batch 312/372] [D loss: 0.6928146481513977] [G loss: 0.7237008213996887]\n",
      "[Epoch 53/1001] [Batch 313/372] [D loss: 0.6937336325645447] [G loss: 0.6476758122444153]\n",
      "[Epoch 53/1001] [Batch 314/372] [D loss: 0.6931096315383911] [G loss: 0.730549156665802]\n",
      "[Epoch 53/1001] [Batch 315/372] [D loss: 0.6935304403305054] [G loss: 0.6381274461746216]\n",
      "[Epoch 53/1001] [Batch 316/372] [D loss: 0.6921392679214478] [G loss: 0.7479180097579956]\n",
      "[Epoch 53/1001] [Batch 317/372] [D loss: 0.696039080619812] [G loss: 0.6154998540878296]\n",
      "[Epoch 53/1001] [Batch 318/372] [D loss: 0.6948448419570923] [G loss: 0.7753905057907104]\n",
      "[Epoch 53/1001] [Batch 319/372] [D loss: 0.6967323422431946] [G loss: 0.5921199321746826]\n",
      "[Epoch 53/1001] [Batch 320/372] [D loss: 0.6966740489006042] [G loss: 0.8072346448898315]\n",
      "[Epoch 53/1001] [Batch 321/372] [D loss: 0.6985818147659302] [G loss: 0.5668773055076599]\n",
      "[Epoch 53/1001] [Batch 322/372] [D loss: 0.69855797290802] [G loss: 0.8383485674858093]\n",
      "[Epoch 53/1001] [Batch 323/372] [D loss: 0.6993945240974426] [G loss: 0.5622376203536987]\n",
      "[Epoch 53/1001] [Batch 324/372] [D loss: 0.7027700543403625] [G loss: 0.8271249532699585]\n",
      "[Epoch 53/1001] [Batch 325/372] [D loss: 0.7006227374076843] [G loss: 0.581600546836853]\n",
      "[Epoch 53/1001] [Batch 326/372] [D loss: 0.6987004280090332] [G loss: 0.7740972638130188]\n",
      "[Epoch 53/1001] [Batch 327/372] [D loss: 0.6953241229057312] [G loss: 0.6331430077552795]\n",
      "[Epoch 53/1001] [Batch 328/372] [D loss: 0.6947020292282104] [G loss: 0.7194858193397522]\n",
      "[Epoch 53/1001] [Batch 329/372] [D loss: 0.6926963329315186] [G loss: 0.6646541357040405]\n",
      "[Epoch 53/1001] [Batch 330/372] [D loss: 0.690488338470459] [G loss: 0.6919973492622375]\n",
      "[Epoch 53/1001] [Batch 331/372] [D loss: 0.6941592693328857] [G loss: 0.6993306279182434]\n",
      "[Epoch 53/1001] [Batch 332/372] [D loss: 0.691569447517395] [G loss: 0.6628264784812927]\n",
      "[Epoch 53/1001] [Batch 333/372] [D loss: 0.6907162666320801] [G loss: 0.7105686664581299]\n",
      "[Epoch 53/1001] [Batch 334/372] [D loss: 0.6940047740936279] [G loss: 0.6629598736763]\n",
      "[Epoch 53/1001] [Batch 335/372] [D loss: 0.6913606524467468] [G loss: 0.706400454044342]\n",
      "[Epoch 53/1001] [Batch 336/372] [D loss: 0.6934177875518799] [G loss: 0.6693770289421082]\n",
      "[Epoch 53/1001] [Batch 337/372] [D loss: 0.6935422420501709] [G loss: 0.7013146877288818]\n",
      "[Epoch 53/1001] [Batch 338/372] [D loss: 0.6932700872421265] [G loss: 0.6683690547943115]\n",
      "[Epoch 53/1001] [Batch 339/372] [D loss: 0.6910862922668457] [G loss: 0.7002583742141724]\n",
      "[Epoch 53/1001] [Batch 340/372] [D loss: 0.6913831830024719] [G loss: 0.6832258105278015]\n",
      "[Epoch 53/1001] [Batch 341/372] [D loss: 0.6919997930526733] [G loss: 0.6911754608154297]\n",
      "[Epoch 53/1001] [Batch 342/372] [D loss: 0.6951489448547363] [G loss: 0.6709779500961304]\n",
      "[Epoch 53/1001] [Batch 343/372] [D loss: 0.6926629543304443] [G loss: 0.7023788690567017]\n",
      "[Epoch 53/1001] [Batch 344/372] [D loss: 0.6927320957183838] [G loss: 0.668538510799408]\n",
      "[Epoch 53/1001] [Batch 345/372] [D loss: 0.6919608116149902] [G loss: 0.7030407786369324]\n",
      "[Epoch 53/1001] [Batch 346/372] [D loss: 0.6920807361602783] [G loss: 0.6727571487426758]\n",
      "[Epoch 53/1001] [Batch 347/372] [D loss: 0.692885160446167] [G loss: 0.7022716999053955]\n",
      "[Epoch 53/1001] [Batch 348/372] [D loss: 0.6917732954025269] [G loss: 0.6662306189537048]\n",
      "[Epoch 53/1001] [Batch 349/372] [D loss: 0.692973256111145] [G loss: 0.7181795835494995]\n",
      "[Epoch 53/1001] [Batch 350/372] [D loss: 0.694934070110321] [G loss: 0.6522145867347717]\n",
      "[Epoch 53/1001] [Batch 351/372] [D loss: 0.6926355361938477] [G loss: 0.7220371961593628]\n",
      "[Epoch 53/1001] [Batch 352/372] [D loss: 0.692764163017273] [G loss: 0.6535605192184448]\n",
      "[Epoch 53/1001] [Batch 353/372] [D loss: 0.6915748119354248] [G loss: 0.722621500492096]\n",
      "[Epoch 53/1001] [Batch 354/372] [D loss: 0.6921170949935913] [G loss: 0.6473056674003601]\n",
      "[Epoch 53/1001] [Batch 355/372] [D loss: 0.6924598217010498] [G loss: 0.7345793843269348]\n",
      "[Epoch 53/1001] [Batch 356/372] [D loss: 0.6948989629745483] [G loss: 0.6316707134246826]\n",
      "[Epoch 53/1001] [Batch 357/372] [D loss: 0.6939440965652466] [G loss: 0.7404154539108276]\n",
      "[Epoch 53/1001] [Batch 358/372] [D loss: 0.6944301128387451] [G loss: 0.6273916363716125]\n",
      "[Epoch 53/1001] [Batch 359/372] [D loss: 0.693898618221283] [G loss: 0.7622161507606506]\n",
      "[Epoch 53/1001] [Batch 360/372] [D loss: 0.6944246292114258] [G loss: 0.6077438592910767]\n",
      "[Epoch 53/1001] [Batch 361/372] [D loss: 0.6950559616088867] [G loss: 0.7773208022117615]\n",
      "[Epoch 53/1001] [Batch 362/372] [D loss: 0.6951122283935547] [G loss: 0.5955190062522888]\n",
      "[Epoch 53/1001] [Batch 363/372] [D loss: 0.6979867219924927] [G loss: 0.8065664172172546]\n",
      "[Epoch 53/1001] [Batch 364/372] [D loss: 0.698327898979187] [G loss: 0.5739843845367432]\n",
      "[Epoch 53/1001] [Batch 365/372] [D loss: 0.7009847164154053] [G loss: 0.8121408224105835]\n",
      "[Epoch 53/1001] [Batch 366/372] [D loss: 0.6978532075881958] [G loss: 0.5886836051940918]\n",
      "[Epoch 53/1001] [Batch 367/372] [D loss: 0.6984193921089172] [G loss: 0.7735889554023743]\n",
      "[Epoch 53/1001] [Batch 368/372] [D loss: 0.6950096487998962] [G loss: 0.6219215393066406]\n",
      "[Epoch 53/1001] [Batch 369/372] [D loss: 0.6943894624710083] [G loss: 0.7427347302436829]\n",
      "[Epoch 53/1001] [Batch 370/372] [D loss: 0.6941461563110352] [G loss: 0.6492559313774109]\n",
      "[Epoch 53/1001] [Batch 371/372] [D loss: 0.690869927406311] [G loss: 0.7071825861930847]\n",
      "[Epoch 54/1001] [Batch 0/372] [D loss: 0.6924581527709961] [G loss: 0.6805355548858643]\n",
      "[Epoch 54/1001] [Batch 1/372] [D loss: 0.6921659708023071] [G loss: 0.6865428686141968]\n",
      "[Epoch 54/1001] [Batch 2/372] [D loss: 0.6931555271148682] [G loss: 0.689367413520813]\n",
      "[Epoch 54/1001] [Batch 3/372] [D loss: 0.6923997402191162] [G loss: 0.6799750328063965]\n",
      "[Epoch 54/1001] [Batch 4/372] [D loss: 0.6918808817863464] [G loss: 0.6875644326210022]\n",
      "[Epoch 54/1001] [Batch 5/372] [D loss: 0.6910146474838257] [G loss: 0.6836179494857788]\n",
      "[Epoch 54/1001] [Batch 6/372] [D loss: 0.6909992694854736] [G loss: 0.6720585823059082]\n",
      "[Epoch 54/1001] [Batch 7/372] [D loss: 0.6921500563621521] [G loss: 0.6899271011352539]\n",
      "[Epoch 54/1001] [Batch 8/372] [D loss: 0.6913754940032959] [G loss: 0.6741538643836975]\n",
      "[Epoch 54/1001] [Batch 9/372] [D loss: 0.6917836666107178] [G loss: 0.6909612417221069]\n",
      "[Epoch 54/1001] [Batch 10/372] [D loss: 0.6927523612976074] [G loss: 0.6737424731254578]\n",
      "[Epoch 54/1001] [Batch 11/372] [D loss: 0.6930534839630127] [G loss: 0.6986290812492371]\n",
      "[Epoch 54/1001] [Batch 12/372] [D loss: 0.6908226013183594] [G loss: 0.6680610179901123]\n",
      "[Epoch 54/1001] [Batch 13/372] [D loss: 0.6918747425079346] [G loss: 0.6996259689331055]\n",
      "[Epoch 54/1001] [Batch 14/372] [D loss: 0.6887340545654297] [G loss: 0.6575404405593872]\n",
      "[Epoch 54/1001] [Batch 15/372] [D loss: 0.6904221773147583] [G loss: 0.7289752960205078]\n",
      "[Epoch 54/1001] [Batch 16/372] [D loss: 0.692069411277771] [G loss: 0.6370616555213928]\n",
      "[Epoch 54/1001] [Batch 17/372] [D loss: 0.6899387836456299] [G loss: 0.7424029111862183]\n",
      "[Epoch 54/1001] [Batch 18/372] [D loss: 0.6913402080535889] [G loss: 0.6265870332717896]\n",
      "[Epoch 54/1001] [Batch 19/372] [D loss: 0.6954153776168823] [G loss: 0.7573957443237305]\n",
      "[Epoch 54/1001] [Batch 20/372] [D loss: 0.6956264972686768] [G loss: 0.6032577157020569]\n",
      "[Epoch 54/1001] [Batch 21/372] [D loss: 0.6941097974777222] [G loss: 0.7787163257598877]\n",
      "[Epoch 54/1001] [Batch 22/372] [D loss: 0.6961886882781982] [G loss: 0.5950629115104675]\n",
      "[Epoch 54/1001] [Batch 23/372] [D loss: 0.6921622157096863] [G loss: 0.7846404910087585]\n",
      "[Epoch 54/1001] [Batch 24/372] [D loss: 0.6944808959960938] [G loss: 0.5910269618034363]\n",
      "[Epoch 54/1001] [Batch 25/372] [D loss: 0.6982736587524414] [G loss: 0.7998458743095398]\n",
      "[Epoch 54/1001] [Batch 26/372] [D loss: 0.6978239417076111] [G loss: 0.5839135646820068]\n",
      "[Epoch 54/1001] [Batch 27/372] [D loss: 0.6985213160514832] [G loss: 0.7848727703094482]\n",
      "[Epoch 54/1001] [Batch 28/372] [D loss: 0.6955230832099915] [G loss: 0.6031231880187988]\n",
      "[Epoch 54/1001] [Batch 29/372] [D loss: 0.6960914134979248] [G loss: 0.7604354619979858]\n",
      "[Epoch 54/1001] [Batch 30/372] [D loss: 0.6932746171951294] [G loss: 0.6136927008628845]\n",
      "[Epoch 54/1001] [Batch 31/372] [D loss: 0.6913505792617798] [G loss: 0.7666191458702087]\n",
      "[Epoch 54/1001] [Batch 32/372] [D loss: 0.693986177444458] [G loss: 0.5913841724395752]\n",
      "[Epoch 54/1001] [Batch 33/372] [D loss: 0.6995935440063477] [G loss: 0.809051513671875]\n",
      "[Epoch 54/1001] [Batch 34/372] [D loss: 0.6996612548828125] [G loss: 0.5646771192550659]\n",
      "[Epoch 54/1001] [Batch 35/372] [D loss: 0.6999065279960632] [G loss: 0.8086737394332886]\n",
      "[Epoch 54/1001] [Batch 36/372] [D loss: 0.6980896592140198] [G loss: 0.5851285457611084]\n",
      "[Epoch 54/1001] [Batch 37/372] [D loss: 0.6971843838691711] [G loss: 0.7735709547996521]\n",
      "[Epoch 54/1001] [Batch 38/372] [D loss: 0.6964623928070068] [G loss: 0.6262610554695129]\n",
      "[Epoch 54/1001] [Batch 39/372] [D loss: 0.6927419304847717] [G loss: 0.7199387550354004]\n",
      "[Epoch 54/1001] [Batch 40/372] [D loss: 0.6920744180679321] [G loss: 0.6645811796188354]\n",
      "[Epoch 54/1001] [Batch 41/372] [D loss: 0.692607045173645] [G loss: 0.6911895871162415]\n",
      "[Epoch 54/1001] [Batch 42/372] [D loss: 0.6919922828674316] [G loss: 0.6805945634841919]\n",
      "[Epoch 54/1001] [Batch 43/372] [D loss: 0.6940990686416626] [G loss: 0.6837466359138489]\n",
      "[Epoch 54/1001] [Batch 44/372] [D loss: 0.6875772476196289] [G loss: 0.6907424926757812]\n",
      "[Epoch 54/1001] [Batch 45/372] [D loss: 0.6910315155982971] [G loss: 0.6770904064178467]\n",
      "[Epoch 54/1001] [Batch 46/372] [D loss: 0.6901979446411133] [G loss: 0.7043446898460388]\n",
      "[Epoch 54/1001] [Batch 47/372] [D loss: 0.6923809051513672] [G loss: 0.6563593149185181]\n",
      "[Epoch 54/1001] [Batch 48/372] [D loss: 0.6928623914718628] [G loss: 0.718870997428894]\n",
      "[Epoch 54/1001] [Batch 49/372] [D loss: 0.6930698752403259] [G loss: 0.6524845361709595]\n",
      "[Epoch 54/1001] [Batch 50/372] [D loss: 0.6927366256713867] [G loss: 0.7156490087509155]\n",
      "[Epoch 54/1001] [Batch 51/372] [D loss: 0.6906232833862305] [G loss: 0.6606389284133911]\n",
      "[Epoch 54/1001] [Batch 52/372] [D loss: 0.6904902458190918] [G loss: 0.7136856317520142]\n",
      "[Epoch 54/1001] [Batch 53/372] [D loss: 0.6917041540145874] [G loss: 0.6565697193145752]\n",
      "[Epoch 54/1001] [Batch 54/372] [D loss: 0.6924155950546265] [G loss: 0.7203714847564697]\n",
      "[Epoch 54/1001] [Batch 55/372] [D loss: 0.6926167011260986] [G loss: 0.6531708836555481]\n",
      "[Epoch 54/1001] [Batch 56/372] [D loss: 0.6915848255157471] [G loss: 0.7163892388343811]\n",
      "[Epoch 54/1001] [Batch 57/372] [D loss: 0.6909078359603882] [G loss: 0.6503204703330994]\n",
      "[Epoch 54/1001] [Batch 58/372] [D loss: 0.692671537399292] [G loss: 0.7282150387763977]\n",
      "[Epoch 54/1001] [Batch 59/372] [D loss: 0.6903581619262695] [G loss: 0.634462833404541]\n",
      "[Epoch 54/1001] [Batch 60/372] [D loss: 0.6923491358757019] [G loss: 0.7716434001922607]\n",
      "[Epoch 54/1001] [Batch 61/372] [D loss: 0.6959698796272278] [G loss: 0.5878422260284424]\n",
      "[Epoch 54/1001] [Batch 62/372] [D loss: 0.6982604265213013] [G loss: 0.7967678308486938]\n",
      "[Epoch 54/1001] [Batch 63/372] [D loss: 0.696287989616394] [G loss: 0.5863920450210571]\n",
      "[Epoch 54/1001] [Batch 64/372] [D loss: 0.6985443234443665] [G loss: 0.7883932590484619]\n",
      "[Epoch 54/1001] [Batch 65/372] [D loss: 0.6974490880966187] [G loss: 0.6009534001350403]\n",
      "[Epoch 54/1001] [Batch 66/372] [D loss: 0.6967184543609619] [G loss: 0.7646797299385071]\n",
      "[Epoch 54/1001] [Batch 67/372] [D loss: 0.6947433948516846] [G loss: 0.6213452816009521]\n",
      "[Epoch 54/1001] [Batch 68/372] [D loss: 0.6933950781822205] [G loss: 0.7407727241516113]\n",
      "[Epoch 54/1001] [Batch 69/372] [D loss: 0.6926312446594238] [G loss: 0.6403038501739502]\n",
      "[Epoch 54/1001] [Batch 70/372] [D loss: 0.693108320236206] [G loss: 0.738730788230896]\n",
      "[Epoch 54/1001] [Batch 71/372] [D loss: 0.691332221031189] [G loss: 0.6311816573143005]\n",
      "[Epoch 54/1001] [Batch 72/372] [D loss: 0.6911174058914185] [G loss: 0.7583779096603394]\n",
      "[Epoch 54/1001] [Batch 73/372] [D loss: 0.6926220655441284] [G loss: 0.6160173416137695]\n",
      "[Epoch 54/1001] [Batch 74/372] [D loss: 0.6949540972709656] [G loss: 0.7734633684158325]\n",
      "[Epoch 54/1001] [Batch 75/372] [D loss: 0.6946942806243896] [G loss: 0.5922817587852478]\n",
      "[Epoch 54/1001] [Batch 76/372] [D loss: 0.697628378868103] [G loss: 0.7894158959388733]\n",
      "[Epoch 54/1001] [Batch 77/372] [D loss: 0.6953391432762146] [G loss: 0.5909273028373718]\n",
      "[Epoch 54/1001] [Batch 78/372] [D loss: 0.6971944570541382] [G loss: 0.7922105193138123]\n",
      "[Epoch 54/1001] [Batch 79/372] [D loss: 0.6946675777435303] [G loss: 0.591708242893219]\n",
      "[Epoch 54/1001] [Batch 80/372] [D loss: 0.6957725286483765] [G loss: 0.778910756111145]\n",
      "[Epoch 54/1001] [Batch 81/372] [D loss: 0.6968069076538086] [G loss: 0.6161669492721558]\n",
      "[Epoch 54/1001] [Batch 82/372] [D loss: 0.694005012512207] [G loss: 0.7466509938240051]\n",
      "[Epoch 54/1001] [Batch 83/372] [D loss: 0.6911448240280151] [G loss: 0.637267529964447]\n",
      "[Epoch 54/1001] [Batch 84/372] [D loss: 0.6945763826370239] [G loss: 0.7285279631614685]\n",
      "[Epoch 54/1001] [Batch 85/372] [D loss: 0.6921741962432861] [G loss: 0.6451848745346069]\n",
      "[Epoch 54/1001] [Batch 86/372] [D loss: 0.6912041902542114] [G loss: 0.7329314947128296]\n",
      "[Epoch 54/1001] [Batch 87/372] [D loss: 0.6917850971221924] [G loss: 0.6404026746749878]\n",
      "[Epoch 54/1001] [Batch 88/372] [D loss: 0.6915113925933838] [G loss: 0.7264025211334229]\n",
      "[Epoch 54/1001] [Batch 89/372] [D loss: 0.6923052072525024] [G loss: 0.6508071422576904]\n",
      "[Epoch 54/1001] [Batch 90/372] [D loss: 0.6926379799842834] [G loss: 0.7234434485435486]\n",
      "[Epoch 54/1001] [Batch 91/372] [D loss: 0.6956844329833984] [G loss: 0.6494491696357727]\n",
      "[Epoch 54/1001] [Batch 92/372] [D loss: 0.6901847720146179] [G loss: 0.7083555459976196]\n",
      "[Epoch 54/1001] [Batch 93/372] [D loss: 0.6933777332305908] [G loss: 0.6732925176620483]\n",
      "[Epoch 54/1001] [Batch 94/372] [D loss: 0.6894981861114502] [G loss: 0.689048171043396]\n",
      "[Epoch 54/1001] [Batch 95/372] [D loss: 0.6917544603347778] [G loss: 0.6859562397003174]\n",
      "[Epoch 54/1001] [Batch 96/372] [D loss: 0.6909885406494141] [G loss: 0.6928505897521973]\n",
      "[Epoch 54/1001] [Batch 97/372] [D loss: 0.6915410161018372] [G loss: 0.6663855910301208]\n",
      "[Epoch 54/1001] [Batch 98/372] [D loss: 0.6931423544883728] [G loss: 0.7024596929550171]\n",
      "[Epoch 54/1001] [Batch 99/372] [D loss: 0.688903272151947] [G loss: 0.6714723706245422]\n",
      "[Epoch 54/1001] [Batch 100/372] [D loss: 0.6887759566307068] [G loss: 0.6921275854110718]\n",
      "[Epoch 54/1001] [Batch 101/372] [D loss: 0.6916265487670898] [G loss: 0.6761031150817871]\n",
      "[Epoch 54/1001] [Batch 102/372] [D loss: 0.6923373937606812] [G loss: 0.6915726661682129]\n",
      "[Epoch 54/1001] [Batch 103/372] [D loss: 0.6915051341056824] [G loss: 0.6795694231987]\n",
      "[Epoch 54/1001] [Batch 104/372] [D loss: 0.6894525289535522] [G loss: 0.6800670027732849]\n",
      "[Epoch 54/1001] [Batch 105/372] [D loss: 0.6905192136764526] [G loss: 0.679412305355072]\n",
      "[Epoch 54/1001] [Batch 106/372] [D loss: 0.69069504737854] [G loss: 0.7174063324928284]\n",
      "[Epoch 54/1001] [Batch 107/372] [D loss: 0.692684531211853] [G loss: 0.6178053021430969]\n",
      "[Epoch 54/1001] [Batch 108/372] [D loss: 0.6948114633560181] [G loss: 0.8029791712760925]\n",
      "[Epoch 54/1001] [Batch 109/372] [D loss: 0.6997188329696655] [G loss: 0.5317403674125671]\n",
      "[Epoch 54/1001] [Batch 110/372] [D loss: 0.7058155536651611] [G loss: 0.9177078604698181]\n",
      "[Epoch 54/1001] [Batch 111/372] [D loss: 0.7095851898193359] [G loss: 0.49050062894821167]\n",
      "[Epoch 54/1001] [Batch 112/372] [D loss: 0.7142703533172607] [G loss: 0.8956007361412048]\n",
      "[Epoch 54/1001] [Batch 113/372] [D loss: 0.7083547711372375] [G loss: 0.5531929135322571]\n",
      "[Epoch 54/1001] [Batch 114/372] [D loss: 0.7016363143920898] [G loss: 0.7779087424278259]\n",
      "[Epoch 54/1001] [Batch 115/372] [D loss: 0.6943174600601196] [G loss: 0.6425253748893738]\n",
      "[Epoch 54/1001] [Batch 116/372] [D loss: 0.6932508945465088] [G loss: 0.6857522130012512]\n",
      "[Epoch 54/1001] [Batch 117/372] [D loss: 0.6938430666923523] [G loss: 0.7068229913711548]\n",
      "[Epoch 54/1001] [Batch 118/372] [D loss: 0.692656397819519] [G loss: 0.6512923240661621]\n",
      "[Epoch 54/1001] [Batch 119/372] [D loss: 0.6933366060256958] [G loss: 0.711957573890686]\n",
      "[Epoch 54/1001] [Batch 120/372] [D loss: 0.6918788552284241] [G loss: 0.6679844260215759]\n",
      "[Epoch 54/1001] [Batch 121/372] [D loss: 0.6933899521827698] [G loss: 0.6876434087753296]\n",
      "[Epoch 54/1001] [Batch 122/372] [D loss: 0.693871796131134] [G loss: 0.6894211769104004]\n",
      "[Epoch 54/1001] [Batch 123/372] [D loss: 0.6930975317955017] [G loss: 0.6804068684577942]\n",
      "[Epoch 54/1001] [Batch 124/372] [D loss: 0.6902420520782471] [G loss: 0.6877154111862183]\n",
      "[Epoch 54/1001] [Batch 125/372] [D loss: 0.6935539245605469] [G loss: 0.6875178217887878]\n",
      "[Epoch 54/1001] [Batch 126/372] [D loss: 0.6910404562950134] [G loss: 0.680695652961731]\n",
      "[Epoch 54/1001] [Batch 127/372] [D loss: 0.6895171999931335] [G loss: 0.6884215474128723]\n",
      "[Epoch 54/1001] [Batch 128/372] [D loss: 0.6907690763473511] [G loss: 0.6878294348716736]\n",
      "[Epoch 54/1001] [Batch 129/372] [D loss: 0.6932748556137085] [G loss: 0.6840490698814392]\n",
      "[Epoch 54/1001] [Batch 130/372] [D loss: 0.690440833568573] [G loss: 0.6778076887130737]\n",
      "[Epoch 54/1001] [Batch 131/372] [D loss: 0.6919896602630615] [G loss: 0.695118248462677]\n",
      "[Epoch 54/1001] [Batch 132/372] [D loss: 0.6896243095397949] [G loss: 0.6693544983863831]\n",
      "[Epoch 54/1001] [Batch 133/372] [D loss: 0.6923096179962158] [G loss: 0.7044986486434937]\n",
      "[Epoch 54/1001] [Batch 134/372] [D loss: 0.692926287651062] [G loss: 0.6603491902351379]\n",
      "[Epoch 54/1001] [Batch 135/372] [D loss: 0.689573347568512] [G loss: 0.7133069038391113]\n",
      "[Epoch 54/1001] [Batch 136/372] [D loss: 0.692927360534668] [G loss: 0.6518515348434448]\n",
      "[Epoch 54/1001] [Batch 137/372] [D loss: 0.6892886161804199] [G loss: 0.7203902006149292]\n",
      "[Epoch 54/1001] [Batch 138/372] [D loss: 0.6940252780914307] [G loss: 0.6508479118347168]\n",
      "[Epoch 54/1001] [Batch 139/372] [D loss: 0.6947089433670044] [G loss: 0.7122257351875305]\n",
      "[Epoch 54/1001] [Batch 140/372] [D loss: 0.69198077917099] [G loss: 0.6538709402084351]\n",
      "[Epoch 54/1001] [Batch 141/372] [D loss: 0.6927539110183716] [G loss: 0.7288272380828857]\n",
      "[Epoch 54/1001] [Batch 142/372] [D loss: 0.69265216588974] [G loss: 0.6339537501335144]\n",
      "[Epoch 54/1001] [Batch 143/372] [D loss: 0.6935188174247742] [G loss: 0.7380189299583435]\n",
      "[Epoch 54/1001] [Batch 144/372] [D loss: 0.6921244859695435] [G loss: 0.6320368647575378]\n",
      "[Epoch 54/1001] [Batch 145/372] [D loss: 0.6938832998275757] [G loss: 0.7559323906898499]\n",
      "[Epoch 54/1001] [Batch 146/372] [D loss: 0.6964887976646423] [G loss: 0.613822340965271]\n",
      "[Epoch 54/1001] [Batch 147/372] [D loss: 0.6955974698066711] [G loss: 0.7587111592292786]\n",
      "[Epoch 54/1001] [Batch 148/372] [D loss: 0.6941612362861633] [G loss: 0.6133098602294922]\n",
      "[Epoch 54/1001] [Batch 149/372] [D loss: 0.6940189599990845] [G loss: 0.7674473524093628]\n",
      "[Epoch 54/1001] [Batch 150/372] [D loss: 0.6914079785346985] [G loss: 0.6121513843536377]\n",
      "[Epoch 54/1001] [Batch 151/372] [D loss: 0.6947264671325684] [G loss: 0.7668052911758423]\n",
      "[Epoch 54/1001] [Batch 152/372] [D loss: 0.6956264972686768] [G loss: 0.6063612103462219]\n",
      "[Epoch 54/1001] [Batch 153/372] [D loss: 0.69573974609375] [G loss: 0.759591281414032]\n",
      "[Epoch 54/1001] [Batch 154/372] [D loss: 0.69501131772995] [G loss: 0.6144662499427795]\n",
      "[Epoch 54/1001] [Batch 155/372] [D loss: 0.693286657333374] [G loss: 0.7471823692321777]\n",
      "[Epoch 54/1001] [Batch 156/372] [D loss: 0.696157693862915] [G loss: 0.6309528350830078]\n",
      "[Epoch 54/1001] [Batch 157/372] [D loss: 0.6920400857925415] [G loss: 0.7458789944648743]\n",
      "[Epoch 54/1001] [Batch 158/372] [D loss: 0.6911540627479553] [G loss: 0.6261662840843201]\n",
      "[Epoch 54/1001] [Batch 159/372] [D loss: 0.6926771402359009] [G loss: 0.7406594753265381]\n",
      "[Epoch 54/1001] [Batch 160/372] [D loss: 0.6939334869384766] [G loss: 0.6324649453163147]\n",
      "[Epoch 54/1001] [Batch 161/372] [D loss: 0.6953861713409424] [G loss: 0.7287065982818604]\n",
      "[Epoch 54/1001] [Batch 162/372] [D loss: 0.6945847868919373] [G loss: 0.645973265171051]\n",
      "[Epoch 54/1001] [Batch 163/372] [D loss: 0.6937252879142761] [G loss: 0.7195475697517395]\n",
      "[Epoch 54/1001] [Batch 164/372] [D loss: 0.694732666015625] [G loss: 0.6428711414337158]\n",
      "[Epoch 54/1001] [Batch 165/372] [D loss: 0.6901030540466309] [G loss: 0.7234205603599548]\n",
      "[Epoch 54/1001] [Batch 166/372] [D loss: 0.6936146020889282] [G loss: 0.6603102087974548]\n",
      "[Epoch 54/1001] [Batch 167/372] [D loss: 0.6910509467124939] [G loss: 0.6859232783317566]\n",
      "[Epoch 54/1001] [Batch 168/372] [D loss: 0.6914893388748169] [G loss: 0.7020719647407532]\n",
      "[Epoch 54/1001] [Batch 169/372] [D loss: 0.689257800579071] [G loss: 0.6538798809051514]\n",
      "[Epoch 54/1001] [Batch 170/372] [D loss: 0.690969705581665] [G loss: 0.7364939451217651]\n",
      "[Epoch 54/1001] [Batch 171/372] [D loss: 0.6929334402084351] [G loss: 0.6285616755485535]\n",
      "[Epoch 54/1001] [Batch 172/372] [D loss: 0.6936941146850586] [G loss: 0.7464139461517334]\n",
      "[Epoch 54/1001] [Batch 173/372] [D loss: 0.694502592086792] [G loss: 0.6239532232284546]\n",
      "[Epoch 54/1001] [Batch 174/372] [D loss: 0.6920473575592041] [G loss: 0.7499961853027344]\n",
      "[Epoch 54/1001] [Batch 175/372] [D loss: 0.6936584711074829] [G loss: 0.6288418173789978]\n",
      "[Epoch 54/1001] [Batch 176/372] [D loss: 0.6929870843887329] [G loss: 0.7437430620193481]\n",
      "[Epoch 54/1001] [Batch 177/372] [D loss: 0.687419056892395] [G loss: 0.6302568912506104]\n",
      "[Epoch 54/1001] [Batch 178/372] [D loss: 0.6921648979187012] [G loss: 0.7514652013778687]\n",
      "[Epoch 54/1001] [Batch 179/372] [D loss: 0.6950957775115967] [G loss: 0.6133174896240234]\n",
      "[Epoch 54/1001] [Batch 180/372] [D loss: 0.6961135268211365] [G loss: 0.7548763751983643]\n",
      "[Epoch 54/1001] [Batch 181/372] [D loss: 0.6936431527137756] [G loss: 0.6242506504058838]\n",
      "[Epoch 54/1001] [Batch 182/372] [D loss: 0.6950284242630005] [G loss: 0.7494485378265381]\n",
      "[Epoch 54/1001] [Batch 183/372] [D loss: 0.6946315765380859] [G loss: 0.6280871629714966]\n",
      "[Epoch 54/1001] [Batch 184/372] [D loss: 0.6918299198150635] [G loss: 0.7434201240539551]\n",
      "[Epoch 54/1001] [Batch 185/372] [D loss: 0.6925472617149353] [G loss: 0.6273373365402222]\n",
      "[Epoch 54/1001] [Batch 186/372] [D loss: 0.691839337348938] [G loss: 0.7506375312805176]\n",
      "[Epoch 54/1001] [Batch 187/372] [D loss: 0.6944495439529419] [G loss: 0.6188474893569946]\n",
      "[Epoch 54/1001] [Batch 188/372] [D loss: 0.6913039684295654] [G loss: 0.7542963624000549]\n",
      "[Epoch 54/1001] [Batch 189/372] [D loss: 0.6952908039093018] [G loss: 0.6140342354774475]\n",
      "[Epoch 54/1001] [Batch 190/372] [D loss: 0.6972280144691467] [G loss: 0.7689392566680908]\n",
      "[Epoch 54/1001] [Batch 191/372] [D loss: 0.6951934099197388] [G loss: 0.6031801700592041]\n",
      "[Epoch 54/1001] [Batch 192/372] [D loss: 0.6945388317108154] [G loss: 0.7717157006263733]\n",
      "[Epoch 54/1001] [Batch 193/372] [D loss: 0.6956713795661926] [G loss: 0.6057519316673279]\n",
      "[Epoch 54/1001] [Batch 194/372] [D loss: 0.6949845552444458] [G loss: 0.7677522897720337]\n",
      "[Epoch 54/1001] [Batch 195/372] [D loss: 0.6971993446350098] [G loss: 0.6119993329048157]\n",
      "[Epoch 54/1001] [Batch 196/372] [D loss: 0.6937921047210693] [G loss: 0.7704135179519653]\n",
      "[Epoch 54/1001] [Batch 197/372] [D loss: 0.6962310671806335] [G loss: 0.5912694931030273]\n",
      "[Epoch 54/1001] [Batch 198/372] [D loss: 0.6981737613677979] [G loss: 0.8089910745620728]\n",
      "[Epoch 54/1001] [Batch 199/372] [D loss: 0.6968353986740112] [G loss: 0.5770586729049683]\n",
      "[Epoch 54/1001] [Batch 200/372] [D loss: 0.6990394592285156] [G loss: 0.7923628091812134]\n",
      "[Epoch 54/1001] [Batch 201/372] [D loss: 0.6971390247344971] [G loss: 0.6053540110588074]\n",
      "[Epoch 54/1001] [Batch 202/372] [D loss: 0.693824291229248] [G loss: 0.7433203458786011]\n",
      "[Epoch 54/1001] [Batch 203/372] [D loss: 0.6926645040512085] [G loss: 0.6537380814552307]\n",
      "[Epoch 54/1001] [Batch 204/372] [D loss: 0.6905055642127991] [G loss: 0.7076500058174133]\n",
      "[Epoch 54/1001] [Batch 205/372] [D loss: 0.6930973529815674] [G loss: 0.6713029146194458]\n",
      "[Epoch 54/1001] [Batch 206/372] [D loss: 0.6915848255157471] [G loss: 0.6962341070175171]\n",
      "[Epoch 54/1001] [Batch 207/372] [D loss: 0.6905128955841064] [G loss: 0.6750689744949341]\n",
      "[Epoch 54/1001] [Batch 208/372] [D loss: 0.6933915615081787] [G loss: 0.7050250768661499]\n",
      "[Epoch 54/1001] [Batch 209/372] [D loss: 0.6909987926483154] [G loss: 0.6618753671646118]\n",
      "[Epoch 54/1001] [Batch 210/372] [D loss: 0.6922897696495056] [G loss: 0.6978935599327087]\n",
      "[Epoch 54/1001] [Batch 211/372] [D loss: 0.6924765110015869] [G loss: 0.6772449016571045]\n",
      "[Epoch 54/1001] [Batch 212/372] [D loss: 0.6930896043777466] [G loss: 0.6993240714073181]\n",
      "[Epoch 54/1001] [Batch 213/372] [D loss: 0.6935768723487854] [G loss: 0.663299024105072]\n",
      "[Epoch 54/1001] [Batch 214/372] [D loss: 0.6917144656181335] [G loss: 0.7116638422012329]\n",
      "[Epoch 54/1001] [Batch 215/372] [D loss: 0.6916909217834473] [G loss: 0.6537623405456543]\n",
      "[Epoch 54/1001] [Batch 216/372] [D loss: 0.6947451829910278] [G loss: 0.7275948524475098]\n",
      "[Epoch 54/1001] [Batch 217/372] [D loss: 0.6908786296844482] [G loss: 0.6370981931686401]\n",
      "[Epoch 54/1001] [Batch 218/372] [D loss: 0.6925832629203796] [G loss: 0.7344481945037842]\n",
      "[Epoch 54/1001] [Batch 219/372] [D loss: 0.6886953115463257] [G loss: 0.6343275308609009]\n",
      "[Epoch 54/1001] [Batch 220/372] [D loss: 0.6902807950973511] [G loss: 0.7586159706115723]\n",
      "[Epoch 54/1001] [Batch 221/372] [D loss: 0.694892168045044] [G loss: 0.6098387241363525]\n",
      "[Epoch 54/1001] [Batch 222/372] [D loss: 0.6960430145263672] [G loss: 0.7747715711593628]\n",
      "[Epoch 54/1001] [Batch 223/372] [D loss: 0.6978346109390259] [G loss: 0.5940355062484741]\n",
      "[Epoch 54/1001] [Batch 224/372] [D loss: 0.6985836625099182] [G loss: 0.7749403119087219]\n",
      "[Epoch 54/1001] [Batch 225/372] [D loss: 0.6964112520217896] [G loss: 0.6125597953796387]\n",
      "[Epoch 54/1001] [Batch 226/372] [D loss: 0.6957838535308838] [G loss: 0.74834805727005]\n",
      "[Epoch 54/1001] [Batch 227/372] [D loss: 0.6948812007904053] [G loss: 0.6291948556900024]\n",
      "[Epoch 54/1001] [Batch 228/372] [D loss: 0.6920127272605896] [G loss: 0.7364544868469238]\n",
      "[Epoch 54/1001] [Batch 229/372] [D loss: 0.6897159814834595] [G loss: 0.6344149708747864]\n",
      "[Epoch 54/1001] [Batch 230/372] [D loss: 0.6946792006492615] [G loss: 0.7472710609436035]\n",
      "[Epoch 54/1001] [Batch 231/372] [D loss: 0.6937670707702637] [G loss: 0.6229537725448608]\n",
      "[Epoch 54/1001] [Batch 232/372] [D loss: 0.6935333013534546] [G loss: 0.7403815388679504]\n",
      "[Epoch 54/1001] [Batch 233/372] [D loss: 0.6923183798789978] [G loss: 0.6392399668693542]\n",
      "[Epoch 54/1001] [Batch 234/372] [D loss: 0.6878150701522827] [G loss: 0.7217869758605957]\n",
      "[Epoch 54/1001] [Batch 235/372] [D loss: 0.6913585066795349] [G loss: 0.6553499102592468]\n",
      "[Epoch 54/1001] [Batch 236/372] [D loss: 0.6916495561599731] [G loss: 0.7040890455245972]\n",
      "[Epoch 54/1001] [Batch 237/372] [D loss: 0.6950491666793823] [G loss: 0.6778532266616821]\n",
      "[Epoch 54/1001] [Batch 238/372] [D loss: 0.692063570022583] [G loss: 0.6780785918235779]\n",
      "[Epoch 54/1001] [Batch 239/372] [D loss: 0.6922874450683594] [G loss: 0.6922978758811951]\n",
      "[Epoch 54/1001] [Batch 240/372] [D loss: 0.6921645402908325] [G loss: 0.6617235541343689]\n",
      "[Epoch 54/1001] [Batch 241/372] [D loss: 0.6911208629608154] [G loss: 0.7204505801200867]\n",
      "[Epoch 54/1001] [Batch 242/372] [D loss: 0.6926406621932983] [G loss: 0.6378535628318787]\n",
      "[Epoch 54/1001] [Batch 243/372] [D loss: 0.6949577927589417] [G loss: 0.7421630024909973]\n",
      "[Epoch 54/1001] [Batch 244/372] [D loss: 0.695188045501709] [G loss: 0.6253184676170349]\n",
      "[Epoch 54/1001] [Batch 245/372] [D loss: 0.6945963501930237] [G loss: 0.7435494661331177]\n",
      "[Epoch 54/1001] [Batch 246/372] [D loss: 0.6959210634231567] [G loss: 0.6336819529533386]\n",
      "[Epoch 54/1001] [Batch 247/372] [D loss: 0.692683219909668] [G loss: 0.7260439395904541]\n",
      "[Epoch 54/1001] [Batch 248/372] [D loss: 0.6939937472343445] [G loss: 0.6539023518562317]\n",
      "[Epoch 54/1001] [Batch 249/372] [D loss: 0.691346287727356] [G loss: 0.7148657441139221]\n",
      "[Epoch 54/1001] [Batch 250/372] [D loss: 0.6923227310180664] [G loss: 0.6607771515846252]\n",
      "[Epoch 54/1001] [Batch 251/372] [D loss: 0.6904295086860657] [G loss: 0.6966798305511475]\n",
      "[Epoch 54/1001] [Batch 252/372] [D loss: 0.6894729137420654] [G loss: 0.6776871681213379]\n",
      "[Epoch 54/1001] [Batch 253/372] [D loss: 0.6957313418388367] [G loss: 0.6994348764419556]\n",
      "[Epoch 54/1001] [Batch 254/372] [D loss: 0.6924887895584106] [G loss: 0.669917643070221]\n",
      "[Epoch 54/1001] [Batch 255/372] [D loss: 0.693584680557251] [G loss: 0.6951136589050293]\n",
      "[Epoch 54/1001] [Batch 256/372] [D loss: 0.6937626600265503] [G loss: 0.6708043217658997]\n",
      "[Epoch 54/1001] [Batch 257/372] [D loss: 0.692283034324646] [G loss: 0.6982311010360718]\n",
      "[Epoch 54/1001] [Batch 258/372] [D loss: 0.6919040679931641] [G loss: 0.6696808338165283]\n",
      "[Epoch 54/1001] [Batch 259/372] [D loss: 0.6896452903747559] [G loss: 0.6954705715179443]\n",
      "[Epoch 54/1001] [Batch 260/372] [D loss: 0.6896755695343018] [G loss: 0.6822769045829773]\n",
      "[Epoch 54/1001] [Batch 261/372] [D loss: 0.6929736137390137] [G loss: 0.6787512302398682]\n",
      "[Epoch 54/1001] [Batch 262/372] [D loss: 0.6930120587348938] [G loss: 0.7035815119743347]\n",
      "[Epoch 54/1001] [Batch 263/372] [D loss: 0.6927000284194946] [G loss: 0.6538817286491394]\n",
      "[Epoch 54/1001] [Batch 264/372] [D loss: 0.6943408846855164] [G loss: 0.7340958118438721]\n",
      "[Epoch 54/1001] [Batch 265/372] [D loss: 0.6947252154350281] [G loss: 0.611747682094574]\n",
      "[Epoch 54/1001] [Batch 266/372] [D loss: 0.6930994391441345] [G loss: 0.794039785861969]\n",
      "[Epoch 54/1001] [Batch 267/372] [D loss: 0.6960575580596924] [G loss: 0.561903715133667]\n",
      "[Epoch 54/1001] [Batch 268/372] [D loss: 0.6984921097755432] [G loss: 0.862016499042511]\n",
      "[Epoch 54/1001] [Batch 269/372] [D loss: 0.7040739059448242] [G loss: 0.5188345313072205]\n",
      "[Epoch 54/1001] [Batch 270/372] [D loss: 0.7112497091293335] [G loss: 0.8931048512458801]\n",
      "[Epoch 54/1001] [Batch 271/372] [D loss: 0.7122088074684143] [G loss: 0.5225058197975159]\n",
      "[Epoch 54/1001] [Batch 272/372] [D loss: 0.7075343728065491] [G loss: 0.8350934386253357]\n",
      "[Epoch 54/1001] [Batch 273/372] [D loss: 0.7021940350532532] [G loss: 0.6008313298225403]\n",
      "[Epoch 54/1001] [Batch 274/372] [D loss: 0.6963173151016235] [G loss: 0.7322695851325989]\n",
      "[Epoch 54/1001] [Batch 275/372] [D loss: 0.6943883299827576] [G loss: 0.671194851398468]\n",
      "[Epoch 54/1001] [Batch 276/372] [D loss: 0.6921234130859375] [G loss: 0.6833849549293518]\n",
      "[Epoch 54/1001] [Batch 277/372] [D loss: 0.6927676200866699] [G loss: 0.6884687542915344]\n",
      "[Epoch 54/1001] [Batch 278/372] [D loss: 0.6910349726676941] [G loss: 0.6744553446769714]\n",
      "[Epoch 54/1001] [Batch 279/372] [D loss: 0.6936575174331665] [G loss: 0.6956653594970703]\n",
      "[Epoch 54/1001] [Batch 280/372] [D loss: 0.6945695877075195] [G loss: 0.6842078566551208]\n",
      "[Epoch 54/1001] [Batch 281/372] [D loss: 0.6922534704208374] [G loss: 0.6753355264663696]\n",
      "[Epoch 54/1001] [Batch 282/372] [D loss: 0.6916957497596741] [G loss: 0.6988465785980225]\n",
      "[Epoch 54/1001] [Batch 283/372] [D loss: 0.6914385557174683] [G loss: 0.6788437962532043]\n",
      "[Epoch 54/1001] [Batch 284/372] [D loss: 0.6921870708465576] [G loss: 0.6824589967727661]\n",
      "[Epoch 54/1001] [Batch 285/372] [D loss: 0.6912539005279541] [G loss: 0.6972834467887878]\n",
      "[Epoch 54/1001] [Batch 286/372] [D loss: 0.689706027507782] [G loss: 0.67006915807724]\n",
      "[Epoch 54/1001] [Batch 287/372] [D loss: 0.6925613284111023] [G loss: 0.7085017561912537]\n",
      "[Epoch 54/1001] [Batch 288/372] [D loss: 0.6950612664222717] [G loss: 0.6605848073959351]\n",
      "[Epoch 54/1001] [Batch 289/372] [D loss: 0.692772388458252] [G loss: 0.7067962884902954]\n",
      "[Epoch 54/1001] [Batch 290/372] [D loss: 0.6922250986099243] [G loss: 0.6612972021102905]\n",
      "[Epoch 54/1001] [Batch 291/372] [D loss: 0.6915539503097534] [G loss: 0.7048661112785339]\n",
      "[Epoch 54/1001] [Batch 292/372] [D loss: 0.6907958388328552] [G loss: 0.670356273651123]\n",
      "[Epoch 54/1001] [Batch 293/372] [D loss: 0.6905286312103271] [G loss: 0.6920753121376038]\n",
      "[Epoch 54/1001] [Batch 294/372] [D loss: 0.6931803226470947] [G loss: 0.6827941536903381]\n",
      "[Epoch 54/1001] [Batch 295/372] [D loss: 0.6895914673805237] [G loss: 0.6866639852523804]\n",
      "[Epoch 54/1001] [Batch 296/372] [D loss: 0.6893224120140076] [G loss: 0.6818736791610718]\n",
      "[Epoch 54/1001] [Batch 297/372] [D loss: 0.6884174942970276] [G loss: 0.7044363021850586]\n",
      "[Epoch 54/1001] [Batch 298/372] [D loss: 0.6930917501449585] [G loss: 0.6741580367088318]\n",
      "[Epoch 54/1001] [Batch 299/372] [D loss: 0.6914646625518799] [G loss: 0.6959629058837891]\n",
      "[Epoch 54/1001] [Batch 300/372] [D loss: 0.6910112500190735] [G loss: 0.6674253940582275]\n",
      "[Epoch 54/1001] [Batch 301/372] [D loss: 0.6927825212478638] [G loss: 0.6980199813842773]\n",
      "[Epoch 54/1001] [Batch 302/372] [D loss: 0.6935828924179077] [G loss: 0.6711217761039734]\n",
      "[Epoch 54/1001] [Batch 303/372] [D loss: 0.6930935978889465] [G loss: 0.70505690574646]\n",
      "[Epoch 54/1001] [Batch 304/372] [D loss: 0.6920013427734375] [G loss: 0.6611664295196533]\n",
      "[Epoch 54/1001] [Batch 305/372] [D loss: 0.6936540603637695] [G loss: 0.7085610628128052]\n",
      "[Epoch 54/1001] [Batch 306/372] [D loss: 0.691260576248169] [G loss: 0.6656270027160645]\n",
      "[Epoch 54/1001] [Batch 307/372] [D loss: 0.6911429166793823] [G loss: 0.7096441388130188]\n",
      "[Epoch 54/1001] [Batch 308/372] [D loss: 0.6908133625984192] [G loss: 0.6642354130744934]\n",
      "[Epoch 54/1001] [Batch 309/372] [D loss: 0.6915192008018494] [G loss: 0.7098026275634766]\n",
      "[Epoch 54/1001] [Batch 310/372] [D loss: 0.6919327974319458] [G loss: 0.6564492583274841]\n",
      "[Epoch 54/1001] [Batch 311/372] [D loss: 0.6910597681999207] [G loss: 0.7223471403121948]\n",
      "[Epoch 54/1001] [Batch 312/372] [D loss: 0.6930272579193115] [G loss: 0.6389583349227905]\n",
      "[Epoch 54/1001] [Batch 313/372] [D loss: 0.6933892965316772] [G loss: 0.7394459247589111]\n",
      "[Epoch 54/1001] [Batch 314/372] [D loss: 0.6915518641471863] [G loss: 0.6213455200195312]\n",
      "[Epoch 54/1001] [Batch 315/372] [D loss: 0.6914396286010742] [G loss: 0.7768698334693909]\n",
      "[Epoch 54/1001] [Batch 316/372] [D loss: 0.693278431892395] [G loss: 0.5903695821762085]\n",
      "[Epoch 54/1001] [Batch 317/372] [D loss: 0.6990553140640259] [G loss: 0.8124431371688843]\n",
      "[Epoch 54/1001] [Batch 318/372] [D loss: 0.7003728151321411] [G loss: 0.5594192147254944]\n",
      "[Epoch 54/1001] [Batch 319/372] [D loss: 0.7014044523239136] [G loss: 0.8411105871200562]\n",
      "[Epoch 54/1001] [Batch 320/372] [D loss: 0.7000287771224976] [G loss: 0.5618983507156372]\n",
      "[Epoch 54/1001] [Batch 321/372] [D loss: 0.7012187242507935] [G loss: 0.8016711473464966]\n",
      "[Epoch 54/1001] [Batch 322/372] [D loss: 0.6995235681533813] [G loss: 0.603120744228363]\n",
      "[Epoch 54/1001] [Batch 323/372] [D loss: 0.6965757608413696] [G loss: 0.7474521398544312]\n",
      "[Epoch 54/1001] [Batch 324/372] [D loss: 0.6928685903549194] [G loss: 0.6426146030426025]\n",
      "[Epoch 54/1001] [Batch 325/372] [D loss: 0.6931906938552856] [G loss: 0.7088395357131958]\n",
      "[Epoch 54/1001] [Batch 326/372] [D loss: 0.6900323629379272] [G loss: 0.6685224175453186]\n",
      "[Epoch 54/1001] [Batch 327/372] [D loss: 0.6932927370071411] [G loss: 0.7084072828292847]\n",
      "[Epoch 54/1001] [Batch 328/372] [D loss: 0.6919869184494019] [G loss: 0.6560618281364441]\n",
      "[Epoch 54/1001] [Batch 329/372] [D loss: 0.691042423248291] [G loss: 0.719352126121521]\n",
      "[Epoch 54/1001] [Batch 330/372] [D loss: 0.6919448375701904] [G loss: 0.6516362428665161]\n",
      "[Epoch 54/1001] [Batch 331/372] [D loss: 0.6926552653312683] [G loss: 0.7174481749534607]\n",
      "[Epoch 54/1001] [Batch 332/372] [D loss: 0.6931769847869873] [G loss: 0.6546350121498108]\n",
      "[Epoch 54/1001] [Batch 333/372] [D loss: 0.6932501196861267] [G loss: 0.7132048010826111]\n",
      "[Epoch 54/1001] [Batch 334/372] [D loss: 0.6936748623847961] [G loss: 0.6620057225227356]\n",
      "[Epoch 54/1001] [Batch 335/372] [D loss: 0.6921038627624512] [G loss: 0.699918270111084]\n",
      "[Epoch 54/1001] [Batch 336/372] [D loss: 0.6910732984542847] [G loss: 0.6762586236000061]\n",
      "[Epoch 54/1001] [Batch 337/372] [D loss: 0.6911002397537231] [G loss: 0.6840434074401855]\n",
      "[Epoch 54/1001] [Batch 338/372] [D loss: 0.6909006237983704] [G loss: 0.689399242401123]\n",
      "[Epoch 54/1001] [Batch 339/372] [D loss: 0.6925269961357117] [G loss: 0.6719338893890381]\n",
      "[Epoch 54/1001] [Batch 340/372] [D loss: 0.6907903552055359] [G loss: 0.701431393623352]\n",
      "[Epoch 54/1001] [Batch 341/372] [D loss: 0.6923580169677734] [G loss: 0.6703842878341675]\n",
      "[Epoch 54/1001] [Batch 342/372] [D loss: 0.6924796104431152] [G loss: 0.6999582052230835]\n",
      "[Epoch 54/1001] [Batch 343/372] [D loss: 0.694308876991272] [G loss: 0.6762939095497131]\n",
      "[Epoch 54/1001] [Batch 344/372] [D loss: 0.6925691366195679] [G loss: 0.6886513233184814]\n",
      "[Epoch 54/1001] [Batch 345/372] [D loss: 0.6918612718582153] [G loss: 0.6867108345031738]\n",
      "[Epoch 54/1001] [Batch 346/372] [D loss: 0.6892340183258057] [G loss: 0.6819824576377869]\n",
      "[Epoch 54/1001] [Batch 347/372] [D loss: 0.6924548745155334] [G loss: 0.6878218054771423]\n",
      "[Epoch 54/1001] [Batch 348/372] [D loss: 0.6926596164703369] [G loss: 0.6787252426147461]\n",
      "[Epoch 54/1001] [Batch 349/372] [D loss: 0.6919832229614258] [G loss: 0.6876267790794373]\n",
      "[Epoch 54/1001] [Batch 350/372] [D loss: 0.6927989721298218] [G loss: 0.6909933090209961]\n",
      "[Epoch 54/1001] [Batch 351/372] [D loss: 0.6904929876327515] [G loss: 0.6743659377098083]\n",
      "[Epoch 54/1001] [Batch 352/372] [D loss: 0.6904261112213135] [G loss: 0.6997722387313843]\n",
      "[Epoch 54/1001] [Batch 353/372] [D loss: 0.6926673650741577] [G loss: 0.6721312999725342]\n",
      "[Epoch 54/1001] [Batch 354/372] [D loss: 0.6889091730117798] [G loss: 0.6906962394714355]\n",
      "[Epoch 54/1001] [Batch 355/372] [D loss: 0.6891514658927917] [G loss: 0.6856895685195923]\n",
      "[Epoch 54/1001] [Batch 356/372] [D loss: 0.6921253204345703] [G loss: 0.7038956880569458]\n",
      "[Epoch 54/1001] [Batch 357/372] [D loss: 0.6931864023208618] [G loss: 0.6577977538108826]\n",
      "[Epoch 54/1001] [Batch 358/372] [D loss: 0.6937804222106934] [G loss: 0.7115893959999084]\n",
      "[Epoch 54/1001] [Batch 359/372] [D loss: 0.692016065120697] [G loss: 0.6451415419578552]\n",
      "[Epoch 54/1001] [Batch 360/372] [D loss: 0.6912974119186401] [G loss: 0.7262076139450073]\n",
      "[Epoch 54/1001] [Batch 361/372] [D loss: 0.6904996633529663] [G loss: 0.6381563544273376]\n",
      "[Epoch 54/1001] [Batch 362/372] [D loss: 0.6907976865768433] [G loss: 0.7558383345603943]\n",
      "[Epoch 54/1001] [Batch 363/372] [D loss: 0.6980953216552734] [G loss: 0.5931263566017151]\n",
      "[Epoch 54/1001] [Batch 364/372] [D loss: 0.697993278503418] [G loss: 0.7910064458847046]\n",
      "[Epoch 54/1001] [Batch 365/372] [D loss: 0.6970111131668091] [G loss: 0.5789569616317749]\n",
      "[Epoch 54/1001] [Batch 366/372] [D loss: 0.7000906467437744] [G loss: 0.8311688899993896]\n",
      "[Epoch 54/1001] [Batch 367/372] [D loss: 0.7031362652778625] [G loss: 0.5467715263366699]\n",
      "[Epoch 54/1001] [Batch 368/372] [D loss: 0.7035790681838989] [G loss: 0.8404414057731628]\n",
      "[Epoch 54/1001] [Batch 369/372] [D loss: 0.7015079259872437] [G loss: 0.5718905329704285]\n",
      "[Epoch 54/1001] [Batch 370/372] [D loss: 0.6994298100471497] [G loss: 0.7826002836227417]\n",
      "[Epoch 54/1001] [Batch 371/372] [D loss: 0.6950353384017944] [G loss: 0.6162604689598083]\n",
      "[Epoch 55/1001] [Batch 0/372] [D loss: 0.6938462257385254] [G loss: 0.7308119535446167]\n",
      "[Epoch 55/1001] [Batch 1/372] [D loss: 0.6895928382873535] [G loss: 0.668400764465332]\n",
      "[Epoch 55/1001] [Batch 2/372] [D loss: 0.6907400488853455] [G loss: 0.6898587942123413]\n",
      "[Epoch 55/1001] [Batch 3/372] [D loss: 0.692324161529541] [G loss: 0.6916708946228027]\n",
      "[Epoch 55/1001] [Batch 4/372] [D loss: 0.6893359422683716] [G loss: 0.6660709977149963]\n",
      "[Epoch 55/1001] [Batch 5/372] [D loss: 0.690276026725769] [G loss: 0.7125787138938904]\n",
      "[Epoch 55/1001] [Batch 6/372] [D loss: 0.6914597749710083] [G loss: 0.6553636193275452]\n",
      "[Epoch 55/1001] [Batch 7/372] [D loss: 0.6922290921211243] [G loss: 0.7199447154998779]\n",
      "[Epoch 55/1001] [Batch 8/372] [D loss: 0.6913537383079529] [G loss: 0.6589535474777222]\n",
      "[Epoch 55/1001] [Batch 9/372] [D loss: 0.6897855401039124] [G loss: 0.6991413831710815]\n",
      "[Epoch 55/1001] [Batch 10/372] [D loss: 0.6889408826828003] [G loss: 0.681147575378418]\n",
      "[Epoch 55/1001] [Batch 11/372] [D loss: 0.6922440528869629] [G loss: 0.6778597235679626]\n",
      "[Epoch 55/1001] [Batch 12/372] [D loss: 0.6932454109191895] [G loss: 0.6943688988685608]\n",
      "[Epoch 55/1001] [Batch 13/372] [D loss: 0.6887927651405334] [G loss: 0.6805512309074402]\n",
      "[Epoch 55/1001] [Batch 14/372] [D loss: 0.6889185905456543] [G loss: 0.6880208253860474]\n",
      "[Epoch 55/1001] [Batch 15/372] [D loss: 0.6912260055541992] [G loss: 0.6838865280151367]\n",
      "[Epoch 55/1001] [Batch 16/372] [D loss: 0.6926279067993164] [G loss: 0.6811778545379639]\n",
      "[Epoch 55/1001] [Batch 17/372] [D loss: 0.6916341781616211] [G loss: 0.6973206996917725]\n",
      "[Epoch 55/1001] [Batch 18/372] [D loss: 0.6924537420272827] [G loss: 0.6569854021072388]\n",
      "[Epoch 55/1001] [Batch 19/372] [D loss: 0.6910741329193115] [G loss: 0.7226987481117249]\n",
      "[Epoch 55/1001] [Batch 20/372] [D loss: 0.6925635933876038] [G loss: 0.6469666957855225]\n",
      "[Epoch 55/1001] [Batch 21/372] [D loss: 0.6898682117462158] [G loss: 0.731115460395813]\n",
      "[Epoch 55/1001] [Batch 22/372] [D loss: 0.6902636289596558] [G loss: 0.6383339166641235]\n",
      "[Epoch 55/1001] [Batch 23/372] [D loss: 0.6916961669921875] [G loss: 0.737290620803833]\n",
      "[Epoch 55/1001] [Batch 24/372] [D loss: 0.6944932341575623] [G loss: 0.6283226609230042]\n",
      "[Epoch 55/1001] [Batch 25/372] [D loss: 0.6917005777359009] [G loss: 0.7473627328872681]\n",
      "[Epoch 55/1001] [Batch 26/372] [D loss: 0.6934001445770264] [G loss: 0.630497932434082]\n",
      "[Epoch 55/1001] [Batch 27/372] [D loss: 0.6945396065711975] [G loss: 0.7350412607192993]\n",
      "[Epoch 55/1001] [Batch 28/372] [D loss: 0.6911218166351318] [G loss: 0.6370123028755188]\n",
      "[Epoch 55/1001] [Batch 29/372] [D loss: 0.6911358833312988] [G loss: 0.7416933178901672]\n",
      "[Epoch 55/1001] [Batch 30/372] [D loss: 0.6925531029701233] [G loss: 0.6217159628868103]\n",
      "[Epoch 55/1001] [Batch 31/372] [D loss: 0.691173255443573] [G loss: 0.7837945818901062]\n",
      "[Epoch 55/1001] [Batch 32/372] [D loss: 0.6964521408081055] [G loss: 0.5798392295837402]\n",
      "[Epoch 55/1001] [Batch 33/372] [D loss: 0.6958366632461548] [G loss: 0.8105817437171936]\n",
      "[Epoch 55/1001] [Batch 34/372] [D loss: 0.6970920562744141] [G loss: 0.5752986669540405]\n",
      "[Epoch 55/1001] [Batch 35/372] [D loss: 0.6971985101699829] [G loss: 0.7931150197982788]\n",
      "[Epoch 55/1001] [Batch 36/372] [D loss: 0.697996973991394] [G loss: 0.6021239757537842]\n",
      "[Epoch 55/1001] [Batch 37/372] [D loss: 0.6980639100074768] [G loss: 0.7634474635124207]\n",
      "[Epoch 55/1001] [Batch 38/372] [D loss: 0.6937256455421448] [G loss: 0.6224560737609863]\n",
      "[Epoch 55/1001] [Batch 39/372] [D loss: 0.6936039924621582] [G loss: 0.7293056845664978]\n",
      "[Epoch 55/1001] [Batch 40/372] [D loss: 0.6917507648468018] [G loss: 0.6555849313735962]\n",
      "[Epoch 55/1001] [Batch 41/372] [D loss: 0.693550169467926] [G loss: 0.7043448090553284]\n",
      "[Epoch 55/1001] [Batch 42/372] [D loss: 0.6896225214004517] [G loss: 0.6701112985610962]\n",
      "[Epoch 55/1001] [Batch 43/372] [D loss: 0.6911574006080627] [G loss: 0.7057755589485168]\n",
      "[Epoch 55/1001] [Batch 44/372] [D loss: 0.692456841468811] [G loss: 0.6638299226760864]\n",
      "[Epoch 55/1001] [Batch 45/372] [D loss: 0.6921086311340332] [G loss: 0.7025856971740723]\n",
      "[Epoch 55/1001] [Batch 46/372] [D loss: 0.6923954486846924] [G loss: 0.6691672801971436]\n",
      "[Epoch 55/1001] [Batch 47/372] [D loss: 0.6901984214782715] [G loss: 0.6966757774353027]\n",
      "[Epoch 55/1001] [Batch 48/372] [D loss: 0.6913718581199646] [G loss: 0.6761583089828491]\n",
      "[Epoch 55/1001] [Batch 49/372] [D loss: 0.6930556297302246] [G loss: 0.6821414828300476]\n",
      "[Epoch 55/1001] [Batch 50/372] [D loss: 0.693217396736145] [G loss: 0.6943371295928955]\n",
      "[Epoch 55/1001] [Batch 51/372] [D loss: 0.6907052993774414] [G loss: 0.664695680141449]\n",
      "[Epoch 55/1001] [Batch 52/372] [D loss: 0.6931606531143188] [G loss: 0.71993088722229]\n",
      "[Epoch 55/1001] [Batch 53/372] [D loss: 0.6911997199058533] [G loss: 0.6459112167358398]\n",
      "[Epoch 55/1001] [Batch 54/372] [D loss: 0.6939001083374023] [G loss: 0.7227538228034973]\n",
      "[Epoch 55/1001] [Batch 55/372] [D loss: 0.6904399394989014] [G loss: 0.6469144821166992]\n",
      "[Epoch 55/1001] [Batch 56/372] [D loss: 0.6918267011642456] [G loss: 0.7256351113319397]\n",
      "[Epoch 55/1001] [Batch 57/372] [D loss: 0.6900192499160767] [G loss: 0.6316341757774353]\n",
      "[Epoch 55/1001] [Batch 58/372] [D loss: 0.6948733329772949] [G loss: 0.7653307914733887]\n",
      "[Epoch 55/1001] [Batch 59/372] [D loss: 0.6938560009002686] [G loss: 0.5901713371276855]\n",
      "[Epoch 55/1001] [Batch 60/372] [D loss: 0.6948119401931763] [G loss: 0.8155946731567383]\n",
      "[Epoch 55/1001] [Batch 61/372] [D loss: 0.6973204612731934] [G loss: 0.5506519079208374]\n",
      "[Epoch 55/1001] [Batch 62/372] [D loss: 0.7037498354911804] [G loss: 0.8629268407821655]\n",
      "[Epoch 55/1001] [Batch 63/372] [D loss: 0.7037846446037292] [G loss: 0.5251702070236206]\n",
      "[Epoch 55/1001] [Batch 64/372] [D loss: 0.7049742937088013] [G loss: 0.8666805624961853]\n",
      "[Epoch 55/1001] [Batch 65/372] [D loss: 0.7076944708824158] [G loss: 0.54941326379776]\n",
      "[Epoch 55/1001] [Batch 66/372] [D loss: 0.7020375728607178] [G loss: 0.8167459964752197]\n",
      "[Epoch 55/1001] [Batch 67/372] [D loss: 0.6995102167129517] [G loss: 0.5934195518493652]\n",
      "[Epoch 55/1001] [Batch 68/372] [D loss: 0.6950087547302246] [G loss: 0.7497214078903198]\n",
      "[Epoch 55/1001] [Batch 69/372] [D loss: 0.6913779973983765] [G loss: 0.6463202238082886]\n",
      "[Epoch 55/1001] [Batch 70/372] [D loss: 0.6951033473014832] [G loss: 0.7100851535797119]\n",
      "[Epoch 55/1001] [Batch 71/372] [D loss: 0.6923134326934814] [G loss: 0.6590877771377563]\n",
      "[Epoch 55/1001] [Batch 72/372] [D loss: 0.6927368640899658] [G loss: 0.7033261656761169]\n",
      "[Epoch 55/1001] [Batch 73/372] [D loss: 0.690911591053009] [G loss: 0.6767358183860779]\n",
      "[Epoch 55/1001] [Batch 74/372] [D loss: 0.6915298700332642] [G loss: 0.6950511932373047]\n",
      "[Epoch 55/1001] [Batch 75/372] [D loss: 0.6907230615615845] [G loss: 0.675277829170227]\n",
      "[Epoch 55/1001] [Batch 76/372] [D loss: 0.6913264393806458] [G loss: 0.6818110346794128]\n",
      "[Epoch 55/1001] [Batch 77/372] [D loss: 0.6898367404937744] [G loss: 0.6960833072662354]\n",
      "[Epoch 55/1001] [Batch 78/372] [D loss: 0.6914848685264587] [G loss: 0.6717742085456848]\n",
      "[Epoch 55/1001] [Batch 79/372] [D loss: 0.6913126707077026] [G loss: 0.6984250545501709]\n",
      "[Epoch 55/1001] [Batch 80/372] [D loss: 0.6923643350601196] [G loss: 0.6713030338287354]\n",
      "[Epoch 55/1001] [Batch 81/372] [D loss: 0.6889171004295349] [G loss: 0.7000117897987366]\n",
      "[Epoch 55/1001] [Batch 82/372] [D loss: 0.6902018189430237] [G loss: 0.6666210293769836]\n",
      "[Epoch 55/1001] [Batch 83/372] [D loss: 0.6913732290267944] [G loss: 0.7128363847732544]\n",
      "[Epoch 55/1001] [Batch 84/372] [D loss: 0.6903156042098999] [G loss: 0.6619781255722046]\n",
      "[Epoch 55/1001] [Batch 85/372] [D loss: 0.6911247968673706] [G loss: 0.6966222524642944]\n",
      "[Epoch 55/1001] [Batch 86/372] [D loss: 0.6881615519523621] [G loss: 0.6716749668121338]\n",
      "[Epoch 55/1001] [Batch 87/372] [D loss: 0.6890639066696167] [G loss: 0.7143473625183105]\n",
      "[Epoch 55/1001] [Batch 88/372] [D loss: 0.6944043636322021] [G loss: 0.6512441039085388]\n",
      "[Epoch 55/1001] [Batch 89/372] [D loss: 0.6924643516540527] [G loss: 0.7146613597869873]\n",
      "[Epoch 55/1001] [Batch 90/372] [D loss: 0.6929706335067749] [G loss: 0.6518627405166626]\n",
      "[Epoch 55/1001] [Batch 91/372] [D loss: 0.6915197372436523] [G loss: 0.7192750573158264]\n",
      "[Epoch 55/1001] [Batch 92/372] [D loss: 0.6929370760917664] [G loss: 0.6486672163009644]\n",
      "[Epoch 55/1001] [Batch 93/372] [D loss: 0.6921772956848145] [G loss: 0.7315062284469604]\n",
      "[Epoch 55/1001] [Batch 94/372] [D loss: 0.695095419883728] [G loss: 0.6367226243019104]\n",
      "[Epoch 55/1001] [Batch 95/372] [D loss: 0.6926493048667908] [G loss: 0.7239822149276733]\n",
      "[Epoch 55/1001] [Batch 96/372] [D loss: 0.6929078102111816] [G loss: 0.6411043405532837]\n",
      "[Epoch 55/1001] [Batch 97/372] [D loss: 0.6926606893539429] [G loss: 0.7349658012390137]\n",
      "[Epoch 55/1001] [Batch 98/372] [D loss: 0.6909739971160889] [G loss: 0.6287859678268433]\n",
      "[Epoch 55/1001] [Batch 99/372] [D loss: 0.6947532892227173] [G loss: 0.7557218074798584]\n",
      "[Epoch 55/1001] [Batch 100/372] [D loss: 0.6928195953369141] [G loss: 0.6056320667266846]\n",
      "[Epoch 55/1001] [Batch 101/372] [D loss: 0.696075439453125] [G loss: 0.790454089641571]\n",
      "[Epoch 55/1001] [Batch 102/372] [D loss: 0.6981630325317383] [G loss: 0.5708045959472656]\n",
      "[Epoch 55/1001] [Batch 103/372] [D loss: 0.6980823278427124] [G loss: 0.8227992057800293]\n",
      "[Epoch 55/1001] [Batch 104/372] [D loss: 0.7008281946182251] [G loss: 0.5570464730262756]\n",
      "[Epoch 55/1001] [Batch 105/372] [D loss: 0.7024091482162476] [G loss: 0.8471156358718872]\n",
      "[Epoch 55/1001] [Batch 106/372] [D loss: 0.7031689286231995] [G loss: 0.5556737780570984]\n",
      "[Epoch 55/1001] [Batch 107/372] [D loss: 0.7003692984580994] [G loss: 0.792633056640625]\n",
      "[Epoch 55/1001] [Batch 108/372] [D loss: 0.6989293098449707] [G loss: 0.6255391836166382]\n",
      "[Epoch 55/1001] [Batch 109/372] [D loss: 0.6960042715072632] [G loss: 0.7175536155700684]\n",
      "[Epoch 55/1001] [Batch 110/372] [D loss: 0.6926004886627197] [G loss: 0.6657518148422241]\n",
      "[Epoch 55/1001] [Batch 111/372] [D loss: 0.6927711963653564] [G loss: 0.695730447769165]\n",
      "[Epoch 55/1001] [Batch 112/372] [D loss: 0.6918952465057373] [G loss: 0.678390622138977]\n",
      "[Epoch 55/1001] [Batch 113/372] [D loss: 0.6896045207977295] [G loss: 0.6774799823760986]\n",
      "[Epoch 55/1001] [Batch 114/372] [D loss: 0.6916004419326782] [G loss: 0.7022037506103516]\n",
      "[Epoch 55/1001] [Batch 115/372] [D loss: 0.6905897259712219] [G loss: 0.6595125198364258]\n",
      "[Epoch 55/1001] [Batch 116/372] [D loss: 0.6915512681007385] [G loss: 0.7161046266555786]\n",
      "[Epoch 55/1001] [Batch 117/372] [D loss: 0.6917710304260254] [G loss: 0.6474027633666992]\n",
      "[Epoch 55/1001] [Batch 118/372] [D loss: 0.6935489773750305] [G loss: 0.7220472097396851]\n",
      "[Epoch 55/1001] [Batch 119/372] [D loss: 0.6928615570068359] [G loss: 0.6547548174858093]\n",
      "[Epoch 55/1001] [Batch 120/372] [D loss: 0.6888278722763062] [G loss: 0.7087587118148804]\n",
      "[Epoch 55/1001] [Batch 121/372] [D loss: 0.6924426555633545] [G loss: 0.6694669723510742]\n",
      "[Epoch 55/1001] [Batch 122/372] [D loss: 0.6884510517120361] [G loss: 0.6758902072906494]\n",
      "[Epoch 55/1001] [Batch 123/372] [D loss: 0.6938140988349915] [G loss: 0.7035011649131775]\n",
      "[Epoch 55/1001] [Batch 124/372] [D loss: 0.692786455154419] [G loss: 0.6481768488883972]\n",
      "[Epoch 55/1001] [Batch 125/372] [D loss: 0.6945438385009766] [G loss: 0.7212545275688171]\n",
      "[Epoch 55/1001] [Batch 126/372] [D loss: 0.6942110061645508] [G loss: 0.6509273648262024]\n",
      "[Epoch 55/1001] [Batch 127/372] [D loss: 0.691349446773529] [G loss: 0.7085365056991577]\n",
      "[Epoch 55/1001] [Batch 128/372] [D loss: 0.6909995079040527] [G loss: 0.6741228103637695]\n",
      "[Epoch 55/1001] [Batch 129/372] [D loss: 0.6915435791015625] [G loss: 0.6826666593551636]\n",
      "[Epoch 55/1001] [Batch 130/372] [D loss: 0.6902536153793335] [G loss: 0.6912749409675598]\n",
      "[Epoch 55/1001] [Batch 131/372] [D loss: 0.6914548873901367] [G loss: 0.6767948269844055]\n",
      "[Epoch 55/1001] [Batch 132/372] [D loss: 0.6877373456954956] [G loss: 0.7003058195114136]\n",
      "[Epoch 55/1001] [Batch 133/372] [D loss: 0.6908140182495117] [G loss: 0.6669930219650269]\n",
      "[Epoch 55/1001] [Batch 134/372] [D loss: 0.6920270919799805] [G loss: 0.7144580483436584]\n",
      "[Epoch 55/1001] [Batch 135/372] [D loss: 0.6918346881866455] [G loss: 0.6461033225059509]\n",
      "[Epoch 55/1001] [Batch 136/372] [D loss: 0.693102240562439] [G loss: 0.7257412672042847]\n",
      "[Epoch 55/1001] [Batch 137/372] [D loss: 0.6940992474555969] [G loss: 0.6364408731460571]\n",
      "[Epoch 55/1001] [Batch 138/372] [D loss: 0.691328227519989] [G loss: 0.74202561378479]\n",
      "[Epoch 55/1001] [Batch 139/372] [D loss: 0.6896846890449524] [G loss: 0.6373974680900574]\n",
      "[Epoch 55/1001] [Batch 140/372] [D loss: 0.6915770769119263] [G loss: 0.7419541478157043]\n",
      "[Epoch 55/1001] [Batch 141/372] [D loss: 0.6961212754249573] [G loss: 0.6119017004966736]\n",
      "[Epoch 55/1001] [Batch 142/372] [D loss: 0.697519063949585] [G loss: 0.77109694480896]\n",
      "[Epoch 55/1001] [Batch 143/372] [D loss: 0.6987825036048889] [G loss: 0.5875533819198608]\n",
      "[Epoch 55/1001] [Batch 144/372] [D loss: 0.6983242034912109] [G loss: 0.7963798642158508]\n",
      "[Epoch 55/1001] [Batch 145/372] [D loss: 0.6978069543838501] [G loss: 0.5852025151252747]\n",
      "[Epoch 55/1001] [Batch 146/372] [D loss: 0.6969962120056152] [G loss: 0.7955206632614136]\n",
      "[Epoch 55/1001] [Batch 147/372] [D loss: 0.6956838369369507] [G loss: 0.5926545858383179]\n",
      "[Epoch 55/1001] [Batch 148/372] [D loss: 0.6936444044113159] [G loss: 0.7682169675827026]\n",
      "[Epoch 55/1001] [Batch 149/372] [D loss: 0.6930441856384277] [G loss: 0.6267417669296265]\n",
      "[Epoch 55/1001] [Batch 150/372] [D loss: 0.6924811601638794] [G loss: 0.7263455986976624]\n",
      "[Epoch 55/1001] [Batch 151/372] [D loss: 0.691789984703064] [G loss: 0.6469956636428833]\n",
      "[Epoch 55/1001] [Batch 152/372] [D loss: 0.6912651658058167] [G loss: 0.7371954321861267]\n",
      "[Epoch 55/1001] [Batch 153/372] [D loss: 0.6909865140914917] [G loss: 0.6297696232795715]\n",
      "[Epoch 55/1001] [Batch 154/372] [D loss: 0.6921132206916809] [G loss: 0.7442229390144348]\n",
      "[Epoch 55/1001] [Batch 155/372] [D loss: 0.69368577003479] [G loss: 0.6361064910888672]\n",
      "[Epoch 55/1001] [Batch 156/372] [D loss: 0.6929343938827515] [G loss: 0.7339190244674683]\n",
      "[Epoch 55/1001] [Batch 157/372] [D loss: 0.6921239495277405] [G loss: 0.6433880925178528]\n",
      "[Epoch 55/1001] [Batch 158/372] [D loss: 0.6942828893661499] [G loss: 0.7155903577804565]\n",
      "[Epoch 55/1001] [Batch 159/372] [D loss: 0.6933339238166809] [G loss: 0.6590257883071899]\n",
      "[Epoch 55/1001] [Batch 160/372] [D loss: 0.6917523145675659] [G loss: 0.6998445391654968]\n",
      "[Epoch 55/1001] [Batch 161/372] [D loss: 0.6934000253677368] [G loss: 0.6642586588859558]\n",
      "[Epoch 55/1001] [Batch 162/372] [D loss: 0.6895572543144226] [G loss: 0.6953389644622803]\n",
      "[Epoch 55/1001] [Batch 163/372] [D loss: 0.6889774799346924] [G loss: 0.6515165567398071]\n",
      "[Epoch 55/1001] [Batch 164/372] [D loss: 0.6936798691749573] [G loss: 0.7176306247711182]\n",
      "[Epoch 55/1001] [Batch 165/372] [D loss: 0.6908738017082214] [G loss: 0.6571478247642517]\n",
      "[Epoch 55/1001] [Batch 166/372] [D loss: 0.6916990280151367] [G loss: 0.719896137714386]\n",
      "[Epoch 55/1001] [Batch 167/372] [D loss: 0.6922253966331482] [G loss: 0.6175973415374756]\n",
      "[Epoch 55/1001] [Batch 168/372] [D loss: 0.6948608160018921] [G loss: 0.7750362157821655]\n",
      "[Epoch 55/1001] [Batch 169/372] [D loss: 0.693472146987915] [G loss: 0.5771024227142334]\n",
      "[Epoch 55/1001] [Batch 170/372] [D loss: 0.6997826099395752] [G loss: 0.8260686993598938]\n",
      "[Epoch 55/1001] [Batch 171/372] [D loss: 0.7015265226364136] [G loss: 0.5506303906440735]\n",
      "[Epoch 55/1001] [Batch 172/372] [D loss: 0.7027837038040161] [G loss: 0.8306521773338318]\n",
      "[Epoch 55/1001] [Batch 173/372] [D loss: 0.6999791860580444] [G loss: 0.5652134418487549]\n",
      "[Epoch 55/1001] [Batch 174/372] [D loss: 0.6980994939804077] [G loss: 0.8243885040283203]\n",
      "[Epoch 55/1001] [Batch 175/372] [D loss: 0.6998307108879089] [G loss: 0.572144091129303]\n",
      "[Epoch 55/1001] [Batch 176/372] [D loss: 0.6986904144287109] [G loss: 0.7866500020027161]\n",
      "[Epoch 55/1001] [Batch 177/372] [D loss: 0.6964520215988159] [G loss: 0.6165004968643188]\n",
      "[Epoch 55/1001] [Batch 178/372] [D loss: 0.6960769891738892] [G loss: 0.7285265326499939]\n",
      "[Epoch 55/1001] [Batch 179/372] [D loss: 0.6940997838973999] [G loss: 0.6623876690864563]\n",
      "[Epoch 55/1001] [Batch 180/372] [D loss: 0.6926883459091187] [G loss: 0.6898412108421326]\n",
      "[Epoch 55/1001] [Batch 181/372] [D loss: 0.6890091300010681] [G loss: 0.6864079833030701]\n",
      "[Epoch 55/1001] [Batch 182/372] [D loss: 0.6916918158531189] [G loss: 0.6784031391143799]\n",
      "[Epoch 55/1001] [Batch 183/372] [D loss: 0.69196617603302] [G loss: 0.6817349195480347]\n",
      "[Epoch 55/1001] [Batch 184/372] [D loss: 0.6921645998954773] [G loss: 0.68500155210495]\n",
      "[Epoch 55/1001] [Batch 185/372] [D loss: 0.6914205551147461] [G loss: 0.6720935106277466]\n",
      "[Epoch 55/1001] [Batch 186/372] [D loss: 0.6903164982795715] [G loss: 0.7100709080696106]\n",
      "[Epoch 55/1001] [Batch 187/372] [D loss: 0.69049072265625] [G loss: 0.6445474028587341]\n",
      "[Epoch 55/1001] [Batch 188/372] [D loss: 0.6915380954742432] [G loss: 0.7487022876739502]\n",
      "[Epoch 55/1001] [Batch 189/372] [D loss: 0.6910920739173889] [G loss: 0.607642650604248]\n",
      "[Epoch 55/1001] [Batch 190/372] [D loss: 0.695331335067749] [G loss: 0.7636823058128357]\n",
      "[Epoch 55/1001] [Batch 191/372] [D loss: 0.6925978660583496] [G loss: 0.5961556434631348]\n",
      "[Epoch 55/1001] [Batch 192/372] [D loss: 0.6962341070175171] [G loss: 0.8191706538200378]\n",
      "[Epoch 55/1001] [Batch 193/372] [D loss: 0.6971924901008606] [G loss: 0.5619698762893677]\n",
      "[Epoch 55/1001] [Batch 194/372] [D loss: 0.7009437084197998] [G loss: 0.8042861819267273]\n",
      "[Epoch 55/1001] [Batch 195/372] [D loss: 0.6974060535430908] [G loss: 0.6112694144248962]\n",
      "[Epoch 55/1001] [Batch 196/372] [D loss: 0.6962121725082397] [G loss: 0.7364938259124756]\n",
      "[Epoch 55/1001] [Batch 197/372] [D loss: 0.6931840181350708] [G loss: 0.6487659811973572]\n",
      "[Epoch 55/1001] [Batch 198/372] [D loss: 0.6925346851348877] [G loss: 0.7184538841247559]\n",
      "[Epoch 55/1001] [Batch 199/372] [D loss: 0.6933691501617432] [G loss: 0.660117506980896]\n",
      "[Epoch 55/1001] [Batch 200/372] [D loss: 0.6918027400970459] [G loss: 0.6960026025772095]\n",
      "[Epoch 55/1001] [Batch 201/372] [D loss: 0.6896857023239136] [G loss: 0.6782106757164001]\n",
      "[Epoch 55/1001] [Batch 202/372] [D loss: 0.6921680569648743] [G loss: 0.6834988594055176]\n",
      "[Epoch 55/1001] [Batch 203/372] [D loss: 0.691669762134552] [G loss: 0.6908178925514221]\n",
      "[Epoch 55/1001] [Batch 204/372] [D loss: 0.6931299567222595] [G loss: 0.6790173053741455]\n",
      "[Epoch 55/1001] [Batch 205/372] [D loss: 0.6906329393386841] [G loss: 0.6917868852615356]\n",
      "[Epoch 55/1001] [Batch 206/372] [D loss: 0.6920254826545715] [G loss: 0.6820840835571289]\n",
      "[Epoch 55/1001] [Batch 207/372] [D loss: 0.6914384365081787] [G loss: 0.6896992921829224]\n",
      "[Epoch 55/1001] [Batch 208/372] [D loss: 0.6904788017272949] [G loss: 0.6774045825004578]\n",
      "[Epoch 55/1001] [Batch 209/372] [D loss: 0.6902836561203003] [G loss: 0.6959837079048157]\n",
      "[Epoch 55/1001] [Batch 210/372] [D loss: 0.6910005211830139] [G loss: 0.6748219132423401]\n",
      "[Epoch 55/1001] [Batch 211/372] [D loss: 0.6909615993499756] [G loss: 0.6965251564979553]\n",
      "[Epoch 55/1001] [Batch 212/372] [D loss: 0.6915103197097778] [G loss: 0.6664214134216309]\n",
      "[Epoch 55/1001] [Batch 213/372] [D loss: 0.6922018527984619] [G loss: 0.6989867687225342]\n",
      "[Epoch 55/1001] [Batch 214/372] [D loss: 0.6906033158302307] [G loss: 0.6737052202224731]\n",
      "[Epoch 55/1001] [Batch 215/372] [D loss: 0.6913840174674988] [G loss: 0.6860700845718384]\n",
      "[Epoch 55/1001] [Batch 216/372] [D loss: 0.6902976632118225] [G loss: 0.6827899813652039]\n",
      "[Epoch 55/1001] [Batch 217/372] [D loss: 0.692801833152771] [G loss: 0.6753110885620117]\n",
      "[Epoch 55/1001] [Batch 218/372] [D loss: 0.6896857023239136] [G loss: 0.7139334082603455]\n",
      "[Epoch 55/1001] [Batch 219/372] [D loss: 0.6909222602844238] [G loss: 0.6586103439331055]\n",
      "[Epoch 55/1001] [Batch 220/372] [D loss: 0.6928200721740723] [G loss: 0.7146651744842529]\n",
      "[Epoch 55/1001] [Batch 221/372] [D loss: 0.6936566829681396] [G loss: 0.6439622640609741]\n",
      "[Epoch 55/1001] [Batch 222/372] [D loss: 0.6921318769454956] [G loss: 0.734862208366394]\n",
      "[Epoch 55/1001] [Batch 223/372] [D loss: 0.6899333000183105] [G loss: 0.6363022327423096]\n",
      "[Epoch 55/1001] [Batch 224/372] [D loss: 0.690111517906189] [G loss: 0.7334458827972412]\n",
      "[Epoch 55/1001] [Batch 225/372] [D loss: 0.6907567977905273] [G loss: 0.6473349928855896]\n",
      "[Epoch 55/1001] [Batch 226/372] [D loss: 0.6889578104019165] [G loss: 0.7218857407569885]\n",
      "[Epoch 55/1001] [Batch 227/372] [D loss: 0.6939033269882202] [G loss: 0.6302663683891296]\n",
      "[Epoch 55/1001] [Batch 228/372] [D loss: 0.6910979747772217] [G loss: 0.7759540677070618]\n",
      "[Epoch 55/1001] [Batch 229/372] [D loss: 0.695658802986145] [G loss: 0.566203236579895]\n",
      "[Epoch 55/1001] [Batch 230/372] [D loss: 0.6982032060623169] [G loss: 0.8536288142204285]\n",
      "[Epoch 55/1001] [Batch 231/372] [D loss: 0.7029068470001221] [G loss: 0.5182613730430603]\n",
      "[Epoch 55/1001] [Batch 232/372] [D loss: 0.7093728184700012] [G loss: 0.9082348346710205]\n",
      "[Epoch 55/1001] [Batch 233/372] [D loss: 0.711327075958252] [G loss: 0.5084381699562073]\n",
      "[Epoch 55/1001] [Batch 234/372] [D loss: 0.7104091644287109] [G loss: 0.8434569239616394]\n",
      "[Epoch 55/1001] [Batch 235/372] [D loss: 0.7041509747505188] [G loss: 0.5920863151550293]\n",
      "[Epoch 55/1001] [Batch 236/372] [D loss: 0.6955193281173706] [G loss: 0.737112820148468]\n",
      "[Epoch 55/1001] [Batch 237/372] [D loss: 0.6923666000366211] [G loss: 0.6666149497032166]\n",
      "[Epoch 55/1001] [Batch 238/372] [D loss: 0.6906803250312805] [G loss: 0.6856318116188049]\n",
      "[Epoch 55/1001] [Batch 239/372] [D loss: 0.6923447847366333] [G loss: 0.6876786947250366]\n",
      "[Epoch 55/1001] [Batch 240/372] [D loss: 0.6911792159080505] [G loss: 0.6778385043144226]\n",
      "[Epoch 55/1001] [Batch 241/372] [D loss: 0.6897078156471252] [G loss: 0.691663384437561]\n",
      "[Epoch 55/1001] [Batch 242/372] [D loss: 0.6916049718856812] [G loss: 0.68439120054245]\n",
      "[Epoch 55/1001] [Batch 243/372] [D loss: 0.6884645223617554] [G loss: 0.6777843236923218]\n",
      "[Epoch 55/1001] [Batch 244/372] [D loss: 0.6913764476776123] [G loss: 0.6890931725502014]\n",
      "[Epoch 55/1001] [Batch 245/372] [D loss: 0.6913622617721558] [G loss: 0.6897001266479492]\n",
      "[Epoch 55/1001] [Batch 246/372] [D loss: 0.6898711919784546] [G loss: 0.6764461398124695]\n",
      "[Epoch 55/1001] [Batch 247/372] [D loss: 0.6915607452392578] [G loss: 0.6933478713035583]\n",
      "[Epoch 55/1001] [Batch 248/372] [D loss: 0.6913180351257324] [G loss: 0.6668359637260437]\n",
      "[Epoch 55/1001] [Batch 249/372] [D loss: 0.6884384155273438] [G loss: 0.7034308910369873]\n",
      "[Epoch 55/1001] [Batch 250/372] [D loss: 0.6918551325798035] [G loss: 0.6630403995513916]\n",
      "[Epoch 55/1001] [Batch 251/372] [D loss: 0.6910302639007568] [G loss: 0.7026517391204834]\n",
      "[Epoch 55/1001] [Batch 252/372] [D loss: 0.6924827694892883] [G loss: 0.6581577062606812]\n",
      "[Epoch 55/1001] [Batch 253/372] [D loss: 0.6909837126731873] [G loss: 0.712776780128479]\n",
      "[Epoch 55/1001] [Batch 254/372] [D loss: 0.6944219470024109] [G loss: 0.6502054929733276]\n",
      "[Epoch 55/1001] [Batch 255/372] [D loss: 0.6911685466766357] [G loss: 0.7123989462852478]\n",
      "[Epoch 55/1001] [Batch 256/372] [D loss: 0.6926912069320679] [G loss: 0.6499089598655701]\n",
      "[Epoch 55/1001] [Batch 257/372] [D loss: 0.69109046459198] [G loss: 0.7184585332870483]\n",
      "[Epoch 55/1001] [Batch 258/372] [D loss: 0.6921529769897461] [G loss: 0.6408860683441162]\n",
      "[Epoch 55/1001] [Batch 259/372] [D loss: 0.6959521174430847] [G loss: 0.7295294404029846]\n",
      "[Epoch 55/1001] [Batch 260/372] [D loss: 0.6952390670776367] [G loss: 0.6239237189292908]\n",
      "[Epoch 55/1001] [Batch 261/372] [D loss: 0.6944795846939087] [G loss: 0.748225748538971]\n",
      "[Epoch 55/1001] [Batch 262/372] [D loss: 0.6928958892822266] [G loss: 0.6169989109039307]\n",
      "[Epoch 55/1001] [Batch 263/372] [D loss: 0.6917259693145752] [G loss: 0.7565394639968872]\n",
      "[Epoch 55/1001] [Batch 264/372] [D loss: 0.6946895718574524] [G loss: 0.6155858039855957]\n",
      "[Epoch 55/1001] [Batch 265/372] [D loss: 0.6935377717018127] [G loss: 0.7697170972824097]\n",
      "[Epoch 55/1001] [Batch 266/372] [D loss: 0.6960798501968384] [G loss: 0.6080130934715271]\n",
      "[Epoch 55/1001] [Batch 267/372] [D loss: 0.6938170194625854] [G loss: 0.7579747438430786]\n",
      "[Epoch 55/1001] [Batch 268/372] [D loss: 0.6942267417907715] [G loss: 0.6221081018447876]\n",
      "[Epoch 55/1001] [Batch 269/372] [D loss: 0.6948408484458923] [G loss: 0.736833930015564]\n",
      "[Epoch 55/1001] [Batch 270/372] [D loss: 0.6914417743682861] [G loss: 0.6406612992286682]\n",
      "[Epoch 55/1001] [Batch 271/372] [D loss: 0.6909270286560059] [G loss: 0.7175642251968384]\n",
      "[Epoch 55/1001] [Batch 272/372] [D loss: 0.690578818321228] [G loss: 0.6643009185791016]\n",
      "[Epoch 55/1001] [Batch 273/372] [D loss: 0.691920816898346] [G loss: 0.6966185569763184]\n",
      "[Epoch 55/1001] [Batch 274/372] [D loss: 0.6923482418060303] [G loss: 0.6708740592002869]\n",
      "[Epoch 55/1001] [Batch 275/372] [D loss: 0.6911641359329224] [G loss: 0.6869978308677673]\n",
      "[Epoch 55/1001] [Batch 276/372] [D loss: 0.6934903860092163] [G loss: 0.685897707939148]\n",
      "[Epoch 55/1001] [Batch 277/372] [D loss: 0.691484272480011] [G loss: 0.6808821558952332]\n",
      "[Epoch 55/1001] [Batch 278/372] [D loss: 0.6915491819381714] [G loss: 0.6798685789108276]\n",
      "[Epoch 55/1001] [Batch 279/372] [D loss: 0.6916487812995911] [G loss: 0.6850829720497131]\n",
      "[Epoch 55/1001] [Batch 280/372] [D loss: 0.6894544363021851] [G loss: 0.6708184480667114]\n",
      "[Epoch 55/1001] [Batch 281/372] [D loss: 0.6892719268798828] [G loss: 0.7018070220947266]\n",
      "[Epoch 55/1001] [Batch 282/372] [D loss: 0.690558910369873] [G loss: 0.6601535081863403]\n",
      "[Epoch 55/1001] [Batch 283/372] [D loss: 0.6880682706832886] [G loss: 0.7138602137565613]\n",
      "[Epoch 55/1001] [Batch 284/372] [D loss: 0.6918573379516602] [G loss: 0.639275312423706]\n",
      "[Epoch 55/1001] [Batch 285/372] [D loss: 0.6952880620956421] [G loss: 0.7486053109169006]\n",
      "[Epoch 55/1001] [Batch 286/372] [D loss: 0.6927105188369751] [G loss: 0.5943737626075745]\n",
      "[Epoch 55/1001] [Batch 287/372] [D loss: 0.6957048177719116] [G loss: 0.8123945593833923]\n",
      "[Epoch 55/1001] [Batch 288/372] [D loss: 0.699560284614563] [G loss: 0.5577734112739563]\n",
      "[Epoch 55/1001] [Batch 289/372] [D loss: 0.7005174160003662] [G loss: 0.8340667486190796]\n",
      "[Epoch 55/1001] [Batch 290/372] [D loss: 0.7019027471542358] [G loss: 0.5617793798446655]\n",
      "[Epoch 55/1001] [Batch 291/372] [D loss: 0.699752688407898] [G loss: 0.8034276366233826]\n",
      "[Epoch 55/1001] [Batch 292/372] [D loss: 0.69867342710495] [G loss: 0.6000697612762451]\n",
      "[Epoch 55/1001] [Batch 293/372] [D loss: 0.6928350925445557] [G loss: 0.739364504814148]\n",
      "[Epoch 55/1001] [Batch 294/372] [D loss: 0.6904216408729553] [G loss: 0.657870352268219]\n",
      "[Epoch 55/1001] [Batch 295/372] [D loss: 0.691767692565918] [G loss: 0.6792681813240051]\n",
      "[Epoch 55/1001] [Batch 296/372] [D loss: 0.6915855407714844] [G loss: 0.7108926177024841]\n",
      "[Epoch 55/1001] [Batch 297/372] [D loss: 0.6910865306854248] [G loss: 0.6419957876205444]\n",
      "[Epoch 55/1001] [Batch 298/372] [D loss: 0.6924530267715454] [G loss: 0.7349813580513]\n",
      "[Epoch 55/1001] [Batch 299/372] [D loss: 0.6887807846069336] [G loss: 0.6351506114006042]\n",
      "[Epoch 55/1001] [Batch 300/372] [D loss: 0.6900566816329956] [G loss: 0.7384637594223022]\n",
      "[Epoch 55/1001] [Batch 301/372] [D loss: 0.6925827264785767] [G loss: 0.6359170079231262]\n",
      "[Epoch 55/1001] [Batch 302/372] [D loss: 0.6947930455207825] [G loss: 0.7313514947891235]\n",
      "[Epoch 55/1001] [Batch 303/372] [D loss: 0.6946614980697632] [G loss: 0.6273409128189087]\n",
      "[Epoch 55/1001] [Batch 304/372] [D loss: 0.6934309005737305] [G loss: 0.7373944520950317]\n",
      "[Epoch 55/1001] [Batch 305/372] [D loss: 0.6948696374893188] [G loss: 0.6347578763961792]\n",
      "[Epoch 55/1001] [Batch 306/372] [D loss: 0.6950573921203613] [G loss: 0.7445107698440552]\n",
      "[Epoch 55/1001] [Batch 307/372] [D loss: 0.6939612030982971] [G loss: 0.6285797357559204]\n",
      "[Epoch 55/1001] [Batch 308/372] [D loss: 0.6957412958145142] [G loss: 0.7351616024971008]\n",
      "[Epoch 55/1001] [Batch 309/372] [D loss: 0.6938414573669434] [G loss: 0.6341208219528198]\n",
      "[Epoch 55/1001] [Batch 310/372] [D loss: 0.6938807964324951] [G loss: 0.726716160774231]\n",
      "[Epoch 55/1001] [Batch 311/372] [D loss: 0.6914108991622925] [G loss: 0.6410911083221436]\n",
      "[Epoch 55/1001] [Batch 312/372] [D loss: 0.6916426420211792] [G loss: 0.7326734066009521]\n",
      "[Epoch 55/1001] [Batch 313/372] [D loss: 0.691795825958252] [G loss: 0.6233572959899902]\n",
      "[Epoch 55/1001] [Batch 314/372] [D loss: 0.693193793296814] [G loss: 0.7766912579536438]\n",
      "[Epoch 55/1001] [Batch 315/372] [D loss: 0.695314347743988] [G loss: 0.5705892443656921]\n",
      "[Epoch 55/1001] [Batch 316/372] [D loss: 0.7025371193885803] [G loss: 0.8457698225975037]\n",
      "[Epoch 55/1001] [Batch 317/372] [D loss: 0.7028450965881348] [G loss: 0.5372885465621948]\n",
      "[Epoch 55/1001] [Batch 318/372] [D loss: 0.7029268741607666] [G loss: 0.8521004915237427]\n",
      "[Epoch 55/1001] [Batch 319/372] [D loss: 0.7026551365852356] [G loss: 0.5674862861633301]\n",
      "[Epoch 55/1001] [Batch 320/372] [D loss: 0.7009649276733398] [G loss: 0.782572865486145]\n",
      "[Epoch 55/1001] [Batch 321/372] [D loss: 0.6970697045326233] [G loss: 0.6253332495689392]\n",
      "[Epoch 55/1001] [Batch 322/372] [D loss: 0.6942006945610046] [G loss: 0.7206573486328125]\n",
      "[Epoch 55/1001] [Batch 323/372] [D loss: 0.6959349513053894] [G loss: 0.6749947667121887]\n",
      "[Epoch 55/1001] [Batch 324/372] [D loss: 0.6906687021255493] [G loss: 0.6765739917755127]\n",
      "[Epoch 55/1001] [Batch 325/372] [D loss: 0.6927600502967834] [G loss: 0.6982202529907227]\n",
      "[Epoch 55/1001] [Batch 326/372] [D loss: 0.692535400390625] [G loss: 0.6680303812026978]\n",
      "[Epoch 55/1001] [Batch 327/372] [D loss: 0.6912364959716797] [G loss: 0.6982656121253967]\n",
      "[Epoch 55/1001] [Batch 328/372] [D loss: 0.689704418182373] [G loss: 0.6791312098503113]\n",
      "[Epoch 55/1001] [Batch 329/372] [D loss: 0.6895909309387207] [G loss: 0.6882964372634888]\n",
      "[Epoch 55/1001] [Batch 330/372] [D loss: 0.691024124622345] [G loss: 0.67974853515625]\n",
      "[Epoch 55/1001] [Batch 331/372] [D loss: 0.6904544234275818] [G loss: 0.693405032157898]\n",
      "[Epoch 55/1001] [Batch 332/372] [D loss: 0.6911148428916931] [G loss: 0.6829483509063721]\n",
      "[Epoch 55/1001] [Batch 333/372] [D loss: 0.6912546157836914] [G loss: 0.680605411529541]\n",
      "[Epoch 55/1001] [Batch 334/372] [D loss: 0.6933140754699707] [G loss: 0.68284672498703]\n",
      "[Epoch 55/1001] [Batch 335/372] [D loss: 0.6889299154281616] [G loss: 0.6827630400657654]\n",
      "[Epoch 55/1001] [Batch 336/372] [D loss: 0.6918776035308838] [G loss: 0.6930835843086243]\n",
      "[Epoch 55/1001] [Batch 337/372] [D loss: 0.6940546035766602] [G loss: 0.6760046482086182]\n",
      "[Epoch 55/1001] [Batch 338/372] [D loss: 0.6916083097457886] [G loss: 0.6743804216384888]\n",
      "[Epoch 55/1001] [Batch 339/372] [D loss: 0.6920806169509888] [G loss: 0.6875964403152466]\n",
      "[Epoch 55/1001] [Batch 340/372] [D loss: 0.6919727325439453] [G loss: 0.6725271344184875]\n",
      "[Epoch 55/1001] [Batch 341/372] [D loss: 0.6900205612182617] [G loss: 0.6988493204116821]\n",
      "[Epoch 55/1001] [Batch 342/372] [D loss: 0.6951653957366943] [G loss: 0.6655778884887695]\n",
      "[Epoch 55/1001] [Batch 343/372] [D loss: 0.6903846263885498] [G loss: 0.7044233679771423]\n",
      "[Epoch 55/1001] [Batch 344/372] [D loss: 0.6910781264305115] [G loss: 0.6624925136566162]\n",
      "[Epoch 55/1001] [Batch 345/372] [D loss: 0.6910624504089355] [G loss: 0.6974694728851318]\n",
      "[Epoch 55/1001] [Batch 346/372] [D loss: 0.6889611482620239] [G loss: 0.6708173751831055]\n",
      "[Epoch 55/1001] [Batch 347/372] [D loss: 0.6946921348571777] [G loss: 0.6967670321464539]\n",
      "[Epoch 55/1001] [Batch 348/372] [D loss: 0.6954032778739929] [G loss: 0.6637120246887207]\n",
      "[Epoch 55/1001] [Batch 349/372] [D loss: 0.6923561096191406] [G loss: 0.7272549867630005]\n",
      "[Epoch 55/1001] [Batch 350/372] [D loss: 0.6918054819107056] [G loss: 0.6287482976913452]\n",
      "[Epoch 55/1001] [Batch 351/372] [D loss: 0.6925867199897766] [G loss: 0.7460166811943054]\n",
      "[Epoch 55/1001] [Batch 352/372] [D loss: 0.6940135955810547] [G loss: 0.6283186078071594]\n",
      "[Epoch 55/1001] [Batch 353/372] [D loss: 0.6939501762390137] [G loss: 0.742518961429596]\n",
      "[Epoch 55/1001] [Batch 354/372] [D loss: 0.6922632455825806] [G loss: 0.6179733276367188]\n",
      "[Epoch 55/1001] [Batch 355/372] [D loss: 0.6935641765594482] [G loss: 0.7722821831703186]\n",
      "[Epoch 55/1001] [Batch 356/372] [D loss: 0.6923196315765381] [G loss: 0.5865724086761475]\n",
      "[Epoch 55/1001] [Batch 357/372] [D loss: 0.6971096396446228] [G loss: 0.8204669952392578]\n",
      "[Epoch 55/1001] [Batch 358/372] [D loss: 0.6998323202133179] [G loss: 0.5560976266860962]\n",
      "[Epoch 55/1001] [Batch 359/372] [D loss: 0.6981368660926819] [G loss: 0.8219630122184753]\n",
      "[Epoch 55/1001] [Batch 360/372] [D loss: 0.7014264464378357] [G loss: 0.5806980729103088]\n",
      "[Epoch 55/1001] [Batch 361/372] [D loss: 0.6954210996627808] [G loss: 0.7763593196868896]\n",
      "[Epoch 55/1001] [Batch 362/372] [D loss: 0.6942905187606812] [G loss: 0.6141834855079651]\n",
      "[Epoch 55/1001] [Batch 363/372] [D loss: 0.6959977149963379] [G loss: 0.7349652051925659]\n",
      "[Epoch 55/1001] [Batch 364/372] [D loss: 0.6908113956451416] [G loss: 0.6483039855957031]\n",
      "[Epoch 55/1001] [Batch 365/372] [D loss: 0.6960694789886475] [G loss: 0.7021313905715942]\n",
      "[Epoch 55/1001] [Batch 366/372] [D loss: 0.6931194067001343] [G loss: 0.6634399890899658]\n",
      "[Epoch 55/1001] [Batch 367/372] [D loss: 0.6918707489967346] [G loss: 0.7127109169960022]\n",
      "[Epoch 55/1001] [Batch 368/372] [D loss: 0.694792628288269] [G loss: 0.6473637223243713]\n",
      "[Epoch 55/1001] [Batch 369/372] [D loss: 0.6927857398986816] [G loss: 0.7211586236953735]\n",
      "[Epoch 55/1001] [Batch 370/372] [D loss: 0.6920464038848877] [G loss: 0.6485028266906738]\n",
      "[Epoch 55/1001] [Batch 371/372] [D loss: 0.6940228939056396] [G loss: 0.7121793627738953]\n",
      "[Epoch 56/1001] [Batch 0/372] [D loss: 0.6914392709732056] [G loss: 0.6570476293563843]\n",
      "[Epoch 56/1001] [Batch 1/372] [D loss: 0.6899937391281128] [G loss: 0.7082043886184692]\n",
      "[Epoch 56/1001] [Batch 2/372] [D loss: 0.6934823989868164] [G loss: 0.6627348065376282]\n",
      "[Epoch 56/1001] [Batch 3/372] [D loss: 0.6886191368103027] [G loss: 0.6956787109375]\n",
      "[Epoch 56/1001] [Batch 4/372] [D loss: 0.6913092732429504] [G loss: 0.6873311996459961]\n",
      "[Epoch 56/1001] [Batch 5/372] [D loss: 0.6885977983474731] [G loss: 0.6721675992012024]\n",
      "[Epoch 56/1001] [Batch 6/372] [D loss: 0.6937735080718994] [G loss: 0.7019789218902588]\n",
      "[Epoch 56/1001] [Batch 7/372] [D loss: 0.6904070377349854] [G loss: 0.6644128561019897]\n",
      "[Epoch 56/1001] [Batch 8/372] [D loss: 0.69172203540802] [G loss: 0.6927095055580139]\n",
      "[Epoch 56/1001] [Batch 9/372] [D loss: 0.6906360387802124] [G loss: 0.6935995817184448]\n",
      "[Epoch 56/1001] [Batch 10/372] [D loss: 0.6904547214508057] [G loss: 0.658270001411438]\n",
      "[Epoch 56/1001] [Batch 11/372] [D loss: 0.690605878829956] [G loss: 0.7291746139526367]\n",
      "[Epoch 56/1001] [Batch 12/372] [D loss: 0.6882880926132202] [G loss: 0.6333140134811401]\n",
      "[Epoch 56/1001] [Batch 13/372] [D loss: 0.6919345855712891] [G loss: 0.7448719143867493]\n",
      "[Epoch 56/1001] [Batch 14/372] [D loss: 0.6921650171279907] [G loss: 0.6328576803207397]\n",
      "[Epoch 56/1001] [Batch 15/372] [D loss: 0.6925206184387207] [G loss: 0.7388446927070618]\n",
      "[Epoch 56/1001] [Batch 16/372] [D loss: 0.6923904418945312] [G loss: 0.6195933222770691]\n",
      "[Epoch 56/1001] [Batch 17/372] [D loss: 0.6897823810577393] [G loss: 0.7565408945083618]\n",
      "[Epoch 56/1001] [Batch 18/372] [D loss: 0.6916476488113403] [G loss: 0.6120766401290894]\n",
      "[Epoch 56/1001] [Batch 19/372] [D loss: 0.6945923566818237] [G loss: 0.8169112801551819]\n",
      "[Epoch 56/1001] [Batch 20/372] [D loss: 0.6969481706619263] [G loss: 0.5466055274009705]\n",
      "[Epoch 56/1001] [Batch 21/372] [D loss: 0.7005889415740967] [G loss: 0.8533700704574585]\n",
      "[Epoch 56/1001] [Batch 22/372] [D loss: 0.7023797035217285] [G loss: 0.5564534664154053]\n",
      "[Epoch 56/1001] [Batch 23/372] [D loss: 0.7002431154251099] [G loss: 0.7961665391921997]\n",
      "[Epoch 56/1001] [Batch 24/372] [D loss: 0.6962850093841553] [G loss: 0.6130704283714294]\n",
      "[Epoch 56/1001] [Batch 25/372] [D loss: 0.6965079307556152] [G loss: 0.7361285090446472]\n",
      "[Epoch 56/1001] [Batch 26/372] [D loss: 0.6910831928253174] [G loss: 0.646918773651123]\n",
      "[Epoch 56/1001] [Batch 27/372] [D loss: 0.6917730569839478] [G loss: 0.7132896184921265]\n",
      "[Epoch 56/1001] [Batch 28/372] [D loss: 0.6913177371025085] [G loss: 0.663289487361908]\n",
      "[Epoch 56/1001] [Batch 29/372] [D loss: 0.6918255090713501] [G loss: 0.7023209929466248]\n",
      "[Epoch 56/1001] [Batch 30/372] [D loss: 0.6911345720291138] [G loss: 0.6701547503471375]\n",
      "[Epoch 56/1001] [Batch 31/372] [D loss: 0.6922150254249573] [G loss: 0.6886576414108276]\n",
      "[Epoch 56/1001] [Batch 32/372] [D loss: 0.692909836769104] [G loss: 0.6837824583053589]\n",
      "[Epoch 56/1001] [Batch 33/372] [D loss: 0.6918718814849854] [G loss: 0.6760658025741577]\n",
      "[Epoch 56/1001] [Batch 34/372] [D loss: 0.6915572285652161] [G loss: 0.6931136846542358]\n",
      "[Epoch 56/1001] [Batch 35/372] [D loss: 0.689509391784668] [G loss: 0.6756138801574707]\n",
      "[Epoch 56/1001] [Batch 36/372] [D loss: 0.6892626285552979] [G loss: 0.7028828263282776]\n",
      "[Epoch 56/1001] [Batch 37/372] [D loss: 0.6897624135017395] [G loss: 0.6623234152793884]\n",
      "[Epoch 56/1001] [Batch 38/372] [D loss: 0.6883751153945923] [G loss: 0.7078942656517029]\n",
      "[Epoch 56/1001] [Batch 39/372] [D loss: 0.6907399296760559] [G loss: 0.6522842049598694]\n",
      "[Epoch 56/1001] [Batch 40/372] [D loss: 0.6897498369216919] [G loss: 0.7175860404968262]\n",
      "[Epoch 56/1001] [Batch 41/372] [D loss: 0.6922407150268555] [G loss: 0.6607529520988464]\n",
      "[Epoch 56/1001] [Batch 42/372] [D loss: 0.6898013949394226] [G loss: 0.6951695680618286]\n",
      "[Epoch 56/1001] [Batch 43/372] [D loss: 0.6905096173286438] [G loss: 0.6718866229057312]\n",
      "[Epoch 56/1001] [Batch 44/372] [D loss: 0.6892068386077881] [G loss: 0.703993558883667]\n",
      "[Epoch 56/1001] [Batch 45/372] [D loss: 0.6935105919837952] [G loss: 0.6619846820831299]\n",
      "[Epoch 56/1001] [Batch 46/372] [D loss: 0.6903442740440369] [G loss: 0.7059349417686462]\n",
      "[Epoch 56/1001] [Batch 47/372] [D loss: 0.6924566626548767] [G loss: 0.6530454158782959]\n",
      "[Epoch 56/1001] [Batch 48/372] [D loss: 0.6911064386367798] [G loss: 0.734305739402771]\n",
      "[Epoch 56/1001] [Batch 49/372] [D loss: 0.6921288371086121] [G loss: 0.6058679819107056]\n",
      "[Epoch 56/1001] [Batch 50/372] [D loss: 0.6954066753387451] [G loss: 0.793681800365448]\n",
      "[Epoch 56/1001] [Batch 51/372] [D loss: 0.6989810466766357] [G loss: 0.5707346200942993]\n",
      "[Epoch 56/1001] [Batch 52/372] [D loss: 0.6973816156387329] [G loss: 0.826571524143219]\n",
      "[Epoch 56/1001] [Batch 53/372] [D loss: 0.7017337679862976] [G loss: 0.5580158829689026]\n",
      "[Epoch 56/1001] [Batch 54/372] [D loss: 0.6999396085739136] [G loss: 0.8070624470710754]\n",
      "[Epoch 56/1001] [Batch 55/372] [D loss: 0.6966373920440674] [G loss: 0.6015479564666748]\n",
      "[Epoch 56/1001] [Batch 56/372] [D loss: 0.6956573724746704] [G loss: 0.747374951839447]\n",
      "[Epoch 56/1001] [Batch 57/372] [D loss: 0.6943035125732422] [G loss: 0.6461659669876099]\n",
      "[Epoch 56/1001] [Batch 58/372] [D loss: 0.6909685134887695] [G loss: 0.6937627196311951]\n",
      "[Epoch 56/1001] [Batch 59/372] [D loss: 0.6899058818817139] [G loss: 0.6718224883079529]\n",
      "[Epoch 56/1001] [Batch 60/372] [D loss: 0.6937375068664551] [G loss: 0.6868691444396973]\n",
      "[Epoch 56/1001] [Batch 61/372] [D loss: 0.6891446113586426] [G loss: 0.6823597550392151]\n",
      "[Epoch 56/1001] [Batch 62/372] [D loss: 0.6899737119674683] [G loss: 0.6913901567459106]\n",
      "[Epoch 56/1001] [Batch 63/372] [D loss: 0.6946253180503845] [G loss: 0.6740012168884277]\n",
      "[Epoch 56/1001] [Batch 64/372] [D loss: 0.6918322443962097] [G loss: 0.6918297410011292]\n",
      "[Epoch 56/1001] [Batch 65/372] [D loss: 0.6927813291549683] [G loss: 0.6626273393630981]\n",
      "[Epoch 56/1001] [Batch 66/372] [D loss: 0.691604733467102] [G loss: 0.6946732401847839]\n",
      "[Epoch 56/1001] [Batch 67/372] [D loss: 0.689409613609314] [G loss: 0.6823331117630005]\n",
      "[Epoch 56/1001] [Batch 68/372] [D loss: 0.6917855739593506] [G loss: 0.6835216283798218]\n",
      "[Epoch 56/1001] [Batch 69/372] [D loss: 0.691275954246521] [G loss: 0.691803514957428]\n",
      "[Epoch 56/1001] [Batch 70/372] [D loss: 0.6933614611625671] [G loss: 0.665252149105072]\n",
      "[Epoch 56/1001] [Batch 71/372] [D loss: 0.6915637254714966] [G loss: 0.703187108039856]\n",
      "[Epoch 56/1001] [Batch 72/372] [D loss: 0.6911239624023438] [G loss: 0.661508321762085]\n",
      "[Epoch 56/1001] [Batch 73/372] [D loss: 0.6904867887496948] [G loss: 0.7059847116470337]\n",
      "[Epoch 56/1001] [Batch 74/372] [D loss: 0.6940110921859741] [G loss: 0.6646769046783447]\n",
      "[Epoch 56/1001] [Batch 75/372] [D loss: 0.6907246112823486] [G loss: 0.6879213452339172]\n",
      "[Epoch 56/1001] [Batch 76/372] [D loss: 0.6890110969543457] [G loss: 0.6795876622200012]\n",
      "[Epoch 56/1001] [Batch 77/372] [D loss: 0.6908986568450928] [G loss: 0.6809250712394714]\n",
      "[Epoch 56/1001] [Batch 78/372] [D loss: 0.6903136968612671] [G loss: 0.6775584816932678]\n",
      "[Epoch 56/1001] [Batch 79/372] [D loss: 0.6924123167991638] [G loss: 0.7347114086151123]\n",
      "[Epoch 56/1001] [Batch 80/372] [D loss: 0.6934458017349243] [G loss: 0.6034809350967407]\n",
      "[Epoch 56/1001] [Batch 81/372] [D loss: 0.6946873664855957] [G loss: 0.800333559513092]\n",
      "[Epoch 56/1001] [Batch 82/372] [D loss: 0.6983627080917358] [G loss: 0.5537397265434265]\n",
      "[Epoch 56/1001] [Batch 83/372] [D loss: 0.7000337839126587] [G loss: 0.8499699234962463]\n",
      "[Epoch 56/1001] [Batch 84/372] [D loss: 0.7022403478622437] [G loss: 0.5461193919181824]\n",
      "[Epoch 56/1001] [Batch 85/372] [D loss: 0.7039761543273926] [G loss: 0.8370219469070435]\n",
      "[Epoch 56/1001] [Batch 86/372] [D loss: 0.7005589604377747] [G loss: 0.5710206627845764]\n",
      "[Epoch 56/1001] [Batch 87/372] [D loss: 0.6988136768341064] [G loss: 0.7754879593849182]\n",
      "[Epoch 56/1001] [Batch 88/372] [D loss: 0.694417417049408] [G loss: 0.6347656846046448]\n",
      "[Epoch 56/1001] [Batch 89/372] [D loss: 0.6941522359848022] [G loss: 0.7077089548110962]\n",
      "[Epoch 56/1001] [Batch 90/372] [D loss: 0.6906446218490601] [G loss: 0.6801996231079102]\n",
      "[Epoch 56/1001] [Batch 91/372] [D loss: 0.6896370053291321] [G loss: 0.6731069684028625]\n",
      "[Epoch 56/1001] [Batch 92/372] [D loss: 0.6915900707244873] [G loss: 0.7059878706932068]\n",
      "[Epoch 56/1001] [Batch 93/372] [D loss: 0.6920942068099976] [G loss: 0.6560622453689575]\n",
      "[Epoch 56/1001] [Batch 94/372] [D loss: 0.6930561065673828] [G loss: 0.7114814519882202]\n",
      "[Epoch 56/1001] [Batch 95/372] [D loss: 0.6897296905517578] [G loss: 0.6493808031082153]\n",
      "[Epoch 56/1001] [Batch 96/372] [D loss: 0.691393256187439] [G loss: 0.7224195003509521]\n",
      "[Epoch 56/1001] [Batch 97/372] [D loss: 0.6943391561508179] [G loss: 0.6358755826950073]\n",
      "[Epoch 56/1001] [Batch 98/372] [D loss: 0.6921176910400391] [G loss: 0.7395269870758057]\n",
      "[Epoch 56/1001] [Batch 99/372] [D loss: 0.6932174563407898] [G loss: 0.6494450569152832]\n",
      "[Epoch 56/1001] [Batch 100/372] [D loss: 0.6883035898208618] [G loss: 0.697452187538147]\n",
      "[Epoch 56/1001] [Batch 101/372] [D loss: 0.6911541819572449] [G loss: 0.6721256971359253]\n",
      "[Epoch 56/1001] [Batch 102/372] [D loss: 0.6912952065467834] [G loss: 0.6989589929580688]\n",
      "[Epoch 56/1001] [Batch 103/372] [D loss: 0.6940990090370178] [G loss: 0.6793426871299744]\n",
      "[Epoch 56/1001] [Batch 104/372] [D loss: 0.6919803619384766] [G loss: 0.6737058758735657]\n",
      "[Epoch 56/1001] [Batch 105/372] [D loss: 0.6917481422424316] [G loss: 0.6967739462852478]\n",
      "[Epoch 56/1001] [Batch 106/372] [D loss: 0.694226861000061] [G loss: 0.6584644317626953]\n",
      "[Epoch 56/1001] [Batch 107/372] [D loss: 0.6911196708679199] [G loss: 0.717363715171814]\n",
      "[Epoch 56/1001] [Batch 108/372] [D loss: 0.6946733593940735] [G loss: 0.6348817944526672]\n",
      "[Epoch 56/1001] [Batch 109/372] [D loss: 0.6927177309989929] [G loss: 0.7400918006896973]\n",
      "[Epoch 56/1001] [Batch 110/372] [D loss: 0.6928250193595886] [G loss: 0.6364901065826416]\n",
      "[Epoch 56/1001] [Batch 111/372] [D loss: 0.6910717487335205] [G loss: 0.7293428778648376]\n",
      "[Epoch 56/1001] [Batch 112/372] [D loss: 0.6912136077880859] [G loss: 0.6406165361404419]\n",
      "[Epoch 56/1001] [Batch 113/372] [D loss: 0.6898816823959351] [G loss: 0.7391654849052429]\n",
      "[Epoch 56/1001] [Batch 114/372] [D loss: 0.69157874584198] [G loss: 0.6033190488815308]\n",
      "[Epoch 56/1001] [Batch 115/372] [D loss: 0.6978521943092346] [G loss: 0.800459623336792]\n",
      "[Epoch 56/1001] [Batch 116/372] [D loss: 0.6955721378326416] [G loss: 0.5654158592224121]\n",
      "[Epoch 56/1001] [Batch 117/372] [D loss: 0.6990624070167542] [G loss: 0.8461760878562927]\n",
      "[Epoch 56/1001] [Batch 118/372] [D loss: 0.7038095593452454] [G loss: 0.5390048027038574]\n",
      "[Epoch 56/1001] [Batch 119/372] [D loss: 0.7060997486114502] [G loss: 0.8319628238677979]\n",
      "[Epoch 56/1001] [Batch 120/372] [D loss: 0.700774073600769] [G loss: 0.5859702825546265]\n",
      "[Epoch 56/1001] [Batch 121/372] [D loss: 0.6982071995735168] [G loss: 0.757623553276062]\n",
      "[Epoch 56/1001] [Batch 122/372] [D loss: 0.6922467947006226] [G loss: 0.6531530022621155]\n",
      "[Epoch 56/1001] [Batch 123/372] [D loss: 0.6911835670471191] [G loss: 0.6890004277229309]\n",
      "[Epoch 56/1001] [Batch 124/372] [D loss: 0.6910953521728516] [G loss: 0.6925884485244751]\n",
      "[Epoch 56/1001] [Batch 125/372] [D loss: 0.694199800491333] [G loss: 0.6735699772834778]\n",
      "[Epoch 56/1001] [Batch 126/372] [D loss: 0.6925259828567505] [G loss: 0.6946942210197449]\n",
      "[Epoch 56/1001] [Batch 127/372] [D loss: 0.6932278871536255] [G loss: 0.6776627898216248]\n",
      "[Epoch 56/1001] [Batch 128/372] [D loss: 0.6934579610824585] [G loss: 0.6825550198554993]\n",
      "[Epoch 56/1001] [Batch 129/372] [D loss: 0.6920472383499146] [G loss: 0.6870978474617004]\n",
      "[Epoch 56/1001] [Batch 130/372] [D loss: 0.6929754018783569] [G loss: 0.6831900477409363]\n",
      "[Epoch 56/1001] [Batch 131/372] [D loss: 0.6914212107658386] [G loss: 0.6849427223205566]\n",
      "[Epoch 56/1001] [Batch 132/372] [D loss: 0.6892348527908325] [G loss: 0.691705584526062]\n",
      "[Epoch 56/1001] [Batch 133/372] [D loss: 0.6924070119857788] [G loss: 0.6653913855552673]\n",
      "[Epoch 56/1001] [Batch 134/372] [D loss: 0.6908360123634338] [G loss: 0.7145874500274658]\n",
      "[Epoch 56/1001] [Batch 135/372] [D loss: 0.6899043321609497] [G loss: 0.6474571228027344]\n",
      "[Epoch 56/1001] [Batch 136/372] [D loss: 0.6916917562484741] [G loss: 0.7242874503135681]\n",
      "[Epoch 56/1001] [Batch 137/372] [D loss: 0.6905077695846558] [G loss: 0.6314917802810669]\n",
      "[Epoch 56/1001] [Batch 138/372] [D loss: 0.692581057548523] [G loss: 0.7569906711578369]\n",
      "[Epoch 56/1001] [Batch 139/372] [D loss: 0.6944962739944458] [G loss: 0.6052746176719666]\n",
      "[Epoch 56/1001] [Batch 140/372] [D loss: 0.6920369863510132] [G loss: 0.7697768211364746]\n",
      "[Epoch 56/1001] [Batch 141/372] [D loss: 0.6887334585189819] [G loss: 0.5911109447479248]\n",
      "[Epoch 56/1001] [Batch 142/372] [D loss: 0.6945066452026367] [G loss: 0.8314468860626221]\n",
      "[Epoch 56/1001] [Batch 143/372] [D loss: 0.6985293626785278] [G loss: 0.5367684364318848]\n",
      "[Epoch 56/1001] [Batch 144/372] [D loss: 0.7004404664039612] [G loss: 0.8712117075920105]\n",
      "[Epoch 56/1001] [Batch 145/372] [D loss: 0.698686957359314] [G loss: 0.5454707741737366]\n",
      "[Epoch 56/1001] [Batch 146/372] [D loss: 0.7037050724029541] [G loss: 0.8267093896865845]\n",
      "[Epoch 56/1001] [Batch 147/372] [D loss: 0.7012536525726318] [G loss: 0.5858770608901978]\n",
      "[Epoch 56/1001] [Batch 148/372] [D loss: 0.69679856300354] [G loss: 0.7445801496505737]\n",
      "[Epoch 56/1001] [Batch 149/372] [D loss: 0.6954906582832336] [G loss: 0.6599747538566589]\n",
      "[Epoch 56/1001] [Batch 150/372] [D loss: 0.6913111209869385] [G loss: 0.6807031035423279]\n",
      "[Epoch 56/1001] [Batch 151/372] [D loss: 0.6903238296508789] [G loss: 0.6973702311515808]\n",
      "[Epoch 56/1001] [Batch 152/372] [D loss: 0.6902316808700562] [G loss: 0.6690652966499329]\n",
      "[Epoch 56/1001] [Batch 153/372] [D loss: 0.6883436441421509] [G loss: 0.6905804872512817]\n",
      "[Epoch 56/1001] [Batch 154/372] [D loss: 0.6896762847900391] [G loss: 0.6814493536949158]\n",
      "[Epoch 56/1001] [Batch 155/372] [D loss: 0.6906188726425171] [G loss: 0.6640191674232483]\n",
      "[Epoch 56/1001] [Batch 156/372] [D loss: 0.6903070211410522] [G loss: 0.7015706896781921]\n",
      "[Epoch 56/1001] [Batch 157/372] [D loss: 0.693122923374176] [G loss: 0.670670211315155]\n",
      "[Epoch 56/1001] [Batch 158/372] [D loss: 0.6901977062225342] [G loss: 0.6883299946784973]\n",
      "[Epoch 56/1001] [Batch 159/372] [D loss: 0.6926647424697876] [G loss: 0.6687600016593933]\n",
      "[Epoch 56/1001] [Batch 160/372] [D loss: 0.6913183927536011] [G loss: 0.6899554133415222]\n",
      "[Epoch 56/1001] [Batch 161/372] [D loss: 0.6944601535797119] [G loss: 0.6809771060943604]\n",
      "[Epoch 56/1001] [Batch 162/372] [D loss: 0.6932252049446106] [G loss: 0.6739405393600464]\n",
      "[Epoch 56/1001] [Batch 163/372] [D loss: 0.6928287744522095] [G loss: 0.6840662956237793]\n",
      "[Epoch 56/1001] [Batch 164/372] [D loss: 0.6929218769073486] [G loss: 0.6858949661254883]\n",
      "[Epoch 56/1001] [Batch 165/372] [D loss: 0.6933006048202515] [G loss: 0.6674854755401611]\n",
      "[Epoch 56/1001] [Batch 166/372] [D loss: 0.6920431852340698] [G loss: 0.7056415677070618]\n",
      "[Epoch 56/1001] [Batch 167/372] [D loss: 0.6914125084877014] [G loss: 0.6423860192298889]\n",
      "[Epoch 56/1001] [Batch 168/372] [D loss: 0.691949188709259] [G loss: 0.7201405763626099]\n",
      "[Epoch 56/1001] [Batch 169/372] [D loss: 0.6924507021903992] [G loss: 0.6512520909309387]\n",
      "[Epoch 56/1001] [Batch 170/372] [D loss: 0.6958476305007935] [G loss: 0.7378479242324829]\n",
      "[Epoch 56/1001] [Batch 171/372] [D loss: 0.6930962800979614] [G loss: 0.6075723767280579]\n",
      "[Epoch 56/1001] [Batch 172/372] [D loss: 0.6940997838973999] [G loss: 0.7819516062736511]\n",
      "[Epoch 56/1001] [Batch 173/372] [D loss: 0.6932899951934814] [G loss: 0.5841973423957825]\n",
      "[Epoch 56/1001] [Batch 174/372] [D loss: 0.6964643597602844] [G loss: 0.7926173210144043]\n",
      "[Epoch 56/1001] [Batch 175/372] [D loss: 0.6946424841880798] [G loss: 0.5869892835617065]\n",
      "[Epoch 56/1001] [Batch 176/372] [D loss: 0.695383608341217] [G loss: 0.7827426791191101]\n",
      "[Epoch 56/1001] [Batch 177/372] [D loss: 0.6970656514167786] [G loss: 0.5994308590888977]\n",
      "[Epoch 56/1001] [Batch 178/372] [D loss: 0.6969161629676819] [G loss: 0.7584145665168762]\n",
      "[Epoch 56/1001] [Batch 179/372] [D loss: 0.6947190761566162] [G loss: 0.634833574295044]\n",
      "[Epoch 56/1001] [Batch 180/372] [D loss: 0.690912127494812] [G loss: 0.7120197415351868]\n",
      "[Epoch 56/1001] [Batch 181/372] [D loss: 0.6920457482337952] [G loss: 0.6737792491912842]\n",
      "[Epoch 56/1001] [Batch 182/372] [D loss: 0.6905788779258728] [G loss: 0.6790872812271118]\n",
      "[Epoch 56/1001] [Batch 183/372] [D loss: 0.6915144920349121] [G loss: 0.684959352016449]\n",
      "[Epoch 56/1001] [Batch 184/372] [D loss: 0.6919043064117432] [G loss: 0.6825164556503296]\n",
      "[Epoch 56/1001] [Batch 185/372] [D loss: 0.689659833908081] [G loss: 0.699832022190094]\n",
      "[Epoch 56/1001] [Batch 186/372] [D loss: 0.6924672722816467] [G loss: 0.6535083055496216]\n",
      "[Epoch 56/1001] [Batch 187/372] [D loss: 0.6907747983932495] [G loss: 0.7045540809631348]\n",
      "[Epoch 56/1001] [Batch 188/372] [D loss: 0.6906270980834961] [G loss: 0.6694599390029907]\n",
      "[Epoch 56/1001] [Batch 189/372] [D loss: 0.6927998065948486] [G loss: 0.6898292303085327]\n",
      "[Epoch 56/1001] [Batch 190/372] [D loss: 0.6921950578689575] [G loss: 0.6755268573760986]\n",
      "[Epoch 56/1001] [Batch 191/372] [D loss: 0.6902319192886353] [G loss: 0.6951726675033569]\n",
      "[Epoch 56/1001] [Batch 192/372] [D loss: 0.6914089322090149] [G loss: 0.668562114238739]\n",
      "[Epoch 56/1001] [Batch 193/372] [D loss: 0.6883965730667114] [G loss: 0.7076917290687561]\n",
      "[Epoch 56/1001] [Batch 194/372] [D loss: 0.6915210485458374] [G loss: 0.6574270725250244]\n",
      "[Epoch 56/1001] [Batch 195/372] [D loss: 0.6928302645683289] [G loss: 0.7076557874679565]\n",
      "[Epoch 56/1001] [Batch 196/372] [D loss: 0.6886304616928101] [G loss: 0.650933563709259]\n",
      "[Epoch 56/1001] [Batch 197/372] [D loss: 0.6930623054504395] [G loss: 0.7223085761070251]\n",
      "[Epoch 56/1001] [Batch 198/372] [D loss: 0.6921140551567078] [G loss: 0.6419850587844849]\n",
      "[Epoch 56/1001] [Batch 199/372] [D loss: 0.6904693841934204] [G loss: 0.7383121252059937]\n",
      "[Epoch 56/1001] [Batch 200/372] [D loss: 0.6912120580673218] [G loss: 0.6193058490753174]\n",
      "[Epoch 56/1001] [Batch 201/372] [D loss: 0.6920701265335083] [G loss: 0.773434042930603]\n",
      "[Epoch 56/1001] [Batch 202/372] [D loss: 0.6939605474472046] [G loss: 0.589439332485199]\n",
      "[Epoch 56/1001] [Batch 203/372] [D loss: 0.6979426741600037] [G loss: 0.8004676103591919]\n",
      "[Epoch 56/1001] [Batch 204/372] [D loss: 0.702133297920227] [G loss: 0.5537621974945068]\n",
      "[Epoch 56/1001] [Batch 205/372] [D loss: 0.7023589611053467] [G loss: 0.8307073712348938]\n",
      "[Epoch 56/1001] [Batch 206/372] [D loss: 0.7014292478561401] [G loss: 0.5658344030380249]\n",
      "[Epoch 56/1001] [Batch 207/372] [D loss: 0.698905885219574] [G loss: 0.7912922501564026]\n",
      "[Epoch 56/1001] [Batch 208/372] [D loss: 0.6962445378303528] [G loss: 0.6092721223831177]\n",
      "[Epoch 56/1001] [Batch 209/372] [D loss: 0.6954901218414307] [G loss: 0.7284830808639526]\n",
      "[Epoch 56/1001] [Batch 210/372] [D loss: 0.6911840438842773] [G loss: 0.6610247492790222]\n",
      "[Epoch 56/1001] [Batch 211/372] [D loss: 0.6939493417739868] [G loss: 0.6975033283233643]\n",
      "[Epoch 56/1001] [Batch 212/372] [D loss: 0.692693829536438] [G loss: 0.6760379672050476]\n",
      "[Epoch 56/1001] [Batch 213/372] [D loss: 0.6893611550331116] [G loss: 0.682396411895752]\n",
      "[Epoch 56/1001] [Batch 214/372] [D loss: 0.6920270919799805] [G loss: 0.6732885837554932]\n",
      "[Epoch 56/1001] [Batch 215/372] [D loss: 0.6910763382911682] [G loss: 0.695145845413208]\n",
      "[Epoch 56/1001] [Batch 216/372] [D loss: 0.6909431219100952] [G loss: 0.6654060482978821]\n",
      "[Epoch 56/1001] [Batch 217/372] [D loss: 0.6924827694892883] [G loss: 0.7017425894737244]\n",
      "[Epoch 56/1001] [Batch 218/372] [D loss: 0.6886630058288574] [G loss: 0.6641167402267456]\n",
      "[Epoch 56/1001] [Batch 219/372] [D loss: 0.6945663690567017] [G loss: 0.6953101754188538]\n",
      "[Epoch 56/1001] [Batch 220/372] [D loss: 0.6904649138450623] [G loss: 0.6774075031280518]\n",
      "[Epoch 56/1001] [Batch 221/372] [D loss: 0.6917508840560913] [G loss: 0.6785097718238831]\n",
      "[Epoch 56/1001] [Batch 222/372] [D loss: 0.6912808418273926] [G loss: 0.6937128901481628]\n",
      "[Epoch 56/1001] [Batch 223/372] [D loss: 0.6902391314506531] [G loss: 0.6670304536819458]\n",
      "[Epoch 56/1001] [Batch 224/372] [D loss: 0.6916061043739319] [G loss: 0.7130586504936218]\n",
      "[Epoch 56/1001] [Batch 225/372] [D loss: 0.690737247467041] [G loss: 0.6464074850082397]\n",
      "[Epoch 56/1001] [Batch 226/372] [D loss: 0.6935595273971558] [G loss: 0.732891321182251]\n",
      "[Epoch 56/1001] [Batch 227/372] [D loss: 0.6895595788955688] [G loss: 0.6350260972976685]\n",
      "[Epoch 56/1001] [Batch 228/372] [D loss: 0.6894354820251465] [G loss: 0.7304625511169434]\n",
      "[Epoch 56/1001] [Batch 229/372] [D loss: 0.6933068037033081] [G loss: 0.6278454661369324]\n",
      "[Epoch 56/1001] [Batch 230/372] [D loss: 0.6934419870376587] [G loss: 0.7611969113349915]\n",
      "[Epoch 56/1001] [Batch 231/372] [D loss: 0.6973808407783508] [G loss: 0.6027215123176575]\n",
      "[Epoch 56/1001] [Batch 232/372] [D loss: 0.6940878033638] [G loss: 0.7725476026535034]\n",
      "[Epoch 56/1001] [Batch 233/372] [D loss: 0.6933177709579468] [G loss: 0.5976629257202148]\n",
      "[Epoch 56/1001] [Batch 234/372] [D loss: 0.6945083737373352] [G loss: 0.7948479652404785]\n",
      "[Epoch 56/1001] [Batch 235/372] [D loss: 0.6936403512954712] [G loss: 0.5792946815490723]\n",
      "[Epoch 56/1001] [Batch 236/372] [D loss: 0.6973282098770142] [G loss: 0.7928703427314758]\n",
      "[Epoch 56/1001] [Batch 237/372] [D loss: 0.6922701001167297] [G loss: 0.597133994102478]\n",
      "[Epoch 56/1001] [Batch 238/372] [D loss: 0.6956011056900024] [G loss: 0.7525138854980469]\n",
      "[Epoch 56/1001] [Batch 239/372] [D loss: 0.6932706236839294] [G loss: 0.6370059847831726]\n",
      "[Epoch 56/1001] [Batch 240/372] [D loss: 0.6906229257583618] [G loss: 0.7203392386436462]\n",
      "[Epoch 56/1001] [Batch 241/372] [D loss: 0.6904737949371338] [G loss: 0.6476582288742065]\n",
      "[Epoch 56/1001] [Batch 242/372] [D loss: 0.6928958892822266] [G loss: 0.7123473882675171]\n",
      "[Epoch 56/1001] [Batch 243/372] [D loss: 0.6868312954902649] [G loss: 0.6587942242622375]\n",
      "[Epoch 56/1001] [Batch 244/372] [D loss: 0.6848645210266113] [G loss: 0.7038310170173645]\n",
      "[Epoch 56/1001] [Batch 245/372] [D loss: 0.6946381330490112] [G loss: 0.6710442304611206]\n",
      "[Epoch 56/1001] [Batch 246/372] [D loss: 0.6866675019264221] [G loss: 0.6852023601531982]\n",
      "[Epoch 56/1001] [Batch 247/372] [D loss: 0.6909488439559937] [G loss: 0.6701757907867432]\n",
      "[Epoch 56/1001] [Batch 248/372] [D loss: 0.6923608779907227] [G loss: 0.7129501104354858]\n",
      "[Epoch 56/1001] [Batch 249/372] [D loss: 0.6933087110519409] [G loss: 0.62273108959198]\n",
      "[Epoch 56/1001] [Batch 250/372] [D loss: 0.6892964839935303] [G loss: 0.7571346759796143]\n",
      "[Epoch 56/1001] [Batch 251/372] [D loss: 0.6954619884490967] [G loss: 0.6080917716026306]\n",
      "[Epoch 56/1001] [Batch 252/372] [D loss: 0.6951018571853638] [G loss: 0.7565931081771851]\n",
      "[Epoch 56/1001] [Batch 253/372] [D loss: 0.6889398097991943] [G loss: 0.6010871529579163]\n",
      "[Epoch 56/1001] [Batch 254/372] [D loss: 0.6964937448501587] [G loss: 0.7891181111335754]\n",
      "[Epoch 56/1001] [Batch 255/372] [D loss: 0.6993348598480225] [G loss: 0.5792639255523682]\n",
      "[Epoch 56/1001] [Batch 256/372] [D loss: 0.698840856552124] [G loss: 0.7957205772399902]\n",
      "[Epoch 56/1001] [Batch 257/372] [D loss: 0.695185661315918] [G loss: 0.587155818939209]\n",
      "[Epoch 56/1001] [Batch 258/372] [D loss: 0.6962696313858032] [G loss: 0.782318115234375]\n",
      "[Epoch 56/1001] [Batch 259/372] [D loss: 0.6912896037101746] [G loss: 0.5946955680847168]\n",
      "[Epoch 56/1001] [Batch 260/372] [D loss: 0.6940879225730896] [G loss: 0.7675302028656006]\n",
      "[Epoch 56/1001] [Batch 261/372] [D loss: 0.6929177045822144] [G loss: 0.6149014234542847]\n",
      "[Epoch 56/1001] [Batch 262/372] [D loss: 0.6925978660583496] [G loss: 0.732219398021698]\n",
      "[Epoch 56/1001] [Batch 263/372] [D loss: 0.6933938264846802] [G loss: 0.6485240459442139]\n",
      "[Epoch 56/1001] [Batch 264/372] [D loss: 0.6928970813751221] [G loss: 0.7053667306900024]\n",
      "[Epoch 56/1001] [Batch 265/372] [D loss: 0.6903919577598572] [G loss: 0.652920126914978]\n",
      "[Epoch 56/1001] [Batch 266/372] [D loss: 0.6912054419517517] [G loss: 0.7213367223739624]\n",
      "[Epoch 56/1001] [Batch 267/372] [D loss: 0.6920968890190125] [G loss: 0.6339420676231384]\n",
      "[Epoch 56/1001] [Batch 268/372] [D loss: 0.6926888227462769] [G loss: 0.7303894758224487]\n",
      "[Epoch 56/1001] [Batch 269/372] [D loss: 0.6914240121841431] [G loss: 0.6403650641441345]\n",
      "[Epoch 56/1001] [Batch 270/372] [D loss: 0.6894083023071289] [G loss: 0.7256511449813843]\n",
      "[Epoch 56/1001] [Batch 271/372] [D loss: 0.6905476450920105] [G loss: 0.6460416316986084]\n",
      "[Epoch 56/1001] [Batch 272/372] [D loss: 0.6896840333938599] [G loss: 0.7145976424217224]\n",
      "[Epoch 56/1001] [Batch 273/372] [D loss: 0.6900781989097595] [G loss: 0.6344423294067383]\n",
      "[Epoch 56/1001] [Batch 274/372] [D loss: 0.6888419389724731] [G loss: 0.7689061164855957]\n",
      "[Epoch 56/1001] [Batch 275/372] [D loss: 0.6936181783676147] [G loss: 0.5748958587646484]\n",
      "[Epoch 56/1001] [Batch 276/372] [D loss: 0.6976228952407837] [G loss: 0.8562576770782471]\n",
      "[Epoch 56/1001] [Batch 277/372] [D loss: 0.6999670267105103] [G loss: 0.5034390091896057]\n",
      "[Epoch 56/1001] [Batch 278/372] [D loss: 0.7101091146469116] [G loss: 0.910499095916748]\n",
      "[Epoch 56/1001] [Batch 279/372] [D loss: 0.7121090888977051] [G loss: 0.5234407186508179]\n",
      "[Epoch 56/1001] [Batch 280/372] [D loss: 0.7070200443267822] [G loss: 0.8032271862030029]\n",
      "[Epoch 56/1001] [Batch 281/372] [D loss: 0.6954665184020996] [G loss: 0.6226034760475159]\n",
      "[Epoch 56/1001] [Batch 282/372] [D loss: 0.6896747350692749] [G loss: 0.7100322842597961]\n",
      "[Epoch 56/1001] [Batch 283/372] [D loss: 0.6883875131607056] [G loss: 0.6785386204719543]\n",
      "[Epoch 56/1001] [Batch 284/372] [D loss: 0.6923810839653015] [G loss: 0.6820477843284607]\n",
      "[Epoch 56/1001] [Batch 285/372] [D loss: 0.692436695098877] [G loss: 0.669576108455658]\n",
      "[Epoch 56/1001] [Batch 286/372] [D loss: 0.6890746355056763] [G loss: 0.6866743564605713]\n",
      "[Epoch 56/1001] [Batch 287/372] [D loss: 0.6924574375152588] [G loss: 0.6725045442581177]\n",
      "[Epoch 56/1001] [Batch 288/372] [D loss: 0.6900174617767334] [G loss: 0.6827868223190308]\n",
      "[Epoch 56/1001] [Batch 289/372] [D loss: 0.6905249953269958] [G loss: 0.7052397727966309]\n",
      "[Epoch 56/1001] [Batch 290/372] [D loss: 0.6910642385482788] [G loss: 0.6674304008483887]\n",
      "[Epoch 56/1001] [Batch 291/372] [D loss: 0.690604031085968] [G loss: 0.6956440806388855]\n",
      "[Epoch 56/1001] [Batch 292/372] [D loss: 0.694681704044342] [G loss: 0.6597155928611755]\n",
      "[Epoch 56/1001] [Batch 293/372] [D loss: 0.6958610415458679] [G loss: 0.687129020690918]\n",
      "[Epoch 56/1001] [Batch 294/372] [D loss: 0.6892833709716797] [G loss: 0.6856557130813599]\n",
      "[Epoch 56/1001] [Batch 295/372] [D loss: 0.6901435852050781] [G loss: 0.6863528490066528]\n",
      "[Epoch 56/1001] [Batch 296/372] [D loss: 0.6926053762435913] [G loss: 0.6731870174407959]\n",
      "[Epoch 56/1001] [Batch 297/372] [D loss: 0.6896388530731201] [G loss: 0.6998763680458069]\n",
      "[Epoch 56/1001] [Batch 298/372] [D loss: 0.6893699169158936] [G loss: 0.6646257042884827]\n",
      "[Epoch 56/1001] [Batch 299/372] [D loss: 0.6905674934387207] [G loss: 0.6928679943084717]\n",
      "[Epoch 56/1001] [Batch 300/372] [D loss: 0.6913222074508667] [G loss: 0.6686384677886963]\n",
      "[Epoch 56/1001] [Batch 301/372] [D loss: 0.6917984485626221] [G loss: 0.7006414532661438]\n",
      "[Epoch 56/1001] [Batch 302/372] [D loss: 0.6887807250022888] [G loss: 0.6606260538101196]\n",
      "[Epoch 56/1001] [Batch 303/372] [D loss: 0.6912128925323486] [G loss: 0.7088197469711304]\n",
      "[Epoch 56/1001] [Batch 304/372] [D loss: 0.6910756826400757] [G loss: 0.6533141136169434]\n",
      "[Epoch 56/1001] [Batch 305/372] [D loss: 0.6921199560165405] [G loss: 0.7065975666046143]\n",
      "[Epoch 56/1001] [Batch 306/372] [D loss: 0.6882075071334839] [G loss: 0.6593015789985657]\n",
      "[Epoch 56/1001] [Batch 307/372] [D loss: 0.6909831762313843] [G loss: 0.7124037146568298]\n",
      "[Epoch 56/1001] [Batch 308/372] [D loss: 0.692632257938385] [G loss: 0.648879885673523]\n",
      "[Epoch 56/1001] [Batch 309/372] [D loss: 0.6928096413612366] [G loss: 0.7164561748504639]\n",
      "[Epoch 56/1001] [Batch 310/372] [D loss: 0.6940528154373169] [G loss: 0.6354619860649109]\n",
      "[Epoch 56/1001] [Batch 311/372] [D loss: 0.6937460899353027] [G loss: 0.739059567451477]\n",
      "[Epoch 56/1001] [Batch 312/372] [D loss: 0.6958041787147522] [G loss: 0.6142053604125977]\n",
      "[Epoch 56/1001] [Batch 313/372] [D loss: 0.6944214105606079] [G loss: 0.7618871927261353]\n",
      "[Epoch 56/1001] [Batch 314/372] [D loss: 0.6960006356239319] [G loss: 0.6032404899597168]\n",
      "[Epoch 56/1001] [Batch 315/372] [D loss: 0.6935032606124878] [G loss: 0.7839590907096863]\n",
      "[Epoch 56/1001] [Batch 316/372] [D loss: 0.6967605352401733] [G loss: 0.5782421231269836]\n",
      "[Epoch 56/1001] [Batch 317/372] [D loss: 0.6993645429611206] [G loss: 0.8340034484863281]\n",
      "[Epoch 56/1001] [Batch 318/372] [D loss: 0.7017669677734375] [G loss: 0.5502753257751465]\n",
      "[Epoch 56/1001] [Batch 319/372] [D loss: 0.7016840577125549] [G loss: 0.8165075182914734]\n",
      "[Epoch 56/1001] [Batch 320/372] [D loss: 0.6978163719177246] [G loss: 0.6000708937644958]\n",
      "[Epoch 56/1001] [Batch 321/372] [D loss: 0.6953959465026855] [G loss: 0.7339322566986084]\n",
      "[Epoch 56/1001] [Batch 322/372] [D loss: 0.6931179761886597] [G loss: 0.6614684462547302]\n",
      "[Epoch 56/1001] [Batch 323/372] [D loss: 0.6904017925262451] [G loss: 0.6808950304985046]\n",
      "[Epoch 56/1001] [Batch 324/372] [D loss: 0.6907297968864441] [G loss: 0.702678382396698]\n",
      "[Epoch 56/1001] [Batch 325/372] [D loss: 0.6914247274398804] [G loss: 0.658970296382904]\n",
      "[Epoch 56/1001] [Batch 326/372] [D loss: 0.694354772567749] [G loss: 0.7093368172645569]\n",
      "[Epoch 56/1001] [Batch 327/372] [D loss: 0.6917431354522705] [G loss: 0.6515638828277588]\n",
      "[Epoch 56/1001] [Batch 328/372] [D loss: 0.6896227598190308] [G loss: 0.7137928009033203]\n",
      "[Epoch 56/1001] [Batch 329/372] [D loss: 0.6932817697525024] [G loss: 0.656306266784668]\n",
      "[Epoch 56/1001] [Batch 330/372] [D loss: 0.6934789419174194] [G loss: 0.6952537298202515]\n",
      "[Epoch 56/1001] [Batch 331/372] [D loss: 0.6924259662628174] [G loss: 0.6788048148155212]\n",
      "[Epoch 56/1001] [Batch 332/372] [D loss: 0.6931792497634888] [G loss: 0.6807547807693481]\n",
      "[Epoch 56/1001] [Batch 333/372] [D loss: 0.6910049319267273] [G loss: 0.6797804236412048]\n",
      "[Epoch 56/1001] [Batch 334/372] [D loss: 0.6930276155471802] [G loss: 0.6932976841926575]\n",
      "[Epoch 56/1001] [Batch 335/372] [D loss: 0.6938815116882324] [G loss: 0.6672221422195435]\n",
      "[Epoch 56/1001] [Batch 336/372] [D loss: 0.6915482878684998] [G loss: 0.6939806342124939]\n",
      "[Epoch 56/1001] [Batch 337/372] [D loss: 0.6914894580841064] [G loss: 0.6727858185768127]\n",
      "[Epoch 56/1001] [Batch 338/372] [D loss: 0.6908056735992432] [G loss: 0.6970906257629395]\n",
      "[Epoch 56/1001] [Batch 339/372] [D loss: 0.6904695630073547] [G loss: 0.6738196015357971]\n",
      "[Epoch 56/1001] [Batch 340/372] [D loss: 0.689511239528656] [G loss: 0.6890329122543335]\n",
      "[Epoch 56/1001] [Batch 341/372] [D loss: 0.6911242008209229] [G loss: 0.6745318174362183]\n",
      "[Epoch 56/1001] [Batch 342/372] [D loss: 0.6924655437469482] [G loss: 0.6892809867858887]\n",
      "[Epoch 56/1001] [Batch 343/372] [D loss: 0.6909205913543701] [G loss: 0.6952320337295532]\n",
      "[Epoch 56/1001] [Batch 344/372] [D loss: 0.6927720308303833] [G loss: 0.6601496338844299]\n",
      "[Epoch 56/1001] [Batch 345/372] [D loss: 0.6912786364555359] [G loss: 0.7222384214401245]\n",
      "[Epoch 56/1001] [Batch 346/372] [D loss: 0.6921828389167786] [G loss: 0.6430881023406982]\n",
      "[Epoch 56/1001] [Batch 347/372] [D loss: 0.6919490098953247] [G loss: 0.7171763181686401]\n",
      "[Epoch 56/1001] [Batch 348/372] [D loss: 0.6922930479049683] [G loss: 0.6491069793701172]\n",
      "[Epoch 56/1001] [Batch 349/372] [D loss: 0.6951184272766113] [G loss: 0.7119538187980652]\n",
      "[Epoch 56/1001] [Batch 350/372] [D loss: 0.6891171336174011] [G loss: 0.6630662679672241]\n",
      "[Epoch 56/1001] [Batch 351/372] [D loss: 0.6884156465530396] [G loss: 0.7029433250427246]\n",
      "[Epoch 56/1001] [Batch 352/372] [D loss: 0.6890926361083984] [G loss: 0.659726619720459]\n",
      "[Epoch 56/1001] [Batch 353/372] [D loss: 0.691068172454834] [G loss: 0.7074431777000427]\n",
      "[Epoch 56/1001] [Batch 354/372] [D loss: 0.6925824284553528] [G loss: 0.6420297026634216]\n",
      "[Epoch 56/1001] [Batch 355/372] [D loss: 0.6964598894119263] [G loss: 0.7355703115463257]\n",
      "[Epoch 56/1001] [Batch 356/372] [D loss: 0.6981539726257324] [G loss: 0.6178560256958008]\n",
      "[Epoch 56/1001] [Batch 357/372] [D loss: 0.6966226696968079] [G loss: 0.7522474527359009]\n",
      "[Epoch 56/1001] [Batch 358/372] [D loss: 0.694715142250061] [G loss: 0.6058588624000549]\n",
      "[Epoch 56/1001] [Batch 359/372] [D loss: 0.6919881701469421] [G loss: 0.7811912298202515]\n",
      "[Epoch 56/1001] [Batch 360/372] [D loss: 0.693304717540741] [G loss: 0.5923712253570557]\n",
      "[Epoch 56/1001] [Batch 361/372] [D loss: 0.6974188089370728] [G loss: 0.7885732054710388]\n",
      "[Epoch 56/1001] [Batch 362/372] [D loss: 0.6963401436805725] [G loss: 0.5884890556335449]\n",
      "[Epoch 56/1001] [Batch 363/372] [D loss: 0.6982235908508301] [G loss: 0.7851182222366333]\n",
      "[Epoch 56/1001] [Batch 364/372] [D loss: 0.6951389312744141] [G loss: 0.6014584302902222]\n",
      "[Epoch 56/1001] [Batch 365/372] [D loss: 0.6924649477005005] [G loss: 0.7540224194526672]\n",
      "[Epoch 56/1001] [Batch 366/372] [D loss: 0.6900700926780701] [G loss: 0.6364123225212097]\n",
      "[Epoch 56/1001] [Batch 367/372] [D loss: 0.6914693117141724] [G loss: 0.7256057858467102]\n",
      "[Epoch 56/1001] [Batch 368/372] [D loss: 0.6922245025634766] [G loss: 0.6509281396865845]\n",
      "[Epoch 56/1001] [Batch 369/372] [D loss: 0.6921806335449219] [G loss: 0.711003303527832]\n",
      "[Epoch 56/1001] [Batch 370/372] [D loss: 0.6905013918876648] [G loss: 0.6454455852508545]\n",
      "[Epoch 56/1001] [Batch 371/372] [D loss: 0.6922293901443481] [G loss: 0.7278369069099426]\n",
      "[Epoch 57/1001] [Batch 0/372] [D loss: 0.6874083280563354] [G loss: 0.6102350950241089]\n",
      "[Epoch 57/1001] [Batch 1/372] [D loss: 0.6967983245849609] [G loss: 0.8289486169815063]\n",
      "[Epoch 57/1001] [Batch 2/372] [D loss: 0.701685905456543] [G loss: 0.5092567801475525]\n",
      "[Epoch 57/1001] [Batch 3/372] [D loss: 0.7139889001846313] [G loss: 0.9246616959571838]\n",
      "[Epoch 57/1001] [Batch 4/372] [D loss: 0.7150105237960815] [G loss: 0.5131908655166626]\n",
      "[Epoch 57/1001] [Batch 5/372] [D loss: 0.7102179527282715] [G loss: 0.8125549554824829]\n",
      "[Epoch 57/1001] [Batch 6/372] [D loss: 0.6979165077209473] [G loss: 0.6300877928733826]\n",
      "[Epoch 57/1001] [Batch 7/372] [D loss: 0.6937102675437927] [G loss: 0.6920064687728882]\n",
      "[Epoch 57/1001] [Batch 8/372] [D loss: 0.6901084184646606] [G loss: 0.7050450444221497]\n",
      "[Epoch 57/1001] [Batch 9/372] [D loss: 0.6902593374252319] [G loss: 0.6538591384887695]\n",
      "[Epoch 57/1001] [Batch 10/372] [D loss: 0.6917232275009155] [G loss: 0.718896746635437]\n",
      "[Epoch 57/1001] [Batch 11/372] [D loss: 0.6935510635375977] [G loss: 0.6601759195327759]\n",
      "[Epoch 57/1001] [Batch 12/372] [D loss: 0.6906747221946716] [G loss: 0.6973715424537659]\n",
      "[Epoch 57/1001] [Batch 13/372] [D loss: 0.6922372579574585] [G loss: 0.6749225854873657]\n",
      "[Epoch 57/1001] [Batch 14/372] [D loss: 0.6870931386947632] [G loss: 0.6892322897911072]\n",
      "[Epoch 57/1001] [Batch 15/372] [D loss: 0.691677451133728] [G loss: 0.674557089805603]\n",
      "[Epoch 57/1001] [Batch 16/372] [D loss: 0.6910082101821899] [G loss: 0.7085679769515991]\n",
      "[Epoch 57/1001] [Batch 17/372] [D loss: 0.6907959580421448] [G loss: 0.6501165628433228]\n",
      "[Epoch 57/1001] [Batch 18/372] [D loss: 0.6942715644836426] [G loss: 0.7122960090637207]\n",
      "[Epoch 57/1001] [Batch 19/372] [D loss: 0.6909680962562561] [G loss: 0.6555973887443542]\n",
      "[Epoch 57/1001] [Batch 20/372] [D loss: 0.6899842619895935] [G loss: 0.7217914462089539]\n",
      "[Epoch 57/1001] [Batch 21/372] [D loss: 0.6915014982223511] [G loss: 0.6437779068946838]\n",
      "[Epoch 57/1001] [Batch 22/372] [D loss: 0.6919728517532349] [G loss: 0.7147431373596191]\n",
      "[Epoch 57/1001] [Batch 23/372] [D loss: 0.6928942203521729] [G loss: 0.6625794172286987]\n",
      "[Epoch 57/1001] [Batch 24/372] [D loss: 0.693056046962738] [G loss: 0.7079817056655884]\n",
      "[Epoch 57/1001] [Batch 25/372] [D loss: 0.6948305368423462] [G loss: 0.6583621501922607]\n",
      "[Epoch 57/1001] [Batch 26/372] [D loss: 0.6907305717468262] [G loss: 0.7039719820022583]\n",
      "[Epoch 57/1001] [Batch 27/372] [D loss: 0.6902976632118225] [G loss: 0.6689820289611816]\n",
      "[Epoch 57/1001] [Batch 28/372] [D loss: 0.6911519765853882] [G loss: 0.6901875138282776]\n",
      "[Epoch 57/1001] [Batch 29/372] [D loss: 0.6894278526306152] [G loss: 0.6813226342201233]\n",
      "[Epoch 57/1001] [Batch 30/372] [D loss: 0.6893662810325623] [G loss: 0.6803855895996094]\n",
      "[Epoch 57/1001] [Batch 31/372] [D loss: 0.6906290650367737] [G loss: 0.6874268054962158]\n",
      "[Epoch 57/1001] [Batch 32/372] [D loss: 0.6901726722717285] [G loss: 0.6728359460830688]\n",
      "[Epoch 57/1001] [Batch 33/372] [D loss: 0.6923813223838806] [G loss: 0.6924935579299927]\n",
      "[Epoch 57/1001] [Batch 34/372] [D loss: 0.6938210725784302] [G loss: 0.6835463047027588]\n",
      "[Epoch 57/1001] [Batch 35/372] [D loss: 0.6911735534667969] [G loss: 0.6719698309898376]\n",
      "[Epoch 57/1001] [Batch 36/372] [D loss: 0.690654456615448] [G loss: 0.6966676712036133]\n",
      "[Epoch 57/1001] [Batch 37/372] [D loss: 0.6902626752853394] [G loss: 0.6703886985778809]\n",
      "[Epoch 57/1001] [Batch 38/372] [D loss: 0.6929082870483398] [G loss: 0.6921054720878601]\n",
      "[Epoch 57/1001] [Batch 39/372] [D loss: 0.6898212432861328] [G loss: 0.6684916019439697]\n",
      "[Epoch 57/1001] [Batch 40/372] [D loss: 0.6885514259338379] [G loss: 0.6809536218643188]\n",
      "[Epoch 57/1001] [Batch 41/372] [D loss: 0.687893271446228] [G loss: 0.6836608052253723]\n",
      "[Epoch 57/1001] [Batch 42/372] [D loss: 0.6908695697784424] [G loss: 0.6997584700584412]\n",
      "[Epoch 57/1001] [Batch 43/372] [D loss: 0.6879696846008301] [G loss: 0.6834449172019958]\n",
      "[Epoch 57/1001] [Batch 44/372] [D loss: 0.6872579455375671] [G loss: 0.6694974899291992]\n",
      "[Epoch 57/1001] [Batch 45/372] [D loss: 0.6914627552032471] [G loss: 0.6857011318206787]\n",
      "[Epoch 57/1001] [Batch 46/372] [D loss: 0.6913503408432007] [G loss: 0.6722920536994934]\n",
      "[Epoch 57/1001] [Batch 47/372] [D loss: 0.6891206502914429] [G loss: 0.6911998391151428]\n",
      "[Epoch 57/1001] [Batch 48/372] [D loss: 0.6933097839355469] [G loss: 0.6884365081787109]\n",
      "[Epoch 57/1001] [Batch 49/372] [D loss: 0.6889034509658813] [G loss: 0.6777195930480957]\n",
      "[Epoch 57/1001] [Batch 50/372] [D loss: 0.6920061707496643] [G loss: 0.6805223226547241]\n",
      "[Epoch 57/1001] [Batch 51/372] [D loss: 0.6892774701118469] [G loss: 0.6811482906341553]\n",
      "[Epoch 57/1001] [Batch 52/372] [D loss: 0.6923301815986633] [G loss: 0.6851118206977844]\n",
      "[Epoch 57/1001] [Batch 53/372] [D loss: 0.6924341917037964] [G loss: 0.6685020327568054]\n",
      "[Epoch 57/1001] [Batch 54/372] [D loss: 0.691585898399353] [G loss: 0.703192412853241]\n",
      "[Epoch 57/1001] [Batch 55/372] [D loss: 0.6914951205253601] [G loss: 0.6455873250961304]\n",
      "[Epoch 57/1001] [Batch 56/372] [D loss: 0.6883886456489563] [G loss: 0.738388180732727]\n",
      "[Epoch 57/1001] [Batch 57/372] [D loss: 0.6894947290420532] [G loss: 0.6209907531738281]\n",
      "[Epoch 57/1001] [Batch 58/372] [D loss: 0.694057047367096] [G loss: 0.7787898182868958]\n",
      "[Epoch 57/1001] [Batch 59/372] [D loss: 0.6941089630126953] [G loss: 0.5746257305145264]\n",
      "[Epoch 57/1001] [Batch 60/372] [D loss: 0.6978927850723267] [G loss: 0.8202487230300903]\n",
      "[Epoch 57/1001] [Batch 61/372] [D loss: 0.6979730725288391] [G loss: 0.555232584476471]\n",
      "[Epoch 57/1001] [Batch 62/372] [D loss: 0.6997050046920776] [G loss: 0.8336459398269653]\n",
      "[Epoch 57/1001] [Batch 63/372] [D loss: 0.7050697803497314] [G loss: 0.5560077428817749]\n",
      "[Epoch 57/1001] [Batch 64/372] [D loss: 0.7028079032897949] [G loss: 0.8133800029754639]\n",
      "[Epoch 57/1001] [Batch 65/372] [D loss: 0.6969320774078369] [G loss: 0.5906150937080383]\n",
      "[Epoch 57/1001] [Batch 66/372] [D loss: 0.6964744925498962] [G loss: 0.7485755085945129]\n",
      "[Epoch 57/1001] [Batch 67/372] [D loss: 0.693700909614563] [G loss: 0.6505587100982666]\n",
      "[Epoch 57/1001] [Batch 68/372] [D loss: 0.6927433013916016] [G loss: 0.6938179731369019]\n",
      "[Epoch 57/1001] [Batch 69/372] [D loss: 0.6919616460800171] [G loss: 0.6837522983551025]\n",
      "[Epoch 57/1001] [Batch 70/372] [D loss: 0.6880329847335815] [G loss: 0.6782958507537842]\n",
      "[Epoch 57/1001] [Batch 71/372] [D loss: 0.6891648769378662] [G loss: 0.6991845965385437]\n",
      "[Epoch 57/1001] [Batch 72/372] [D loss: 0.6914874315261841] [G loss: 0.6599023342132568]\n",
      "[Epoch 57/1001] [Batch 73/372] [D loss: 0.6898446083068848] [G loss: 0.7144611477851868]\n",
      "[Epoch 57/1001] [Batch 74/372] [D loss: 0.6906898617744446] [G loss: 0.6468724012374878]\n",
      "[Epoch 57/1001] [Batch 75/372] [D loss: 0.6915183067321777] [G loss: 0.7218152284622192]\n",
      "[Epoch 57/1001] [Batch 76/372] [D loss: 0.6942470073699951] [G loss: 0.6421656608581543]\n",
      "[Epoch 57/1001] [Batch 77/372] [D loss: 0.6903418302536011] [G loss: 0.7230327129364014]\n",
      "[Epoch 57/1001] [Batch 78/372] [D loss: 0.693755030632019] [G loss: 0.639398992061615]\n",
      "[Epoch 57/1001] [Batch 79/372] [D loss: 0.6905196905136108] [G loss: 0.7229253053665161]\n",
      "[Epoch 57/1001] [Batch 80/372] [D loss: 0.6922197341918945] [G loss: 0.6321399807929993]\n",
      "[Epoch 57/1001] [Batch 81/372] [D loss: 0.6940715312957764] [G loss: 0.7407592535018921]\n",
      "[Epoch 57/1001] [Batch 82/372] [D loss: 0.6912490129470825] [G loss: 0.6266827583312988]\n",
      "[Epoch 57/1001] [Batch 83/372] [D loss: 0.6923083066940308] [G loss: 0.7507125735282898]\n",
      "[Epoch 57/1001] [Batch 84/372] [D loss: 0.6914665102958679] [G loss: 0.6029974222183228]\n",
      "[Epoch 57/1001] [Batch 85/372] [D loss: 0.6941596269607544] [G loss: 0.7816154956817627]\n",
      "[Epoch 57/1001] [Batch 86/372] [D loss: 0.6972755193710327] [G loss: 0.5859726071357727]\n",
      "[Epoch 57/1001] [Batch 87/372] [D loss: 0.6919466257095337] [G loss: 0.7984627485275269]\n",
      "[Epoch 57/1001] [Batch 88/372] [D loss: 0.6955758333206177] [G loss: 0.5735146999359131]\n",
      "[Epoch 57/1001] [Batch 89/372] [D loss: 0.699112057685852] [G loss: 0.8168947100639343]\n",
      "[Epoch 57/1001] [Batch 90/372] [D loss: 0.7002438902854919] [G loss: 0.5600756406784058]\n",
      "[Epoch 57/1001] [Batch 91/372] [D loss: 0.7018890380859375] [G loss: 0.8136069774627686]\n",
      "[Epoch 57/1001] [Batch 92/372] [D loss: 0.6997988820075989] [G loss: 0.5767715573310852]\n",
      "[Epoch 57/1001] [Batch 93/372] [D loss: 0.6970846056938171] [G loss: 0.7771342396736145]\n",
      "[Epoch 57/1001] [Batch 94/372] [D loss: 0.6938279867172241] [G loss: 0.6127485632896423]\n",
      "[Epoch 57/1001] [Batch 95/372] [D loss: 0.6923953294754028] [G loss: 0.7544800639152527]\n",
      "[Epoch 57/1001] [Batch 96/372] [D loss: 0.6909557580947876] [G loss: 0.6297782063484192]\n",
      "[Epoch 57/1001] [Batch 97/372] [D loss: 0.6925491690635681] [G loss: 0.7286357283592224]\n",
      "[Epoch 57/1001] [Batch 98/372] [D loss: 0.6932530403137207] [G loss: 0.6596997976303101]\n",
      "[Epoch 57/1001] [Batch 99/372] [D loss: 0.6895698308944702] [G loss: 0.6888545751571655]\n",
      "[Epoch 57/1001] [Batch 100/372] [D loss: 0.6899954080581665] [G loss: 0.6933743357658386]\n",
      "[Epoch 57/1001] [Batch 101/372] [D loss: 0.6914197206497192] [G loss: 0.6688026785850525]\n",
      "[Epoch 57/1001] [Batch 102/372] [D loss: 0.6925173997879028] [G loss: 0.696565568447113]\n",
      "[Epoch 57/1001] [Batch 103/372] [D loss: 0.6909600496292114] [G loss: 0.6797745227813721]\n",
      "[Epoch 57/1001] [Batch 104/372] [D loss: 0.6938512325286865] [G loss: 0.6761461496353149]\n",
      "[Epoch 57/1001] [Batch 105/372] [D loss: 0.6922556161880493] [G loss: 0.6886427998542786]\n",
      "[Epoch 57/1001] [Batch 106/372] [D loss: 0.691286027431488] [G loss: 0.6667217016220093]\n",
      "[Epoch 57/1001] [Batch 107/372] [D loss: 0.692848801612854] [G loss: 0.6987513303756714]\n",
      "[Epoch 57/1001] [Batch 108/372] [D loss: 0.6884217858314514] [G loss: 0.6713476181030273]\n",
      "[Epoch 57/1001] [Batch 109/372] [D loss: 0.6873388290405273] [G loss: 0.6956776976585388]\n",
      "[Epoch 57/1001] [Batch 110/372] [D loss: 0.688463568687439] [G loss: 0.6716926693916321]\n",
      "[Epoch 57/1001] [Batch 111/372] [D loss: 0.688836932182312] [G loss: 0.6901446580886841]\n",
      "[Epoch 57/1001] [Batch 112/372] [D loss: 0.6902572512626648] [G loss: 0.6811833381652832]\n",
      "[Epoch 57/1001] [Batch 113/372] [D loss: 0.6922340989112854] [G loss: 0.6883724927902222]\n",
      "[Epoch 57/1001] [Batch 114/372] [D loss: 0.6929182410240173] [G loss: 0.6576350331306458]\n",
      "[Epoch 57/1001] [Batch 115/372] [D loss: 0.6912170648574829] [G loss: 0.728293776512146]\n",
      "[Epoch 57/1001] [Batch 116/372] [D loss: 0.6931070685386658] [G loss: 0.628922700881958]\n",
      "[Epoch 57/1001] [Batch 117/372] [D loss: 0.6953330039978027] [G loss: 0.7388917803764343]\n",
      "[Epoch 57/1001] [Batch 118/372] [D loss: 0.6896034479141235] [G loss: 0.626613974571228]\n",
      "[Epoch 57/1001] [Batch 119/372] [D loss: 0.6902166604995728] [G loss: 0.7371917963027954]\n",
      "[Epoch 57/1001] [Batch 120/372] [D loss: 0.6920905113220215] [G loss: 0.6245574951171875]\n",
      "[Epoch 57/1001] [Batch 121/372] [D loss: 0.6901190280914307] [G loss: 0.7669978141784668]\n",
      "[Epoch 57/1001] [Batch 122/372] [D loss: 0.6932036280632019] [G loss: 0.5706171989440918]\n",
      "[Epoch 57/1001] [Batch 123/372] [D loss: 0.7016284465789795] [G loss: 0.847637414932251]\n",
      "[Epoch 57/1001] [Batch 124/372] [D loss: 0.7061126232147217] [G loss: 0.5292078256607056]\n",
      "[Epoch 57/1001] [Batch 125/372] [D loss: 0.7007078528404236] [G loss: 0.8404909372329712]\n",
      "[Epoch 57/1001] [Batch 126/372] [D loss: 0.7024788856506348] [G loss: 0.5761803984642029]\n",
      "[Epoch 57/1001] [Batch 127/372] [D loss: 0.7004944086074829] [G loss: 0.7598828673362732]\n",
      "[Epoch 57/1001] [Batch 128/372] [D loss: 0.6933050751686096] [G loss: 0.6291598081588745]\n",
      "[Epoch 57/1001] [Batch 129/372] [D loss: 0.6936187744140625] [G loss: 0.7219808101654053]\n",
      "[Epoch 57/1001] [Batch 130/372] [D loss: 0.686591625213623] [G loss: 0.6572332382202148]\n",
      "[Epoch 57/1001] [Batch 131/372] [D loss: 0.6898977756500244] [G loss: 0.7020448446273804]\n",
      "[Epoch 57/1001] [Batch 132/372] [D loss: 0.692027747631073] [G loss: 0.6787541508674622]\n",
      "[Epoch 57/1001] [Batch 133/372] [D loss: 0.6910061836242676] [G loss: 0.6718049049377441]\n",
      "[Epoch 57/1001] [Batch 134/372] [D loss: 0.6927905082702637] [G loss: 0.7065332531929016]\n",
      "[Epoch 57/1001] [Batch 135/372] [D loss: 0.6894855499267578] [G loss: 0.6579221487045288]\n",
      "[Epoch 57/1001] [Batch 136/372] [D loss: 0.6903122663497925] [G loss: 0.7061002850532532]\n",
      "[Epoch 57/1001] [Batch 137/372] [D loss: 0.6912039518356323] [G loss: 0.6587061285972595]\n",
      "[Epoch 57/1001] [Batch 138/372] [D loss: 0.6878889799118042] [G loss: 0.7048988342285156]\n",
      "[Epoch 57/1001] [Batch 139/372] [D loss: 0.690682590007782] [G loss: 0.6593478918075562]\n",
      "[Epoch 57/1001] [Batch 140/372] [D loss: 0.6900174021720886] [G loss: 0.720444917678833]\n",
      "[Epoch 57/1001] [Batch 141/372] [D loss: 0.6893216371536255] [G loss: 0.6441527605056763]\n",
      "[Epoch 57/1001] [Batch 142/372] [D loss: 0.6900434494018555] [G loss: 0.7254635691642761]\n",
      "[Epoch 57/1001] [Batch 143/372] [D loss: 0.696016788482666] [G loss: 0.6268803477287292]\n",
      "[Epoch 57/1001] [Batch 144/372] [D loss: 0.6913799047470093] [G loss: 0.7324613332748413]\n",
      "[Epoch 57/1001] [Batch 145/372] [D loss: 0.6900336742401123] [G loss: 0.6206109523773193]\n",
      "[Epoch 57/1001] [Batch 146/372] [D loss: 0.6934022903442383] [G loss: 0.774748682975769]\n",
      "[Epoch 57/1001] [Batch 147/372] [D loss: 0.6905158758163452] [G loss: 0.5857540965080261]\n",
      "[Epoch 57/1001] [Batch 148/372] [D loss: 0.6974396705627441] [G loss: 0.8037492632865906]\n",
      "[Epoch 57/1001] [Batch 149/372] [D loss: 0.697012186050415] [G loss: 0.5651542544364929]\n",
      "[Epoch 57/1001] [Batch 150/372] [D loss: 0.6995401382446289] [G loss: 0.8146611452102661]\n",
      "[Epoch 57/1001] [Batch 151/372] [D loss: 0.6999686360359192] [G loss: 0.5718427300453186]\n",
      "[Epoch 57/1001] [Batch 152/372] [D loss: 0.6980692148208618] [G loss: 0.7861272692680359]\n",
      "[Epoch 57/1001] [Batch 153/372] [D loss: 0.6965620517730713] [G loss: 0.6076297760009766]\n",
      "[Epoch 57/1001] [Batch 154/372] [D loss: 0.6930968761444092] [G loss: 0.7409142255783081]\n",
      "[Epoch 57/1001] [Batch 155/372] [D loss: 0.6857261657714844] [G loss: 0.6446146965026855]\n",
      "[Epoch 57/1001] [Batch 156/372] [D loss: 0.6917287111282349] [G loss: 0.7275153398513794]\n",
      "[Epoch 57/1001] [Batch 157/372] [D loss: 0.6930866837501526] [G loss: 0.6302052140235901]\n",
      "[Epoch 57/1001] [Batch 158/372] [D loss: 0.6913613080978394] [G loss: 0.7374297976493835]\n",
      "[Epoch 57/1001] [Batch 159/372] [D loss: 0.6907312870025635] [G loss: 0.6371630430221558]\n",
      "[Epoch 57/1001] [Batch 160/372] [D loss: 0.6906489133834839] [G loss: 0.7213426828384399]\n",
      "[Epoch 57/1001] [Batch 161/372] [D loss: 0.6905818581581116] [G loss: 0.6557605266571045]\n",
      "[Epoch 57/1001] [Batch 162/372] [D loss: 0.693206787109375] [G loss: 0.6931787729263306]\n",
      "[Epoch 57/1001] [Batch 163/372] [D loss: 0.6926741600036621] [G loss: 0.6735863089561462]\n",
      "[Epoch 57/1001] [Batch 164/372] [D loss: 0.6882390975952148] [G loss: 0.691801905632019]\n",
      "[Epoch 57/1001] [Batch 165/372] [D loss: 0.690355658531189] [G loss: 0.6567038893699646]\n",
      "[Epoch 57/1001] [Batch 166/372] [D loss: 0.6955046653747559] [G loss: 0.7314700484275818]\n",
      "[Epoch 57/1001] [Batch 167/372] [D loss: 0.6899279952049255] [G loss: 0.6270055770874023]\n",
      "[Epoch 57/1001] [Batch 168/372] [D loss: 0.6938052177429199] [G loss: 0.7438651323318481]\n",
      "[Epoch 57/1001] [Batch 169/372] [D loss: 0.6892393827438354] [G loss: 0.6273326873779297]\n",
      "[Epoch 57/1001] [Batch 170/372] [D loss: 0.6926507949829102] [G loss: 0.7371581792831421]\n",
      "[Epoch 57/1001] [Batch 171/372] [D loss: 0.692461371421814] [G loss: 0.6338675022125244]\n",
      "[Epoch 57/1001] [Batch 172/372] [D loss: 0.6932754516601562] [G loss: 0.7339484095573425]\n",
      "[Epoch 57/1001] [Batch 173/372] [D loss: 0.6945047974586487] [G loss: 0.6278423070907593]\n",
      "[Epoch 57/1001] [Batch 174/372] [D loss: 0.6931387186050415] [G loss: 0.737862229347229]\n",
      "[Epoch 57/1001] [Batch 175/372] [D loss: 0.6956692934036255] [G loss: 0.631578803062439]\n",
      "[Epoch 57/1001] [Batch 176/372] [D loss: 0.6910054683685303] [G loss: 0.7321103811264038]\n",
      "[Epoch 57/1001] [Batch 177/372] [D loss: 0.6907025575637817] [G loss: 0.6315930485725403]\n",
      "[Epoch 57/1001] [Batch 178/372] [D loss: 0.6915853023529053] [G loss: 0.735984742641449]\n",
      "[Epoch 57/1001] [Batch 179/372] [D loss: 0.6899958252906799] [G loss: 0.6229984164237976]\n",
      "[Epoch 57/1001] [Batch 180/372] [D loss: 0.6949506998062134] [G loss: 0.773845374584198]\n",
      "[Epoch 57/1001] [Batch 181/372] [D loss: 0.6922284960746765] [G loss: 0.5870477557182312]\n",
      "[Epoch 57/1001] [Batch 182/372] [D loss: 0.6925516128540039] [G loss: 0.8035051822662354]\n",
      "[Epoch 57/1001] [Batch 183/372] [D loss: 0.6997736096382141] [G loss: 0.567935049533844]\n",
      "[Epoch 57/1001] [Batch 184/372] [D loss: 0.6991050243377686] [G loss: 0.7926111817359924]\n",
      "[Epoch 57/1001] [Batch 185/372] [D loss: 0.6963963508605957] [G loss: 0.6039484739303589]\n",
      "[Epoch 57/1001] [Batch 186/372] [D loss: 0.6905889511108398] [G loss: 0.728252649307251]\n",
      "[Epoch 57/1001] [Batch 187/372] [D loss: 0.6824712753295898] [G loss: 0.6555746793746948]\n",
      "[Epoch 57/1001] [Batch 188/372] [D loss: 0.6883814334869385] [G loss: 0.6914703249931335]\n",
      "[Epoch 57/1001] [Batch 189/372] [D loss: 0.6946151256561279] [G loss: 0.6873503923416138]\n",
      "[Epoch 57/1001] [Batch 190/372] [D loss: 0.6909792423248291] [G loss: 0.6547156572341919]\n",
      "[Epoch 57/1001] [Batch 191/372] [D loss: 0.6924653053283691] [G loss: 0.7264223098754883]\n",
      "[Epoch 57/1001] [Batch 192/372] [D loss: 0.695116400718689] [G loss: 0.6165579557418823]\n",
      "[Epoch 57/1001] [Batch 193/372] [D loss: 0.6927635669708252] [G loss: 0.7707532644271851]\n",
      "[Epoch 57/1001] [Batch 194/372] [D loss: 0.6961917877197266] [G loss: 0.5886239409446716]\n",
      "[Epoch 57/1001] [Batch 195/372] [D loss: 0.6963731050491333] [G loss: 0.7820687294006348]\n",
      "[Epoch 57/1001] [Batch 196/372] [D loss: 0.6909295916557312] [G loss: 0.5987944006919861]\n",
      "[Epoch 57/1001] [Batch 197/372] [D loss: 0.692746102809906] [G loss: 0.7743120193481445]\n",
      "[Epoch 57/1001] [Batch 198/372] [D loss: 0.6968399286270142] [G loss: 0.5918381214141846]\n",
      "[Epoch 57/1001] [Batch 199/372] [D loss: 0.6960055232048035] [G loss: 0.7940704822540283]\n",
      "[Epoch 57/1001] [Batch 200/372] [D loss: 0.694311797618866] [G loss: 0.5820169448852539]\n",
      "[Epoch 57/1001] [Batch 201/372] [D loss: 0.698437511920929] [G loss: 0.778290867805481]\n",
      "[Epoch 57/1001] [Batch 202/372] [D loss: 0.6934581398963928] [G loss: 0.6081222295761108]\n",
      "[Epoch 57/1001] [Batch 203/372] [D loss: 0.6910699009895325] [G loss: 0.7412331104278564]\n",
      "[Epoch 57/1001] [Batch 204/372] [D loss: 0.6924910545349121] [G loss: 0.6436083316802979]\n",
      "[Epoch 57/1001] [Batch 205/372] [D loss: 0.6946567296981812] [G loss: 0.7086936235427856]\n",
      "[Epoch 57/1001] [Batch 206/372] [D loss: 0.6947524547576904] [G loss: 0.6630954146385193]\n",
      "[Epoch 57/1001] [Batch 207/372] [D loss: 0.693984866142273] [G loss: 0.6846765875816345]\n",
      "[Epoch 57/1001] [Batch 208/372] [D loss: 0.6904464364051819] [G loss: 0.6887532472610474]\n",
      "[Epoch 57/1001] [Batch 209/372] [D loss: 0.6893655061721802] [G loss: 0.671751081943512]\n",
      "[Epoch 57/1001] [Batch 210/372] [D loss: 0.6928964853286743] [G loss: 0.677789568901062]\n",
      "[Epoch 57/1001] [Batch 211/372] [D loss: 0.6894277930259705] [G loss: 0.683483362197876]\n",
      "[Epoch 57/1001] [Batch 212/372] [D loss: 0.6927068829536438] [G loss: 0.6847265958786011]\n",
      "[Epoch 57/1001] [Batch 213/372] [D loss: 0.6932768821716309] [G loss: 0.6876857280731201]\n",
      "[Epoch 57/1001] [Batch 214/372] [D loss: 0.6913872957229614] [G loss: 0.678148090839386]\n",
      "[Epoch 57/1001] [Batch 215/372] [D loss: 0.6895145177841187] [G loss: 0.6800969243049622]\n",
      "[Epoch 57/1001] [Batch 216/372] [D loss: 0.6908772587776184] [G loss: 0.6832361221313477]\n",
      "[Epoch 57/1001] [Batch 217/372] [D loss: 0.688653826713562] [G loss: 0.6831206679344177]\n",
      "[Epoch 57/1001] [Batch 218/372] [D loss: 0.6909418702125549] [G loss: 0.6870810985565186]\n",
      "[Epoch 57/1001] [Batch 219/372] [D loss: 0.6899007558822632] [G loss: 0.6880487203598022]\n",
      "[Epoch 57/1001] [Batch 220/372] [D loss: 0.6928941011428833] [G loss: 0.660504162311554]\n",
      "[Epoch 57/1001] [Batch 221/372] [D loss: 0.6889020800590515] [G loss: 0.7085072994232178]\n",
      "[Epoch 57/1001] [Batch 222/372] [D loss: 0.6892948746681213] [G loss: 0.6502658128738403]\n",
      "[Epoch 57/1001] [Batch 223/372] [D loss: 0.6906957030296326] [G loss: 0.7228389382362366]\n",
      "[Epoch 57/1001] [Batch 224/372] [D loss: 0.6877456903457642] [G loss: 0.625609278678894]\n",
      "[Epoch 57/1001] [Batch 225/372] [D loss: 0.686593770980835] [G loss: 0.7543026208877563]\n",
      "[Epoch 57/1001] [Batch 226/372] [D loss: 0.6929384469985962] [G loss: 0.5982513427734375]\n",
      "[Epoch 57/1001] [Batch 227/372] [D loss: 0.692682147026062] [G loss: 0.7844150066375732]\n",
      "[Epoch 57/1001] [Batch 228/372] [D loss: 0.6934401392936707] [G loss: 0.5702492594718933]\n",
      "[Epoch 57/1001] [Batch 229/372] [D loss: 0.6992943286895752] [G loss: 0.8503697514533997]\n",
      "[Epoch 57/1001] [Batch 230/372] [D loss: 0.7025352716445923] [G loss: 0.5345270037651062]\n",
      "[Epoch 57/1001] [Batch 231/372] [D loss: 0.7032477259635925] [G loss: 0.8427315354347229]\n",
      "[Epoch 57/1001] [Batch 232/372] [D loss: 0.7022624611854553] [G loss: 0.5764604806900024]\n",
      "[Epoch 57/1001] [Batch 233/372] [D loss: 0.6963145732879639] [G loss: 0.7562786936759949]\n",
      "[Epoch 57/1001] [Batch 234/372] [D loss: 0.6896783113479614] [G loss: 0.6406974196434021]\n",
      "[Epoch 57/1001] [Batch 235/372] [D loss: 0.6914865970611572] [G loss: 0.7038463354110718]\n",
      "[Epoch 57/1001] [Batch 236/372] [D loss: 0.6870560646057129] [G loss: 0.6622456908226013]\n",
      "[Epoch 57/1001] [Batch 237/372] [D loss: 0.6921329498291016] [G loss: 0.7038459777832031]\n",
      "[Epoch 57/1001] [Batch 238/372] [D loss: 0.6949962973594666] [G loss: 0.6404694318771362]\n",
      "[Epoch 57/1001] [Batch 239/372] [D loss: 0.6942970752716064] [G loss: 0.7164241075515747]\n",
      "[Epoch 57/1001] [Batch 240/372] [D loss: 0.6921402215957642] [G loss: 0.6549279093742371]\n",
      "[Epoch 57/1001] [Batch 241/372] [D loss: 0.6974623203277588] [G loss: 0.7093515396118164]\n",
      "[Epoch 57/1001] [Batch 242/372] [D loss: 0.6922028064727783] [G loss: 0.6314093470573425]\n",
      "[Epoch 57/1001] [Batch 243/372] [D loss: 0.6953142881393433] [G loss: 0.7497318387031555]\n",
      "[Epoch 57/1001] [Batch 244/372] [D loss: 0.6957537531852722] [G loss: 0.6221513748168945]\n",
      "[Epoch 57/1001] [Batch 245/372] [D loss: 0.6941055655479431] [G loss: 0.7247107028961182]\n",
      "[Epoch 57/1001] [Batch 246/372] [D loss: 0.6907287836074829] [G loss: 0.6584887504577637]\n",
      "[Epoch 57/1001] [Batch 247/372] [D loss: 0.6924252510070801] [G loss: 0.6935679912567139]\n",
      "[Epoch 57/1001] [Batch 248/372] [D loss: 0.6910864114761353] [G loss: 0.6772194504737854]\n",
      "[Epoch 57/1001] [Batch 249/372] [D loss: 0.688232421875] [G loss: 0.6856507658958435]\n",
      "[Epoch 57/1001] [Batch 250/372] [D loss: 0.6925249099731445] [G loss: 0.674954354763031]\n",
      "[Epoch 57/1001] [Batch 251/372] [D loss: 0.689897894859314] [G loss: 0.6970162391662598]\n",
      "[Epoch 57/1001] [Batch 252/372] [D loss: 0.6923378109931946] [G loss: 0.6731424331665039]\n",
      "[Epoch 57/1001] [Batch 253/372] [D loss: 0.6887303590774536] [G loss: 0.6805909872055054]\n",
      "[Epoch 57/1001] [Batch 254/372] [D loss: 0.6886440515518188] [G loss: 0.6835642457008362]\n",
      "[Epoch 57/1001] [Batch 255/372] [D loss: 0.6927735805511475] [G loss: 0.6756685376167297]\n",
      "[Epoch 57/1001] [Batch 256/372] [D loss: 0.6877956986427307] [G loss: 0.6944774389266968]\n",
      "[Epoch 57/1001] [Batch 257/372] [D loss: 0.6941784620285034] [G loss: 0.6632815003395081]\n",
      "[Epoch 57/1001] [Batch 258/372] [D loss: 0.6870490312576294] [G loss: 0.6966092586517334]\n",
      "[Epoch 57/1001] [Batch 259/372] [D loss: 0.690597414970398] [G loss: 0.6644366383552551]\n",
      "[Epoch 57/1001] [Batch 260/372] [D loss: 0.689591109752655] [G loss: 0.7009168267250061]\n",
      "[Epoch 57/1001] [Batch 261/372] [D loss: 0.6912932395935059] [G loss: 0.6493669152259827]\n",
      "[Epoch 57/1001] [Batch 262/372] [D loss: 0.693290650844574] [G loss: 0.7203164100646973]\n",
      "[Epoch 57/1001] [Batch 263/372] [D loss: 0.6969048976898193] [G loss: 0.6353121995925903]\n",
      "[Epoch 57/1001] [Batch 264/372] [D loss: 0.6930060386657715] [G loss: 0.7301268577575684]\n",
      "[Epoch 57/1001] [Batch 265/372] [D loss: 0.6930282115936279] [G loss: 0.6361345052719116]\n",
      "[Epoch 57/1001] [Batch 266/372] [D loss: 0.6936842203140259] [G loss: 0.7336047291755676]\n",
      "[Epoch 57/1001] [Batch 267/372] [D loss: 0.6943178772926331] [G loss: 0.6281810402870178]\n",
      "[Epoch 57/1001] [Batch 268/372] [D loss: 0.6918483972549438] [G loss: 0.7410218715667725]\n",
      "[Epoch 57/1001] [Batch 269/372] [D loss: 0.6936384439468384] [G loss: 0.6165176630020142]\n",
      "[Epoch 57/1001] [Batch 270/372] [D loss: 0.6905194520950317] [G loss: 0.7404173016548157]\n",
      "[Epoch 57/1001] [Batch 271/372] [D loss: 0.6925984025001526] [G loss: 0.6405526399612427]\n",
      "[Epoch 57/1001] [Batch 272/372] [D loss: 0.693901538848877] [G loss: 0.7287886738777161]\n",
      "[Epoch 57/1001] [Batch 273/372] [D loss: 0.6912649869918823] [G loss: 0.622389554977417]\n",
      "[Epoch 57/1001] [Batch 274/372] [D loss: 0.6893450021743774] [G loss: 0.745523989200592]\n",
      "[Epoch 57/1001] [Batch 275/372] [D loss: 0.6913958191871643] [G loss: 0.6177652478218079]\n",
      "[Epoch 57/1001] [Batch 276/372] [D loss: 0.6947780847549438] [G loss: 0.7521656155586243]\n",
      "[Epoch 57/1001] [Batch 277/372] [D loss: 0.6868270635604858] [G loss: 0.6089817881584167]\n",
      "[Epoch 57/1001] [Batch 278/372] [D loss: 0.6934213638305664] [G loss: 0.7706506252288818]\n",
      "[Epoch 57/1001] [Batch 279/372] [D loss: 0.6927694082260132] [G loss: 0.6004024744033813]\n",
      "[Epoch 57/1001] [Batch 280/372] [D loss: 0.6941315531730652] [G loss: 0.7713473439216614]\n",
      "[Epoch 57/1001] [Batch 281/372] [D loss: 0.6944645047187805] [G loss: 0.5853067636489868]\n",
      "[Epoch 57/1001] [Batch 282/372] [D loss: 0.6978983879089355] [G loss: 0.8097110390663147]\n",
      "[Epoch 57/1001] [Batch 283/372] [D loss: 0.6976070404052734] [G loss: 0.5573207139968872]\n",
      "[Epoch 57/1001] [Batch 284/372] [D loss: 0.6973294019699097] [G loss: 0.8484523296356201]\n",
      "[Epoch 57/1001] [Batch 285/372] [D loss: 0.7060400247573853] [G loss: 0.5440191030502319]\n",
      "[Epoch 57/1001] [Batch 286/372] [D loss: 0.70122230052948] [G loss: 0.8220512866973877]\n",
      "[Epoch 57/1001] [Batch 287/372] [D loss: 0.7000458240509033] [G loss: 0.5761503577232361]\n",
      "[Epoch 57/1001] [Batch 288/372] [D loss: 0.6947818994522095] [G loss: 0.7736231088638306]\n",
      "[Epoch 57/1001] [Batch 289/372] [D loss: 0.6930346488952637] [G loss: 0.6289306879043579]\n",
      "[Epoch 57/1001] [Batch 290/372] [D loss: 0.6915076375007629] [G loss: 0.7040689587593079]\n",
      "[Epoch 57/1001] [Batch 291/372] [D loss: 0.6923080682754517] [G loss: 0.6790546178817749]\n",
      "[Epoch 57/1001] [Batch 292/372] [D loss: 0.6891586780548096] [G loss: 0.6664190888404846]\n",
      "[Epoch 57/1001] [Batch 293/372] [D loss: 0.6915503144264221] [G loss: 0.7035232186317444]\n",
      "[Epoch 57/1001] [Batch 294/372] [D loss: 0.6925410032272339] [G loss: 0.6525087952613831]\n",
      "[Epoch 57/1001] [Batch 295/372] [D loss: 0.6916691064834595] [G loss: 0.7172539234161377]\n",
      "[Epoch 57/1001] [Batch 296/372] [D loss: 0.6941845417022705] [G loss: 0.6460099220275879]\n",
      "[Epoch 57/1001] [Batch 297/372] [D loss: 0.6911247968673706] [G loss: 0.7136086225509644]\n",
      "[Epoch 57/1001] [Batch 298/372] [D loss: 0.6911927461624146] [G loss: 0.6597976684570312]\n",
      "[Epoch 57/1001] [Batch 299/372] [D loss: 0.691409707069397] [G loss: 0.7057720422744751]\n",
      "[Epoch 57/1001] [Batch 300/372] [D loss: 0.6899005174636841] [G loss: 0.6542350053787231]\n",
      "[Epoch 57/1001] [Batch 301/372] [D loss: 0.6881296634674072] [G loss: 0.6910049915313721]\n",
      "[Epoch 57/1001] [Batch 302/372] [D loss: 0.695665717124939] [G loss: 0.6998526453971863]\n",
      "[Epoch 57/1001] [Batch 303/372] [D loss: 0.6926781535148621] [G loss: 0.6390265822410583]\n",
      "[Epoch 57/1001] [Batch 304/372] [D loss: 0.6894681453704834] [G loss: 0.7372605204582214]\n",
      "[Epoch 57/1001] [Batch 305/372] [D loss: 0.6944347023963928] [G loss: 0.6325095295906067]\n",
      "[Epoch 57/1001] [Batch 306/372] [D loss: 0.6927920579910278] [G loss: 0.7267618179321289]\n",
      "[Epoch 57/1001] [Batch 307/372] [D loss: 0.6941356658935547] [G loss: 0.6391928195953369]\n",
      "[Epoch 57/1001] [Batch 308/372] [D loss: 0.6934878826141357] [G loss: 0.7192448377609253]\n",
      "[Epoch 57/1001] [Batch 309/372] [D loss: 0.6885162591934204] [G loss: 0.6467317342758179]\n",
      "[Epoch 57/1001] [Batch 310/372] [D loss: 0.6927682161331177] [G loss: 0.7350194454193115]\n",
      "[Epoch 57/1001] [Batch 311/372] [D loss: 0.6884768009185791] [G loss: 0.6369955539703369]\n",
      "[Epoch 57/1001] [Batch 312/372] [D loss: 0.6942646503448486] [G loss: 0.7209961414337158]\n",
      "[Epoch 57/1001] [Batch 313/372] [D loss: 0.6886762380599976] [G loss: 0.6506860852241516]\n",
      "[Epoch 57/1001] [Batch 314/372] [D loss: 0.6921964883804321] [G loss: 0.7018823623657227]\n",
      "[Epoch 57/1001] [Batch 315/372] [D loss: 0.6885563135147095] [G loss: 0.6620789766311646]\n",
      "[Epoch 57/1001] [Batch 316/372] [D loss: 0.6922136545181274] [G loss: 0.7141965627670288]\n",
      "[Epoch 57/1001] [Batch 317/372] [D loss: 0.6897953748703003] [G loss: 0.6437203288078308]\n",
      "[Epoch 57/1001] [Batch 318/372] [D loss: 0.6895841360092163] [G loss: 0.7324791550636292]\n",
      "[Epoch 57/1001] [Batch 319/372] [D loss: 0.690984845161438] [G loss: 0.6100121736526489]\n",
      "[Epoch 57/1001] [Batch 320/372] [D loss: 0.6945673823356628] [G loss: 0.7837153673171997]\n",
      "[Epoch 57/1001] [Batch 321/372] [D loss: 0.6967262029647827] [G loss: 0.5686982870101929]\n",
      "[Epoch 57/1001] [Batch 322/372] [D loss: 0.6989339590072632] [G loss: 0.8230152130126953]\n",
      "[Epoch 57/1001] [Batch 323/372] [D loss: 0.7012064456939697] [G loss: 0.5460073351860046]\n",
      "[Epoch 57/1001] [Batch 324/372] [D loss: 0.6962440013885498] [G loss: 0.8468418717384338]\n",
      "[Epoch 57/1001] [Batch 325/372] [D loss: 0.7010749578475952] [G loss: 0.551956057548523]\n",
      "[Epoch 57/1001] [Batch 326/372] [D loss: 0.7072650194168091] [G loss: 0.8042147755622864]\n",
      "[Epoch 57/1001] [Batch 327/372] [D loss: 0.6993371248245239] [G loss: 0.5984622240066528]\n",
      "[Epoch 57/1001] [Batch 328/372] [D loss: 0.6963739395141602] [G loss: 0.750779390335083]\n",
      "[Epoch 57/1001] [Batch 329/372] [D loss: 0.6921836137771606] [G loss: 0.6454988718032837]\n",
      "[Epoch 57/1001] [Batch 330/372] [D loss: 0.6915152072906494] [G loss: 0.6930129528045654]\n",
      "[Epoch 57/1001] [Batch 331/372] [D loss: 0.6879482269287109] [G loss: 0.6884496808052063]\n",
      "[Epoch 57/1001] [Batch 332/372] [D loss: 0.6905341744422913] [G loss: 0.6683984398841858]\n",
      "[Epoch 57/1001] [Batch 333/372] [D loss: 0.6948798894882202] [G loss: 0.7000146508216858]\n",
      "[Epoch 57/1001] [Batch 334/372] [D loss: 0.6890254616737366] [G loss: 0.6646823287010193]\n",
      "[Epoch 57/1001] [Batch 335/372] [D loss: 0.6893986463546753] [G loss: 0.6984429359436035]\n",
      "[Epoch 57/1001] [Batch 336/372] [D loss: 0.6905931234359741] [G loss: 0.6633561849594116]\n",
      "[Epoch 57/1001] [Batch 337/372] [D loss: 0.6923990249633789] [G loss: 0.7047709226608276]\n",
      "[Epoch 57/1001] [Batch 338/372] [D loss: 0.6938825845718384] [G loss: 0.6545769572257996]\n",
      "[Epoch 57/1001] [Batch 339/372] [D loss: 0.6942545175552368] [G loss: 0.7131935954093933]\n",
      "[Epoch 57/1001] [Batch 340/372] [D loss: 0.6935681104660034] [G loss: 0.6504924893379211]\n",
      "[Epoch 57/1001] [Batch 341/372] [D loss: 0.6922533512115479] [G loss: 0.7157118320465088]\n",
      "[Epoch 57/1001] [Batch 342/372] [D loss: 0.6920395493507385] [G loss: 0.6505033373832703]\n",
      "[Epoch 57/1001] [Batch 343/372] [D loss: 0.6866926550865173] [G loss: 0.7044914960861206]\n",
      "[Epoch 57/1001] [Batch 344/372] [D loss: 0.6900744438171387] [G loss: 0.6633414030075073]\n",
      "[Epoch 57/1001] [Batch 345/372] [D loss: 0.6878798007965088] [G loss: 0.6965571641921997]\n",
      "[Epoch 57/1001] [Batch 346/372] [D loss: 0.6915254592895508] [G loss: 0.6686288118362427]\n",
      "[Epoch 57/1001] [Batch 347/372] [D loss: 0.6919149160385132] [G loss: 0.7045336365699768]\n",
      "[Epoch 57/1001] [Batch 348/372] [D loss: 0.6951899528503418] [G loss: 0.6372698545455933]\n",
      "[Epoch 57/1001] [Batch 349/372] [D loss: 0.6938220262527466] [G loss: 0.7283199429512024]\n",
      "[Epoch 57/1001] [Batch 350/372] [D loss: 0.693466067314148] [G loss: 0.6510263681411743]\n",
      "[Epoch 57/1001] [Batch 351/372] [D loss: 0.6925663948059082] [G loss: 0.7042426466941833]\n",
      "[Epoch 57/1001] [Batch 352/372] [D loss: 0.6918684244155884] [G loss: 0.6741341352462769]\n",
      "[Epoch 57/1001] [Batch 353/372] [D loss: 0.6921800374984741] [G loss: 0.6904694437980652]\n",
      "[Epoch 57/1001] [Batch 354/372] [D loss: 0.6893867254257202] [G loss: 0.6648823022842407]\n",
      "[Epoch 57/1001] [Batch 355/372] [D loss: 0.6922412514686584] [G loss: 0.7029985189437866]\n",
      "[Epoch 57/1001] [Batch 356/372] [D loss: 0.6880354881286621] [G loss: 0.6582137942314148]\n",
      "[Epoch 57/1001] [Batch 357/372] [D loss: 0.6935006380081177] [G loss: 0.6990121603012085]\n",
      "[Epoch 57/1001] [Batch 358/372] [D loss: 0.6907532215118408] [G loss: 0.6582468152046204]\n",
      "[Epoch 57/1001] [Batch 359/372] [D loss: 0.6919001340866089] [G loss: 0.7104805707931519]\n",
      "[Epoch 57/1001] [Batch 360/372] [D loss: 0.695248007774353] [G loss: 0.6460699439048767]\n",
      "[Epoch 57/1001] [Batch 361/372] [D loss: 0.6903378963470459] [G loss: 0.7324329614639282]\n",
      "[Epoch 57/1001] [Batch 362/372] [D loss: 0.6879937052726746] [G loss: 0.6128609776496887]\n",
      "[Epoch 57/1001] [Batch 363/372] [D loss: 0.6908975839614868] [G loss: 0.7833260297775269]\n",
      "[Epoch 57/1001] [Batch 364/372] [D loss: 0.6974560022354126] [G loss: 0.5721679329872131]\n",
      "[Epoch 57/1001] [Batch 365/372] [D loss: 0.699212908744812] [G loss: 0.8288790583610535]\n",
      "[Epoch 57/1001] [Batch 366/372] [D loss: 0.6997789144515991] [G loss: 0.552882969379425]\n",
      "[Epoch 57/1001] [Batch 367/372] [D loss: 0.6979950666427612] [G loss: 0.8247041702270508]\n",
      "[Epoch 57/1001] [Batch 368/372] [D loss: 0.7024118304252625] [G loss: 0.5654984712600708]\n",
      "[Epoch 57/1001] [Batch 369/372] [D loss: 0.6990275382995605] [G loss: 0.802888035774231]\n",
      "[Epoch 57/1001] [Batch 370/372] [D loss: 0.7019875049591064] [G loss: 0.5887183547019958]\n",
      "[Epoch 57/1001] [Batch 371/372] [D loss: 0.6981195211410522] [G loss: 0.7505221962928772]\n",
      "[Epoch 58/1001] [Batch 0/372] [D loss: 0.6929051876068115] [G loss: 0.6350443959236145]\n",
      "[Epoch 58/1001] [Batch 1/372] [D loss: 0.6932863593101501] [G loss: 0.7221336960792542]\n",
      "[Epoch 58/1001] [Batch 2/372] [D loss: 0.6899899244308472] [G loss: 0.6516781449317932]\n",
      "[Epoch 58/1001] [Batch 3/372] [D loss: 0.6927868127822876] [G loss: 0.7185304164886475]\n",
      "[Epoch 58/1001] [Batch 4/372] [D loss: 0.6855469942092896] [G loss: 0.6456042528152466]\n",
      "[Epoch 58/1001] [Batch 5/372] [D loss: 0.6956931352615356] [G loss: 0.7074779272079468]\n",
      "[Epoch 58/1001] [Batch 6/372] [D loss: 0.6942002773284912] [G loss: 0.6774318218231201]\n",
      "[Epoch 58/1001] [Batch 7/372] [D loss: 0.6898598670959473] [G loss: 0.6730324029922485]\n",
      "[Epoch 58/1001] [Batch 8/372] [D loss: 0.6904474496841431] [G loss: 0.6949735283851624]\n",
      "[Epoch 58/1001] [Batch 9/372] [D loss: 0.6911301612854004] [G loss: 0.6830562949180603]\n",
      "[Epoch 58/1001] [Batch 10/372] [D loss: 0.6909817457199097] [G loss: 0.685835063457489]\n",
      "[Epoch 58/1001] [Batch 11/372] [D loss: 0.6898291110992432] [G loss: 0.6812413930892944]\n",
      "[Epoch 58/1001] [Batch 12/372] [D loss: 0.6912743449211121] [G loss: 0.6833046674728394]\n",
      "[Epoch 58/1001] [Batch 13/372] [D loss: 0.6917202472686768] [G loss: 0.6830591559410095]\n",
      "[Epoch 58/1001] [Batch 14/372] [D loss: 0.6911488771438599] [G loss: 0.678695797920227]\n",
      "[Epoch 58/1001] [Batch 15/372] [D loss: 0.6900753378868103] [G loss: 0.6824831366539001]\n",
      "[Epoch 58/1001] [Batch 16/372] [D loss: 0.6881111860275269] [G loss: 0.6797136068344116]\n",
      "[Epoch 58/1001] [Batch 17/372] [D loss: 0.6898318529129028] [G loss: 0.7060494422912598]\n",
      "[Epoch 58/1001] [Batch 18/372] [D loss: 0.6921734809875488] [G loss: 0.6356974244117737]\n",
      "[Epoch 58/1001] [Batch 19/372] [D loss: 0.6884666681289673] [G loss: 0.7472490668296814]\n",
      "[Epoch 58/1001] [Batch 20/372] [D loss: 0.6910891532897949] [G loss: 0.6085857152938843]\n",
      "[Epoch 58/1001] [Batch 21/372] [D loss: 0.6948930621147156] [G loss: 0.786296546459198]\n",
      "[Epoch 58/1001] [Batch 22/372] [D loss: 0.6961966753005981] [G loss: 0.5810425281524658]\n",
      "[Epoch 58/1001] [Batch 23/372] [D loss: 0.6977927684783936] [G loss: 0.7912447452545166]\n",
      "[Epoch 58/1001] [Batch 24/372] [D loss: 0.698994517326355] [G loss: 0.5903053283691406]\n",
      "[Epoch 58/1001] [Batch 25/372] [D loss: 0.6931183934211731] [G loss: 0.7796248197555542]\n",
      "[Epoch 58/1001] [Batch 26/372] [D loss: 0.6932244300842285] [G loss: 0.6125414371490479]\n",
      "[Epoch 58/1001] [Batch 27/372] [D loss: 0.6893947720527649] [G loss: 0.7559337615966797]\n",
      "[Epoch 58/1001] [Batch 28/372] [D loss: 0.6898351907730103] [G loss: 0.6261691451072693]\n",
      "[Epoch 58/1001] [Batch 29/372] [D loss: 0.6936947703361511] [G loss: 0.7298884391784668]\n",
      "[Epoch 58/1001] [Batch 30/372] [D loss: 0.6876964569091797] [G loss: 0.6459566950798035]\n",
      "[Epoch 58/1001] [Batch 31/372] [D loss: 0.6883954405784607] [G loss: 0.7416293621063232]\n",
      "[Epoch 58/1001] [Batch 32/372] [D loss: 0.6902703046798706] [G loss: 0.6240962743759155]\n",
      "[Epoch 58/1001] [Batch 33/372] [D loss: 0.6936372518539429] [G loss: 0.7398698925971985]\n",
      "[Epoch 58/1001] [Batch 34/372] [D loss: 0.6949163675308228] [G loss: 0.6264274716377258]\n",
      "[Epoch 58/1001] [Batch 35/372] [D loss: 0.6907609701156616] [G loss: 0.7332017421722412]\n",
      "[Epoch 58/1001] [Batch 36/372] [D loss: 0.6952115297317505] [G loss: 0.6347688436508179]\n",
      "[Epoch 58/1001] [Batch 37/372] [D loss: 0.6985388994216919] [G loss: 0.7383387088775635]\n",
      "[Epoch 58/1001] [Batch 38/372] [D loss: 0.6898932456970215] [G loss: 0.6226961016654968]\n",
      "[Epoch 58/1001] [Batch 39/372] [D loss: 0.6917724013328552] [G loss: 0.7583045363426208]\n",
      "[Epoch 58/1001] [Batch 40/372] [D loss: 0.6929171681404114] [G loss: 0.607690155506134]\n",
      "[Epoch 58/1001] [Batch 41/372] [D loss: 0.694642961025238] [G loss: 0.7680648565292358]\n",
      "[Epoch 58/1001] [Batch 42/372] [D loss: 0.6940641403198242] [G loss: 0.6150327920913696]\n",
      "[Epoch 58/1001] [Batch 43/372] [D loss: 0.6919262409210205] [G loss: 0.7540826797485352]\n",
      "[Epoch 58/1001] [Batch 44/372] [D loss: 0.693401038646698] [G loss: 0.6280485391616821]\n",
      "[Epoch 58/1001] [Batch 45/372] [D loss: 0.6926974058151245] [G loss: 0.729196310043335]\n",
      "[Epoch 58/1001] [Batch 46/372] [D loss: 0.6920773983001709] [G loss: 0.6460148096084595]\n",
      "[Epoch 58/1001] [Batch 47/372] [D loss: 0.6911685466766357] [G loss: 0.708686351776123]\n",
      "[Epoch 58/1001] [Batch 48/372] [D loss: 0.6892439126968384] [G loss: 0.6689571142196655]\n",
      "[Epoch 58/1001] [Batch 49/372] [D loss: 0.6869904398918152] [G loss: 0.6929095983505249]\n",
      "[Epoch 58/1001] [Batch 50/372] [D loss: 0.6914844512939453] [G loss: 0.6757234334945679]\n",
      "[Epoch 58/1001] [Batch 51/372] [D loss: 0.6874929666519165] [G loss: 0.6913522481918335]\n",
      "[Epoch 58/1001] [Batch 52/372] [D loss: 0.6903636455535889] [G loss: 0.6649145483970642]\n",
      "[Epoch 58/1001] [Batch 53/372] [D loss: 0.6926484704017639] [G loss: 0.7098546624183655]\n",
      "[Epoch 58/1001] [Batch 54/372] [D loss: 0.689845621585846] [G loss: 0.6442316770553589]\n",
      "[Epoch 58/1001] [Batch 55/372] [D loss: 0.69175124168396] [G loss: 0.7300921678543091]\n",
      "[Epoch 58/1001] [Batch 56/372] [D loss: 0.6902037262916565] [G loss: 0.6277426481246948]\n",
      "[Epoch 58/1001] [Batch 57/372] [D loss: 0.6923002004623413] [G loss: 0.7421370148658752]\n",
      "[Epoch 58/1001] [Batch 58/372] [D loss: 0.6917401552200317] [G loss: 0.6153829097747803]\n",
      "[Epoch 58/1001] [Batch 59/372] [D loss: 0.6928129196166992] [G loss: 0.75291907787323]\n",
      "[Epoch 58/1001] [Batch 60/372] [D loss: 0.6917927265167236] [G loss: 0.6104547381401062]\n",
      "[Epoch 58/1001] [Batch 61/372] [D loss: 0.694147527217865] [G loss: 0.7782608866691589]\n",
      "[Epoch 58/1001] [Batch 62/372] [D loss: 0.693321704864502] [G loss: 0.5683655142784119]\n",
      "[Epoch 58/1001] [Batch 63/372] [D loss: 0.6948268413543701] [G loss: 0.8429679870605469]\n",
      "[Epoch 58/1001] [Batch 64/372] [D loss: 0.6961102485656738] [G loss: 0.5227152109146118]\n",
      "[Epoch 58/1001] [Batch 65/372] [D loss: 0.7092810869216919] [G loss: 0.9181225299835205]\n",
      "[Epoch 58/1001] [Batch 66/372] [D loss: 0.7141718864440918] [G loss: 0.4904552698135376]\n",
      "[Epoch 58/1001] [Batch 67/372] [D loss: 0.7147624492645264] [G loss: 0.8864068984985352]\n",
      "[Epoch 58/1001] [Batch 68/372] [D loss: 0.7129022479057312] [G loss: 0.5681796073913574]\n",
      "[Epoch 58/1001] [Batch 69/372] [D loss: 0.7023794651031494] [G loss: 0.7431825995445251]\n",
      "[Epoch 58/1001] [Batch 70/372] [D loss: 0.6954866647720337] [G loss: 0.6753992438316345]\n",
      "[Epoch 58/1001] [Batch 71/372] [D loss: 0.6943002343177795] [G loss: 0.6527370810508728]\n",
      "[Epoch 58/1001] [Batch 72/372] [D loss: 0.6886231303215027] [G loss: 0.7099041938781738]\n",
      "[Epoch 58/1001] [Batch 73/372] [D loss: 0.6863471269607544] [G loss: 0.6751085519790649]\n",
      "[Epoch 58/1001] [Batch 74/372] [D loss: 0.6939462423324585] [G loss: 0.6930718421936035]\n",
      "[Epoch 58/1001] [Batch 75/372] [D loss: 0.6915323734283447] [G loss: 0.6794872879981995]\n",
      "[Epoch 58/1001] [Batch 76/372] [D loss: 0.6889879107475281] [G loss: 0.6811735033988953]\n",
      "[Epoch 58/1001] [Batch 77/372] [D loss: 0.6923054456710815] [G loss: 0.6971802115440369]\n",
      "[Epoch 58/1001] [Batch 78/372] [D loss: 0.690176784992218] [G loss: 0.6716622710227966]\n",
      "[Epoch 58/1001] [Batch 79/372] [D loss: 0.6912384033203125] [G loss: 0.6940348148345947]\n",
      "[Epoch 58/1001] [Batch 80/372] [D loss: 0.6918197870254517] [G loss: 0.6762039661407471]\n",
      "[Epoch 58/1001] [Batch 81/372] [D loss: 0.6894006133079529] [G loss: 0.6891841888427734]\n",
      "[Epoch 58/1001] [Batch 82/372] [D loss: 0.6892415285110474] [G loss: 0.6794126033782959]\n",
      "[Epoch 58/1001] [Batch 83/372] [D loss: 0.6902625560760498] [G loss: 0.6855291128158569]\n",
      "[Epoch 58/1001] [Batch 84/372] [D loss: 0.6887929439544678] [G loss: 0.6809287667274475]\n",
      "[Epoch 58/1001] [Batch 85/372] [D loss: 0.6898460984230042] [G loss: 0.6823016405105591]\n",
      "[Epoch 58/1001] [Batch 86/372] [D loss: 0.6918056011199951] [G loss: 0.6909939050674438]\n",
      "[Epoch 58/1001] [Batch 87/372] [D loss: 0.6905503273010254] [G loss: 0.683780312538147]\n",
      "[Epoch 58/1001] [Batch 88/372] [D loss: 0.688407838344574] [G loss: 0.6908932328224182]\n",
      "[Epoch 58/1001] [Batch 89/372] [D loss: 0.6899740695953369] [G loss: 0.6790609359741211]\n",
      "[Epoch 58/1001] [Batch 90/372] [D loss: 0.6917017102241516] [G loss: 0.6843050122261047]\n",
      "[Epoch 58/1001] [Batch 91/372] [D loss: 0.6929070353507996] [G loss: 0.6728290319442749]\n",
      "[Epoch 58/1001] [Batch 92/372] [D loss: 0.6901354193687439] [G loss: 0.7019444704055786]\n",
      "[Epoch 58/1001] [Batch 93/372] [D loss: 0.6916438341140747] [G loss: 0.6488372683525085]\n",
      "[Epoch 58/1001] [Batch 94/372] [D loss: 0.692168116569519] [G loss: 0.7175213098526001]\n",
      "[Epoch 58/1001] [Batch 95/372] [D loss: 0.692851185798645] [G loss: 0.654563307762146]\n",
      "[Epoch 58/1001] [Batch 96/372] [D loss: 0.6928858757019043] [G loss: 0.6963644027709961]\n",
      "[Epoch 58/1001] [Batch 97/372] [D loss: 0.6912648677825928] [G loss: 0.6687447428703308]\n",
      "[Epoch 58/1001] [Batch 98/372] [D loss: 0.6880236864089966] [G loss: 0.7067078948020935]\n",
      "[Epoch 58/1001] [Batch 99/372] [D loss: 0.688165545463562] [G loss: 0.6572114825248718]\n",
      "[Epoch 58/1001] [Batch 100/372] [D loss: 0.6917997598648071] [G loss: 0.711994469165802]\n",
      "[Epoch 58/1001] [Batch 101/372] [D loss: 0.6915479898452759] [G loss: 0.6556663513183594]\n",
      "[Epoch 58/1001] [Batch 102/372] [D loss: 0.6881461143493652] [G loss: 0.7016803026199341]\n",
      "[Epoch 58/1001] [Batch 103/372] [D loss: 0.6876910924911499] [G loss: 0.6578370332717896]\n",
      "[Epoch 58/1001] [Batch 104/372] [D loss: 0.6820958852767944] [G loss: 0.7461626529693604]\n",
      "[Epoch 58/1001] [Batch 105/372] [D loss: 0.6913340091705322] [G loss: 0.6008270978927612]\n",
      "[Epoch 58/1001] [Batch 106/372] [D loss: 0.6986365914344788] [G loss: 0.7771753072738647]\n",
      "[Epoch 58/1001] [Batch 107/372] [D loss: 0.6925841569900513] [G loss: 0.590181291103363]\n",
      "[Epoch 58/1001] [Batch 108/372] [D loss: 0.6947190761566162] [G loss: 0.7946313619613647]\n",
      "[Epoch 58/1001] [Batch 109/372] [D loss: 0.6991732120513916] [G loss: 0.5839456915855408]\n",
      "[Epoch 58/1001] [Batch 110/372] [D loss: 0.6952446699142456] [G loss: 0.7732459902763367]\n",
      "[Epoch 58/1001] [Batch 111/372] [D loss: 0.6993029117584229] [G loss: 0.612108051776886]\n",
      "[Epoch 58/1001] [Batch 112/372] [D loss: 0.6976351737976074] [G loss: 0.7323800921440125]\n",
      "[Epoch 58/1001] [Batch 113/372] [D loss: 0.691719651222229] [G loss: 0.6496731638908386]\n",
      "[Epoch 58/1001] [Batch 114/372] [D loss: 0.6899830102920532] [G loss: 0.709327220916748]\n",
      "[Epoch 58/1001] [Batch 115/372] [D loss: 0.6921287775039673] [G loss: 0.6646350622177124]\n",
      "[Epoch 58/1001] [Batch 116/372] [D loss: 0.691602349281311] [G loss: 0.696614146232605]\n",
      "[Epoch 58/1001] [Batch 117/372] [D loss: 0.689816951751709] [G loss: 0.6744731068611145]\n",
      "[Epoch 58/1001] [Batch 118/372] [D loss: 0.693871021270752] [G loss: 0.6885285973548889]\n",
      "[Epoch 58/1001] [Batch 119/372] [D loss: 0.6921067833900452] [G loss: 0.6727767586708069]\n",
      "[Epoch 58/1001] [Batch 120/372] [D loss: 0.6904385089874268] [G loss: 0.6982491612434387]\n",
      "[Epoch 58/1001] [Batch 121/372] [D loss: 0.691730260848999] [G loss: 0.6647009253501892]\n",
      "[Epoch 58/1001] [Batch 122/372] [D loss: 0.690386950969696] [G loss: 0.6987049579620361]\n",
      "[Epoch 58/1001] [Batch 123/372] [D loss: 0.6926007270812988] [G loss: 0.6717122793197632]\n",
      "[Epoch 58/1001] [Batch 124/372] [D loss: 0.689558744430542] [G loss: 0.6878105998039246]\n",
      "[Epoch 58/1001] [Batch 125/372] [D loss: 0.6925787925720215] [G loss: 0.6886886358261108]\n",
      "[Epoch 58/1001] [Batch 126/372] [D loss: 0.6890708208084106] [G loss: 0.6764017939567566]\n",
      "[Epoch 58/1001] [Batch 127/372] [D loss: 0.6898660659790039] [G loss: 0.6964967250823975]\n",
      "[Epoch 58/1001] [Batch 128/372] [D loss: 0.6890009045600891] [G loss: 0.6561091542243958]\n",
      "[Epoch 58/1001] [Batch 129/372] [D loss: 0.6903278827667236] [G loss: 0.7000123262405396]\n",
      "[Epoch 58/1001] [Batch 130/372] [D loss: 0.6864055395126343] [G loss: 0.6898399591445923]\n",
      "[Epoch 58/1001] [Batch 131/372] [D loss: 0.6957937479019165] [G loss: 0.6605086326599121]\n",
      "[Epoch 58/1001] [Batch 132/372] [D loss: 0.6917722821235657] [G loss: 0.7264343500137329]\n",
      "[Epoch 58/1001] [Batch 133/372] [D loss: 0.6910859942436218] [G loss: 0.6139964461326599]\n",
      "[Epoch 58/1001] [Batch 134/372] [D loss: 0.6955538988113403] [G loss: 0.7634427547454834]\n",
      "[Epoch 58/1001] [Batch 135/372] [D loss: 0.6956096887588501] [G loss: 0.5875834226608276]\n",
      "[Epoch 58/1001] [Batch 136/372] [D loss: 0.6947715282440186] [G loss: 0.8084949851036072]\n",
      "[Epoch 58/1001] [Batch 137/372] [D loss: 0.701930046081543] [G loss: 0.5458681583404541]\n",
      "[Epoch 58/1001] [Batch 138/372] [D loss: 0.7017199397087097] [G loss: 0.8762693405151367]\n",
      "[Epoch 58/1001] [Batch 139/372] [D loss: 0.7058506608009338] [G loss: 0.5249419212341309]\n",
      "[Epoch 58/1001] [Batch 140/372] [D loss: 0.7023149728775024] [G loss: 0.8420647382736206]\n",
      "[Epoch 58/1001] [Batch 141/372] [D loss: 0.6997965574264526] [G loss: 0.5775169730186462]\n",
      "[Epoch 58/1001] [Batch 142/372] [D loss: 0.6969475746154785] [G loss: 0.7635927200317383]\n",
      "[Epoch 58/1001] [Batch 143/372] [D loss: 0.6951272487640381] [G loss: 0.6326332688331604]\n",
      "[Epoch 58/1001] [Batch 144/372] [D loss: 0.6907018423080444] [G loss: 0.7094649076461792]\n",
      "[Epoch 58/1001] [Batch 145/372] [D loss: 0.6872416734695435] [G loss: 0.670339047908783]\n",
      "[Epoch 58/1001] [Batch 146/372] [D loss: 0.68974769115448] [G loss: 0.6860154271125793]\n",
      "[Epoch 58/1001] [Batch 147/372] [D loss: 0.6911903619766235] [G loss: 0.6719646453857422]\n",
      "[Epoch 58/1001] [Batch 148/372] [D loss: 0.6882022619247437] [G loss: 0.6782562136650085]\n",
      "[Epoch 58/1001] [Batch 149/372] [D loss: 0.6920437812805176] [G loss: 0.6988858580589294]\n",
      "[Epoch 58/1001] [Batch 150/372] [D loss: 0.6893656253814697] [G loss: 0.6550242900848389]\n",
      "[Epoch 58/1001] [Batch 151/372] [D loss: 0.6915202140808105] [G loss: 0.718795120716095]\n",
      "[Epoch 58/1001] [Batch 152/372] [D loss: 0.693254828453064] [G loss: 0.6428788304328918]\n",
      "[Epoch 58/1001] [Batch 153/372] [D loss: 0.6912655830383301] [G loss: 0.725843071937561]\n",
      "[Epoch 58/1001] [Batch 154/372] [D loss: 0.68886798620224] [G loss: 0.6479232907295227]\n",
      "[Epoch 58/1001] [Batch 155/372] [D loss: 0.6892381906509399] [G loss: 0.7148300409317017]\n",
      "[Epoch 58/1001] [Batch 156/372] [D loss: 0.6880532503128052] [G loss: 0.6588555574417114]\n",
      "[Epoch 58/1001] [Batch 157/372] [D loss: 0.6923648118972778] [G loss: 0.7155966758728027]\n",
      "[Epoch 58/1001] [Batch 158/372] [D loss: 0.6905313730239868] [G loss: 0.6387369632720947]\n",
      "[Epoch 58/1001] [Batch 159/372] [D loss: 0.6876493692398071] [G loss: 0.7240093946456909]\n",
      "[Epoch 58/1001] [Batch 160/372] [D loss: 0.6889725923538208] [G loss: 0.6630503535270691]\n",
      "[Epoch 58/1001] [Batch 161/372] [D loss: 0.6902337670326233] [G loss: 0.7210086584091187]\n",
      "[Epoch 58/1001] [Batch 162/372] [D loss: 0.6882875561714172] [G loss: 0.6209402680397034]\n",
      "[Epoch 58/1001] [Batch 163/372] [D loss: 0.6870912909507751] [G loss: 0.7861512899398804]\n",
      "[Epoch 58/1001] [Batch 164/372] [D loss: 0.6969754695892334] [G loss: 0.5391265749931335]\n",
      "[Epoch 58/1001] [Batch 165/372] [D loss: 0.7071473002433777] [G loss: 0.9250056743621826]\n",
      "[Epoch 58/1001] [Batch 166/372] [D loss: 0.7116281390190125] [G loss: 0.47742587327957153]\n",
      "[Epoch 58/1001] [Batch 167/372] [D loss: 0.7189598679542542] [G loss: 0.883499026298523]\n",
      "[Epoch 58/1001] [Batch 168/372] [D loss: 0.7097285985946655] [G loss: 0.585171103477478]\n",
      "[Epoch 58/1001] [Batch 169/372] [D loss: 0.6972630620002747] [G loss: 0.7101378440856934]\n",
      "[Epoch 58/1001] [Batch 170/372] [D loss: 0.6907023191452026] [G loss: 0.6914647221565247]\n",
      "[Epoch 58/1001] [Batch 171/372] [D loss: 0.6890442967414856] [G loss: 0.6646933555603027]\n",
      "[Epoch 58/1001] [Batch 172/372] [D loss: 0.6919009685516357] [G loss: 0.6986478567123413]\n",
      "[Epoch 58/1001] [Batch 173/372] [D loss: 0.6898335218429565] [G loss: 0.6711257696151733]\n",
      "[Epoch 58/1001] [Batch 174/372] [D loss: 0.6943957805633545] [G loss: 0.681387722492218]\n",
      "[Epoch 58/1001] [Batch 175/372] [D loss: 0.6909803748130798] [G loss: 0.6780010461807251]\n",
      "[Epoch 58/1001] [Batch 176/372] [D loss: 0.6899421215057373] [G loss: 0.6900026798248291]\n",
      "[Epoch 58/1001] [Batch 177/372] [D loss: 0.6875125169754028] [G loss: 0.6702160835266113]\n",
      "[Epoch 58/1001] [Batch 178/372] [D loss: 0.6868630647659302] [G loss: 0.7068919539451599]\n",
      "[Epoch 58/1001] [Batch 179/372] [D loss: 0.6898152828216553] [G loss: 0.6592614054679871]\n",
      "[Epoch 58/1001] [Batch 180/372] [D loss: 0.6903567910194397] [G loss: 0.6971051692962646]\n",
      "[Epoch 58/1001] [Batch 181/372] [D loss: 0.6860101222991943] [G loss: 0.6710774898529053]\n",
      "[Epoch 58/1001] [Batch 182/372] [D loss: 0.6884517669677734] [G loss: 0.7003403902053833]\n",
      "[Epoch 58/1001] [Batch 183/372] [D loss: 0.6888530254364014] [G loss: 0.6624716520309448]\n",
      "[Epoch 58/1001] [Batch 184/372] [D loss: 0.6910651326179504] [G loss: 0.7065929770469666]\n",
      "[Epoch 58/1001] [Batch 185/372] [D loss: 0.6912721991539001] [G loss: 0.6455354690551758]\n",
      "[Epoch 58/1001] [Batch 186/372] [D loss: 0.6899098753929138] [G loss: 0.7283567190170288]\n",
      "[Epoch 58/1001] [Batch 187/372] [D loss: 0.6915827393531799] [G loss: 0.6438112258911133]\n",
      "[Epoch 58/1001] [Batch 188/372] [D loss: 0.6949713826179504] [G loss: 0.7013936638832092]\n",
      "[Epoch 58/1001] [Batch 189/372] [D loss: 0.6960848569869995] [G loss: 0.6609842777252197]\n",
      "[Epoch 58/1001] [Batch 190/372] [D loss: 0.6929668188095093] [G loss: 0.6942427158355713]\n",
      "[Epoch 58/1001] [Batch 191/372] [D loss: 0.690446138381958] [G loss: 0.6686540842056274]\n",
      "[Epoch 58/1001] [Batch 192/372] [D loss: 0.692481517791748] [G loss: 0.7059134244918823]\n",
      "[Epoch 58/1001] [Batch 193/372] [D loss: 0.6893978118896484] [G loss: 0.6596359610557556]\n",
      "[Epoch 58/1001] [Batch 194/372] [D loss: 0.693300724029541] [G loss: 0.6989085674285889]\n",
      "[Epoch 58/1001] [Batch 195/372] [D loss: 0.6918221712112427] [G loss: 0.6717007756233215]\n",
      "[Epoch 58/1001] [Batch 196/372] [D loss: 0.6892921924591064] [G loss: 0.6852507591247559]\n",
      "[Epoch 58/1001] [Batch 197/372] [D loss: 0.6871362924575806] [G loss: 0.6869081854820251]\n",
      "[Epoch 58/1001] [Batch 198/372] [D loss: 0.6901280879974365] [G loss: 0.6824436187744141]\n",
      "[Epoch 58/1001] [Batch 199/372] [D loss: 0.6873561143875122] [G loss: 0.6736544370651245]\n",
      "[Epoch 58/1001] [Batch 200/372] [D loss: 0.6933870315551758] [G loss: 0.6900627613067627]\n",
      "[Epoch 58/1001] [Batch 201/372] [D loss: 0.6860240697860718] [G loss: 0.6624824404716492]\n",
      "[Epoch 58/1001] [Batch 202/372] [D loss: 0.6928722262382507] [G loss: 0.7057994604110718]\n",
      "[Epoch 58/1001] [Batch 203/372] [D loss: 0.6924085021018982] [G loss: 0.6554253101348877]\n",
      "[Epoch 58/1001] [Batch 204/372] [D loss: 0.6925920248031616] [G loss: 0.7054501175880432]\n",
      "[Epoch 58/1001] [Batch 205/372] [D loss: 0.6919500827789307] [G loss: 0.6493854522705078]\n",
      "[Epoch 58/1001] [Batch 206/372] [D loss: 0.6887045502662659] [G loss: 0.7187217473983765]\n",
      "[Epoch 58/1001] [Batch 207/372] [D loss: 0.6891354322433472] [G loss: 0.6324474215507507]\n",
      "[Epoch 58/1001] [Batch 208/372] [D loss: 0.6934239864349365] [G loss: 0.7613959312438965]\n",
      "[Epoch 58/1001] [Batch 209/372] [D loss: 0.691007137298584] [G loss: 0.6000245809555054]\n",
      "[Epoch 58/1001] [Batch 210/372] [D loss: 0.6948423385620117] [G loss: 0.7637900114059448]\n",
      "[Epoch 58/1001] [Batch 211/372] [D loss: 0.6918922066688538] [G loss: 0.6138413548469543]\n",
      "[Epoch 58/1001] [Batch 212/372] [D loss: 0.6966886520385742] [G loss: 0.7536704540252686]\n",
      "[Epoch 58/1001] [Batch 213/372] [D loss: 0.6912561655044556] [G loss: 0.6122875809669495]\n",
      "[Epoch 58/1001] [Batch 214/372] [D loss: 0.6961635947227478] [G loss: 0.7591480612754822]\n",
      "[Epoch 58/1001] [Batch 215/372] [D loss: 0.6922639608383179] [G loss: 0.6199004054069519]\n",
      "[Epoch 58/1001] [Batch 216/372] [D loss: 0.6965038776397705] [G loss: 0.7424276471138]\n",
      "[Epoch 58/1001] [Batch 217/372] [D loss: 0.695315420627594] [G loss: 0.6212640404701233]\n",
      "[Epoch 58/1001] [Batch 218/372] [D loss: 0.6926907300949097] [G loss: 0.7284168004989624]\n",
      "[Epoch 58/1001] [Batch 219/372] [D loss: 0.692314863204956] [G loss: 0.6444591283798218]\n",
      "[Epoch 58/1001] [Batch 220/372] [D loss: 0.6891622543334961] [G loss: 0.7117629051208496]\n",
      "[Epoch 58/1001] [Batch 221/372] [D loss: 0.6890950798988342] [G loss: 0.6697341203689575]\n",
      "[Epoch 58/1001] [Batch 222/372] [D loss: 0.6883200407028198] [G loss: 0.6865628957748413]\n",
      "[Epoch 58/1001] [Batch 223/372] [D loss: 0.696627140045166] [G loss: 0.6777765154838562]\n",
      "[Epoch 58/1001] [Batch 224/372] [D loss: 0.6953158378601074] [G loss: 0.6737093925476074]\n",
      "[Epoch 58/1001] [Batch 225/372] [D loss: 0.6911917924880981] [G loss: 0.6709177494049072]\n",
      "[Epoch 58/1001] [Batch 226/372] [D loss: 0.6960293054580688] [G loss: 0.6999391317367554]\n",
      "[Epoch 58/1001] [Batch 227/372] [D loss: 0.693037748336792] [G loss: 0.6504439115524292]\n",
      "[Epoch 58/1001] [Batch 228/372] [D loss: 0.6900129318237305] [G loss: 0.716475248336792]\n",
      "[Epoch 58/1001] [Batch 229/372] [D loss: 0.688811182975769] [G loss: 0.6488428115844727]\n",
      "[Epoch 58/1001] [Batch 230/372] [D loss: 0.6934131979942322] [G loss: 0.7183560729026794]\n",
      "[Epoch 58/1001] [Batch 231/372] [D loss: 0.6910288333892822] [G loss: 0.639344334602356]\n",
      "[Epoch 58/1001] [Batch 232/372] [D loss: 0.6930694580078125] [G loss: 0.7299251556396484]\n",
      "[Epoch 58/1001] [Batch 233/372] [D loss: 0.6879335641860962] [G loss: 0.61324542760849]\n",
      "[Epoch 58/1001] [Batch 234/372] [D loss: 0.6900179982185364] [G loss: 0.7703930735588074]\n",
      "[Epoch 58/1001] [Batch 235/372] [D loss: 0.6934497356414795] [G loss: 0.5999959707260132]\n",
      "[Epoch 58/1001] [Batch 236/372] [D loss: 0.6928718090057373] [G loss: 0.788790762424469]\n",
      "[Epoch 58/1001] [Batch 237/372] [D loss: 0.6959351301193237] [G loss: 0.5784077644348145]\n",
      "[Epoch 58/1001] [Batch 238/372] [D loss: 0.6956207156181335] [G loss: 0.7966908812522888]\n",
      "[Epoch 58/1001] [Batch 239/372] [D loss: 0.6996468305587769] [G loss: 0.5911778211593628]\n",
      "[Epoch 58/1001] [Batch 240/372] [D loss: 0.6957001686096191] [G loss: 0.7543184757232666]\n",
      "[Epoch 58/1001] [Batch 241/372] [D loss: 0.6936295032501221] [G loss: 0.6307321786880493]\n",
      "[Epoch 58/1001] [Batch 242/372] [D loss: 0.6936287879943848] [G loss: 0.7142019271850586]\n",
      "[Epoch 58/1001] [Batch 243/372] [D loss: 0.6933627128601074] [G loss: 0.6583333611488342]\n",
      "[Epoch 58/1001] [Batch 244/372] [D loss: 0.6917130947113037] [G loss: 0.7073997855186462]\n",
      "[Epoch 58/1001] [Batch 245/372] [D loss: 0.6895812153816223] [G loss: 0.6586215496063232]\n",
      "[Epoch 58/1001] [Batch 246/372] [D loss: 0.6880373954772949] [G loss: 0.6965189576148987]\n",
      "[Epoch 58/1001] [Batch 247/372] [D loss: 0.6908132433891296] [G loss: 0.6648558378219604]\n",
      "[Epoch 58/1001] [Batch 248/372] [D loss: 0.687281608581543] [G loss: 0.7073395252227783]\n",
      "[Epoch 58/1001] [Batch 249/372] [D loss: 0.6877962350845337] [G loss: 0.6504335403442383]\n",
      "[Epoch 58/1001] [Batch 250/372] [D loss: 0.688593864440918] [G loss: 0.7335417866706848]\n",
      "[Epoch 58/1001] [Batch 251/372] [D loss: 0.6918033361434937] [G loss: 0.6096731424331665]\n",
      "[Epoch 58/1001] [Batch 252/372] [D loss: 0.6944496035575867] [G loss: 0.8007490038871765]\n",
      "[Epoch 58/1001] [Batch 253/372] [D loss: 0.6946021318435669] [G loss: 0.5419164896011353]\n",
      "[Epoch 58/1001] [Batch 254/372] [D loss: 0.7035700082778931] [G loss: 0.9009682536125183]\n",
      "[Epoch 58/1001] [Batch 255/372] [D loss: 0.7114933133125305] [G loss: 0.5005636811256409]\n",
      "[Epoch 58/1001] [Batch 256/372] [D loss: 0.7108025550842285] [G loss: 0.8770080804824829]\n",
      "[Epoch 58/1001] [Batch 257/372] [D loss: 0.7090362310409546] [G loss: 0.5623888969421387]\n",
      "[Epoch 58/1001] [Batch 258/372] [D loss: 0.6965253353118896] [G loss: 0.7543790936470032]\n",
      "[Epoch 58/1001] [Batch 259/372] [D loss: 0.688239336013794] [G loss: 0.6504456996917725]\n",
      "[Epoch 58/1001] [Batch 260/372] [D loss: 0.6936284303665161] [G loss: 0.7042047381401062]\n",
      "[Epoch 58/1001] [Batch 261/372] [D loss: 0.691971480846405] [G loss: 0.6584886312484741]\n",
      "[Epoch 58/1001] [Batch 262/372] [D loss: 0.6911884546279907] [G loss: 0.6899593472480774]\n",
      "[Epoch 58/1001] [Batch 263/372] [D loss: 0.690563440322876] [G loss: 0.7035388350486755]\n",
      "[Epoch 58/1001] [Batch 264/372] [D loss: 0.6905210018157959] [G loss: 0.6372252702713013]\n",
      "[Epoch 58/1001] [Batch 265/372] [D loss: 0.6913623809814453] [G loss: 0.7445415258407593]\n",
      "[Epoch 58/1001] [Batch 266/372] [D loss: 0.6935144066810608] [G loss: 0.6138442754745483]\n",
      "[Epoch 58/1001] [Batch 267/372] [D loss: 0.6965821981430054] [G loss: 0.7317319512367249]\n",
      "[Epoch 58/1001] [Batch 268/372] [D loss: 0.6931641101837158] [G loss: 0.638198971748352]\n",
      "[Epoch 58/1001] [Batch 269/372] [D loss: 0.692497730255127] [G loss: 0.7075622081756592]\n",
      "[Epoch 58/1001] [Batch 270/372] [D loss: 0.6916515231132507] [G loss: 0.6595914363861084]\n",
      "[Epoch 58/1001] [Batch 271/372] [D loss: 0.6896947622299194] [G loss: 0.6935141086578369]\n",
      "[Epoch 58/1001] [Batch 272/372] [D loss: 0.6935857534408569] [G loss: 0.6750895977020264]\n",
      "[Epoch 58/1001] [Batch 273/372] [D loss: 0.690491795539856] [G loss: 0.6841954588890076]\n",
      "[Epoch 58/1001] [Batch 274/372] [D loss: 0.686768651008606] [G loss: 0.6827602982521057]\n",
      "[Epoch 58/1001] [Batch 275/372] [D loss: 0.6893261671066284] [G loss: 0.6788922548294067]\n",
      "[Epoch 58/1001] [Batch 276/372] [D loss: 0.6884831190109253] [G loss: 0.6778852939605713]\n",
      "[Epoch 58/1001] [Batch 277/372] [D loss: 0.6919469833374023] [G loss: 0.6802251935005188]\n",
      "[Epoch 58/1001] [Batch 278/372] [D loss: 0.6946002244949341] [G loss: 0.6834443211555481]\n",
      "[Epoch 58/1001] [Batch 279/372] [D loss: 0.6903361082077026] [G loss: 0.6805682182312012]\n",
      "[Epoch 58/1001] [Batch 280/372] [D loss: 0.688563883304596] [G loss: 0.673396110534668]\n",
      "[Epoch 58/1001] [Batch 281/372] [D loss: 0.6897318363189697] [G loss: 0.6892351508140564]\n",
      "[Epoch 58/1001] [Batch 282/372] [D loss: 0.6911222338676453] [G loss: 0.6714872717857361]\n",
      "[Epoch 58/1001] [Batch 283/372] [D loss: 0.6896030902862549] [G loss: 0.7079946398735046]\n",
      "[Epoch 58/1001] [Batch 284/372] [D loss: 0.6829638481140137] [G loss: 0.6556867361068726]\n",
      "[Epoch 58/1001] [Batch 285/372] [D loss: 0.6867561340332031] [G loss: 0.6991662383079529]\n",
      "[Epoch 58/1001] [Batch 286/372] [D loss: 0.6933472156524658] [G loss: 0.682655930519104]\n",
      "[Epoch 58/1001] [Batch 287/372] [D loss: 0.6892184019088745] [G loss: 0.6854335069656372]\n",
      "[Epoch 58/1001] [Batch 288/372] [D loss: 0.691964864730835] [G loss: 0.6683857440948486]\n",
      "[Epoch 58/1001] [Batch 289/372] [D loss: 0.6912024021148682] [G loss: 0.6748170852661133]\n",
      "[Epoch 58/1001] [Batch 290/372] [D loss: 0.6941275596618652] [G loss: 0.6864101886749268]\n",
      "[Epoch 58/1001] [Batch 291/372] [D loss: 0.6892974972724915] [G loss: 0.6626286506652832]\n",
      "[Epoch 58/1001] [Batch 292/372] [D loss: 0.6878930330276489] [G loss: 0.7180445790290833]\n",
      "[Epoch 58/1001] [Batch 293/372] [D loss: 0.6856724619865417] [G loss: 0.6112648844718933]\n",
      "[Epoch 58/1001] [Batch 294/372] [D loss: 0.6999518871307373] [G loss: 0.8022786974906921]\n",
      "[Epoch 58/1001] [Batch 295/372] [D loss: 0.6968292593955994] [G loss: 0.5393397212028503]\n",
      "[Epoch 58/1001] [Batch 296/372] [D loss: 0.7049094438552856] [G loss: 0.8802564144134521]\n",
      "[Epoch 58/1001] [Batch 297/372] [D loss: 0.7120299339294434] [G loss: 0.5044270157814026]\n",
      "[Epoch 58/1001] [Batch 298/372] [D loss: 0.7092846632003784] [G loss: 0.8536859750747681]\n",
      "[Epoch 58/1001] [Batch 299/372] [D loss: 0.7043933868408203] [G loss: 0.5731586217880249]\n",
      "[Epoch 58/1001] [Batch 300/372] [D loss: 0.6992264986038208] [G loss: 0.7653246521949768]\n",
      "[Epoch 58/1001] [Batch 301/372] [D loss: 0.694384753704071] [G loss: 0.6273629665374756]\n",
      "[Epoch 58/1001] [Batch 302/372] [D loss: 0.6925740242004395] [G loss: 0.7158259749412537]\n",
      "[Epoch 58/1001] [Batch 303/372] [D loss: 0.6912986040115356] [G loss: 0.6681799292564392]\n",
      "[Epoch 58/1001] [Batch 304/372] [D loss: 0.6915837526321411] [G loss: 0.678490400314331]\n",
      "[Epoch 58/1001] [Batch 305/372] [D loss: 0.6883971095085144] [G loss: 0.6957511305809021]\n",
      "[Epoch 58/1001] [Batch 306/372] [D loss: 0.6910943388938904] [G loss: 0.671349823474884]\n",
      "[Epoch 58/1001] [Batch 307/372] [D loss: 0.6892011761665344] [G loss: 0.6839311718940735]\n",
      "[Epoch 58/1001] [Batch 308/372] [D loss: 0.6901296377182007] [G loss: 0.6907860636711121]\n",
      "[Epoch 58/1001] [Batch 309/372] [D loss: 0.6921520233154297] [G loss: 0.6644497513771057]\n",
      "[Epoch 58/1001] [Batch 310/372] [D loss: 0.6918445825576782] [G loss: 0.7047926783561707]\n",
      "[Epoch 58/1001] [Batch 311/372] [D loss: 0.6911185383796692] [G loss: 0.6643010377883911]\n",
      "[Epoch 58/1001] [Batch 312/372] [D loss: 0.6925925612449646] [G loss: 0.6811411380767822]\n",
      "[Epoch 58/1001] [Batch 313/372] [D loss: 0.6909767389297485] [G loss: 0.6736806035041809]\n",
      "[Epoch 58/1001] [Batch 314/372] [D loss: 0.6858861446380615] [G loss: 0.6908529996871948]\n",
      "[Epoch 58/1001] [Batch 315/372] [D loss: 0.6917511224746704] [G loss: 0.6763085126876831]\n",
      "[Epoch 58/1001] [Batch 316/372] [D loss: 0.6922405362129211] [G loss: 0.6790860295295715]\n",
      "[Epoch 58/1001] [Batch 317/372] [D loss: 0.6930282115936279] [G loss: 0.6947680711746216]\n",
      "[Epoch 58/1001] [Batch 318/372] [D loss: 0.6920175552368164] [G loss: 0.6632535457611084]\n",
      "[Epoch 58/1001] [Batch 319/372] [D loss: 0.691779375076294] [G loss: 0.7019546031951904]\n",
      "[Epoch 58/1001] [Batch 320/372] [D loss: 0.6910775899887085] [G loss: 0.6499539613723755]\n",
      "[Epoch 58/1001] [Batch 321/372] [D loss: 0.6910301446914673] [G loss: 0.7108350992202759]\n",
      "[Epoch 58/1001] [Batch 322/372] [D loss: 0.6904099583625793] [G loss: 0.6466152667999268]\n",
      "[Epoch 58/1001] [Batch 323/372] [D loss: 0.6930926442146301] [G loss: 0.7542212605476379]\n",
      "[Epoch 58/1001] [Batch 324/372] [D loss: 0.694577693939209] [G loss: 0.5934754014015198]\n",
      "[Epoch 58/1001] [Batch 325/372] [D loss: 0.6954097151756287] [G loss: 0.7851295471191406]\n",
      "[Epoch 58/1001] [Batch 326/372] [D loss: 0.6932034492492676] [G loss: 0.5783200263977051]\n",
      "[Epoch 58/1001] [Batch 327/372] [D loss: 0.7005793452262878] [G loss: 0.8031197190284729]\n",
      "[Epoch 58/1001] [Batch 328/372] [D loss: 0.7005164623260498] [G loss: 0.5712465643882751]\n",
      "[Epoch 58/1001] [Batch 329/372] [D loss: 0.6977502703666687] [G loss: 0.7817323207855225]\n",
      "[Epoch 58/1001] [Batch 330/372] [D loss: 0.6959465146064758] [G loss: 0.6144214272499084]\n",
      "[Epoch 58/1001] [Batch 331/372] [D loss: 0.6971276998519897] [G loss: 0.7314286231994629]\n",
      "[Epoch 58/1001] [Batch 332/372] [D loss: 0.6926209926605225] [G loss: 0.6456519961357117]\n",
      "[Epoch 58/1001] [Batch 333/372] [D loss: 0.6937062740325928] [G loss: 0.7117406129837036]\n",
      "[Epoch 58/1001] [Batch 334/372] [D loss: 0.6908416152000427] [G loss: 0.6566001772880554]\n",
      "[Epoch 58/1001] [Batch 335/372] [D loss: 0.6894621849060059] [G loss: 0.6907926201820374]\n",
      "[Epoch 58/1001] [Batch 336/372] [D loss: 0.6910631656646729] [G loss: 0.6903983354568481]\n",
      "[Epoch 58/1001] [Batch 337/372] [D loss: 0.6851431131362915] [G loss: 0.6585291624069214]\n",
      "[Epoch 58/1001] [Batch 338/372] [D loss: 0.6893366575241089] [G loss: 0.7188071608543396]\n",
      "[Epoch 58/1001] [Batch 339/372] [D loss: 0.6879830956459045] [G loss: 0.6469224691390991]\n",
      "[Epoch 58/1001] [Batch 340/372] [D loss: 0.6898335814476013] [G loss: 0.7123539447784424]\n",
      "[Epoch 58/1001] [Batch 341/372] [D loss: 0.6894121170043945] [G loss: 0.644102156162262]\n",
      "[Epoch 58/1001] [Batch 342/372] [D loss: 0.692939817905426] [G loss: 0.7177026271820068]\n",
      "[Epoch 58/1001] [Batch 343/372] [D loss: 0.6924315690994263] [G loss: 0.640102207660675]\n",
      "[Epoch 58/1001] [Batch 344/372] [D loss: 0.692500114440918] [G loss: 0.7366746664047241]\n",
      "[Epoch 58/1001] [Batch 345/372] [D loss: 0.6932821869850159] [G loss: 0.6288094520568848]\n",
      "[Epoch 58/1001] [Batch 346/372] [D loss: 0.6946529746055603] [G loss: 0.721651017665863]\n",
      "[Epoch 58/1001] [Batch 347/372] [D loss: 0.6904721260070801] [G loss: 0.6405920386314392]\n",
      "[Epoch 58/1001] [Batch 348/372] [D loss: 0.6937626600265503] [G loss: 0.7269256711006165]\n",
      "[Epoch 58/1001] [Batch 349/372] [D loss: 0.693173885345459] [G loss: 0.6472476720809937]\n",
      "[Epoch 58/1001] [Batch 350/372] [D loss: 0.694028377532959] [G loss: 0.6991583704948425]\n",
      "[Epoch 58/1001] [Batch 351/372] [D loss: 0.6909960508346558] [G loss: 0.6750471591949463]\n",
      "[Epoch 58/1001] [Batch 352/372] [D loss: 0.6912741661071777] [G loss: 0.6831001043319702]\n",
      "[Epoch 58/1001] [Batch 353/372] [D loss: 0.6906493306159973] [G loss: 0.6917425990104675]\n",
      "[Epoch 58/1001] [Batch 354/372] [D loss: 0.6926942467689514] [G loss: 0.6730222702026367]\n",
      "[Epoch 58/1001] [Batch 355/372] [D loss: 0.690729022026062] [G loss: 0.6766906380653381]\n",
      "[Epoch 58/1001] [Batch 356/372] [D loss: 0.6916359663009644] [G loss: 0.6857874989509583]\n",
      "[Epoch 58/1001] [Batch 357/372] [D loss: 0.6907041072845459] [G loss: 0.6672887802124023]\n",
      "[Epoch 58/1001] [Batch 358/372] [D loss: 0.6883881092071533] [G loss: 0.6983602643013]\n",
      "[Epoch 58/1001] [Batch 359/372] [D loss: 0.6917028427124023] [G loss: 0.6582746505737305]\n",
      "[Epoch 58/1001] [Batch 360/372] [D loss: 0.6903777122497559] [G loss: 0.7142516374588013]\n",
      "[Epoch 58/1001] [Batch 361/372] [D loss: 0.6924536228179932] [G loss: 0.6468992829322815]\n",
      "[Epoch 58/1001] [Batch 362/372] [D loss: 0.6926372051239014] [G loss: 0.720170795917511]\n",
      "[Epoch 58/1001] [Batch 363/372] [D loss: 0.6921594142913818] [G loss: 0.6385752558708191]\n",
      "[Epoch 58/1001] [Batch 364/372] [D loss: 0.6926846504211426] [G loss: 0.7333893775939941]\n",
      "[Epoch 58/1001] [Batch 365/372] [D loss: 0.6898784637451172] [G loss: 0.6410853266716003]\n",
      "[Epoch 58/1001] [Batch 366/372] [D loss: 0.689537763595581] [G loss: 0.6962737441062927]\n",
      "[Epoch 58/1001] [Batch 367/372] [D loss: 0.6906574964523315] [G loss: 0.6747020483016968]\n",
      "[Epoch 58/1001] [Batch 368/372] [D loss: 0.6870073080062866] [G loss: 0.6897313594818115]\n",
      "[Epoch 58/1001] [Batch 369/372] [D loss: 0.6833399534225464] [G loss: 0.6834061741828918]\n",
      "[Epoch 58/1001] [Batch 370/372] [D loss: 0.6925444602966309] [G loss: 0.6765760183334351]\n",
      "[Epoch 58/1001] [Batch 371/372] [D loss: 0.6876569986343384] [G loss: 0.6824829578399658]\n",
      "[Epoch 59/1001] [Batch 0/372] [D loss: 0.689529538154602] [G loss: 0.6753168106079102]\n",
      "[Epoch 59/1001] [Batch 1/372] [D loss: 0.6918542385101318] [G loss: 0.6787332892417908]\n",
      "[Epoch 59/1001] [Batch 2/372] [D loss: 0.6930463314056396] [G loss: 0.6836778521537781]\n",
      "[Epoch 59/1001] [Batch 3/372] [D loss: 0.6896306276321411] [G loss: 0.6811345815658569]\n",
      "[Epoch 59/1001] [Batch 4/372] [D loss: 0.6881077885627747] [G loss: 0.672171950340271]\n",
      "[Epoch 59/1001] [Batch 5/372] [D loss: 0.6875388622283936] [G loss: 0.7043545842170715]\n",
      "[Epoch 59/1001] [Batch 6/372] [D loss: 0.6851956844329834] [G loss: 0.6332020163536072]\n",
      "[Epoch 59/1001] [Batch 7/372] [D loss: 0.6904284954071045] [G loss: 0.769066333770752]\n",
      "[Epoch 59/1001] [Batch 8/372] [D loss: 0.6896365880966187] [G loss: 0.5655560493469238]\n",
      "[Epoch 59/1001] [Batch 9/372] [D loss: 0.6983788013458252] [G loss: 0.8879052996635437]\n",
      "[Epoch 59/1001] [Batch 10/372] [D loss: 0.710434079170227] [G loss: 0.4698951840400696]\n",
      "[Epoch 59/1001] [Batch 11/372] [D loss: 0.7228236794471741] [G loss: 0.9766236543655396]\n",
      "[Epoch 59/1001] [Batch 12/372] [D loss: 0.7292506694793701] [G loss: 0.47157514095306396]\n",
      "[Epoch 59/1001] [Batch 13/372] [D loss: 0.7175700068473816] [G loss: 0.8688962459564209]\n",
      "[Epoch 59/1001] [Batch 14/372] [D loss: 0.705707311630249] [G loss: 0.5966751575469971]\n",
      "[Epoch 59/1001] [Batch 15/372] [D loss: 0.6931153535842896] [G loss: 0.7150073051452637]\n",
      "[Epoch 59/1001] [Batch 16/372] [D loss: 0.6922237873077393] [G loss: 0.6820274591445923]\n",
      "[Epoch 59/1001] [Batch 17/372] [D loss: 0.6869474649429321] [G loss: 0.6525799036026001]\n",
      "[Epoch 59/1001] [Batch 18/372] [D loss: 0.6902533769607544] [G loss: 0.7201358079910278]\n",
      "[Epoch 59/1001] [Batch 19/372] [D loss: 0.6909859776496887] [G loss: 0.6574281454086304]\n",
      "[Epoch 59/1001] [Batch 20/372] [D loss: 0.6894018054008484] [G loss: 0.6887727975845337]\n",
      "[Epoch 59/1001] [Batch 21/372] [D loss: 0.6916736364364624] [G loss: 0.6956676244735718]\n",
      "[Epoch 59/1001] [Batch 22/372] [D loss: 0.6901195049285889] [G loss: 0.6555262804031372]\n",
      "[Epoch 59/1001] [Batch 23/372] [D loss: 0.6889277696609497] [G loss: 0.7191035747528076]\n",
      "[Epoch 59/1001] [Batch 24/372] [D loss: 0.6842381954193115] [G loss: 0.6576303243637085]\n",
      "[Epoch 59/1001] [Batch 25/372] [D loss: 0.6904251575469971] [G loss: 0.723976731300354]\n",
      "[Epoch 59/1001] [Batch 26/372] [D loss: 0.6902302503585815] [G loss: 0.6486767530441284]\n",
      "[Epoch 59/1001] [Batch 27/372] [D loss: 0.6920385360717773] [G loss: 0.6980942487716675]\n",
      "[Epoch 59/1001] [Batch 28/372] [D loss: 0.6889959573745728] [G loss: 0.6808449029922485]\n",
      "[Epoch 59/1001] [Batch 29/372] [D loss: 0.693001389503479] [G loss: 0.6790903806686401]\n",
      "[Epoch 59/1001] [Batch 30/372] [D loss: 0.6887795329093933] [G loss: 0.6907886266708374]\n",
      "[Epoch 59/1001] [Batch 31/372] [D loss: 0.6946824789047241] [G loss: 0.6675559282302856]\n",
      "[Epoch 59/1001] [Batch 32/372] [D loss: 0.6895873546600342] [G loss: 0.6906258463859558]\n",
      "[Epoch 59/1001] [Batch 33/372] [D loss: 0.6913774609565735] [G loss: 0.6773922443389893]\n",
      "[Epoch 59/1001] [Batch 34/372] [D loss: 0.6913913488388062] [G loss: 0.6768563985824585]\n",
      "[Epoch 59/1001] [Batch 35/372] [D loss: 0.6853090524673462] [G loss: 0.6888189315795898]\n",
      "[Epoch 59/1001] [Batch 36/372] [D loss: 0.6895606517791748] [G loss: 0.6804576516151428]\n",
      "[Epoch 59/1001] [Batch 37/372] [D loss: 0.6842299699783325] [G loss: 0.6855106949806213]\n",
      "[Epoch 59/1001] [Batch 38/372] [D loss: 0.6848719120025635] [G loss: 0.6855155229568481]\n",
      "[Epoch 59/1001] [Batch 39/372] [D loss: 0.6910467743873596] [G loss: 0.6940639019012451]\n",
      "[Epoch 59/1001] [Batch 40/372] [D loss: 0.6928635239601135] [G loss: 0.6544023156166077]\n",
      "[Epoch 59/1001] [Batch 41/372] [D loss: 0.6942428350448608] [G loss: 0.703658401966095]\n",
      "[Epoch 59/1001] [Batch 42/372] [D loss: 0.6897029876708984] [G loss: 0.6560207009315491]\n",
      "[Epoch 59/1001] [Batch 43/372] [D loss: 0.6946797966957092] [G loss: 0.7065106630325317]\n",
      "[Epoch 59/1001] [Batch 44/372] [D loss: 0.6922041177749634] [G loss: 0.6540042161941528]\n",
      "[Epoch 59/1001] [Batch 45/372] [D loss: 0.6900304555892944] [G loss: 0.7160398364067078]\n",
      "[Epoch 59/1001] [Batch 46/372] [D loss: 0.6913986206054688] [G loss: 0.6444453001022339]\n",
      "[Epoch 59/1001] [Batch 47/372] [D loss: 0.6882700324058533] [G loss: 0.7185978293418884]\n",
      "[Epoch 59/1001] [Batch 48/372] [D loss: 0.690506100654602] [G loss: 0.6506273150444031]\n",
      "[Epoch 59/1001] [Batch 49/372] [D loss: 0.6871644258499146] [G loss: 0.7132974863052368]\n",
      "[Epoch 59/1001] [Batch 50/372] [D loss: 0.6878343820571899] [G loss: 0.6396036744117737]\n",
      "[Epoch 59/1001] [Batch 51/372] [D loss: 0.6915076375007629] [G loss: 0.7826619148254395]\n",
      "[Epoch 59/1001] [Batch 52/372] [D loss: 0.6942137479782104] [G loss: 0.5578100681304932]\n",
      "[Epoch 59/1001] [Batch 53/372] [D loss: 0.6989489197731018] [G loss: 0.8487804532051086]\n",
      "[Epoch 59/1001] [Batch 54/372] [D loss: 0.6994595527648926] [G loss: 0.5428957939147949]\n",
      "[Epoch 59/1001] [Batch 55/372] [D loss: 0.700779914855957] [G loss: 0.8154044151306152]\n",
      "[Epoch 59/1001] [Batch 56/372] [D loss: 0.69371497631073] [G loss: 0.5973689556121826]\n",
      "[Epoch 59/1001] [Batch 57/372] [D loss: 0.6929188370704651] [G loss: 0.7526603937149048]\n",
      "[Epoch 59/1001] [Batch 58/372] [D loss: 0.6906261444091797] [G loss: 0.6325227618217468]\n",
      "[Epoch 59/1001] [Batch 59/372] [D loss: 0.6886929273605347] [G loss: 0.7393038868904114]\n",
      "[Epoch 59/1001] [Batch 60/372] [D loss: 0.6891816854476929] [G loss: 0.6325899958610535]\n",
      "[Epoch 59/1001] [Batch 61/372] [D loss: 0.6908878087997437] [G loss: 0.7224563360214233]\n",
      "[Epoch 59/1001] [Batch 62/372] [D loss: 0.694425106048584] [G loss: 0.6423826813697815]\n",
      "[Epoch 59/1001] [Batch 63/372] [D loss: 0.6960405111312866] [G loss: 0.7009334564208984]\n",
      "[Epoch 59/1001] [Batch 64/372] [D loss: 0.6919288635253906] [G loss: 0.6754195690155029]\n",
      "[Epoch 59/1001] [Batch 65/372] [D loss: 0.6904687881469727] [G loss: 0.6848751902580261]\n",
      "[Epoch 59/1001] [Batch 66/372] [D loss: 0.6915252208709717] [G loss: 0.683246910572052]\n",
      "[Epoch 59/1001] [Batch 67/372] [D loss: 0.6882345676422119] [G loss: 0.664752721786499]\n",
      "[Epoch 59/1001] [Batch 68/372] [D loss: 0.691078782081604] [G loss: 0.6986737251281738]\n",
      "[Epoch 59/1001] [Batch 69/372] [D loss: 0.6930568218231201] [G loss: 0.6586218476295471]\n",
      "[Epoch 59/1001] [Batch 70/372] [D loss: 0.6899881362915039] [G loss: 0.6906749606132507]\n",
      "[Epoch 59/1001] [Batch 71/372] [D loss: 0.6903538703918457] [G loss: 0.6718209385871887]\n",
      "[Epoch 59/1001] [Batch 72/372] [D loss: 0.6909346580505371] [G loss: 0.6939769983291626]\n",
      "[Epoch 59/1001] [Batch 73/372] [D loss: 0.6883352994918823] [G loss: 0.6714240312576294]\n",
      "[Epoch 59/1001] [Batch 74/372] [D loss: 0.6862728595733643] [G loss: 0.6871786713600159]\n",
      "[Epoch 59/1001] [Batch 75/372] [D loss: 0.6867202520370483] [G loss: 0.6854835748672485]\n",
      "[Epoch 59/1001] [Batch 76/372] [D loss: 0.6921311616897583] [G loss: 0.6601911187171936]\n",
      "[Epoch 59/1001] [Batch 77/372] [D loss: 0.6964552402496338] [G loss: 0.6948607563972473]\n",
      "[Epoch 59/1001] [Batch 78/372] [D loss: 0.6861056089401245] [G loss: 0.6691163778305054]\n",
      "[Epoch 59/1001] [Batch 79/372] [D loss: 0.688552975654602] [G loss: 0.6992177367210388]\n",
      "[Epoch 59/1001] [Batch 80/372] [D loss: 0.6903741359710693] [G loss: 0.6588538289070129]\n",
      "[Epoch 59/1001] [Batch 81/372] [D loss: 0.6869958639144897] [G loss: 0.7073884010314941]\n",
      "[Epoch 59/1001] [Batch 82/372] [D loss: 0.6858899593353271] [G loss: 0.6489923000335693]\n",
      "[Epoch 59/1001] [Batch 83/372] [D loss: 0.6892663240432739] [G loss: 0.7102714776992798]\n",
      "[Epoch 59/1001] [Batch 84/372] [D loss: 0.6967116594314575] [G loss: 0.6430894732475281]\n",
      "[Epoch 59/1001] [Batch 85/372] [D loss: 0.6955536603927612] [G loss: 0.6975529193878174]\n",
      "[Epoch 59/1001] [Batch 86/372] [D loss: 0.6964238286018372] [G loss: 0.6435835361480713]\n",
      "[Epoch 59/1001] [Batch 87/372] [D loss: 0.6920375227928162] [G loss: 0.7322223782539368]\n",
      "[Epoch 59/1001] [Batch 88/372] [D loss: 0.6928784847259521] [G loss: 0.6122604608535767]\n",
      "[Epoch 59/1001] [Batch 89/372] [D loss: 0.6906594038009644] [G loss: 0.7718520760536194]\n",
      "[Epoch 59/1001] [Batch 90/372] [D loss: 0.6967140436172485] [G loss: 0.5756132006645203]\n",
      "[Epoch 59/1001] [Batch 91/372] [D loss: 0.6950008273124695] [G loss: 0.814600944519043]\n",
      "[Epoch 59/1001] [Batch 92/372] [D loss: 0.6980167031288147] [G loss: 0.5683454871177673]\n",
      "[Epoch 59/1001] [Batch 93/372] [D loss: 0.6980447769165039] [G loss: 0.7787526249885559]\n",
      "[Epoch 59/1001] [Batch 94/372] [D loss: 0.6927704811096191] [G loss: 0.6110392212867737]\n",
      "[Epoch 59/1001] [Batch 95/372] [D loss: 0.6945849657058716] [G loss: 0.7327365279197693]\n",
      "[Epoch 59/1001] [Batch 96/372] [D loss: 0.6940432190895081] [G loss: 0.638480544090271]\n",
      "[Epoch 59/1001] [Batch 97/372] [D loss: 0.6930503845214844] [G loss: 0.719108521938324]\n",
      "[Epoch 59/1001] [Batch 98/372] [D loss: 0.6940075159072876] [G loss: 0.6422086954116821]\n",
      "[Epoch 59/1001] [Batch 99/372] [D loss: 0.6893713474273682] [G loss: 0.7120416760444641]\n",
      "[Epoch 59/1001] [Batch 100/372] [D loss: 0.694724440574646] [G loss: 0.6685938835144043]\n",
      "[Epoch 59/1001] [Batch 101/372] [D loss: 0.6920275092124939] [G loss: 0.6808098554611206]\n",
      "[Epoch 59/1001] [Batch 102/372] [D loss: 0.6894631385803223] [G loss: 0.6755127906799316]\n",
      "[Epoch 59/1001] [Batch 103/372] [D loss: 0.6906452178955078] [G loss: 0.6865750551223755]\n",
      "[Epoch 59/1001] [Batch 104/372] [D loss: 0.6916077733039856] [G loss: 0.6744768023490906]\n",
      "[Epoch 59/1001] [Batch 105/372] [D loss: 0.6918569803237915] [G loss: 0.6854662895202637]\n",
      "[Epoch 59/1001] [Batch 106/372] [D loss: 0.6909035444259644] [G loss: 0.6559858918190002]\n",
      "[Epoch 59/1001] [Batch 107/372] [D loss: 0.6892416477203369] [G loss: 0.7134966254234314]\n",
      "[Epoch 59/1001] [Batch 108/372] [D loss: 0.6909440755844116] [G loss: 0.6486215591430664]\n",
      "[Epoch 59/1001] [Batch 109/372] [D loss: 0.6902977824211121] [G loss: 0.7309863567352295]\n",
      "[Epoch 59/1001] [Batch 110/372] [D loss: 0.6884249448776245] [G loss: 0.6258619427680969]\n",
      "[Epoch 59/1001] [Batch 111/372] [D loss: 0.6931727528572083] [G loss: 0.7419729232788086]\n",
      "[Epoch 59/1001] [Batch 112/372] [D loss: 0.689407229423523] [G loss: 0.6227190494537354]\n",
      "[Epoch 59/1001] [Batch 113/372] [D loss: 0.6947018504142761] [G loss: 0.7434512376785278]\n",
      "[Epoch 59/1001] [Batch 114/372] [D loss: 0.6931092739105225] [G loss: 0.6210407018661499]\n",
      "[Epoch 59/1001] [Batch 115/372] [D loss: 0.6921131610870361] [G loss: 0.7473393678665161]\n",
      "[Epoch 59/1001] [Batch 116/372] [D loss: 0.6909577250480652] [G loss: 0.598112940788269]\n",
      "[Epoch 59/1001] [Batch 117/372] [D loss: 0.6963038444519043] [G loss: 0.8088907599449158]\n",
      "[Epoch 59/1001] [Batch 118/372] [D loss: 0.6875650882720947] [G loss: 0.5668196082115173]\n",
      "[Epoch 59/1001] [Batch 119/372] [D loss: 0.6976474523544312] [G loss: 0.8123601675033569]\n",
      "[Epoch 59/1001] [Batch 120/372] [D loss: 0.6958118677139282] [G loss: 0.562493085861206]\n",
      "[Epoch 59/1001] [Batch 121/372] [D loss: 0.692173957824707] [G loss: 0.8049256205558777]\n",
      "[Epoch 59/1001] [Batch 122/372] [D loss: 0.6940196752548218] [G loss: 0.5715911388397217]\n",
      "[Epoch 59/1001] [Batch 123/372] [D loss: 0.7005928754806519] [G loss: 0.8193352222442627]\n",
      "[Epoch 59/1001] [Batch 124/372] [D loss: 0.6977765560150146] [G loss: 0.55070561170578]\n",
      "[Epoch 59/1001] [Batch 125/372] [D loss: 0.7024465799331665] [G loss: 0.8511108160018921]\n",
      "[Epoch 59/1001] [Batch 126/372] [D loss: 0.7028458118438721] [G loss: 0.5498796105384827]\n",
      "[Epoch 59/1001] [Batch 127/372] [D loss: 0.7009897232055664] [G loss: 0.7972193360328674]\n",
      "[Epoch 59/1001] [Batch 128/372] [D loss: 0.6947616934776306] [G loss: 0.6215178370475769]\n",
      "[Epoch 59/1001] [Batch 129/372] [D loss: 0.6908411979675293] [G loss: 0.7059047818183899]\n",
      "[Epoch 59/1001] [Batch 130/372] [D loss: 0.6908602118492126] [G loss: 0.6830303072929382]\n",
      "[Epoch 59/1001] [Batch 131/372] [D loss: 0.6937634944915771] [G loss: 0.6619440317153931]\n",
      "[Epoch 59/1001] [Batch 132/372] [D loss: 0.6862133145332336] [G loss: 0.702290415763855]\n",
      "[Epoch 59/1001] [Batch 133/372] [D loss: 0.6885274648666382] [G loss: 0.6513230800628662]\n",
      "[Epoch 59/1001] [Batch 134/372] [D loss: 0.6894795894622803] [G loss: 0.7179325819015503]\n",
      "[Epoch 59/1001] [Batch 135/372] [D loss: 0.6903742551803589] [G loss: 0.6391516923904419]\n",
      "[Epoch 59/1001] [Batch 136/372] [D loss: 0.6903908252716064] [G loss: 0.7011406421661377]\n",
      "[Epoch 59/1001] [Batch 137/372] [D loss: 0.6932183504104614] [G loss: 0.6742947101593018]\n",
      "[Epoch 59/1001] [Batch 138/372] [D loss: 0.6879802942276001] [G loss: 0.6735971570014954]\n",
      "[Epoch 59/1001] [Batch 139/372] [D loss: 0.6890546083450317] [G loss: 0.6932604312896729]\n",
      "[Epoch 59/1001] [Batch 140/372] [D loss: 0.6896264553070068] [G loss: 0.6741992831230164]\n",
      "[Epoch 59/1001] [Batch 141/372] [D loss: 0.6916422843933105] [G loss: 0.6913644075393677]\n",
      "[Epoch 59/1001] [Batch 142/372] [D loss: 0.6932314038276672] [G loss: 0.6483757495880127]\n",
      "[Epoch 59/1001] [Batch 143/372] [D loss: 0.6920815706253052] [G loss: 0.717755913734436]\n",
      "[Epoch 59/1001] [Batch 144/372] [D loss: 0.6914752125740051] [G loss: 0.6346433758735657]\n",
      "[Epoch 59/1001] [Batch 145/372] [D loss: 0.6939676403999329] [G loss: 0.7339112758636475]\n",
      "[Epoch 59/1001] [Batch 146/372] [D loss: 0.6922237873077393] [G loss: 0.6399717330932617]\n",
      "[Epoch 59/1001] [Batch 147/372] [D loss: 0.6898102760314941] [G loss: 0.7158127427101135]\n",
      "[Epoch 59/1001] [Batch 148/372] [D loss: 0.6909846663475037] [G loss: 0.6463518738746643]\n",
      "[Epoch 59/1001] [Batch 149/372] [D loss: 0.6914267539978027] [G loss: 0.7201835513114929]\n",
      "[Epoch 59/1001] [Batch 150/372] [D loss: 0.6862990856170654] [G loss: 0.6530213356018066]\n",
      "[Epoch 59/1001] [Batch 151/372] [D loss: 0.6907325983047485] [G loss: 0.6911839842796326]\n",
      "[Epoch 59/1001] [Batch 152/372] [D loss: 0.689570426940918] [G loss: 0.667529821395874]\n",
      "[Epoch 59/1001] [Batch 153/372] [D loss: 0.6892105340957642] [G loss: 0.692403256893158]\n",
      "[Epoch 59/1001] [Batch 154/372] [D loss: 0.6896315813064575] [G loss: 0.6683233976364136]\n",
      "[Epoch 59/1001] [Batch 155/372] [D loss: 0.6905673742294312] [G loss: 0.69196617603302]\n",
      "[Epoch 59/1001] [Batch 156/372] [D loss: 0.688065767288208] [G loss: 0.6768059134483337]\n",
      "[Epoch 59/1001] [Batch 157/372] [D loss: 0.6878969669342041] [G loss: 0.6821637153625488]\n",
      "[Epoch 59/1001] [Batch 158/372] [D loss: 0.6914787292480469] [G loss: 0.669544517993927]\n",
      "[Epoch 59/1001] [Batch 159/372] [D loss: 0.6883575320243835] [G loss: 0.7076777219772339]\n",
      "[Epoch 59/1001] [Batch 160/372] [D loss: 0.6963544487953186] [G loss: 0.6282768845558167]\n",
      "[Epoch 59/1001] [Batch 161/372] [D loss: 0.6930693984031677] [G loss: 0.7569249272346497]\n",
      "[Epoch 59/1001] [Batch 162/372] [D loss: 0.6922680139541626] [G loss: 0.5888268351554871]\n",
      "[Epoch 59/1001] [Batch 163/372] [D loss: 0.6965914368629456] [G loss: 0.8275473117828369]\n",
      "[Epoch 59/1001] [Batch 164/372] [D loss: 0.7008711695671082] [G loss: 0.5390436053276062]\n",
      "[Epoch 59/1001] [Batch 165/372] [D loss: 0.7014611959457397] [G loss: 0.8358216285705566]\n",
      "[Epoch 59/1001] [Batch 166/372] [D loss: 0.7020797729492188] [G loss: 0.5575985908508301]\n",
      "[Epoch 59/1001] [Batch 167/372] [D loss: 0.6988446712493896] [G loss: 0.8188627362251282]\n",
      "[Epoch 59/1001] [Batch 168/372] [D loss: 0.7004992365837097] [G loss: 0.5796325206756592]\n",
      "[Epoch 59/1001] [Batch 169/372] [D loss: 0.6963461637496948] [G loss: 0.7711359262466431]\n",
      "[Epoch 59/1001] [Batch 170/372] [D loss: 0.69645094871521] [G loss: 0.6232132315635681]\n",
      "[Epoch 59/1001] [Batch 171/372] [D loss: 0.6944905519485474] [G loss: 0.6984093189239502]\n",
      "[Epoch 59/1001] [Batch 172/372] [D loss: 0.6865687370300293] [G loss: 0.696742594242096]\n",
      "[Epoch 59/1001] [Batch 173/372] [D loss: 0.6894401907920837] [G loss: 0.6670006513595581]\n",
      "[Epoch 59/1001] [Batch 174/372] [D loss: 0.6925550699234009] [G loss: 0.6974850296974182]\n",
      "[Epoch 59/1001] [Batch 175/372] [D loss: 0.6931363344192505] [G loss: 0.6616256833076477]\n",
      "[Epoch 59/1001] [Batch 176/372] [D loss: 0.6887738704681396] [G loss: 0.6950326561927795]\n",
      "[Epoch 59/1001] [Batch 177/372] [D loss: 0.6864497065544128] [G loss: 0.6678056716918945]\n",
      "[Epoch 59/1001] [Batch 178/372] [D loss: 0.6906411647796631] [G loss: 0.6871033906936646]\n",
      "[Epoch 59/1001] [Batch 179/372] [D loss: 0.6870943307876587] [G loss: 0.6771975755691528]\n",
      "[Epoch 59/1001] [Batch 180/372] [D loss: 0.6919516324996948] [G loss: 0.6814674735069275]\n",
      "[Epoch 59/1001] [Batch 181/372] [D loss: 0.6873911023139954] [G loss: 0.6904968023300171]\n",
      "[Epoch 59/1001] [Batch 182/372] [D loss: 0.6906872391700745] [G loss: 0.6636096239089966]\n",
      "[Epoch 59/1001] [Batch 183/372] [D loss: 0.691166877746582] [G loss: 0.7039725184440613]\n",
      "[Epoch 59/1001] [Batch 184/372] [D loss: 0.6881847381591797] [G loss: 0.6483983993530273]\n",
      "[Epoch 59/1001] [Batch 185/372] [D loss: 0.685111939907074] [G loss: 0.7208085060119629]\n",
      "[Epoch 59/1001] [Batch 186/372] [D loss: 0.6888397932052612] [G loss: 0.6360982656478882]\n",
      "[Epoch 59/1001] [Batch 187/372] [D loss: 0.6895036101341248] [G loss: 0.7465221881866455]\n",
      "[Epoch 59/1001] [Batch 188/372] [D loss: 0.69393390417099] [G loss: 0.6006261110305786]\n",
      "[Epoch 59/1001] [Batch 189/372] [D loss: 0.6884375810623169] [G loss: 0.7751056551933289]\n",
      "[Epoch 59/1001] [Batch 190/372] [D loss: 0.690266489982605] [G loss: 0.5747186541557312]\n",
      "[Epoch 59/1001] [Batch 191/372] [D loss: 0.7012317180633545] [G loss: 0.8640637993812561]\n",
      "[Epoch 59/1001] [Batch 192/372] [D loss: 0.7078234553337097] [G loss: 0.502752959728241]\n",
      "[Epoch 59/1001] [Batch 193/372] [D loss: 0.7075409889221191] [G loss: 0.8816034197807312]\n",
      "[Epoch 59/1001] [Batch 194/372] [D loss: 0.7075468301773071] [G loss: 0.5600191354751587]\n",
      "[Epoch 59/1001] [Batch 195/372] [D loss: 0.7016259431838989] [G loss: 0.7574304342269897]\n",
      "[Epoch 59/1001] [Batch 196/372] [D loss: 0.6935679912567139] [G loss: 0.6378983855247498]\n",
      "[Epoch 59/1001] [Batch 197/372] [D loss: 0.6885857582092285] [G loss: 0.6983212232589722]\n",
      "[Epoch 59/1001] [Batch 198/372] [D loss: 0.6942527294158936] [G loss: 0.6882069110870361]\n",
      "[Epoch 59/1001] [Batch 199/372] [D loss: 0.6879289150238037] [G loss: 0.6580556631088257]\n",
      "[Epoch 59/1001] [Batch 200/372] [D loss: 0.6911945939064026] [G loss: 0.714215874671936]\n",
      "[Epoch 59/1001] [Batch 201/372] [D loss: 0.6922807097434998] [G loss: 0.6562395095825195]\n",
      "[Epoch 59/1001] [Batch 202/372] [D loss: 0.6931668519973755] [G loss: 0.6950392127037048]\n",
      "[Epoch 59/1001] [Batch 203/372] [D loss: 0.6893998384475708] [G loss: 0.6712724566459656]\n",
      "[Epoch 59/1001] [Batch 204/372] [D loss: 0.6886012554168701] [G loss: 0.686007022857666]\n",
      "[Epoch 59/1001] [Batch 205/372] [D loss: 0.6919252276420593] [G loss: 0.6766226887702942]\n",
      "[Epoch 59/1001] [Batch 206/372] [D loss: 0.6903332471847534] [G loss: 0.6726193428039551]\n",
      "[Epoch 59/1001] [Batch 207/372] [D loss: 0.6889386177062988] [G loss: 0.6941913366317749]\n",
      "[Epoch 59/1001] [Batch 208/372] [D loss: 0.6886999607086182] [G loss: 0.6587137579917908]\n",
      "[Epoch 59/1001] [Batch 209/372] [D loss: 0.6873806118965149] [G loss: 0.7025920748710632]\n",
      "[Epoch 59/1001] [Batch 210/372] [D loss: 0.69557785987854] [G loss: 0.664777398109436]\n",
      "[Epoch 59/1001] [Batch 211/372] [D loss: 0.6893645524978638] [G loss: 0.6903336048126221]\n",
      "[Epoch 59/1001] [Batch 212/372] [D loss: 0.6918137669563293] [G loss: 0.6601337194442749]\n",
      "[Epoch 59/1001] [Batch 213/372] [D loss: 0.6867159008979797] [G loss: 0.7034399509429932]\n",
      "[Epoch 59/1001] [Batch 214/372] [D loss: 0.6913927793502808] [G loss: 0.627982497215271]\n",
      "[Epoch 59/1001] [Batch 215/372] [D loss: 0.6920531988143921] [G loss: 0.7598970532417297]\n",
      "[Epoch 59/1001] [Batch 216/372] [D loss: 0.6973327994346619] [G loss: 0.5775925517082214]\n",
      "[Epoch 59/1001] [Batch 217/372] [D loss: 0.6972472071647644] [G loss: 0.814652681350708]\n",
      "[Epoch 59/1001] [Batch 218/372] [D loss: 0.6943941116333008] [G loss: 0.5275543332099915]\n",
      "[Epoch 59/1001] [Batch 219/372] [D loss: 0.7016569375991821] [G loss: 0.8837233185768127]\n",
      "[Epoch 59/1001] [Batch 220/372] [D loss: 0.7033778429031372] [G loss: 0.5316466689109802]\n",
      "[Epoch 59/1001] [Batch 221/372] [D loss: 0.7061969041824341] [G loss: 0.8315333127975464]\n",
      "[Epoch 59/1001] [Batch 222/372] [D loss: 0.6976971626281738] [G loss: 0.5836966037750244]\n",
      "[Epoch 59/1001] [Batch 223/372] [D loss: 0.6996231079101562] [G loss: 0.7335667014122009]\n",
      "[Epoch 59/1001] [Batch 224/372] [D loss: 0.6886028051376343] [G loss: 0.6717219352722168]\n",
      "[Epoch 59/1001] [Batch 225/372] [D loss: 0.69287109375] [G loss: 0.6653909683227539]\n",
      "[Epoch 59/1001] [Batch 226/372] [D loss: 0.6859315633773804] [G loss: 0.6940993070602417]\n",
      "[Epoch 59/1001] [Batch 227/372] [D loss: 0.6884942650794983] [G loss: 0.6582337617874146]\n",
      "[Epoch 59/1001] [Batch 228/372] [D loss: 0.6897499561309814] [G loss: 0.7061538696289062]\n",
      "[Epoch 59/1001] [Batch 229/372] [D loss: 0.6889170408248901] [G loss: 0.670072615146637]\n",
      "[Epoch 59/1001] [Batch 230/372] [D loss: 0.6896371841430664] [G loss: 0.6762466430664062]\n",
      "[Epoch 59/1001] [Batch 231/372] [D loss: 0.6903363466262817] [G loss: 0.6889703273773193]\n",
      "[Epoch 59/1001] [Batch 232/372] [D loss: 0.6925338506698608] [G loss: 0.6700943112373352]\n",
      "[Epoch 59/1001] [Batch 233/372] [D loss: 0.6894375085830688] [G loss: 0.6925246119499207]\n",
      "[Epoch 59/1001] [Batch 234/372] [D loss: 0.691975474357605] [G loss: 0.6589243412017822]\n",
      "[Epoch 59/1001] [Batch 235/372] [D loss: 0.6922897696495056] [G loss: 0.7028684616088867]\n",
      "[Epoch 59/1001] [Batch 236/372] [D loss: 0.6925158500671387] [G loss: 0.6575837135314941]\n",
      "[Epoch 59/1001] [Batch 237/372] [D loss: 0.6894991397857666] [G loss: 0.6944169998168945]\n",
      "[Epoch 59/1001] [Batch 238/372] [D loss: 0.6874808073043823] [G loss: 0.6707146167755127]\n",
      "[Epoch 59/1001] [Batch 239/372] [D loss: 0.6897073984146118] [G loss: 0.6783841252326965]\n",
      "[Epoch 59/1001] [Batch 240/372] [D loss: 0.6888664960861206] [G loss: 0.6946569085121155]\n",
      "[Epoch 59/1001] [Batch 241/372] [D loss: 0.6945692896842957] [G loss: 0.6849958896636963]\n",
      "[Epoch 59/1001] [Batch 242/372] [D loss: 0.6884881854057312] [G loss: 0.6737743616104126]\n",
      "[Epoch 59/1001] [Batch 243/372] [D loss: 0.6906371712684631] [G loss: 0.6794828176498413]\n",
      "[Epoch 59/1001] [Batch 244/372] [D loss: 0.6875356435775757] [G loss: 0.6821913719177246]\n",
      "[Epoch 59/1001] [Batch 245/372] [D loss: 0.6926217079162598] [G loss: 0.6740875244140625]\n",
      "[Epoch 59/1001] [Batch 246/372] [D loss: 0.6880807876586914] [G loss: 0.6811388731002808]\n",
      "[Epoch 59/1001] [Batch 247/372] [D loss: 0.689705491065979] [G loss: 0.6889234781265259]\n",
      "[Epoch 59/1001] [Batch 248/372] [D loss: 0.689129114151001] [G loss: 0.6623194813728333]\n",
      "[Epoch 59/1001] [Batch 249/372] [D loss: 0.6938086748123169] [G loss: 0.7039676904678345]\n",
      "[Epoch 59/1001] [Batch 250/372] [D loss: 0.692841649055481] [G loss: 0.6367497444152832]\n",
      "[Epoch 59/1001] [Batch 251/372] [D loss: 0.6942687630653381] [G loss: 0.7198681831359863]\n",
      "[Epoch 59/1001] [Batch 252/372] [D loss: 0.6919798851013184] [G loss: 0.6521548628807068]\n",
      "[Epoch 59/1001] [Batch 253/372] [D loss: 0.6948719620704651] [G loss: 0.7060257196426392]\n",
      "[Epoch 59/1001] [Batch 254/372] [D loss: 0.6921133995056152] [G loss: 0.6431609392166138]\n",
      "[Epoch 59/1001] [Batch 255/372] [D loss: 0.6936078667640686] [G loss: 0.7246623635292053]\n",
      "[Epoch 59/1001] [Batch 256/372] [D loss: 0.6920373439788818] [G loss: 0.6262259483337402]\n",
      "[Epoch 59/1001] [Batch 257/372] [D loss: 0.6904236674308777] [G loss: 0.7343152761459351]\n",
      "[Epoch 59/1001] [Batch 258/372] [D loss: 0.6922597885131836] [G loss: 0.6339665651321411]\n",
      "[Epoch 59/1001] [Batch 259/372] [D loss: 0.6868149042129517] [G loss: 0.7289872169494629]\n",
      "[Epoch 59/1001] [Batch 260/372] [D loss: 0.6867884397506714] [G loss: 0.646407961845398]\n",
      "[Epoch 59/1001] [Batch 261/372] [D loss: 0.6902064085006714] [G loss: 0.7332867383956909]\n",
      "[Epoch 59/1001] [Batch 262/372] [D loss: 0.6980041265487671] [G loss: 0.6156800389289856]\n",
      "[Epoch 59/1001] [Batch 263/372] [D loss: 0.6903780698776245] [G loss: 0.7281768321990967]\n",
      "[Epoch 59/1001] [Batch 264/372] [D loss: 0.6963272094726562] [G loss: 0.6524664163589478]\n",
      "[Epoch 59/1001] [Batch 265/372] [D loss: 0.6930370330810547] [G loss: 0.6916363835334778]\n",
      "[Epoch 59/1001] [Batch 266/372] [D loss: 0.6910781860351562] [G loss: 0.656546950340271]\n",
      "[Epoch 59/1001] [Batch 267/372] [D loss: 0.690943717956543] [G loss: 0.7232602834701538]\n",
      "[Epoch 59/1001] [Batch 268/372] [D loss: 0.6951563358306885] [G loss: 0.6255515217781067]\n",
      "[Epoch 59/1001] [Batch 269/372] [D loss: 0.6900902986526489] [G loss: 0.7487679719924927]\n",
      "[Epoch 59/1001] [Batch 270/372] [D loss: 0.691265881061554] [G loss: 0.6082720160484314]\n",
      "[Epoch 59/1001] [Batch 271/372] [D loss: 0.6924015283584595] [G loss: 0.7507325410842896]\n",
      "[Epoch 59/1001] [Batch 272/372] [D loss: 0.688169002532959] [G loss: 0.6178110837936401]\n",
      "[Epoch 59/1001] [Batch 273/372] [D loss: 0.695363461971283] [G loss: 0.7646975517272949]\n",
      "[Epoch 59/1001] [Batch 274/372] [D loss: 0.6938282251358032] [G loss: 0.5697718858718872]\n",
      "[Epoch 59/1001] [Batch 275/372] [D loss: 0.6999818682670593] [G loss: 0.8234136700630188]\n",
      "[Epoch 59/1001] [Batch 276/372] [D loss: 0.7052052021026611] [G loss: 0.5345061421394348]\n",
      "[Epoch 59/1001] [Batch 277/372] [D loss: 0.7024544477462769] [G loss: 0.8523287773132324]\n",
      "[Epoch 59/1001] [Batch 278/372] [D loss: 0.6999405026435852] [G loss: 0.5596113204956055]\n",
      "[Epoch 59/1001] [Batch 279/372] [D loss: 0.6980896592140198] [G loss: 0.7785016894340515]\n",
      "[Epoch 59/1001] [Batch 280/372] [D loss: 0.6905435919761658] [G loss: 0.6115237474441528]\n",
      "[Epoch 59/1001] [Batch 281/372] [D loss: 0.6880900859832764] [G loss: 0.7223641872406006]\n",
      "[Epoch 59/1001] [Batch 282/372] [D loss: 0.6902928948402405] [G loss: 0.6708484888076782]\n",
      "[Epoch 59/1001] [Batch 283/372] [D loss: 0.6857136487960815] [G loss: 0.6846352219581604]\n",
      "[Epoch 59/1001] [Batch 284/372] [D loss: 0.6867947578430176] [G loss: 0.6766872406005859]\n",
      "[Epoch 59/1001] [Batch 285/372] [D loss: 0.6909236311912537] [G loss: 0.6779793500900269]\n",
      "[Epoch 59/1001] [Batch 286/372] [D loss: 0.6865025162696838] [G loss: 0.7006425261497498]\n",
      "[Epoch 59/1001] [Batch 287/372] [D loss: 0.6953058242797852] [G loss: 0.6423648595809937]\n",
      "[Epoch 59/1001] [Batch 288/372] [D loss: 0.6934269070625305] [G loss: 0.718744695186615]\n",
      "[Epoch 59/1001] [Batch 289/372] [D loss: 0.6940411329269409] [G loss: 0.6403695344924927]\n",
      "[Epoch 59/1001] [Batch 290/372] [D loss: 0.6928026080131531] [G loss: 0.7106819152832031]\n",
      "[Epoch 59/1001] [Batch 291/372] [D loss: 0.6936497092247009] [G loss: 0.6674307584762573]\n",
      "[Epoch 59/1001] [Batch 292/372] [D loss: 0.6920058727264404] [G loss: 0.6883364915847778]\n",
      "[Epoch 59/1001] [Batch 293/372] [D loss: 0.6948952674865723] [G loss: 0.6703343391418457]\n",
      "[Epoch 59/1001] [Batch 294/372] [D loss: 0.691450834274292] [G loss: 0.682135283946991]\n",
      "[Epoch 59/1001] [Batch 295/372] [D loss: 0.6929402947425842] [G loss: 0.681610643863678]\n",
      "[Epoch 59/1001] [Batch 296/372] [D loss: 0.6922307014465332] [G loss: 0.67960125207901]\n",
      "[Epoch 59/1001] [Batch 297/372] [D loss: 0.6918166279792786] [G loss: 0.6766396760940552]\n",
      "[Epoch 59/1001] [Batch 298/372] [D loss: 0.6873718500137329] [G loss: 0.6920384168624878]\n",
      "[Epoch 59/1001] [Batch 299/372] [D loss: 0.6895512938499451] [G loss: 0.6772551536560059]\n",
      "[Epoch 59/1001] [Batch 300/372] [D loss: 0.6913590431213379] [G loss: 0.6721081733703613]\n",
      "[Epoch 59/1001] [Batch 301/372] [D loss: 0.6928691864013672] [G loss: 0.6940976977348328]\n",
      "[Epoch 59/1001] [Batch 302/372] [D loss: 0.693727433681488] [G loss: 0.663128137588501]\n",
      "[Epoch 59/1001] [Batch 303/372] [D loss: 0.6875334978103638] [G loss: 0.6884422302246094]\n",
      "[Epoch 59/1001] [Batch 304/372] [D loss: 0.6877997517585754] [G loss: 0.673200249671936]\n",
      "[Epoch 59/1001] [Batch 305/372] [D loss: 0.6920986175537109] [G loss: 0.67266446352005]\n",
      "[Epoch 59/1001] [Batch 306/372] [D loss: 0.6912858486175537] [G loss: 0.6932967901229858]\n",
      "[Epoch 59/1001] [Batch 307/372] [D loss: 0.6854292750358582] [G loss: 0.6604443788528442]\n",
      "[Epoch 59/1001] [Batch 308/372] [D loss: 0.6896429657936096] [G loss: 0.6983620524406433]\n",
      "[Epoch 59/1001] [Batch 309/372] [D loss: 0.6930072903633118] [G loss: 0.6443040370941162]\n",
      "[Epoch 59/1001] [Batch 310/372] [D loss: 0.6889783143997192] [G loss: 0.7257646322250366]\n",
      "[Epoch 59/1001] [Batch 311/372] [D loss: 0.691930890083313] [G loss: 0.6246137022972107]\n",
      "[Epoch 59/1001] [Batch 312/372] [D loss: 0.6936826705932617] [G loss: 0.776716947555542]\n",
      "[Epoch 59/1001] [Batch 313/372] [D loss: 0.6956495046615601] [G loss: 0.5618711113929749]\n",
      "[Epoch 59/1001] [Batch 314/372] [D loss: 0.6989852786064148] [G loss: 0.8392641544342041]\n",
      "[Epoch 59/1001] [Batch 315/372] [D loss: 0.7005990743637085] [G loss: 0.5326632261276245]\n",
      "[Epoch 59/1001] [Batch 316/372] [D loss: 0.7004740238189697] [G loss: 0.8846770524978638]\n",
      "[Epoch 59/1001] [Batch 317/372] [D loss: 0.7067878246307373] [G loss: 0.5171626210212708]\n",
      "[Epoch 59/1001] [Batch 318/372] [D loss: 0.7045217752456665] [G loss: 0.8587239384651184]\n",
      "[Epoch 59/1001] [Batch 319/372] [D loss: 0.7068678140640259] [G loss: 0.5675562620162964]\n",
      "[Epoch 59/1001] [Batch 320/372] [D loss: 0.6974120140075684] [G loss: 0.7582038044929504]\n",
      "[Epoch 59/1001] [Batch 321/372] [D loss: 0.6908640265464783] [G loss: 0.6419118046760559]\n",
      "[Epoch 59/1001] [Batch 322/372] [D loss: 0.6934702396392822] [G loss: 0.6871064901351929]\n",
      "[Epoch 59/1001] [Batch 323/372] [D loss: 0.6887699365615845] [G loss: 0.6948500871658325]\n",
      "[Epoch 59/1001] [Batch 324/372] [D loss: 0.6902447938919067] [G loss: 0.6619489192962646]\n",
      "[Epoch 59/1001] [Batch 325/372] [D loss: 0.6931804418563843] [G loss: 0.6912897825241089]\n",
      "[Epoch 59/1001] [Batch 326/372] [D loss: 0.6949068307876587] [G loss: 0.6723468899726868]\n",
      "[Epoch 59/1001] [Batch 327/372] [D loss: 0.6916680335998535] [G loss: 0.6860764622688293]\n",
      "[Epoch 59/1001] [Batch 328/372] [D loss: 0.6869964599609375] [G loss: 0.6888573169708252]\n",
      "[Epoch 59/1001] [Batch 329/372] [D loss: 0.6929071545600891] [G loss: 0.6588811278343201]\n",
      "[Epoch 59/1001] [Batch 330/372] [D loss: 0.6922783851623535] [G loss: 0.7139073014259338]\n",
      "[Epoch 59/1001] [Batch 331/372] [D loss: 0.6920657157897949] [G loss: 0.6248796582221985]\n",
      "[Epoch 59/1001] [Batch 332/372] [D loss: 0.688549816608429] [G loss: 0.7600581645965576]\n",
      "[Epoch 59/1001] [Batch 333/372] [D loss: 0.6939395070075989] [G loss: 0.6192017793655396]\n",
      "[Epoch 59/1001] [Batch 334/372] [D loss: 0.6932125091552734] [G loss: 0.7419378161430359]\n",
      "[Epoch 59/1001] [Batch 335/372] [D loss: 0.693453311920166] [G loss: 0.6295926570892334]\n",
      "[Epoch 59/1001] [Batch 336/372] [D loss: 0.6923978924751282] [G loss: 0.7196177244186401]\n",
      "[Epoch 59/1001] [Batch 337/372] [D loss: 0.6866620779037476] [G loss: 0.6448006629943848]\n",
      "[Epoch 59/1001] [Batch 338/372] [D loss: 0.6973329782485962] [G loss: 0.7437031865119934]\n",
      "[Epoch 59/1001] [Batch 339/372] [D loss: 0.6936557292938232] [G loss: 0.6205267906188965]\n",
      "[Epoch 59/1001] [Batch 340/372] [D loss: 0.6929439306259155] [G loss: 0.7160606384277344]\n",
      "[Epoch 59/1001] [Batch 341/372] [D loss: 0.6897585391998291] [G loss: 0.6733128428459167]\n",
      "[Epoch 59/1001] [Batch 342/372] [D loss: 0.6895275115966797] [G loss: 0.6824451088905334]\n",
      "[Epoch 59/1001] [Batch 343/372] [D loss: 0.6919736862182617] [G loss: 0.6845123171806335]\n",
      "[Epoch 59/1001] [Batch 344/372] [D loss: 0.6909124255180359] [G loss: 0.6684656143188477]\n",
      "[Epoch 59/1001] [Batch 345/372] [D loss: 0.6910475492477417] [G loss: 0.6927598118782043]\n",
      "[Epoch 59/1001] [Batch 346/372] [D loss: 0.684951901435852] [G loss: 0.6793781518936157]\n",
      "[Epoch 59/1001] [Batch 347/372] [D loss: 0.6862767934799194] [G loss: 0.6615055799484253]\n",
      "[Epoch 59/1001] [Batch 348/372] [D loss: 0.6895663738250732] [G loss: 0.7207847237586975]\n",
      "[Epoch 59/1001] [Batch 349/372] [D loss: 0.6889891624450684] [G loss: 0.6285131573677063]\n",
      "[Epoch 59/1001] [Batch 350/372] [D loss: 0.6904056072235107] [G loss: 0.7592359185218811]\n",
      "[Epoch 59/1001] [Batch 351/372] [D loss: 0.6904284954071045] [G loss: 0.5740518569946289]\n",
      "[Epoch 59/1001] [Batch 352/372] [D loss: 0.7003225088119507] [G loss: 0.8637852668762207]\n",
      "[Epoch 59/1001] [Batch 353/372] [D loss: 0.7024519443511963] [G loss: 0.5166743993759155]\n",
      "[Epoch 59/1001] [Batch 354/372] [D loss: 0.7084850072860718] [G loss: 0.8613731265068054]\n",
      "[Epoch 59/1001] [Batch 355/372] [D loss: 0.7026277184486389] [G loss: 0.564826250076294]\n",
      "[Epoch 59/1001] [Batch 356/372] [D loss: 0.7027577757835388] [G loss: 0.7769471406936646]\n",
      "[Epoch 59/1001] [Batch 357/372] [D loss: 0.694521427154541] [G loss: 0.6343241333961487]\n",
      "[Epoch 59/1001] [Batch 358/372] [D loss: 0.6948071718215942] [G loss: 0.6998624205589294]\n",
      "[Epoch 59/1001] [Batch 359/372] [D loss: 0.6915351748466492] [G loss: 0.6850065588951111]\n",
      "[Epoch 59/1001] [Batch 360/372] [D loss: 0.6900526285171509] [G loss: 0.6633017659187317]\n",
      "[Epoch 59/1001] [Batch 361/372] [D loss: 0.691596269607544] [G loss: 0.7047596573829651]\n",
      "[Epoch 59/1001] [Batch 362/372] [D loss: 0.6907126307487488] [G loss: 0.6590723395347595]\n",
      "[Epoch 59/1001] [Batch 363/372] [D loss: 0.687110424041748] [G loss: 0.7017605304718018]\n",
      "[Epoch 59/1001] [Batch 364/372] [D loss: 0.6930500864982605] [G loss: 0.666507363319397]\n",
      "[Epoch 59/1001] [Batch 365/372] [D loss: 0.6884262561798096] [G loss: 0.701992928981781]\n",
      "[Epoch 59/1001] [Batch 366/372] [D loss: 0.690453052520752] [G loss: 0.673884928226471]\n",
      "[Epoch 59/1001] [Batch 367/372] [D loss: 0.6947132349014282] [G loss: 0.6653584241867065]\n",
      "[Epoch 59/1001] [Batch 368/372] [D loss: 0.6895731687545776] [G loss: 0.7086623907089233]\n",
      "[Epoch 59/1001] [Batch 369/372] [D loss: 0.6914106607437134] [G loss: 0.6626963019371033]\n",
      "[Epoch 59/1001] [Batch 370/372] [D loss: 0.6931277513504028] [G loss: 0.684441864490509]\n",
      "[Epoch 59/1001] [Batch 371/372] [D loss: 0.6908290386199951] [G loss: 0.6846145391464233]\n",
      "[Epoch 60/1001] [Batch 0/372] [D loss: 0.6918206810951233] [G loss: 0.676927924156189]\n",
      "[Epoch 60/1001] [Batch 1/372] [D loss: 0.6911957263946533] [G loss: 0.6870642900466919]\n",
      "[Epoch 60/1001] [Batch 2/372] [D loss: 0.6901583075523376] [G loss: 0.6784443855285645]\n",
      "[Epoch 60/1001] [Batch 3/372] [D loss: 0.6885401606559753] [G loss: 0.6696979999542236]\n",
      "[Epoch 60/1001] [Batch 4/372] [D loss: 0.6902599334716797] [G loss: 0.7065392136573792]\n",
      "[Epoch 60/1001] [Batch 5/372] [D loss: 0.6909852027893066] [G loss: 0.6473959684371948]\n",
      "[Epoch 60/1001] [Batch 6/372] [D loss: 0.6912922859191895] [G loss: 0.7121175527572632]\n",
      "[Epoch 60/1001] [Batch 7/372] [D loss: 0.6876378059387207] [G loss: 0.6465811729431152]\n",
      "[Epoch 60/1001] [Batch 8/372] [D loss: 0.691227912902832] [G loss: 0.753356397151947]\n",
      "[Epoch 60/1001] [Batch 9/372] [D loss: 0.6918917894363403] [G loss: 0.5940061807632446]\n",
      "[Epoch 60/1001] [Batch 10/372] [D loss: 0.6943433284759521] [G loss: 0.7775065302848816]\n",
      "[Epoch 60/1001] [Batch 11/372] [D loss: 0.6917717456817627] [G loss: 0.6003241539001465]\n",
      "[Epoch 60/1001] [Batch 12/372] [D loss: 0.698061466217041] [G loss: 0.7543960809707642]\n",
      "[Epoch 60/1001] [Batch 13/372] [D loss: 0.6922891736030579] [G loss: 0.616887092590332]\n",
      "[Epoch 60/1001] [Batch 14/372] [D loss: 0.6902767419815063] [G loss: 0.7383973598480225]\n",
      "[Epoch 60/1001] [Batch 15/372] [D loss: 0.6915090084075928] [G loss: 0.6396227478981018]\n",
      "[Epoch 60/1001] [Batch 16/372] [D loss: 0.6923386454582214] [G loss: 0.7100164890289307]\n",
      "[Epoch 60/1001] [Batch 17/372] [D loss: 0.6944876909255981] [G loss: 0.6665287613868713]\n",
      "[Epoch 60/1001] [Batch 18/372] [D loss: 0.6879360675811768] [G loss: 0.6899818778038025]\n",
      "[Epoch 60/1001] [Batch 19/372] [D loss: 0.690446138381958] [G loss: 0.672340989112854]\n",
      "[Epoch 60/1001] [Batch 20/372] [D loss: 0.6904768347740173] [G loss: 0.6924383640289307]\n",
      "[Epoch 60/1001] [Batch 21/372] [D loss: 0.6896634101867676] [G loss: 0.6661422848701477]\n",
      "[Epoch 60/1001] [Batch 22/372] [D loss: 0.6902205944061279] [G loss: 0.691551685333252]\n",
      "[Epoch 60/1001] [Batch 23/372] [D loss: 0.6870840787887573] [G loss: 0.6592043042182922]\n",
      "[Epoch 60/1001] [Batch 24/372] [D loss: 0.6899583339691162] [G loss: 0.7230017185211182]\n",
      "[Epoch 60/1001] [Batch 25/372] [D loss: 0.6912171840667725] [G loss: 0.6243447065353394]\n",
      "[Epoch 60/1001] [Batch 26/372] [D loss: 0.6926972270011902] [G loss: 0.7586400508880615]\n",
      "[Epoch 60/1001] [Batch 27/372] [D loss: 0.6933009624481201] [G loss: 0.6108637452125549]\n",
      "[Epoch 60/1001] [Batch 28/372] [D loss: 0.6894791126251221] [G loss: 0.7499345541000366]\n",
      "[Epoch 60/1001] [Batch 29/372] [D loss: 0.6870771646499634] [G loss: 0.6170034408569336]\n",
      "[Epoch 60/1001] [Batch 30/372] [D loss: 0.6909589767456055] [G loss: 0.7471083998680115]\n",
      "[Epoch 60/1001] [Batch 31/372] [D loss: 0.689900279045105] [G loss: 0.6273711323738098]\n",
      "[Epoch 60/1001] [Batch 32/372] [D loss: 0.6882938146591187] [G loss: 0.7378955483436584]\n",
      "[Epoch 60/1001] [Batch 33/372] [D loss: 0.6921794414520264] [G loss: 0.6209858655929565]\n",
      "[Epoch 60/1001] [Batch 34/372] [D loss: 0.6914675235748291] [G loss: 0.7752367854118347]\n",
      "[Epoch 60/1001] [Batch 35/372] [D loss: 0.6961263418197632] [G loss: 0.5676265954971313]\n",
      "[Epoch 60/1001] [Batch 36/372] [D loss: 0.7012425661087036] [G loss: 0.7937609553337097]\n",
      "[Epoch 60/1001] [Batch 37/372] [D loss: 0.6916346549987793] [G loss: 0.5884631872177124]\n",
      "[Epoch 60/1001] [Batch 38/372] [D loss: 0.6991329789161682] [G loss: 0.7748988270759583]\n",
      "[Epoch 60/1001] [Batch 39/372] [D loss: 0.6972544193267822] [G loss: 0.578563928604126]\n",
      "[Epoch 60/1001] [Batch 40/372] [D loss: 0.6968145370483398] [G loss: 0.8198822140693665]\n",
      "[Epoch 60/1001] [Batch 41/372] [D loss: 0.6971118450164795] [G loss: 0.5578703284263611]\n",
      "[Epoch 60/1001] [Batch 42/372] [D loss: 0.6985167264938354] [G loss: 0.8033449649810791]\n",
      "[Epoch 60/1001] [Batch 43/372] [D loss: 0.7007214426994324] [G loss: 0.6004650592803955]\n",
      "[Epoch 60/1001] [Batch 44/372] [D loss: 0.6905306577682495] [G loss: 0.7352777123451233]\n",
      "[Epoch 60/1001] [Batch 45/372] [D loss: 0.6916371583938599] [G loss: 0.6533752679824829]\n",
      "[Epoch 60/1001] [Batch 46/372] [D loss: 0.6897619962692261] [G loss: 0.6936653852462769]\n",
      "[Epoch 60/1001] [Batch 47/372] [D loss: 0.6864204406738281] [G loss: 0.6941556930541992]\n",
      "[Epoch 60/1001] [Batch 48/372] [D loss: 0.6911296844482422] [G loss: 0.6719717979431152]\n",
      "[Epoch 60/1001] [Batch 49/372] [D loss: 0.6883924603462219] [G loss: 0.6804547905921936]\n",
      "[Epoch 60/1001] [Batch 50/372] [D loss: 0.6911063194274902] [G loss: 0.6836400628089905]\n",
      "[Epoch 60/1001] [Batch 51/372] [D loss: 0.6870505809783936] [G loss: 0.68214350938797]\n",
      "[Epoch 60/1001] [Batch 52/372] [D loss: 0.6913471817970276] [G loss: 0.6874226331710815]\n",
      "[Epoch 60/1001] [Batch 53/372] [D loss: 0.6882226467132568] [G loss: 0.6742047071456909]\n",
      "[Epoch 60/1001] [Batch 54/372] [D loss: 0.6898448467254639] [G loss: 0.6889509558677673]\n",
      "[Epoch 60/1001] [Batch 55/372] [D loss: 0.6917510032653809] [G loss: 0.6684218645095825]\n",
      "[Epoch 60/1001] [Batch 56/372] [D loss: 0.6894220113754272] [G loss: 0.6940646767616272]\n",
      "[Epoch 60/1001] [Batch 57/372] [D loss: 0.6877048015594482] [G loss: 0.6644681096076965]\n",
      "[Epoch 60/1001] [Batch 58/372] [D loss: 0.6871644258499146] [G loss: 0.696905255317688]\n",
      "[Epoch 60/1001] [Batch 59/372] [D loss: 0.6941579580307007] [G loss: 0.6583202481269836]\n",
      "[Epoch 60/1001] [Batch 60/372] [D loss: 0.6910967826843262] [G loss: 0.7009318470954895]\n",
      "[Epoch 60/1001] [Batch 61/372] [D loss: 0.6938596963882446] [G loss: 0.6509668827056885]\n",
      "[Epoch 60/1001] [Batch 62/372] [D loss: 0.6931525468826294] [G loss: 0.7165908217430115]\n",
      "[Epoch 60/1001] [Batch 63/372] [D loss: 0.6902453899383545] [G loss: 0.6377293467521667]\n",
      "[Epoch 60/1001] [Batch 64/372] [D loss: 0.6924508810043335] [G loss: 0.751514732837677]\n",
      "[Epoch 60/1001] [Batch 65/372] [D loss: 0.699100136756897] [G loss: 0.5967246890068054]\n",
      "[Epoch 60/1001] [Batch 66/372] [D loss: 0.6921869516372681] [G loss: 0.7855788469314575]\n",
      "[Epoch 60/1001] [Batch 67/372] [D loss: 0.6962001323699951] [G loss: 0.5817753076553345]\n",
      "[Epoch 60/1001] [Batch 68/372] [D loss: 0.6970145106315613] [G loss: 0.7809717655181885]\n",
      "[Epoch 60/1001] [Batch 69/372] [D loss: 0.6903241872787476] [G loss: 0.6143083572387695]\n",
      "[Epoch 60/1001] [Batch 70/372] [D loss: 0.6937339901924133] [G loss: 0.7238715887069702]\n",
      "[Epoch 60/1001] [Batch 71/372] [D loss: 0.6881005167961121] [G loss: 0.6597800850868225]\n",
      "[Epoch 60/1001] [Batch 72/372] [D loss: 0.690360426902771] [G loss: 0.6929171085357666]\n",
      "[Epoch 60/1001] [Batch 73/372] [D loss: 0.6913853883743286] [G loss: 0.6718764901161194]\n",
      "[Epoch 60/1001] [Batch 74/372] [D loss: 0.6899840235710144] [G loss: 0.6791646480560303]\n",
      "[Epoch 60/1001] [Batch 75/372] [D loss: 0.689009428024292] [G loss: 0.6802794933319092]\n",
      "[Epoch 60/1001] [Batch 76/372] [D loss: 0.690911054611206] [G loss: 0.6979814767837524]\n",
      "[Epoch 60/1001] [Batch 77/372] [D loss: 0.6885417699813843] [G loss: 0.6656697988510132]\n",
      "[Epoch 60/1001] [Batch 78/372] [D loss: 0.687499463558197] [G loss: 0.7008498311042786]\n",
      "[Epoch 60/1001] [Batch 79/372] [D loss: 0.6883299946784973] [G loss: 0.6606826782226562]\n",
      "[Epoch 60/1001] [Batch 80/372] [D loss: 0.686820924282074] [G loss: 0.7150304317474365]\n",
      "[Epoch 60/1001] [Batch 81/372] [D loss: 0.691171407699585] [G loss: 0.6345669627189636]\n",
      "[Epoch 60/1001] [Batch 82/372] [D loss: 0.6878520250320435] [G loss: 0.7384697198867798]\n",
      "[Epoch 60/1001] [Batch 83/372] [D loss: 0.6915722489356995] [G loss: 0.6184200048446655]\n",
      "[Epoch 60/1001] [Batch 84/372] [D loss: 0.6957255005836487] [G loss: 0.7690759301185608]\n",
      "[Epoch 60/1001] [Batch 85/372] [D loss: 0.6943001747131348] [G loss: 0.5812321901321411]\n",
      "[Epoch 60/1001] [Batch 86/372] [D loss: 0.6931533813476562] [G loss: 0.7991299629211426]\n",
      "[Epoch 60/1001] [Batch 87/372] [D loss: 0.6981449127197266] [G loss: 0.5836517214775085]\n",
      "[Epoch 60/1001] [Batch 88/372] [D loss: 0.6938515901565552] [G loss: 0.7678130865097046]\n",
      "[Epoch 60/1001] [Batch 89/372] [D loss: 0.691083550453186] [G loss: 0.6282517910003662]\n",
      "[Epoch 60/1001] [Batch 90/372] [D loss: 0.6922007203102112] [G loss: 0.7135839462280273]\n",
      "[Epoch 60/1001] [Batch 91/372] [D loss: 0.6927586197853088] [G loss: 0.6636675000190735]\n",
      "[Epoch 60/1001] [Batch 92/372] [D loss: 0.6905264258384705] [G loss: 0.6911013126373291]\n",
      "[Epoch 60/1001] [Batch 93/372] [D loss: 0.6887147426605225] [G loss: 0.6804251074790955]\n",
      "[Epoch 60/1001] [Batch 94/372] [D loss: 0.6908378005027771] [G loss: 0.6574347019195557]\n",
      "[Epoch 60/1001] [Batch 95/372] [D loss: 0.687665581703186] [G loss: 0.7202631235122681]\n",
      "[Epoch 60/1001] [Batch 96/372] [D loss: 0.6883035898208618] [G loss: 0.6428519487380981]\n",
      "[Epoch 60/1001] [Batch 97/372] [D loss: 0.6933568120002747] [G loss: 0.7187094688415527]\n",
      "[Epoch 60/1001] [Batch 98/372] [D loss: 0.6925898790359497] [G loss: 0.6365854144096375]\n",
      "[Epoch 60/1001] [Batch 99/372] [D loss: 0.6885337233543396] [G loss: 0.725448727607727]\n",
      "[Epoch 60/1001] [Batch 100/372] [D loss: 0.6904955506324768] [G loss: 0.6412942409515381]\n",
      "[Epoch 60/1001] [Batch 101/372] [D loss: 0.6897953152656555] [G loss: 0.7218770384788513]\n",
      "[Epoch 60/1001] [Batch 102/372] [D loss: 0.6906384229660034] [G loss: 0.6356753706932068]\n",
      "[Epoch 60/1001] [Batch 103/372] [D loss: 0.6892788410186768] [G loss: 0.7271623611450195]\n",
      "[Epoch 60/1001] [Batch 104/372] [D loss: 0.6951682567596436] [G loss: 0.6350176334381104]\n",
      "[Epoch 60/1001] [Batch 105/372] [D loss: 0.6884952783584595] [G loss: 0.7216933965682983]\n",
      "[Epoch 60/1001] [Batch 106/372] [D loss: 0.6922382712364197] [G loss: 0.6454187035560608]\n",
      "[Epoch 60/1001] [Batch 107/372] [D loss: 0.6892426013946533] [G loss: 0.7109249830245972]\n",
      "[Epoch 60/1001] [Batch 108/372] [D loss: 0.6867718696594238] [G loss: 0.6586260199546814]\n",
      "[Epoch 60/1001] [Batch 109/372] [D loss: 0.6897441148757935] [G loss: 0.7003336548805237]\n",
      "[Epoch 60/1001] [Batch 110/372] [D loss: 0.6895891427993774] [G loss: 0.6658586263656616]\n",
      "[Epoch 60/1001] [Batch 111/372] [D loss: 0.6951960325241089] [G loss: 0.6964319944381714]\n",
      "[Epoch 60/1001] [Batch 112/372] [D loss: 0.6904618740081787] [G loss: 0.6690243482589722]\n",
      "[Epoch 60/1001] [Batch 113/372] [D loss: 0.6884341239929199] [G loss: 0.6776663661003113]\n",
      "[Epoch 60/1001] [Batch 114/372] [D loss: 0.6893635988235474] [G loss: 0.6879916787147522]\n",
      "[Epoch 60/1001] [Batch 115/372] [D loss: 0.6898754835128784] [G loss: 0.681094765663147]\n",
      "[Epoch 60/1001] [Batch 116/372] [D loss: 0.6882041096687317] [G loss: 0.6629623174667358]\n",
      "[Epoch 60/1001] [Batch 117/372] [D loss: 0.6859913468360901] [G loss: 0.702394425868988]\n",
      "[Epoch 60/1001] [Batch 118/372] [D loss: 0.6879326701164246] [G loss: 0.6540836095809937]\n",
      "[Epoch 60/1001] [Batch 119/372] [D loss: 0.6901921629905701] [G loss: 0.7289505004882812]\n",
      "[Epoch 60/1001] [Batch 120/372] [D loss: 0.6951174139976501] [G loss: 0.616081953048706]\n",
      "[Epoch 60/1001] [Batch 121/372] [D loss: 0.6959823369979858] [G loss: 0.766793966293335]\n",
      "[Epoch 60/1001] [Batch 122/372] [D loss: 0.6931313276290894] [G loss: 0.570950448513031]\n",
      "[Epoch 60/1001] [Batch 123/372] [D loss: 0.6974344253540039] [G loss: 0.8288276791572571]\n",
      "[Epoch 60/1001] [Batch 124/372] [D loss: 0.6953266263008118] [G loss: 0.5257107019424438]\n",
      "[Epoch 60/1001] [Batch 125/372] [D loss: 0.7016733884811401] [G loss: 0.945792555809021]\n",
      "[Epoch 60/1001] [Batch 126/372] [D loss: 0.7204359769821167] [G loss: 0.4468660354614258]\n",
      "[Epoch 60/1001] [Batch 127/372] [D loss: 0.7230384945869446] [G loss: 0.9734641909599304]\n",
      "[Epoch 60/1001] [Batch 128/372] [D loss: 0.7258210182189941] [G loss: 0.5057529807090759]\n",
      "[Epoch 60/1001] [Batch 129/372] [D loss: 0.7054388523101807] [G loss: 0.8210025429725647]\n",
      "[Epoch 60/1001] [Batch 130/372] [D loss: 0.6966310143470764] [G loss: 0.6176908612251282]\n",
      "[Epoch 60/1001] [Batch 131/372] [D loss: 0.6949343681335449] [G loss: 0.6973740458488464]\n",
      "[Epoch 60/1001] [Batch 132/372] [D loss: 0.6883713603019714] [G loss: 0.7004430294036865]\n",
      "[Epoch 60/1001] [Batch 133/372] [D loss: 0.6893086433410645] [G loss: 0.6566598415374756]\n",
      "[Epoch 60/1001] [Batch 134/372] [D loss: 0.689390242099762] [G loss: 0.7032379508018494]\n",
      "[Epoch 60/1001] [Batch 135/372] [D loss: 0.6838454008102417] [G loss: 0.6764233708381653]\n",
      "[Epoch 60/1001] [Batch 136/372] [D loss: 0.6930190920829773] [G loss: 0.6835980415344238]\n",
      "[Epoch 60/1001] [Batch 137/372] [D loss: 0.6886048913002014] [G loss: 0.684788167476654]\n",
      "[Epoch 60/1001] [Batch 138/372] [D loss: 0.6861128211021423] [G loss: 0.681383490562439]\n",
      "[Epoch 60/1001] [Batch 139/372] [D loss: 0.6914075613021851] [G loss: 0.6755561828613281]\n",
      "[Epoch 60/1001] [Batch 140/372] [D loss: 0.6916272044181824] [G loss: 0.6695139408111572]\n",
      "[Epoch 60/1001] [Batch 141/372] [D loss: 0.6894888281822205] [G loss: 0.6913133263587952]\n",
      "[Epoch 60/1001] [Batch 142/372] [D loss: 0.6889169812202454] [G loss: 0.6745511889457703]\n",
      "[Epoch 60/1001] [Batch 143/372] [D loss: 0.6917603015899658] [G loss: 0.6899617910385132]\n",
      "[Epoch 60/1001] [Batch 144/372] [D loss: 0.6878147125244141] [G loss: 0.6686702370643616]\n",
      "[Epoch 60/1001] [Batch 145/372] [D loss: 0.6880375146865845] [G loss: 0.6872691512107849]\n",
      "[Epoch 60/1001] [Batch 146/372] [D loss: 0.687002420425415] [G loss: 0.6751585006713867]\n",
      "[Epoch 60/1001] [Batch 147/372] [D loss: 0.6911071538925171] [G loss: 0.6779136657714844]\n",
      "[Epoch 60/1001] [Batch 148/372] [D loss: 0.6910806894302368] [G loss: 0.6771842837333679]\n",
      "[Epoch 60/1001] [Batch 149/372] [D loss: 0.6888728141784668] [G loss: 0.6708914637565613]\n",
      "[Epoch 60/1001] [Batch 150/372] [D loss: 0.6927529573440552] [G loss: 0.693404495716095]\n",
      "[Epoch 60/1001] [Batch 151/372] [D loss: 0.6930186748504639] [G loss: 0.6638317704200745]\n",
      "[Epoch 60/1001] [Batch 152/372] [D loss: 0.6924265623092651] [G loss: 0.6930730938911438]\n",
      "[Epoch 60/1001] [Batch 153/372] [D loss: 0.6823973655700684] [G loss: 0.6645525693893433]\n",
      "[Epoch 60/1001] [Batch 154/372] [D loss: 0.692505955696106] [G loss: 0.7080549597740173]\n",
      "[Epoch 60/1001] [Batch 155/372] [D loss: 0.6858035326004028] [G loss: 0.6422392129898071]\n",
      "[Epoch 60/1001] [Batch 156/372] [D loss: 0.6872642040252686] [G loss: 0.7532811164855957]\n",
      "[Epoch 60/1001] [Batch 157/372] [D loss: 0.6937295198440552] [G loss: 0.6127118468284607]\n",
      "[Epoch 60/1001] [Batch 158/372] [D loss: 0.6915463209152222] [G loss: 0.7371113300323486]\n",
      "[Epoch 60/1001] [Batch 159/372] [D loss: 0.6925042867660522] [G loss: 0.6352114081382751]\n",
      "[Epoch 60/1001] [Batch 160/372] [D loss: 0.6934475898742676] [G loss: 0.7320809364318848]\n",
      "[Epoch 60/1001] [Batch 161/372] [D loss: 0.6926940679550171] [G loss: 0.621903657913208]\n",
      "[Epoch 60/1001] [Batch 162/372] [D loss: 0.6901096105575562] [G loss: 0.7588624358177185]\n",
      "[Epoch 60/1001] [Batch 163/372] [D loss: 0.6906418800354004] [G loss: 0.6044777631759644]\n",
      "[Epoch 60/1001] [Batch 164/372] [D loss: 0.6917994618415833] [G loss: 0.7630873918533325]\n",
      "[Epoch 60/1001] [Batch 165/372] [D loss: 0.6868623495101929] [G loss: 0.6216885447502136]\n",
      "[Epoch 60/1001] [Batch 166/372] [D loss: 0.6978338956832886] [G loss: 0.7204949855804443]\n",
      "[Epoch 60/1001] [Batch 167/372] [D loss: 0.6926438808441162] [G loss: 0.6480894088745117]\n",
      "[Epoch 60/1001] [Batch 168/372] [D loss: 0.6936526298522949] [G loss: 0.7123394012451172]\n",
      "[Epoch 60/1001] [Batch 169/372] [D loss: 0.6917040348052979] [G loss: 0.6327844858169556]\n",
      "[Epoch 60/1001] [Batch 170/372] [D loss: 0.6917832493782043] [G loss: 0.7279004454612732]\n",
      "[Epoch 60/1001] [Batch 171/372] [D loss: 0.6884996891021729] [G loss: 0.6531193256378174]\n",
      "[Epoch 60/1001] [Batch 172/372] [D loss: 0.6901089549064636] [G loss: 0.6873243451118469]\n",
      "[Epoch 60/1001] [Batch 173/372] [D loss: 0.693577766418457] [G loss: 0.6819415092468262]\n",
      "[Epoch 60/1001] [Batch 174/372] [D loss: 0.6910096406936646] [G loss: 0.6742196679115295]\n",
      "[Epoch 60/1001] [Batch 175/372] [D loss: 0.690156102180481] [G loss: 0.6608080863952637]\n",
      "[Epoch 60/1001] [Batch 176/372] [D loss: 0.6852343082427979] [G loss: 0.7204508185386658]\n",
      "[Epoch 60/1001] [Batch 177/372] [D loss: 0.6890941262245178] [G loss: 0.6408845782279968]\n",
      "[Epoch 60/1001] [Batch 178/372] [D loss: 0.692694365978241] [G loss: 0.7122550010681152]\n",
      "[Epoch 60/1001] [Batch 179/372] [D loss: 0.6876895427703857] [G loss: 0.6414141654968262]\n",
      "[Epoch 60/1001] [Batch 180/372] [D loss: 0.6885362863540649] [G loss: 0.7433839440345764]\n",
      "[Epoch 60/1001] [Batch 181/372] [D loss: 0.6829415559768677] [G loss: 0.612348198890686]\n",
      "[Epoch 60/1001] [Batch 182/372] [D loss: 0.6944221258163452] [G loss: 0.7500549554824829]\n",
      "[Epoch 60/1001] [Batch 183/372] [D loss: 0.6898387670516968] [G loss: 0.6111999154090881]\n",
      "[Epoch 60/1001] [Batch 184/372] [D loss: 0.6928750276565552] [G loss: 0.7710615396499634]\n",
      "[Epoch 60/1001] [Batch 185/372] [D loss: 0.6886939406394958] [G loss: 0.5788044929504395]\n",
      "[Epoch 60/1001] [Batch 186/372] [D loss: 0.6956350803375244] [G loss: 0.8115963935852051]\n",
      "[Epoch 60/1001] [Batch 187/372] [D loss: 0.7027764916419983] [G loss: 0.553692102432251]\n",
      "[Epoch 60/1001] [Batch 188/372] [D loss: 0.6950788497924805] [G loss: 0.8402602672576904]\n",
      "[Epoch 60/1001] [Batch 189/372] [D loss: 0.699059247970581] [G loss: 0.5641329884529114]\n",
      "[Epoch 60/1001] [Batch 190/372] [D loss: 0.6994979977607727] [G loss: 0.7959874272346497]\n",
      "[Epoch 60/1001] [Batch 191/372] [D loss: 0.6918063163757324] [G loss: 0.5928056836128235]\n",
      "[Epoch 60/1001] [Batch 192/372] [D loss: 0.6960886120796204] [G loss: 0.7447022199630737]\n",
      "[Epoch 60/1001] [Batch 193/372] [D loss: 0.6898380517959595] [G loss: 0.6391859650611877]\n",
      "[Epoch 60/1001] [Batch 194/372] [D loss: 0.6847577691078186] [G loss: 0.731861412525177]\n",
      "[Epoch 60/1001] [Batch 195/372] [D loss: 0.6854619979858398] [G loss: 0.626129686832428]\n",
      "[Epoch 60/1001] [Batch 196/372] [D loss: 0.6894327998161316] [G loss: 0.7091574668884277]\n",
      "[Epoch 60/1001] [Batch 197/372] [D loss: 0.691825270652771] [G loss: 0.6577416062355042]\n",
      "[Epoch 60/1001] [Batch 198/372] [D loss: 0.6928539276123047] [G loss: 0.6912618279457092]\n",
      "[Epoch 60/1001] [Batch 199/372] [D loss: 0.6882271766662598] [G loss: 0.6604439616203308]\n",
      "[Epoch 60/1001] [Batch 200/372] [D loss: 0.6887685060501099] [G loss: 0.7003339529037476]\n",
      "[Epoch 60/1001] [Batch 201/372] [D loss: 0.6910653114318848] [G loss: 0.6563789248466492]\n",
      "[Epoch 60/1001] [Batch 202/372] [D loss: 0.6925756335258484] [G loss: 0.6755408048629761]\n",
      "[Epoch 60/1001] [Batch 203/372] [D loss: 0.6866942644119263] [G loss: 0.7086400985717773]\n",
      "[Epoch 60/1001] [Batch 204/372] [D loss: 0.6983924508094788] [G loss: 0.6162183284759521]\n",
      "[Epoch 60/1001] [Batch 205/372] [D loss: 0.6924757957458496] [G loss: 0.760899543762207]\n",
      "[Epoch 60/1001] [Batch 206/372] [D loss: 0.6965371370315552] [G loss: 0.596382200717926]\n",
      "[Epoch 60/1001] [Batch 207/372] [D loss: 0.6951187252998352] [G loss: 0.7745051383972168]\n",
      "[Epoch 60/1001] [Batch 208/372] [D loss: 0.6964057683944702] [G loss: 0.5896883010864258]\n",
      "[Epoch 60/1001] [Batch 209/372] [D loss: 0.6931775808334351] [G loss: 0.7640970945358276]\n",
      "[Epoch 60/1001] [Batch 210/372] [D loss: 0.687126874923706] [G loss: 0.6185176968574524]\n",
      "[Epoch 60/1001] [Batch 211/372] [D loss: 0.6900863647460938] [G loss: 0.7296207547187805]\n",
      "[Epoch 60/1001] [Batch 212/372] [D loss: 0.6909224987030029] [G loss: 0.6442581415176392]\n",
      "[Epoch 60/1001] [Batch 213/372] [D loss: 0.6888328790664673] [G loss: 0.7262529730796814]\n",
      "[Epoch 60/1001] [Batch 214/372] [D loss: 0.6886264681816101] [G loss: 0.6617586612701416]\n",
      "[Epoch 60/1001] [Batch 215/372] [D loss: 0.6920360326766968] [G loss: 0.666240930557251]\n",
      "[Epoch 60/1001] [Batch 216/372] [D loss: 0.6884928345680237] [G loss: 0.7193077206611633]\n",
      "[Epoch 60/1001] [Batch 217/372] [D loss: 0.692193865776062] [G loss: 0.6279250383377075]\n",
      "[Epoch 60/1001] [Batch 218/372] [D loss: 0.6903855204582214] [G loss: 0.7532325983047485]\n",
      "[Epoch 60/1001] [Batch 219/372] [D loss: 0.6907517910003662] [G loss: 0.596512496471405]\n",
      "[Epoch 60/1001] [Batch 220/372] [D loss: 0.6970618963241577] [G loss: 0.7871231436729431]\n",
      "[Epoch 60/1001] [Batch 221/372] [D loss: 0.6941757202148438] [G loss: 0.5686659812927246]\n",
      "[Epoch 60/1001] [Batch 222/372] [D loss: 0.6971208453178406] [G loss: 0.8163754940032959]\n",
      "[Epoch 60/1001] [Batch 223/372] [D loss: 0.698946475982666] [G loss: 0.5657026767730713]\n",
      "[Epoch 60/1001] [Batch 224/372] [D loss: 0.6947861909866333] [G loss: 0.8053503036499023]\n",
      "[Epoch 60/1001] [Batch 225/372] [D loss: 0.6981009244918823] [G loss: 0.5719585418701172]\n",
      "[Epoch 60/1001] [Batch 226/372] [D loss: 0.695192813873291] [G loss: 0.7785158753395081]\n",
      "[Epoch 60/1001] [Batch 227/372] [D loss: 0.6920043230056763] [G loss: 0.5948876142501831]\n",
      "[Epoch 60/1001] [Batch 228/372] [D loss: 0.6942054033279419] [G loss: 0.7685299515724182]\n",
      "[Epoch 60/1001] [Batch 229/372] [D loss: 0.6947681903839111] [G loss: 0.6031337380409241]\n",
      "[Epoch 60/1001] [Batch 230/372] [D loss: 0.6910065412521362] [G loss: 0.7615057826042175]\n",
      "[Epoch 60/1001] [Batch 231/372] [D loss: 0.6965799331665039] [G loss: 0.6107474565505981]\n",
      "[Epoch 60/1001] [Batch 232/372] [D loss: 0.692579984664917] [G loss: 0.7394176721572876]\n",
      "[Epoch 60/1001] [Batch 233/372] [D loss: 0.6927326917648315] [G loss: 0.6474902629852295]\n",
      "[Epoch 60/1001] [Batch 234/372] [D loss: 0.6901242733001709] [G loss: 0.6896974444389343]\n",
      "[Epoch 60/1001] [Batch 235/372] [D loss: 0.6884959936141968] [G loss: 0.6842710971832275]\n",
      "[Epoch 60/1001] [Batch 236/372] [D loss: 0.6909448504447937] [G loss: 0.6916010975837708]\n",
      "[Epoch 60/1001] [Batch 237/372] [D loss: 0.6897526979446411] [G loss: 0.6638778448104858]\n",
      "[Epoch 60/1001] [Batch 238/372] [D loss: 0.6905468702316284] [G loss: 0.6926143765449524]\n",
      "[Epoch 60/1001] [Batch 239/372] [D loss: 0.6908620595932007] [G loss: 0.6753311157226562]\n",
      "[Epoch 60/1001] [Batch 240/372] [D loss: 0.6920464038848877] [G loss: 0.690851092338562]\n",
      "[Epoch 60/1001] [Batch 241/372] [D loss: 0.6889375448226929] [G loss: 0.671623945236206]\n",
      "[Epoch 60/1001] [Batch 242/372] [D loss: 0.6862813234329224] [G loss: 0.6823282837867737]\n",
      "[Epoch 60/1001] [Batch 243/372] [D loss: 0.6930737495422363] [G loss: 0.6814039945602417]\n",
      "[Epoch 60/1001] [Batch 244/372] [D loss: 0.6880879998207092] [G loss: 0.674402117729187]\n",
      "[Epoch 60/1001] [Batch 245/372] [D loss: 0.6896560788154602] [G loss: 0.6797866225242615]\n",
      "[Epoch 60/1001] [Batch 246/372] [D loss: 0.6884243488311768] [G loss: 0.6831018924713135]\n",
      "[Epoch 60/1001] [Batch 247/372] [D loss: 0.6867848634719849] [G loss: 0.6546907424926758]\n",
      "[Epoch 60/1001] [Batch 248/372] [D loss: 0.6903470754623413] [G loss: 0.7470402121543884]\n",
      "[Epoch 60/1001] [Batch 249/372] [D loss: 0.6905045509338379] [G loss: 0.5856985449790955]\n",
      "[Epoch 60/1001] [Batch 250/372] [D loss: 0.692287027835846] [G loss: 0.8294999003410339]\n",
      "[Epoch 60/1001] [Batch 251/372] [D loss: 0.6987752914428711] [G loss: 0.5323439836502075]\n",
      "[Epoch 60/1001] [Batch 252/372] [D loss: 0.7062729597091675] [G loss: 0.8603997230529785]\n",
      "[Epoch 60/1001] [Batch 253/372] [D loss: 0.7084527015686035] [G loss: 0.5375252366065979]\n",
      "[Epoch 60/1001] [Batch 254/372] [D loss: 0.7040644884109497] [G loss: 0.8052687048912048]\n",
      "[Epoch 60/1001] [Batch 255/372] [D loss: 0.6997862458229065] [G loss: 0.5984586477279663]\n",
      "[Epoch 60/1001] [Batch 256/372] [D loss: 0.6948884725570679] [G loss: 0.7362042665481567]\n",
      "[Epoch 60/1001] [Batch 257/372] [D loss: 0.6908893585205078] [G loss: 0.6461604237556458]\n",
      "[Epoch 60/1001] [Batch 258/372] [D loss: 0.6931052207946777] [G loss: 0.6887255311012268]\n",
      "[Epoch 60/1001] [Batch 259/372] [D loss: 0.6893177032470703] [G loss: 0.683054506778717]\n",
      "[Epoch 60/1001] [Batch 260/372] [D loss: 0.6869621276855469] [G loss: 0.6697015166282654]\n",
      "[Epoch 60/1001] [Batch 261/372] [D loss: 0.6918694972991943] [G loss: 0.6887469291687012]\n",
      "[Epoch 60/1001] [Batch 262/372] [D loss: 0.6914784908294678] [G loss: 0.6678170561790466]\n",
      "[Epoch 60/1001] [Batch 263/372] [D loss: 0.6884571313858032] [G loss: 0.6895942687988281]\n",
      "[Epoch 60/1001] [Batch 264/372] [D loss: 0.6937848329544067] [G loss: 0.6744314432144165]\n",
      "[Epoch 60/1001] [Batch 265/372] [D loss: 0.6898190379142761] [G loss: 0.6690711379051208]\n",
      "[Epoch 60/1001] [Batch 266/372] [D loss: 0.6867960691452026] [G loss: 0.6897732615470886]\n",
      "[Epoch 60/1001] [Batch 267/372] [D loss: 0.6910073757171631] [G loss: 0.6690847873687744]\n",
      "[Epoch 60/1001] [Batch 268/372] [D loss: 0.6899886131286621] [G loss: 0.6727638840675354]\n",
      "[Epoch 60/1001] [Batch 269/372] [D loss: 0.6958388090133667] [G loss: 0.7002876400947571]\n",
      "[Epoch 60/1001] [Batch 270/372] [D loss: 0.6918355226516724] [G loss: 0.6537069082260132]\n",
      "[Epoch 60/1001] [Batch 271/372] [D loss: 0.6904555559158325] [G loss: 0.700977087020874]\n",
      "[Epoch 60/1001] [Batch 272/372] [D loss: 0.6896772384643555] [G loss: 0.6647546291351318]\n",
      "[Epoch 60/1001] [Batch 273/372] [D loss: 0.6881520748138428] [G loss: 0.6859644651412964]\n",
      "[Epoch 60/1001] [Batch 274/372] [D loss: 0.685681939125061] [G loss: 0.6661590337753296]\n",
      "[Epoch 60/1001] [Batch 275/372] [D loss: 0.6933321952819824] [G loss: 0.7039970755577087]\n",
      "[Epoch 60/1001] [Batch 276/372] [D loss: 0.6847196817398071] [G loss: 0.6410627961158752]\n",
      "[Epoch 60/1001] [Batch 277/372] [D loss: 0.6912956833839417] [G loss: 0.765062689781189]\n",
      "[Epoch 60/1001] [Batch 278/372] [D loss: 0.6871241927146912] [G loss: 0.5825322270393372]\n",
      "[Epoch 60/1001] [Batch 279/372] [D loss: 0.6932113170623779] [G loss: 0.7896332740783691]\n",
      "[Epoch 60/1001] [Batch 280/372] [D loss: 0.689117968082428] [G loss: 0.5738216638565063]\n",
      "[Epoch 60/1001] [Batch 281/372] [D loss: 0.6950771808624268] [G loss: 0.8273849487304688]\n",
      "[Epoch 60/1001] [Batch 282/372] [D loss: 0.69304358959198] [G loss: 0.5419240593910217]\n",
      "[Epoch 60/1001] [Batch 283/372] [D loss: 0.7002137899398804] [G loss: 0.8799392580986023]\n",
      "[Epoch 60/1001] [Batch 284/372] [D loss: 0.707600474357605] [G loss: 0.5124473571777344]\n",
      "[Epoch 60/1001] [Batch 285/372] [D loss: 0.708830714225769] [G loss: 0.8381975889205933]\n",
      "[Epoch 60/1001] [Batch 286/372] [D loss: 0.7040705680847168] [G loss: 0.5882126688957214]\n",
      "[Epoch 60/1001] [Batch 287/372] [D loss: 0.6997743844985962] [G loss: 0.7189351320266724]\n",
      "[Epoch 60/1001] [Batch 288/372] [D loss: 0.6907023191452026] [G loss: 0.672827959060669]\n",
      "[Epoch 60/1001] [Batch 289/372] [D loss: 0.6885247826576233] [G loss: 0.6586547493934631]\n",
      "[Epoch 60/1001] [Batch 290/372] [D loss: 0.6860244274139404] [G loss: 0.7155272364616394]\n",
      "[Epoch 60/1001] [Batch 291/372] [D loss: 0.6909877061843872] [G loss: 0.6744251251220703]\n",
      "[Epoch 60/1001] [Batch 292/372] [D loss: 0.6920929551124573] [G loss: 0.6694612503051758]\n",
      "[Epoch 60/1001] [Batch 293/372] [D loss: 0.6911776065826416] [G loss: 0.6985446214675903]\n",
      "[Epoch 60/1001] [Batch 294/372] [D loss: 0.6913552284240723] [G loss: 0.6535667181015015]\n",
      "[Epoch 60/1001] [Batch 295/372] [D loss: 0.6907765865325928] [G loss: 0.6886100769042969]\n",
      "[Epoch 60/1001] [Batch 296/372] [D loss: 0.6858099699020386] [G loss: 0.6707369685173035]\n",
      "[Epoch 60/1001] [Batch 297/372] [D loss: 0.6950548887252808] [G loss: 0.6936184167861938]\n",
      "[Epoch 60/1001] [Batch 298/372] [D loss: 0.6910783052444458] [G loss: 0.6687935590744019]\n",
      "[Epoch 60/1001] [Batch 299/372] [D loss: 0.690198540687561] [G loss: 0.6884269714355469]\n",
      "[Epoch 60/1001] [Batch 300/372] [D loss: 0.6910216808319092] [G loss: 0.673460841178894]\n",
      "[Epoch 60/1001] [Batch 301/372] [D loss: 0.686379611492157] [G loss: 0.6808103322982788]\n",
      "[Epoch 60/1001] [Batch 302/372] [D loss: 0.6909869909286499] [G loss: 0.6904470324516296]\n",
      "[Epoch 60/1001] [Batch 303/372] [D loss: 0.6888457536697388] [G loss: 0.668075680732727]\n",
      "[Epoch 60/1001] [Batch 304/372] [D loss: 0.692091166973114] [G loss: 0.6893860697746277]\n",
      "[Epoch 60/1001] [Batch 305/372] [D loss: 0.6909008026123047] [G loss: 0.6854829788208008]\n",
      "[Epoch 60/1001] [Batch 306/372] [D loss: 0.6920593976974487] [G loss: 0.6626322269439697]\n",
      "[Epoch 60/1001] [Batch 307/372] [D loss: 0.6873632073402405] [G loss: 0.7119283080101013]\n",
      "[Epoch 60/1001] [Batch 308/372] [D loss: 0.6910349726676941] [G loss: 0.6439051032066345]\n",
      "[Epoch 60/1001] [Batch 309/372] [D loss: 0.6915327310562134] [G loss: 0.7210285663604736]\n",
      "[Epoch 60/1001] [Batch 310/372] [D loss: 0.6890459656715393] [G loss: 0.6212610006332397]\n",
      "[Epoch 60/1001] [Batch 311/372] [D loss: 0.6920018196105957] [G loss: 0.7552638649940491]\n",
      "[Epoch 60/1001] [Batch 312/372] [D loss: 0.6876994371414185] [G loss: 0.6105687022209167]\n",
      "[Epoch 60/1001] [Batch 313/372] [D loss: 0.6894755363464355] [G loss: 0.7704402208328247]\n",
      "[Epoch 60/1001] [Batch 314/372] [D loss: 0.6914012432098389] [G loss: 0.5954298973083496]\n",
      "[Epoch 60/1001] [Batch 315/372] [D loss: 0.6957131028175354] [G loss: 0.7633211016654968]\n",
      "[Epoch 60/1001] [Batch 316/372] [D loss: 0.6934412717819214] [G loss: 0.5959951877593994]\n",
      "[Epoch 60/1001] [Batch 317/372] [D loss: 0.6951771974563599] [G loss: 0.777061939239502]\n",
      "[Epoch 60/1001] [Batch 318/372] [D loss: 0.6968119740486145] [G loss: 0.5760185122489929]\n",
      "[Epoch 60/1001] [Batch 319/372] [D loss: 0.7025609016418457] [G loss: 0.8094928860664368]\n",
      "[Epoch 60/1001] [Batch 320/372] [D loss: 0.6988648176193237] [G loss: 0.5756442546844482]\n",
      "[Epoch 60/1001] [Batch 321/372] [D loss: 0.6998015642166138] [G loss: 0.7605188488960266]\n",
      "[Epoch 60/1001] [Batch 322/372] [D loss: 0.6965198516845703] [G loss: 0.6260130405426025]\n",
      "[Epoch 60/1001] [Batch 323/372] [D loss: 0.6907004117965698] [G loss: 0.7139145135879517]\n",
      "[Epoch 60/1001] [Batch 324/372] [D loss: 0.6910154819488525] [G loss: 0.6615623831748962]\n",
      "[Epoch 60/1001] [Batch 325/372] [D loss: 0.6923149824142456] [G loss: 0.6758172512054443]\n",
      "[Epoch 60/1001] [Batch 326/372] [D loss: 0.6910661458969116] [G loss: 0.6948908567428589]\n",
      "[Epoch 60/1001] [Batch 327/372] [D loss: 0.6850709319114685] [G loss: 0.6518141031265259]\n",
      "[Epoch 60/1001] [Batch 328/372] [D loss: 0.6894567012786865] [G loss: 0.7391635775566101]\n",
      "[Epoch 60/1001] [Batch 329/372] [D loss: 0.6938257813453674] [G loss: 0.6176037192344666]\n",
      "[Epoch 60/1001] [Batch 330/372] [D loss: 0.695338249206543] [G loss: 0.7441071271896362]\n",
      "[Epoch 60/1001] [Batch 331/372] [D loss: 0.690921425819397] [G loss: 0.621278703212738]\n",
      "[Epoch 60/1001] [Batch 332/372] [D loss: 0.6943026781082153] [G loss: 0.737513542175293]\n",
      "[Epoch 60/1001] [Batch 333/372] [D loss: 0.6938109397888184] [G loss: 0.6256693005561829]\n",
      "[Epoch 60/1001] [Batch 334/372] [D loss: 0.6915667653083801] [G loss: 0.73689866065979]\n",
      "[Epoch 60/1001] [Batch 335/372] [D loss: 0.6910874843597412] [G loss: 0.64128178358078]\n",
      "[Epoch 60/1001] [Batch 336/372] [D loss: 0.689088761806488] [G loss: 0.7148661017417908]\n",
      "[Epoch 60/1001] [Batch 337/372] [D loss: 0.6924179792404175] [G loss: 0.6523834466934204]\n",
      "[Epoch 60/1001] [Batch 338/372] [D loss: 0.6927751898765564] [G loss: 0.6978185176849365]\n",
      "[Epoch 60/1001] [Batch 339/372] [D loss: 0.6901627779006958] [G loss: 0.6786319017410278]\n",
      "[Epoch 60/1001] [Batch 340/372] [D loss: 0.6911338567733765] [G loss: 0.674619197845459]\n",
      "[Epoch 60/1001] [Batch 341/372] [D loss: 0.6896872520446777] [G loss: 0.6930084228515625]\n",
      "[Epoch 60/1001] [Batch 342/372] [D loss: 0.6916854381561279] [G loss: 0.6611759662628174]\n",
      "[Epoch 60/1001] [Batch 343/372] [D loss: 0.696610689163208] [G loss: 0.6915117502212524]\n",
      "[Epoch 60/1001] [Batch 344/372] [D loss: 0.6918675303459167] [G loss: 0.6820200681686401]\n",
      "[Epoch 60/1001] [Batch 345/372] [D loss: 0.6914064884185791] [G loss: 0.6800457239151001]\n",
      "[Epoch 60/1001] [Batch 346/372] [D loss: 0.6892989873886108] [G loss: 0.6599875092506409]\n",
      "[Epoch 60/1001] [Batch 347/372] [D loss: 0.6930776834487915] [G loss: 0.7131323218345642]\n",
      "[Epoch 60/1001] [Batch 348/372] [D loss: 0.6918857097625732] [G loss: 0.6340196132659912]\n",
      "[Epoch 60/1001] [Batch 349/372] [D loss: 0.6919837594032288] [G loss: 0.7531763315200806]\n",
      "[Epoch 60/1001] [Batch 350/372] [D loss: 0.6893928050994873] [G loss: 0.6044721603393555]\n",
      "[Epoch 60/1001] [Batch 351/372] [D loss: 0.6967223286628723] [G loss: 0.7770463228225708]\n",
      "[Epoch 60/1001] [Batch 352/372] [D loss: 0.6957637071609497] [G loss: 0.5820282697677612]\n",
      "[Epoch 60/1001] [Batch 353/372] [D loss: 0.697169840335846] [G loss: 0.7867279052734375]\n",
      "[Epoch 60/1001] [Batch 354/372] [D loss: 0.6954611539840698] [G loss: 0.5933957695960999]\n",
      "[Epoch 60/1001] [Batch 355/372] [D loss: 0.6935855150222778] [G loss: 0.7669910788536072]\n",
      "[Epoch 60/1001] [Batch 356/372] [D loss: 0.6968954205513] [G loss: 0.6159631013870239]\n",
      "[Epoch 60/1001] [Batch 357/372] [D loss: 0.6912448406219482] [G loss: 0.724951446056366]\n",
      "[Epoch 60/1001] [Batch 358/372] [D loss: 0.6902834177017212] [G loss: 0.6591495275497437]\n",
      "[Epoch 60/1001] [Batch 359/372] [D loss: 0.689162015914917] [G loss: 0.697788417339325]\n",
      "[Epoch 60/1001] [Batch 360/372] [D loss: 0.6896778345108032] [G loss: 0.6440331339836121]\n",
      "[Epoch 60/1001] [Batch 361/372] [D loss: 0.6890528202056885] [G loss: 0.7205086350440979]\n",
      "[Epoch 60/1001] [Batch 362/372] [D loss: 0.6904986500740051] [G loss: 0.640762209892273]\n",
      "[Epoch 60/1001] [Batch 363/372] [D loss: 0.68840491771698] [G loss: 0.7381402254104614]\n",
      "[Epoch 60/1001] [Batch 364/372] [D loss: 0.6930314302444458] [G loss: 0.6183837652206421]\n",
      "[Epoch 60/1001] [Batch 365/372] [D loss: 0.6958116292953491] [G loss: 0.7437373995780945]\n",
      "[Epoch 60/1001] [Batch 366/372] [D loss: 0.6975253224372864] [G loss: 0.608132541179657]\n",
      "[Epoch 60/1001] [Batch 367/372] [D loss: 0.6895490884780884] [G loss: 0.7587054371833801]\n",
      "[Epoch 60/1001] [Batch 368/372] [D loss: 0.6919122934341431] [G loss: 0.6130197644233704]\n",
      "[Epoch 60/1001] [Batch 369/372] [D loss: 0.6923333406448364] [G loss: 0.7608819007873535]\n",
      "[Epoch 60/1001] [Batch 370/372] [D loss: 0.6910688877105713] [G loss: 0.605739414691925]\n",
      "[Epoch 60/1001] [Batch 371/372] [D loss: 0.6908811330795288] [G loss: 0.7564029097557068]\n",
      "[Epoch 61/1001] [Batch 0/372] [D loss: 0.6920626163482666] [G loss: 0.6163567304611206]\n",
      "[Epoch 61/1001] [Batch 1/372] [D loss: 0.6912558078765869] [G loss: 0.7376344203948975]\n",
      "[Epoch 61/1001] [Batch 2/372] [D loss: 0.6866224408149719] [G loss: 0.6405227184295654]\n",
      "[Epoch 61/1001] [Batch 3/372] [D loss: 0.6906766891479492] [G loss: 0.7020190954208374]\n",
      "[Epoch 61/1001] [Batch 4/372] [D loss: 0.6911170482635498] [G loss: 0.6621796488761902]\n",
      "[Epoch 61/1001] [Batch 5/372] [D loss: 0.6874183416366577] [G loss: 0.7052063941955566]\n",
      "[Epoch 61/1001] [Batch 6/372] [D loss: 0.6825824975967407] [G loss: 0.6458787322044373]\n",
      "[Epoch 61/1001] [Batch 7/372] [D loss: 0.687879204750061] [G loss: 0.7145661115646362]\n",
      "[Epoch 61/1001] [Batch 8/372] [D loss: 0.6869281530380249] [G loss: 0.650847852230072]\n",
      "[Epoch 61/1001] [Batch 9/372] [D loss: 0.6926394701004028] [G loss: 0.7255297899246216]\n",
      "[Epoch 61/1001] [Batch 10/372] [D loss: 0.688457727432251] [G loss: 0.6270754933357239]\n",
      "[Epoch 61/1001] [Batch 11/372] [D loss: 0.6933742165565491] [G loss: 0.7456705570220947]\n",
      "[Epoch 61/1001] [Batch 12/372] [D loss: 0.6923300623893738] [G loss: 0.6082326769828796]\n",
      "[Epoch 61/1001] [Batch 13/372] [D loss: 0.6935059428215027] [G loss: 0.7755961418151855]\n",
      "[Epoch 61/1001] [Batch 14/372] [D loss: 0.6882514357566833] [G loss: 0.5998071432113647]\n",
      "[Epoch 61/1001] [Batch 15/372] [D loss: 0.6943227052688599] [G loss: 0.7577138543128967]\n",
      "[Epoch 61/1001] [Batch 16/372] [D loss: 0.6943169236183167] [G loss: 0.6274372935295105]\n",
      "[Epoch 61/1001] [Batch 17/372] [D loss: 0.6925556659698486] [G loss: 0.7171762585639954]\n",
      "[Epoch 61/1001] [Batch 18/372] [D loss: 0.6940107345581055] [G loss: 0.6553828716278076]\n",
      "[Epoch 61/1001] [Batch 19/372] [D loss: 0.6878137588500977] [G loss: 0.7030867338180542]\n",
      "[Epoch 61/1001] [Batch 20/372] [D loss: 0.6850374937057495] [G loss: 0.6594682931900024]\n",
      "[Epoch 61/1001] [Batch 21/372] [D loss: 0.6867334246635437] [G loss: 0.7086666822433472]\n",
      "[Epoch 61/1001] [Batch 22/372] [D loss: 0.6892834901809692] [G loss: 0.6716407537460327]\n",
      "[Epoch 61/1001] [Batch 23/372] [D loss: 0.6899019479751587] [G loss: 0.6747704744338989]\n",
      "[Epoch 61/1001] [Batch 24/372] [D loss: 0.689214289188385] [G loss: 0.6735551953315735]\n",
      "[Epoch 61/1001] [Batch 25/372] [D loss: 0.6866528987884521] [G loss: 0.7031117677688599]\n",
      "[Epoch 61/1001] [Batch 26/372] [D loss: 0.6911615133285522] [G loss: 0.662634015083313]\n",
      "[Epoch 61/1001] [Batch 27/372] [D loss: 0.69151371717453] [G loss: 0.6805810332298279]\n",
      "[Epoch 61/1001] [Batch 28/372] [D loss: 0.6875814199447632] [G loss: 0.6657393574714661]\n",
      "[Epoch 61/1001] [Batch 29/372] [D loss: 0.6907104253768921] [G loss: 0.7023100852966309]\n",
      "[Epoch 61/1001] [Batch 30/372] [D loss: 0.6892558336257935] [G loss: 0.6554145216941833]\n",
      "[Epoch 61/1001] [Batch 31/372] [D loss: 0.6896927356719971] [G loss: 0.6963731646537781]\n",
      "[Epoch 61/1001] [Batch 32/372] [D loss: 0.6879662275314331] [G loss: 0.6651327013969421]\n",
      "[Epoch 61/1001] [Batch 33/372] [D loss: 0.6913813352584839] [G loss: 0.6943836808204651]\n",
      "[Epoch 61/1001] [Batch 34/372] [D loss: 0.6846539378166199] [G loss: 0.6654540300369263]\n",
      "[Epoch 61/1001] [Batch 35/372] [D loss: 0.6924843788146973] [G loss: 0.7021726369857788]\n",
      "[Epoch 61/1001] [Batch 36/372] [D loss: 0.6887654066085815] [G loss: 0.6538980007171631]\n",
      "[Epoch 61/1001] [Batch 37/372] [D loss: 0.6890812516212463] [G loss: 0.7168249487876892]\n",
      "[Epoch 61/1001] [Batch 38/372] [D loss: 0.6865503787994385] [G loss: 0.640697717666626]\n",
      "[Epoch 61/1001] [Batch 39/372] [D loss: 0.6908367872238159] [G loss: 0.7228455543518066]\n",
      "[Epoch 61/1001] [Batch 40/372] [D loss: 0.6927319169044495] [G loss: 0.6255427598953247]\n",
      "[Epoch 61/1001] [Batch 41/372] [D loss: 0.6880533695220947] [G loss: 0.7578821778297424]\n",
      "[Epoch 61/1001] [Batch 42/372] [D loss: 0.6975091695785522] [G loss: 0.584191083908081]\n",
      "[Epoch 61/1001] [Batch 43/372] [D loss: 0.6951658725738525] [G loss: 0.7910770177841187]\n",
      "[Epoch 61/1001] [Batch 44/372] [D loss: 0.6985496878623962] [G loss: 0.5727920532226562]\n",
      "[Epoch 61/1001] [Batch 45/372] [D loss: 0.6972464323043823] [G loss: 0.8125991821289062]\n",
      "[Epoch 61/1001] [Batch 46/372] [D loss: 0.7048414945602417] [G loss: 0.5577592849731445]\n",
      "[Epoch 61/1001] [Batch 47/372] [D loss: 0.701030969619751] [G loss: 0.8097430467605591]\n",
      "[Epoch 61/1001] [Batch 48/372] [D loss: 0.693942666053772] [G loss: 0.5891271233558655]\n",
      "[Epoch 61/1001] [Batch 49/372] [D loss: 0.6972631216049194] [G loss: 0.7461556196212769]\n",
      "[Epoch 61/1001] [Batch 50/372] [D loss: 0.6917192935943604] [G loss: 0.6460250020027161]\n",
      "[Epoch 61/1001] [Batch 51/372] [D loss: 0.6928234100341797] [G loss: 0.6974706649780273]\n",
      "[Epoch 61/1001] [Batch 52/372] [D loss: 0.6917548179626465] [G loss: 0.6761594414710999]\n",
      "[Epoch 61/1001] [Batch 53/372] [D loss: 0.6873838901519775] [G loss: 0.6908268332481384]\n",
      "[Epoch 61/1001] [Batch 54/372] [D loss: 0.688713788986206] [G loss: 0.6814466714859009]\n",
      "[Epoch 61/1001] [Batch 55/372] [D loss: 0.6893408894538879] [G loss: 0.6827309727668762]\n",
      "[Epoch 61/1001] [Batch 56/372] [D loss: 0.6871423721313477] [G loss: 0.6831107139587402]\n",
      "[Epoch 61/1001] [Batch 57/372] [D loss: 0.6898747682571411] [G loss: 0.6942410469055176]\n",
      "[Epoch 61/1001] [Batch 58/372] [D loss: 0.6908652782440186] [G loss: 0.6694235801696777]\n",
      "[Epoch 61/1001] [Batch 59/372] [D loss: 0.6906172037124634] [G loss: 0.7052112817764282]\n",
      "[Epoch 61/1001] [Batch 60/372] [D loss: 0.691699743270874] [G loss: 0.6427580118179321]\n",
      "[Epoch 61/1001] [Batch 61/372] [D loss: 0.692326009273529] [G loss: 0.7158405184745789]\n",
      "[Epoch 61/1001] [Batch 62/372] [D loss: 0.6872942447662354] [G loss: 0.6373743414878845]\n",
      "[Epoch 61/1001] [Batch 63/372] [D loss: 0.690997838973999] [G loss: 0.7516438364982605]\n",
      "[Epoch 61/1001] [Batch 64/372] [D loss: 0.6908391714096069] [G loss: 0.582134485244751]\n",
      "[Epoch 61/1001] [Batch 65/372] [D loss: 0.6931548118591309] [G loss: 0.828766942024231]\n",
      "[Epoch 61/1001] [Batch 66/372] [D loss: 0.7009277939796448] [G loss: 0.5067223906517029]\n",
      "[Epoch 61/1001] [Batch 67/372] [D loss: 0.7130862474441528] [G loss: 0.9461179971694946]\n",
      "[Epoch 61/1001] [Batch 68/372] [D loss: 0.7197372317314148] [G loss: 0.47446396946907043]\n",
      "[Epoch 61/1001] [Batch 69/372] [D loss: 0.7168242335319519] [G loss: 0.8581529855728149]\n",
      "[Epoch 61/1001] [Batch 70/372] [D loss: 0.7028419971466064] [G loss: 0.596653163433075]\n",
      "[Epoch 61/1001] [Batch 71/372] [D loss: 0.6901321411132812] [G loss: 0.7181586027145386]\n",
      "[Epoch 61/1001] [Batch 72/372] [D loss: 0.6911131143569946] [G loss: 0.6819349527359009]\n",
      "[Epoch 61/1001] [Batch 73/372] [D loss: 0.6880658864974976] [G loss: 0.6568312644958496]\n",
      "[Epoch 61/1001] [Batch 74/372] [D loss: 0.6903098821640015] [G loss: 0.7165530920028687]\n",
      "[Epoch 61/1001] [Batch 75/372] [D loss: 0.6876428127288818] [G loss: 0.6609686613082886]\n",
      "[Epoch 61/1001] [Batch 76/372] [D loss: 0.6886803507804871] [G loss: 0.688874363899231]\n",
      "[Epoch 61/1001] [Batch 77/372] [D loss: 0.6875016093254089] [G loss: 0.6742753386497498]\n",
      "[Epoch 61/1001] [Batch 78/372] [D loss: 0.6916043758392334] [G loss: 0.6851114630699158]\n",
      "[Epoch 61/1001] [Batch 79/372] [D loss: 0.6890405416488647] [G loss: 0.6817417740821838]\n",
      "[Epoch 61/1001] [Batch 80/372] [D loss: 0.6886922717094421] [G loss: 0.6698819398880005]\n",
      "[Epoch 61/1001] [Batch 81/372] [D loss: 0.6938527822494507] [G loss: 0.6748385429382324]\n",
      "[Epoch 61/1001] [Batch 82/372] [D loss: 0.6884814500808716] [G loss: 0.6900913715362549]\n",
      "[Epoch 61/1001] [Batch 83/372] [D loss: 0.6857468485832214] [G loss: 0.6488542556762695]\n",
      "[Epoch 61/1001] [Batch 84/372] [D loss: 0.6886873245239258] [G loss: 0.7105637788772583]\n",
      "[Epoch 61/1001] [Batch 85/372] [D loss: 0.690874457359314] [G loss: 0.6535922884941101]\n",
      "[Epoch 61/1001] [Batch 86/372] [D loss: 0.6819673180580139] [G loss: 0.6802577376365662]\n",
      "[Epoch 61/1001] [Batch 87/372] [D loss: 0.6853692531585693] [G loss: 0.6844025254249573]\n",
      "[Epoch 61/1001] [Batch 88/372] [D loss: 0.6991702914237976] [G loss: 0.7086234092712402]\n",
      "[Epoch 61/1001] [Batch 89/372] [D loss: 0.6878550052642822] [G loss: 0.6299846172332764]\n",
      "[Epoch 61/1001] [Batch 90/372] [D loss: 0.6930730938911438] [G loss: 0.7439190745353699]\n",
      "[Epoch 61/1001] [Batch 91/372] [D loss: 0.6913710832595825] [G loss: 0.6111599206924438]\n",
      "[Epoch 61/1001] [Batch 92/372] [D loss: 0.6909604072570801] [G loss: 0.7630237936973572]\n",
      "[Epoch 61/1001] [Batch 93/372] [D loss: 0.694025993347168] [G loss: 0.596432089805603]\n",
      "[Epoch 61/1001] [Batch 94/372] [D loss: 0.692972719669342] [G loss: 0.7695046067237854]\n",
      "[Epoch 61/1001] [Batch 95/372] [D loss: 0.690392255783081] [G loss: 0.6047545671463013]\n",
      "[Epoch 61/1001] [Batch 96/372] [D loss: 0.6890803575515747] [G loss: 0.7532723546028137]\n",
      "[Epoch 61/1001] [Batch 97/372] [D loss: 0.6844188570976257] [G loss: 0.6168339848518372]\n",
      "[Epoch 61/1001] [Batch 98/372] [D loss: 0.6929811239242554] [G loss: 0.7584916353225708]\n",
      "[Epoch 61/1001] [Batch 99/372] [D loss: 0.6938743591308594] [G loss: 0.5878434181213379]\n",
      "[Epoch 61/1001] [Batch 100/372] [D loss: 0.693903923034668] [G loss: 0.801770806312561]\n",
      "[Epoch 61/1001] [Batch 101/372] [D loss: 0.6978702545166016] [G loss: 0.5414472222328186]\n",
      "[Epoch 61/1001] [Batch 102/372] [D loss: 0.7047461271286011] [G loss: 0.8787360787391663]\n",
      "[Epoch 61/1001] [Batch 103/372] [D loss: 0.702181339263916] [G loss: 0.5276318788528442]\n",
      "[Epoch 61/1001] [Batch 104/372] [D loss: 0.7002670764923096] [G loss: 0.8547492027282715]\n",
      "[Epoch 61/1001] [Batch 105/372] [D loss: 0.6984880566596985] [G loss: 0.577324628829956]\n",
      "[Epoch 61/1001] [Batch 106/372] [D loss: 0.6958681344985962] [G loss: 0.7328841686248779]\n",
      "[Epoch 61/1001] [Batch 107/372] [D loss: 0.6900972127914429] [G loss: 0.6726798415184021]\n",
      "[Epoch 61/1001] [Batch 108/372] [D loss: 0.6822605133056641] [G loss: 0.6617305874824524]\n",
      "[Epoch 61/1001] [Batch 109/372] [D loss: 0.6909863352775574] [G loss: 0.7061008810997009]\n",
      "[Epoch 61/1001] [Batch 110/372] [D loss: 0.6925842761993408] [G loss: 0.6626471877098083]\n",
      "[Epoch 61/1001] [Batch 111/372] [D loss: 0.6880388259887695] [G loss: 0.6807090640068054]\n",
      "[Epoch 61/1001] [Batch 112/372] [D loss: 0.6915799379348755] [G loss: 0.7042768001556396]\n",
      "[Epoch 61/1001] [Batch 113/372] [D loss: 0.6946782469749451] [G loss: 0.6471513509750366]\n",
      "[Epoch 61/1001] [Batch 114/372] [D loss: 0.6908949017524719] [G loss: 0.7032384276390076]\n",
      "[Epoch 61/1001] [Batch 115/372] [D loss: 0.6894013285636902] [G loss: 0.667059063911438]\n",
      "[Epoch 61/1001] [Batch 116/372] [D loss: 0.6961089372634888] [G loss: 0.681085467338562]\n",
      "[Epoch 61/1001] [Batch 117/372] [D loss: 0.6880797147750854] [G loss: 0.6722610592842102]\n",
      "[Epoch 61/1001] [Batch 118/372] [D loss: 0.6875481605529785] [G loss: 0.6933085918426514]\n",
      "[Epoch 61/1001] [Batch 119/372] [D loss: 0.6902273893356323] [G loss: 0.6778319478034973]\n",
      "[Epoch 61/1001] [Batch 120/372] [D loss: 0.6887824535369873] [G loss: 0.679253101348877]\n",
      "[Epoch 61/1001] [Batch 121/372] [D loss: 0.6877508163452148] [G loss: 0.6810078620910645]\n",
      "[Epoch 61/1001] [Batch 122/372] [D loss: 0.689621090888977] [G loss: 0.6873801350593567]\n",
      "[Epoch 61/1001] [Batch 123/372] [D loss: 0.6851179599761963] [G loss: 0.6746034622192383]\n",
      "[Epoch 61/1001] [Batch 124/372] [D loss: 0.688325822353363] [G loss: 0.6944112777709961]\n",
      "[Epoch 61/1001] [Batch 125/372] [D loss: 0.6907563209533691] [G loss: 0.6607497334480286]\n",
      "[Epoch 61/1001] [Batch 126/372] [D loss: 0.6853652000427246] [G loss: 0.6991162896156311]\n",
      "[Epoch 61/1001] [Batch 127/372] [D loss: 0.6875998973846436] [G loss: 0.6720741987228394]\n",
      "[Epoch 61/1001] [Batch 128/372] [D loss: 0.6870625019073486] [G loss: 0.6816596984863281]\n",
      "[Epoch 61/1001] [Batch 129/372] [D loss: 0.6860519051551819] [G loss: 0.6922649145126343]\n",
      "[Epoch 61/1001] [Batch 130/372] [D loss: 0.6957817077636719] [G loss: 0.6542471051216125]\n",
      "[Epoch 61/1001] [Batch 131/372] [D loss: 0.6846596002578735] [G loss: 0.7140930891036987]\n",
      "[Epoch 61/1001] [Batch 132/372] [D loss: 0.6885180473327637] [G loss: 0.644300103187561]\n",
      "[Epoch 61/1001] [Batch 133/372] [D loss: 0.6884214878082275] [G loss: 0.7206693291664124]\n",
      "[Epoch 61/1001] [Batch 134/372] [D loss: 0.6880303621292114] [G loss: 0.643887996673584]\n",
      "[Epoch 61/1001] [Batch 135/372] [D loss: 0.6862033605575562] [G loss: 0.7268587350845337]\n",
      "[Epoch 61/1001] [Batch 136/372] [D loss: 0.6930899620056152] [G loss: 0.6342377066612244]\n",
      "[Epoch 61/1001] [Batch 137/372] [D loss: 0.6918531656265259] [G loss: 0.7034325003623962]\n",
      "[Epoch 61/1001] [Batch 138/372] [D loss: 0.6900650262832642] [G loss: 0.6597771048545837]\n",
      "[Epoch 61/1001] [Batch 139/372] [D loss: 0.6880557537078857] [G loss: 0.7052587866783142]\n",
      "[Epoch 61/1001] [Batch 140/372] [D loss: 0.6885472536087036] [G loss: 0.6503332257270813]\n",
      "[Epoch 61/1001] [Batch 141/372] [D loss: 0.6910418272018433] [G loss: 0.7087979316711426]\n",
      "[Epoch 61/1001] [Batch 142/372] [D loss: 0.6892473697662354] [G loss: 0.6450686454772949]\n",
      "[Epoch 61/1001] [Batch 143/372] [D loss: 0.68937087059021] [G loss: 0.7140514850616455]\n",
      "[Epoch 61/1001] [Batch 144/372] [D loss: 0.6917858123779297] [G loss: 0.6355254650115967]\n",
      "[Epoch 61/1001] [Batch 145/372] [D loss: 0.6879785060882568] [G loss: 0.7405209541320801]\n",
      "[Epoch 61/1001] [Batch 146/372] [D loss: 0.6932944655418396] [G loss: 0.608601987361908]\n",
      "[Epoch 61/1001] [Batch 147/372] [D loss: 0.6930814981460571] [G loss: 0.7747615575790405]\n",
      "[Epoch 61/1001] [Batch 148/372] [D loss: 0.6885205507278442] [G loss: 0.5893259644508362]\n",
      "[Epoch 61/1001] [Batch 149/372] [D loss: 0.6912920475006104] [G loss: 0.7874236702919006]\n",
      "[Epoch 61/1001] [Batch 150/372] [D loss: 0.6956993341445923] [G loss: 0.5791462659835815]\n",
      "[Epoch 61/1001] [Batch 151/372] [D loss: 0.6947089433670044] [G loss: 0.7965474128723145]\n",
      "[Epoch 61/1001] [Batch 152/372] [D loss: 0.6937086582183838] [G loss: 0.5728114247322083]\n",
      "[Epoch 61/1001] [Batch 153/372] [D loss: 0.6967236995697021] [G loss: 0.803375244140625]\n",
      "[Epoch 61/1001] [Batch 154/372] [D loss: 0.6957594156265259] [G loss: 0.5669390559196472]\n",
      "[Epoch 61/1001] [Batch 155/372] [D loss: 0.6961601376533508] [G loss: 0.8066109418869019]\n",
      "[Epoch 61/1001] [Batch 156/372] [D loss: 0.6985282897949219] [G loss: 0.5612457394599915]\n",
      "[Epoch 61/1001] [Batch 157/372] [D loss: 0.6982123851776123] [G loss: 0.7964420318603516]\n",
      "[Epoch 61/1001] [Batch 158/372] [D loss: 0.6956716775894165] [G loss: 0.5777725577354431]\n",
      "[Epoch 61/1001] [Batch 159/372] [D loss: 0.6919987201690674] [G loss: 0.7770476341247559]\n",
      "[Epoch 61/1001] [Batch 160/372] [D loss: 0.7006077766418457] [G loss: 0.5848361849784851]\n",
      "[Epoch 61/1001] [Batch 161/372] [D loss: 0.6890987157821655] [G loss: 0.7607887983322144]\n",
      "[Epoch 61/1001] [Batch 162/372] [D loss: 0.688575804233551] [G loss: 0.6364973187446594]\n",
      "[Epoch 61/1001] [Batch 163/372] [D loss: 0.6925671100616455] [G loss: 0.7113907337188721]\n",
      "[Epoch 61/1001] [Batch 164/372] [D loss: 0.688511848449707] [G loss: 0.6634506583213806]\n",
      "[Epoch 61/1001] [Batch 165/372] [D loss: 0.6896278858184814] [G loss: 0.6914228796958923]\n",
      "[Epoch 61/1001] [Batch 166/372] [D loss: 0.6881624460220337] [G loss: 0.6645243763923645]\n",
      "[Epoch 61/1001] [Batch 167/372] [D loss: 0.685754656791687] [G loss: 0.6796034574508667]\n",
      "[Epoch 61/1001] [Batch 168/372] [D loss: 0.6912157535552979] [G loss: 0.6855128407478333]\n",
      "[Epoch 61/1001] [Batch 169/372] [D loss: 0.6879854202270508] [G loss: 0.6888052225112915]\n",
      "[Epoch 61/1001] [Batch 170/372] [D loss: 0.6874812841415405] [G loss: 0.6831621527671814]\n",
      "[Epoch 61/1001] [Batch 171/372] [D loss: 0.6901755332946777] [G loss: 0.6720057725906372]\n",
      "[Epoch 61/1001] [Batch 172/372] [D loss: 0.6946314573287964] [G loss: 0.6840257048606873]\n",
      "[Epoch 61/1001] [Batch 173/372] [D loss: 0.6889051198959351] [G loss: 0.6760731935501099]\n",
      "[Epoch 61/1001] [Batch 174/372] [D loss: 0.6910356283187866] [G loss: 0.6707701683044434]\n",
      "[Epoch 61/1001] [Batch 175/372] [D loss: 0.6848831176757812] [G loss: 0.6812120676040649]\n",
      "[Epoch 61/1001] [Batch 176/372] [D loss: 0.6881527304649353] [G loss: 0.6636974215507507]\n",
      "[Epoch 61/1001] [Batch 177/372] [D loss: 0.6843056678771973] [G loss: 0.699025571346283]\n",
      "[Epoch 61/1001] [Batch 178/372] [D loss: 0.6905208826065063] [G loss: 0.6829037666320801]\n",
      "[Epoch 61/1001] [Batch 179/372] [D loss: 0.6875177621841431] [G loss: 0.6485323905944824]\n",
      "[Epoch 61/1001] [Batch 180/372] [D loss: 0.6901955604553223] [G loss: 0.726544976234436]\n",
      "[Epoch 61/1001] [Batch 181/372] [D loss: 0.687440812587738] [G loss: 0.6273422241210938]\n",
      "[Epoch 61/1001] [Batch 182/372] [D loss: 0.6867474913597107] [G loss: 0.7296604514122009]\n",
      "[Epoch 61/1001] [Batch 183/372] [D loss: 0.6891658902168274] [G loss: 0.6556787490844727]\n",
      "[Epoch 61/1001] [Batch 184/372] [D loss: 0.6851674318313599] [G loss: 0.6872632503509521]\n",
      "[Epoch 61/1001] [Batch 185/372] [D loss: 0.6906942129135132] [G loss: 0.6690724492073059]\n",
      "[Epoch 61/1001] [Batch 186/372] [D loss: 0.6891382932662964] [G loss: 0.6780290603637695]\n",
      "[Epoch 61/1001] [Batch 187/372] [D loss: 0.6873025894165039] [G loss: 0.6871863603591919]\n",
      "[Epoch 61/1001] [Batch 188/372] [D loss: 0.6900366544723511] [G loss: 0.6510785222053528]\n",
      "[Epoch 61/1001] [Batch 189/372] [D loss: 0.6946539878845215] [G loss: 0.6952323317527771]\n",
      "[Epoch 61/1001] [Batch 190/372] [D loss: 0.6953191161155701] [G loss: 0.6274398565292358]\n",
      "[Epoch 61/1001] [Batch 191/372] [D loss: 0.6890060901641846] [G loss: 0.7603262662887573]\n",
      "[Epoch 61/1001] [Batch 192/372] [D loss: 0.6947126388549805] [G loss: 0.5723748803138733]\n",
      "[Epoch 61/1001] [Batch 193/372] [D loss: 0.691012442111969] [G loss: 0.8404996395111084]\n",
      "[Epoch 61/1001] [Batch 194/372] [D loss: 0.6926999092102051] [G loss: 0.520771861076355]\n",
      "[Epoch 61/1001] [Batch 195/372] [D loss: 0.7022789716720581] [G loss: 0.9022789001464844]\n",
      "[Epoch 61/1001] [Batch 196/372] [D loss: 0.7056287527084351] [G loss: 0.510999858379364]\n",
      "[Epoch 61/1001] [Batch 197/372] [D loss: 0.7075985074043274] [G loss: 0.8724719285964966]\n",
      "[Epoch 61/1001] [Batch 198/372] [D loss: 0.7036715149879456] [G loss: 0.5658767819404602]\n",
      "[Epoch 61/1001] [Batch 199/372] [D loss: 0.6965951919555664] [G loss: 0.7551122903823853]\n",
      "[Epoch 61/1001] [Batch 200/372] [D loss: 0.6940047740936279] [G loss: 0.6548397541046143]\n",
      "[Epoch 61/1001] [Batch 201/372] [D loss: 0.6913875341415405] [G loss: 0.6974284052848816]\n",
      "[Epoch 61/1001] [Batch 202/372] [D loss: 0.6975057721138] [G loss: 0.6633045077323914]\n",
      "[Epoch 61/1001] [Batch 203/372] [D loss: 0.6917537450790405] [G loss: 0.6672577261924744]\n",
      "[Epoch 61/1001] [Batch 204/372] [D loss: 0.6917349100112915] [G loss: 0.6842881441116333]\n",
      "[Epoch 61/1001] [Batch 205/372] [D loss: 0.6896460652351379] [G loss: 0.6730754375457764]\n",
      "[Epoch 61/1001] [Batch 206/372] [D loss: 0.688557505607605] [G loss: 0.6779546737670898]\n",
      "[Epoch 61/1001] [Batch 207/372] [D loss: 0.6889041662216187] [G loss: 0.6789212226867676]\n",
      "[Epoch 61/1001] [Batch 208/372] [D loss: 0.69334876537323] [G loss: 0.6628625988960266]\n",
      "[Epoch 61/1001] [Batch 209/372] [D loss: 0.6844310760498047] [G loss: 0.6922929286956787]\n",
      "[Epoch 61/1001] [Batch 210/372] [D loss: 0.6907883882522583] [G loss: 0.6715824007987976]\n",
      "[Epoch 61/1001] [Batch 211/372] [D loss: 0.6931809186935425] [G loss: 0.6817220449447632]\n",
      "[Epoch 61/1001] [Batch 212/372] [D loss: 0.6867021918296814] [G loss: 0.6807276606559753]\n",
      "[Epoch 61/1001] [Batch 213/372] [D loss: 0.6859853267669678] [G loss: 0.6667419672012329]\n",
      "[Epoch 61/1001] [Batch 214/372] [D loss: 0.6924237608909607] [G loss: 0.6878681182861328]\n",
      "[Epoch 61/1001] [Batch 215/372] [D loss: 0.6966310143470764] [G loss: 0.6687896251678467]\n",
      "[Epoch 61/1001] [Batch 216/372] [D loss: 0.6875165700912476] [G loss: 0.6643504500389099]\n",
      "[Epoch 61/1001] [Batch 217/372] [D loss: 0.6834006905555725] [G loss: 0.7040777206420898]\n",
      "[Epoch 61/1001] [Batch 218/372] [D loss: 0.6892057657241821] [G loss: 0.6431277394294739]\n",
      "[Epoch 61/1001] [Batch 219/372] [D loss: 0.690188467502594] [G loss: 0.7463092803955078]\n",
      "[Epoch 61/1001] [Batch 220/372] [D loss: 0.6875332593917847] [G loss: 0.5965175628662109]\n",
      "[Epoch 61/1001] [Batch 221/372] [D loss: 0.692280650138855] [G loss: 0.8006121516227722]\n",
      "[Epoch 61/1001] [Batch 222/372] [D loss: 0.6979501247406006] [G loss: 0.5589485764503479]\n",
      "[Epoch 61/1001] [Batch 223/372] [D loss: 0.6974800825119019] [G loss: 0.8153586983680725]\n",
      "[Epoch 61/1001] [Batch 224/372] [D loss: 0.6939746737480164] [G loss: 0.5628014206886292]\n",
      "[Epoch 61/1001] [Batch 225/372] [D loss: 0.7019367218017578] [G loss: 0.8114641904830933]\n",
      "[Epoch 61/1001] [Batch 226/372] [D loss: 0.69469153881073] [G loss: 0.5792950391769409]\n",
      "[Epoch 61/1001] [Batch 227/372] [D loss: 0.7017094492912292] [G loss: 0.7713736891746521]\n",
      "[Epoch 61/1001] [Batch 228/372] [D loss: 0.6970603466033936] [G loss: 0.6061626672744751]\n",
      "[Epoch 61/1001] [Batch 229/372] [D loss: 0.6954292058944702] [G loss: 0.7276160717010498]\n",
      "[Epoch 61/1001] [Batch 230/372] [D loss: 0.6920636296272278] [G loss: 0.663701057434082]\n",
      "[Epoch 61/1001] [Batch 231/372] [D loss: 0.6920233964920044] [G loss: 0.6774798035621643]\n",
      "[Epoch 61/1001] [Batch 232/372] [D loss: 0.6928086876869202] [G loss: 0.6946918964385986]\n",
      "[Epoch 61/1001] [Batch 233/372] [D loss: 0.6837254762649536] [G loss: 0.6590167284011841]\n",
      "[Epoch 61/1001] [Batch 234/372] [D loss: 0.6867638826370239] [G loss: 0.6885173320770264]\n",
      "[Epoch 61/1001] [Batch 235/372] [D loss: 0.6923238039016724] [G loss: 0.6694830656051636]\n",
      "[Epoch 61/1001] [Batch 236/372] [D loss: 0.6913750767707825] [G loss: 0.6831331253051758]\n",
      "[Epoch 61/1001] [Batch 237/372] [D loss: 0.6968889236450195] [G loss: 0.6605042219161987]\n",
      "[Epoch 61/1001] [Batch 238/372] [D loss: 0.68904709815979] [G loss: 0.68636554479599]\n",
      "[Epoch 61/1001] [Batch 239/372] [D loss: 0.68687903881073] [G loss: 0.6849766969680786]\n",
      "[Epoch 61/1001] [Batch 240/372] [D loss: 0.6907910108566284] [G loss: 0.6588088274002075]\n",
      "[Epoch 61/1001] [Batch 241/372] [D loss: 0.6866863369941711] [G loss: 0.7103719711303711]\n",
      "[Epoch 61/1001] [Batch 242/372] [D loss: 0.6898685693740845] [G loss: 0.6355676651000977]\n",
      "[Epoch 61/1001] [Batch 243/372] [D loss: 0.6881586313247681] [G loss: 0.739659309387207]\n",
      "[Epoch 61/1001] [Batch 244/372] [D loss: 0.6905199289321899] [G loss: 0.5980179309844971]\n",
      "[Epoch 61/1001] [Batch 245/372] [D loss: 0.6909301280975342] [G loss: 0.7916306853294373]\n",
      "[Epoch 61/1001] [Batch 246/372] [D loss: 0.6923097372055054] [G loss: 0.5539973974227905]\n",
      "[Epoch 61/1001] [Batch 247/372] [D loss: 0.6931300759315491] [G loss: 0.8390007615089417]\n",
      "[Epoch 61/1001] [Batch 248/372] [D loss: 0.7024680376052856] [G loss: 0.4994208812713623]\n",
      "[Epoch 61/1001] [Batch 249/372] [D loss: 0.7168697118759155] [G loss: 0.961929440498352]\n",
      "[Epoch 61/1001] [Batch 250/372] [D loss: 0.7232744693756104] [G loss: 0.4649215340614319]\n",
      "[Epoch 61/1001] [Batch 251/372] [D loss: 0.7175001502037048] [G loss: 0.865525484085083]\n",
      "[Epoch 61/1001] [Batch 252/372] [D loss: 0.7009392976760864] [G loss: 0.6121188402175903]\n",
      "[Epoch 61/1001] [Batch 253/372] [D loss: 0.6928309202194214] [G loss: 0.6866615414619446]\n",
      "[Epoch 61/1001] [Batch 254/372] [D loss: 0.694861888885498] [G loss: 0.6964075565338135]\n",
      "[Epoch 61/1001] [Batch 255/372] [D loss: 0.6888121962547302] [G loss: 0.6515452861785889]\n",
      "[Epoch 61/1001] [Batch 256/372] [D loss: 0.6924160718917847] [G loss: 0.7020387649536133]\n",
      "[Epoch 61/1001] [Batch 257/372] [D loss: 0.6877236366271973] [G loss: 0.6605764627456665]\n",
      "[Epoch 61/1001] [Batch 258/372] [D loss: 0.6952805519104004] [G loss: 0.675986647605896]\n",
      "[Epoch 61/1001] [Batch 259/372] [D loss: 0.6870877742767334] [G loss: 0.6918497085571289]\n",
      "[Epoch 61/1001] [Batch 260/372] [D loss: 0.6908015012741089] [G loss: 0.6699517965316772]\n",
      "[Epoch 61/1001] [Batch 261/372] [D loss: 0.6907083988189697] [G loss: 0.6780614852905273]\n",
      "[Epoch 61/1001] [Batch 262/372] [D loss: 0.6892762780189514] [G loss: 0.681079089641571]\n",
      "[Epoch 61/1001] [Batch 263/372] [D loss: 0.6910474300384521] [G loss: 0.6736124753952026]\n",
      "[Epoch 61/1001] [Batch 264/372] [D loss: 0.688705325126648] [G loss: 0.6647447943687439]\n",
      "[Epoch 61/1001] [Batch 265/372] [D loss: 0.6954216957092285] [G loss: 0.7236066460609436]\n",
      "[Epoch 61/1001] [Batch 266/372] [D loss: 0.6860069036483765] [G loss: 0.6175292730331421]\n",
      "[Epoch 61/1001] [Batch 267/372] [D loss: 0.6921474933624268] [G loss: 0.7652026414871216]\n",
      "[Epoch 61/1001] [Batch 268/372] [D loss: 0.696264386177063] [G loss: 0.6084229350090027]\n",
      "[Epoch 61/1001] [Batch 269/372] [D loss: 0.6916279792785645] [G loss: 0.7347796559333801]\n",
      "[Epoch 61/1001] [Batch 270/372] [D loss: 0.6906683444976807] [G loss: 0.6473844647407532]\n",
      "[Epoch 61/1001] [Batch 271/372] [D loss: 0.6871145963668823] [G loss: 0.7147966623306274]\n",
      "[Epoch 61/1001] [Batch 272/372] [D loss: 0.6869996786117554] [G loss: 0.6549172401428223]\n",
      "[Epoch 61/1001] [Batch 273/372] [D loss: 0.6894558072090149] [G loss: 0.7055374979972839]\n",
      "[Epoch 61/1001] [Batch 274/372] [D loss: 0.6884213089942932] [G loss: 0.68006831407547]\n",
      "[Epoch 61/1001] [Batch 275/372] [D loss: 0.6877267360687256] [G loss: 0.665468156337738]\n",
      "[Epoch 61/1001] [Batch 276/372] [D loss: 0.6870445013046265] [G loss: 0.6972799301147461]\n",
      "[Epoch 61/1001] [Batch 277/372] [D loss: 0.6875472068786621] [G loss: 0.667537271976471]\n",
      "[Epoch 61/1001] [Batch 278/372] [D loss: 0.687721848487854] [G loss: 0.6900317668914795]\n",
      "[Epoch 61/1001] [Batch 279/372] [D loss: 0.6893987059593201] [G loss: 0.6624571681022644]\n",
      "[Epoch 61/1001] [Batch 280/372] [D loss: 0.6843023300170898] [G loss: 0.6765434741973877]\n",
      "[Epoch 61/1001] [Batch 281/372] [D loss: 0.6884386539459229] [G loss: 0.6946113705635071]\n",
      "[Epoch 61/1001] [Batch 282/372] [D loss: 0.6929740905761719] [G loss: 0.6636847257614136]\n",
      "[Epoch 61/1001] [Batch 283/372] [D loss: 0.6825814247131348] [G loss: 0.6843379735946655]\n",
      "[Epoch 61/1001] [Batch 284/372] [D loss: 0.6817657947540283] [G loss: 0.6922532916069031]\n",
      "[Epoch 61/1001] [Batch 285/372] [D loss: 0.6903367042541504] [G loss: 0.6413979530334473]\n",
      "[Epoch 61/1001] [Batch 286/372] [D loss: 0.6931912899017334] [G loss: 0.7443355321884155]\n",
      "[Epoch 61/1001] [Batch 287/372] [D loss: 0.6945464611053467] [G loss: 0.5914961099624634]\n",
      "[Epoch 61/1001] [Batch 288/372] [D loss: 0.6887289881706238] [G loss: 0.7898439764976501]\n",
      "[Epoch 61/1001] [Batch 289/372] [D loss: 0.6944270133972168] [G loss: 0.5716087818145752]\n",
      "[Epoch 61/1001] [Batch 290/372] [D loss: 0.6926332712173462] [G loss: 0.7967323064804077]\n",
      "[Epoch 61/1001] [Batch 291/372] [D loss: 0.6959401369094849] [G loss: 0.5792811512947083]\n",
      "[Epoch 61/1001] [Batch 292/372] [D loss: 0.691441535949707] [G loss: 0.7759219408035278]\n",
      "[Epoch 61/1001] [Batch 293/372] [D loss: 0.6924843788146973] [G loss: 0.6048085689544678]\n",
      "[Epoch 61/1001] [Batch 294/372] [D loss: 0.6879768371582031] [G loss: 0.7463618516921997]\n",
      "[Epoch 61/1001] [Batch 295/372] [D loss: 0.692223072052002] [G loss: 0.6106857061386108]\n",
      "[Epoch 61/1001] [Batch 296/372] [D loss: 0.6989681720733643] [G loss: 0.7642625570297241]\n",
      "[Epoch 61/1001] [Batch 297/372] [D loss: 0.6891310214996338] [G loss: 0.607783317565918]\n",
      "[Epoch 61/1001] [Batch 298/372] [D loss: 0.6935168504714966] [G loss: 0.740023136138916]\n",
      "[Epoch 61/1001] [Batch 299/372] [D loss: 0.689620852470398] [G loss: 0.6346340775489807]\n",
      "[Epoch 61/1001] [Batch 300/372] [D loss: 0.6909693479537964] [G loss: 0.7272996306419373]\n",
      "[Epoch 61/1001] [Batch 301/372] [D loss: 0.6901177763938904] [G loss: 0.610385537147522]\n",
      "[Epoch 61/1001] [Batch 302/372] [D loss: 0.6993531584739685] [G loss: 0.7456783056259155]\n",
      "[Epoch 61/1001] [Batch 303/372] [D loss: 0.6925148367881775] [G loss: 0.6203100681304932]\n",
      "[Epoch 61/1001] [Batch 304/372] [D loss: 0.6917788982391357] [G loss: 0.7196456789970398]\n",
      "[Epoch 61/1001] [Batch 305/372] [D loss: 0.6936513185501099] [G loss: 0.6441447734832764]\n",
      "[Epoch 61/1001] [Batch 306/372] [D loss: 0.6904715895652771] [G loss: 0.7120720148086548]\n",
      "[Epoch 61/1001] [Batch 307/372] [D loss: 0.6887641549110413] [G loss: 0.654280960559845]\n",
      "[Epoch 61/1001] [Batch 308/372] [D loss: 0.6869783997535706] [G loss: 0.708449125289917]\n",
      "[Epoch 61/1001] [Batch 309/372] [D loss: 0.692071795463562] [G loss: 0.6380103826522827]\n",
      "[Epoch 61/1001] [Batch 310/372] [D loss: 0.6891483068466187] [G loss: 0.7242380976676941]\n",
      "[Epoch 61/1001] [Batch 311/372] [D loss: 0.6911451816558838] [G loss: 0.6071411371231079]\n",
      "[Epoch 61/1001] [Batch 312/372] [D loss: 0.6903417110443115] [G loss: 0.7643356323242188]\n",
      "[Epoch 61/1001] [Batch 313/372] [D loss: 0.6952214241027832] [G loss: 0.585545539855957]\n",
      "[Epoch 61/1001] [Batch 314/372] [D loss: 0.6936854720115662] [G loss: 0.7901264429092407]\n",
      "[Epoch 61/1001] [Batch 315/372] [D loss: 0.6955898404121399] [G loss: 0.5824083089828491]\n",
      "[Epoch 61/1001] [Batch 316/372] [D loss: 0.6920437812805176] [G loss: 0.7722717523574829]\n",
      "[Epoch 61/1001] [Batch 317/372] [D loss: 0.6912052631378174] [G loss: 0.6228827834129333]\n",
      "[Epoch 61/1001] [Batch 318/372] [D loss: 0.6913683414459229] [G loss: 0.7044186592102051]\n",
      "[Epoch 61/1001] [Batch 319/372] [D loss: 0.6837056875228882] [G loss: 0.6642800569534302]\n",
      "[Epoch 61/1001] [Batch 320/372] [D loss: 0.6865750551223755] [G loss: 0.7061618566513062]\n",
      "[Epoch 61/1001] [Batch 321/372] [D loss: 0.6908336877822876] [G loss: 0.6494161486625671]\n",
      "[Epoch 61/1001] [Batch 322/372] [D loss: 0.6938306093215942] [G loss: 0.7196967601776123]\n",
      "[Epoch 61/1001] [Batch 323/372] [D loss: 0.6965414881706238] [G loss: 0.627448558807373]\n",
      "[Epoch 61/1001] [Batch 324/372] [D loss: 0.6899334788322449] [G loss: 0.7268197536468506]\n",
      "[Epoch 61/1001] [Batch 325/372] [D loss: 0.6977125406265259] [G loss: 0.6385502815246582]\n",
      "[Epoch 61/1001] [Batch 326/372] [D loss: 0.6920521259307861] [G loss: 0.7103539705276489]\n",
      "[Epoch 61/1001] [Batch 327/372] [D loss: 0.6865779161453247] [G loss: 0.6474103331565857]\n",
      "[Epoch 61/1001] [Batch 328/372] [D loss: 0.6908685564994812] [G loss: 0.7101160287857056]\n",
      "[Epoch 61/1001] [Batch 329/372] [D loss: 0.6926378011703491] [G loss: 0.6398845911026001]\n",
      "[Epoch 61/1001] [Batch 330/372] [D loss: 0.6850889921188354] [G loss: 0.7768809199333191]\n",
      "[Epoch 61/1001] [Batch 331/372] [D loss: 0.6931276917457581] [G loss: 0.5689115524291992]\n",
      "[Epoch 61/1001] [Batch 332/372] [D loss: 0.6974228024482727] [G loss: 0.8143450021743774]\n",
      "[Epoch 61/1001] [Batch 333/372] [D loss: 0.6986699104309082] [G loss: 0.5516448616981506]\n",
      "[Epoch 61/1001] [Batch 334/372] [D loss: 0.698961615562439] [G loss: 0.8179664611816406]\n",
      "[Epoch 61/1001] [Batch 335/372] [D loss: 0.697186291217804] [G loss: 0.5881922245025635]\n",
      "[Epoch 61/1001] [Batch 336/372] [D loss: 0.6947762966156006] [G loss: 0.7439810633659363]\n",
      "[Epoch 61/1001] [Batch 337/372] [D loss: 0.6975421905517578] [G loss: 0.6424080729484558]\n",
      "[Epoch 61/1001] [Batch 338/372] [D loss: 0.6901900768280029] [G loss: 0.6975167989730835]\n",
      "[Epoch 61/1001] [Batch 339/372] [D loss: 0.6925709247589111] [G loss: 0.666115403175354]\n",
      "[Epoch 61/1001] [Batch 340/372] [D loss: 0.6903188824653625] [G loss: 0.68501877784729]\n",
      "[Epoch 61/1001] [Batch 341/372] [D loss: 0.6898549795150757] [G loss: 0.6754189729690552]\n",
      "[Epoch 61/1001] [Batch 342/372] [D loss: 0.6916952729225159] [G loss: 0.6810539960861206]\n",
      "[Epoch 61/1001] [Batch 343/372] [D loss: 0.688732922077179] [G loss: 0.6534401178359985]\n",
      "[Epoch 61/1001] [Batch 344/372] [D loss: 0.6892215013504028] [G loss: 0.7472769021987915]\n",
      "[Epoch 61/1001] [Batch 345/372] [D loss: 0.6898118257522583] [G loss: 0.5802842378616333]\n",
      "[Epoch 61/1001] [Batch 346/372] [D loss: 0.6993471384048462] [G loss: 0.8466486930847168]\n",
      "[Epoch 61/1001] [Batch 347/372] [D loss: 0.702153742313385] [G loss: 0.502345085144043]\n",
      "[Epoch 61/1001] [Batch 348/372] [D loss: 0.709083080291748] [G loss: 0.898979902267456]\n",
      "[Epoch 61/1001] [Batch 349/372] [D loss: 0.7082018256187439] [G loss: 0.540564775466919]\n",
      "[Epoch 61/1001] [Batch 350/372] [D loss: 0.7026492953300476] [G loss: 0.7806620001792908]\n",
      "[Epoch 61/1001] [Batch 351/372] [D loss: 0.6921576261520386] [G loss: 0.6316757202148438]\n",
      "[Epoch 61/1001] [Batch 352/372] [D loss: 0.6922395825386047] [G loss: 0.7101854681968689]\n",
      "[Epoch 61/1001] [Batch 353/372] [D loss: 0.6899676322937012] [G loss: 0.6805027723312378]\n",
      "[Epoch 61/1001] [Batch 354/372] [D loss: 0.6884322166442871] [G loss: 0.6675059795379639]\n",
      "[Epoch 61/1001] [Batch 355/372] [D loss: 0.6871544718742371] [G loss: 0.6920774579048157]\n",
      "[Epoch 61/1001] [Batch 356/372] [D loss: 0.6867227554321289] [G loss: 0.6754491925239563]\n",
      "[Epoch 61/1001] [Batch 357/372] [D loss: 0.6846461892127991] [G loss: 0.6816885471343994]\n",
      "[Epoch 61/1001] [Batch 358/372] [D loss: 0.6921435594558716] [G loss: 0.6814051270484924]\n",
      "[Epoch 61/1001] [Batch 359/372] [D loss: 0.6911258697509766] [G loss: 0.6858823299407959]\n",
      "[Epoch 61/1001] [Batch 360/372] [D loss: 0.6914633512496948] [G loss: 0.6468259692192078]\n",
      "[Epoch 61/1001] [Batch 361/372] [D loss: 0.6900890469551086] [G loss: 0.726077675819397]\n",
      "[Epoch 61/1001] [Batch 362/372] [D loss: 0.6942957639694214] [G loss: 0.638807475566864]\n",
      "[Epoch 61/1001] [Batch 363/372] [D loss: 0.6919770240783691] [G loss: 0.706021785736084]\n",
      "[Epoch 61/1001] [Batch 364/372] [D loss: 0.6865829229354858] [G loss: 0.6527255773544312]\n",
      "[Epoch 61/1001] [Batch 365/372] [D loss: 0.6928831934928894] [G loss: 0.6844190955162048]\n",
      "[Epoch 61/1001] [Batch 366/372] [D loss: 0.6923035383224487] [G loss: 0.677937924861908]\n",
      "[Epoch 61/1001] [Batch 367/372] [D loss: 0.6900238394737244] [G loss: 0.6842103600502014]\n",
      "[Epoch 61/1001] [Batch 368/372] [D loss: 0.6946368217468262] [G loss: 0.6630603075027466]\n",
      "[Epoch 61/1001] [Batch 369/372] [D loss: 0.6911147832870483] [G loss: 0.6943889856338501]\n",
      "[Epoch 61/1001] [Batch 370/372] [D loss: 0.6882200241088867] [G loss: 0.648076593875885]\n",
      "[Epoch 61/1001] [Batch 371/372] [D loss: 0.6887059807777405] [G loss: 0.7346795201301575]\n",
      "[Epoch 62/1001] [Batch 0/372] [D loss: 0.6918103694915771] [G loss: 0.62831050157547]\n",
      "[Epoch 62/1001] [Batch 1/372] [D loss: 0.685733437538147] [G loss: 0.7426857948303223]\n",
      "[Epoch 62/1001] [Batch 2/372] [D loss: 0.6839306354522705] [G loss: 0.6109716892242432]\n",
      "[Epoch 62/1001] [Batch 3/372] [D loss: 0.6924543976783752] [G loss: 0.7617653608322144]\n",
      "[Epoch 62/1001] [Batch 4/372] [D loss: 0.6943773627281189] [G loss: 0.6089903116226196]\n",
      "[Epoch 62/1001] [Batch 5/372] [D loss: 0.6900750994682312] [G loss: 0.751822292804718]\n",
      "[Epoch 62/1001] [Batch 6/372] [D loss: 0.6931481957435608] [G loss: 0.6184175610542297]\n",
      "[Epoch 62/1001] [Batch 7/372] [D loss: 0.6893911361694336] [G loss: 0.7232896089553833]\n",
      "[Epoch 62/1001] [Batch 8/372] [D loss: 0.6885026693344116] [G loss: 0.6621732711791992]\n",
      "[Epoch 62/1001] [Batch 9/372] [D loss: 0.6898055076599121] [G loss: 0.6860171556472778]\n",
      "[Epoch 62/1001] [Batch 10/372] [D loss: 0.6902024149894714] [G loss: 0.6715865731239319]\n",
      "[Epoch 62/1001] [Batch 11/372] [D loss: 0.6896634101867676] [G loss: 0.6934340000152588]\n",
      "[Epoch 62/1001] [Batch 12/372] [D loss: 0.6892398595809937] [G loss: 0.6723216772079468]\n",
      "[Epoch 62/1001] [Batch 13/372] [D loss: 0.6899075508117676] [G loss: 0.6807581782341003]\n",
      "[Epoch 62/1001] [Batch 14/372] [D loss: 0.6805161237716675] [G loss: 0.6901583075523376]\n",
      "[Epoch 62/1001] [Batch 15/372] [D loss: 0.6853753328323364] [G loss: 0.6858307123184204]\n",
      "[Epoch 62/1001] [Batch 16/372] [D loss: 0.689384937286377] [G loss: 0.6536180377006531]\n",
      "[Epoch 62/1001] [Batch 17/372] [D loss: 0.6871538758277893] [G loss: 0.6925480365753174]\n",
      "[Epoch 62/1001] [Batch 18/372] [D loss: 0.695118248462677] [G loss: 0.6939117908477783]\n",
      "[Epoch 62/1001] [Batch 19/372] [D loss: 0.6919732093811035] [G loss: 0.6360884308815002]\n",
      "[Epoch 62/1001] [Batch 20/372] [D loss: 0.6927191019058228] [G loss: 0.7302798628807068]\n",
      "[Epoch 62/1001] [Batch 21/372] [D loss: 0.6871721744537354] [G loss: 0.616675615310669]\n",
      "[Epoch 62/1001] [Batch 22/372] [D loss: 0.6929789781570435] [G loss: 0.7551792860031128]\n",
      "[Epoch 62/1001] [Batch 23/372] [D loss: 0.6911294460296631] [G loss: 0.5851207375526428]\n",
      "[Epoch 62/1001] [Batch 24/372] [D loss: 0.6967282295227051] [G loss: 0.7916836142539978]\n",
      "[Epoch 62/1001] [Batch 25/372] [D loss: 0.6926736235618591] [G loss: 0.5514919757843018]\n",
      "[Epoch 62/1001] [Batch 26/372] [D loss: 0.7049922943115234] [G loss: 0.8950294256210327]\n",
      "[Epoch 62/1001] [Batch 27/372] [D loss: 0.7042480707168579] [G loss: 0.5155496597290039]\n",
      "[Epoch 62/1001] [Batch 28/372] [D loss: 0.7074553370475769] [G loss: 0.8294382095336914]\n",
      "[Epoch 62/1001] [Batch 29/372] [D loss: 0.6997677087783813] [G loss: 0.5982720255851746]\n",
      "[Epoch 62/1001] [Batch 30/372] [D loss: 0.6963324546813965] [G loss: 0.7148199081420898]\n",
      "[Epoch 62/1001] [Batch 31/372] [D loss: 0.6862629652023315] [G loss: 0.6924425363540649]\n",
      "[Epoch 62/1001] [Batch 32/372] [D loss: 0.6884674429893494] [G loss: 0.6463333368301392]\n",
      "[Epoch 62/1001] [Batch 33/372] [D loss: 0.6896795034408569] [G loss: 0.7171434164047241]\n",
      "[Epoch 62/1001] [Batch 34/372] [D loss: 0.6896183490753174] [G loss: 0.6533553600311279]\n",
      "[Epoch 62/1001] [Batch 35/372] [D loss: 0.6836771368980408] [G loss: 0.685992956161499]\n",
      "[Epoch 62/1001] [Batch 36/372] [D loss: 0.691988468170166] [G loss: 0.6881971955299377]\n",
      "[Epoch 62/1001] [Batch 37/372] [D loss: 0.6887018084526062] [G loss: 0.6651426553726196]\n",
      "[Epoch 62/1001] [Batch 38/372] [D loss: 0.6908390522003174] [G loss: 0.698401927947998]\n",
      "[Epoch 62/1001] [Batch 39/372] [D loss: 0.688156247138977] [G loss: 0.6721214652061462]\n",
      "[Epoch 62/1001] [Batch 40/372] [D loss: 0.6870265007019043] [G loss: 0.688662052154541]\n",
      "[Epoch 62/1001] [Batch 41/372] [D loss: 0.690562903881073] [G loss: 0.6818876266479492]\n",
      "[Epoch 62/1001] [Batch 42/372] [D loss: 0.688396692276001] [G loss: 0.6739019155502319]\n",
      "[Epoch 62/1001] [Batch 43/372] [D loss: 0.6885817050933838] [G loss: 0.6731748580932617]\n",
      "[Epoch 62/1001] [Batch 44/372] [D loss: 0.6892936825752258] [G loss: 0.7001628875732422]\n",
      "[Epoch 62/1001] [Batch 45/372] [D loss: 0.6894484758377075] [G loss: 0.661487877368927]\n",
      "[Epoch 62/1001] [Batch 46/372] [D loss: 0.6893545389175415] [G loss: 0.7056856751441956]\n",
      "[Epoch 62/1001] [Batch 47/372] [D loss: 0.6838362216949463] [G loss: 0.6466304063796997]\n",
      "[Epoch 62/1001] [Batch 48/372] [D loss: 0.6907815933227539] [G loss: 0.7198448181152344]\n",
      "[Epoch 62/1001] [Batch 49/372] [D loss: 0.6888018250465393] [G loss: 0.6399359107017517]\n",
      "[Epoch 62/1001] [Batch 50/372] [D loss: 0.6849475502967834] [G loss: 0.7239014506340027]\n",
      "[Epoch 62/1001] [Batch 51/372] [D loss: 0.6878393888473511] [G loss: 0.6458023190498352]\n",
      "[Epoch 62/1001] [Batch 52/372] [D loss: 0.6902240514755249] [G loss: 0.7225476503372192]\n",
      "[Epoch 62/1001] [Batch 53/372] [D loss: 0.6869332790374756] [G loss: 0.6208909153938293]\n",
      "[Epoch 62/1001] [Batch 54/372] [D loss: 0.6943479776382446] [G loss: 0.7659681439399719]\n",
      "[Epoch 62/1001] [Batch 55/372] [D loss: 0.6931496262550354] [G loss: 0.5871873497962952]\n",
      "[Epoch 62/1001] [Batch 56/372] [D loss: 0.6984131932258606] [G loss: 0.7725192308425903]\n",
      "[Epoch 62/1001] [Batch 57/372] [D loss: 0.6944869756698608] [G loss: 0.598620593547821]\n",
      "[Epoch 62/1001] [Batch 58/372] [D loss: 0.6883519887924194] [G loss: 0.7595686912536621]\n",
      "[Epoch 62/1001] [Batch 59/372] [D loss: 0.6969331502914429] [G loss: 0.6104063391685486]\n",
      "[Epoch 62/1001] [Batch 60/372] [D loss: 0.6922891139984131] [G loss: 0.7328665852546692]\n",
      "[Epoch 62/1001] [Batch 61/372] [D loss: 0.6915364265441895] [G loss: 0.6400094628334045]\n",
      "[Epoch 62/1001] [Batch 62/372] [D loss: 0.6900263428688049] [G loss: 0.7171498537063599]\n",
      "[Epoch 62/1001] [Batch 63/372] [D loss: 0.6913630962371826] [G loss: 0.6361846327781677]\n",
      "[Epoch 62/1001] [Batch 64/372] [D loss: 0.6923178434371948] [G loss: 0.7184785604476929]\n",
      "[Epoch 62/1001] [Batch 65/372] [D loss: 0.6955177783966064] [G loss: 0.6403436064720154]\n",
      "[Epoch 62/1001] [Batch 66/372] [D loss: 0.6883220672607422] [G loss: 0.7163844108581543]\n",
      "[Epoch 62/1001] [Batch 67/372] [D loss: 0.6945475339889526] [G loss: 0.6473404765129089]\n",
      "[Epoch 62/1001] [Batch 68/372] [D loss: 0.6830039024353027] [G loss: 0.7098480463027954]\n",
      "[Epoch 62/1001] [Batch 69/372] [D loss: 0.6863587498664856] [G loss: 0.6639500260353088]\n",
      "[Epoch 62/1001] [Batch 70/372] [D loss: 0.6885713338851929] [G loss: 0.6784670948982239]\n",
      "[Epoch 62/1001] [Batch 71/372] [D loss: 0.6904013752937317] [G loss: 0.6977065205574036]\n",
      "[Epoch 62/1001] [Batch 72/372] [D loss: 0.6871582269668579] [G loss: 0.6651490330696106]\n",
      "[Epoch 62/1001] [Batch 73/372] [D loss: 0.6853033304214478] [G loss: 0.703168511390686]\n",
      "[Epoch 62/1001] [Batch 74/372] [D loss: 0.6941859722137451] [G loss: 0.6319143176078796]\n",
      "[Epoch 62/1001] [Batch 75/372] [D loss: 0.6909035444259644] [G loss: 0.7243051528930664]\n",
      "[Epoch 62/1001] [Batch 76/372] [D loss: 0.6834332942962646] [G loss: 0.6189245581626892]\n",
      "[Epoch 62/1001] [Batch 77/372] [D loss: 0.6916489601135254] [G loss: 0.7602696418762207]\n",
      "[Epoch 62/1001] [Batch 78/372] [D loss: 0.6916308403015137] [G loss: 0.6080003380775452]\n",
      "[Epoch 62/1001] [Batch 79/372] [D loss: 0.6912187337875366] [G loss: 0.7524548172950745]\n",
      "[Epoch 62/1001] [Batch 80/372] [D loss: 0.6901953220367432] [G loss: 0.6063846349716187]\n",
      "[Epoch 62/1001] [Batch 81/372] [D loss: 0.6915324926376343] [G loss: 0.7614745497703552]\n",
      "[Epoch 62/1001] [Batch 82/372] [D loss: 0.696111261844635] [G loss: 0.5908874273300171]\n",
      "[Epoch 62/1001] [Batch 83/372] [D loss: 0.6949813365936279] [G loss: 0.7560093998908997]\n",
      "[Epoch 62/1001] [Batch 84/372] [D loss: 0.6881937980651855] [G loss: 0.6221413016319275]\n",
      "[Epoch 62/1001] [Batch 85/372] [D loss: 0.6899644136428833] [G loss: 0.7220520973205566]\n",
      "[Epoch 62/1001] [Batch 86/372] [D loss: 0.6921846866607666] [G loss: 0.6468185186386108]\n",
      "[Epoch 62/1001] [Batch 87/372] [D loss: 0.6886012554168701] [G loss: 0.688328742980957]\n",
      "[Epoch 62/1001] [Batch 88/372] [D loss: 0.686429500579834] [G loss: 0.6701313257217407]\n",
      "[Epoch 62/1001] [Batch 89/372] [D loss: 0.6845988035202026] [G loss: 0.696530818939209]\n",
      "[Epoch 62/1001] [Batch 90/372] [D loss: 0.6896936297416687] [G loss: 0.6719034910202026]\n",
      "[Epoch 62/1001] [Batch 91/372] [D loss: 0.6905224323272705] [G loss: 0.6751130819320679]\n",
      "[Epoch 62/1001] [Batch 92/372] [D loss: 0.6862083673477173] [G loss: 0.6639789938926697]\n",
      "[Epoch 62/1001] [Batch 93/372] [D loss: 0.6842092275619507] [G loss: 0.704380989074707]\n",
      "[Epoch 62/1001] [Batch 94/372] [D loss: 0.6830170154571533] [G loss: 0.6740429401397705]\n",
      "[Epoch 62/1001] [Batch 95/372] [D loss: 0.6893696784973145] [G loss: 0.6831300854682922]\n",
      "[Epoch 62/1001] [Batch 96/372] [D loss: 0.690109372138977] [G loss: 0.6814298033714294]\n",
      "[Epoch 62/1001] [Batch 97/372] [D loss: 0.6900712251663208] [G loss: 0.669612467288971]\n",
      "[Epoch 62/1001] [Batch 98/372] [D loss: 0.6940690279006958] [G loss: 0.6776298880577087]\n",
      "[Epoch 62/1001] [Batch 99/372] [D loss: 0.6894457340240479] [G loss: 0.6766351461410522]\n",
      "[Epoch 62/1001] [Batch 100/372] [D loss: 0.686623752117157] [G loss: 0.6913809180259705]\n",
      "[Epoch 62/1001] [Batch 101/372] [D loss: 0.6900140643119812] [G loss: 0.6519787311553955]\n",
      "[Epoch 62/1001] [Batch 102/372] [D loss: 0.6982448697090149] [G loss: 0.7212353944778442]\n",
      "[Epoch 62/1001] [Batch 103/372] [D loss: 0.691605806350708] [G loss: 0.6237010955810547]\n",
      "[Epoch 62/1001] [Batch 104/372] [D loss: 0.6931656002998352] [G loss: 0.7457011938095093]\n",
      "[Epoch 62/1001] [Batch 105/372] [D loss: 0.6926427483558655] [G loss: 0.6070332527160645]\n",
      "[Epoch 62/1001] [Batch 106/372] [D loss: 0.6877667903900146] [G loss: 0.7706975936889648]\n",
      "[Epoch 62/1001] [Batch 107/372] [D loss: 0.6939455270767212] [G loss: 0.5787635445594788]\n",
      "[Epoch 62/1001] [Batch 108/372] [D loss: 0.6947493553161621] [G loss: 0.7850692868232727]\n",
      "[Epoch 62/1001] [Batch 109/372] [D loss: 0.6940237283706665] [G loss: 0.5925240516662598]\n",
      "[Epoch 62/1001] [Batch 110/372] [D loss: 0.6915087699890137] [G loss: 0.7463315725326538]\n",
      "[Epoch 62/1001] [Batch 111/372] [D loss: 0.6907014846801758] [G loss: 0.6212196350097656]\n",
      "[Epoch 62/1001] [Batch 112/372] [D loss: 0.6909713745117188] [G loss: 0.756917417049408]\n",
      "[Epoch 62/1001] [Batch 113/372] [D loss: 0.6919600963592529] [G loss: 0.5916191935539246]\n",
      "[Epoch 62/1001] [Batch 114/372] [D loss: 0.6901940703392029] [G loss: 0.7892860174179077]\n",
      "[Epoch 62/1001] [Batch 115/372] [D loss: 0.6973090767860413] [G loss: 0.5640350580215454]\n",
      "[Epoch 62/1001] [Batch 116/372] [D loss: 0.6938762664794922] [G loss: 0.8202993273735046]\n",
      "[Epoch 62/1001] [Batch 117/372] [D loss: 0.6921877861022949] [G loss: 0.5581749677658081]\n",
      "[Epoch 62/1001] [Batch 118/372] [D loss: 0.6992589831352234] [G loss: 0.807542085647583]\n",
      "[Epoch 62/1001] [Batch 119/372] [D loss: 0.6986287236213684] [G loss: 0.5869426727294922]\n",
      "[Epoch 62/1001] [Batch 120/372] [D loss: 0.6916143298149109] [G loss: 0.7534684538841248]\n",
      "[Epoch 62/1001] [Batch 121/372] [D loss: 0.6940068602561951] [G loss: 0.6399675607681274]\n",
      "[Epoch 62/1001] [Batch 122/372] [D loss: 0.6877944469451904] [G loss: 0.7025196552276611]\n",
      "[Epoch 62/1001] [Batch 123/372] [D loss: 0.6850181818008423] [G loss: 0.6708901524543762]\n",
      "[Epoch 62/1001] [Batch 124/372] [D loss: 0.6865500211715698] [G loss: 0.6734355688095093]\n",
      "[Epoch 62/1001] [Batch 125/372] [D loss: 0.6874010562896729] [G loss: 0.69331955909729]\n",
      "[Epoch 62/1001] [Batch 126/372] [D loss: 0.6870086193084717] [G loss: 0.6578052043914795]\n",
      "[Epoch 62/1001] [Batch 127/372] [D loss: 0.6946631669998169] [G loss: 0.7247582674026489]\n",
      "[Epoch 62/1001] [Batch 128/372] [D loss: 0.6929430365562439] [G loss: 0.6169823408126831]\n",
      "[Epoch 62/1001] [Batch 129/372] [D loss: 0.6964166164398193] [G loss: 0.7360206842422485]\n",
      "[Epoch 62/1001] [Batch 130/372] [D loss: 0.6899962425231934] [G loss: 0.6151648163795471]\n",
      "[Epoch 62/1001] [Batch 131/372] [D loss: 0.6881566047668457] [G loss: 0.7465251684188843]\n",
      "[Epoch 62/1001] [Batch 132/372] [D loss: 0.6888031363487244] [G loss: 0.6314724087715149]\n",
      "[Epoch 62/1001] [Batch 133/372] [D loss: 0.6849370002746582] [G loss: 0.7169197201728821]\n",
      "[Epoch 62/1001] [Batch 134/372] [D loss: 0.6881897449493408] [G loss: 0.6648290157318115]\n",
      "[Epoch 62/1001] [Batch 135/372] [D loss: 0.6933454275131226] [G loss: 0.6836380958557129]\n",
      "[Epoch 62/1001] [Batch 136/372] [D loss: 0.6855818629264832] [G loss: 0.6745892763137817]\n",
      "[Epoch 62/1001] [Batch 137/372] [D loss: 0.6952924728393555] [G loss: 0.6746731996536255]\n",
      "[Epoch 62/1001] [Batch 138/372] [D loss: 0.689507782459259] [G loss: 0.6727466583251953]\n",
      "[Epoch 62/1001] [Batch 139/372] [D loss: 0.6879391074180603] [G loss: 0.69877690076828]\n",
      "[Epoch 62/1001] [Batch 140/372] [D loss: 0.6947872638702393] [G loss: 0.6460201144218445]\n",
      "[Epoch 62/1001] [Batch 141/372] [D loss: 0.6921908259391785] [G loss: 0.7176634073257446]\n",
      "[Epoch 62/1001] [Batch 142/372] [D loss: 0.6857701539993286] [G loss: 0.628190279006958]\n",
      "[Epoch 62/1001] [Batch 143/372] [D loss: 0.6961451172828674] [G loss: 0.7772376537322998]\n",
      "[Epoch 62/1001] [Batch 144/372] [D loss: 0.6907559633255005] [G loss: 0.5681002140045166]\n",
      "[Epoch 62/1001] [Batch 145/372] [D loss: 0.6954719424247742] [G loss: 0.8330641984939575]\n",
      "[Epoch 62/1001] [Batch 146/372] [D loss: 0.6956545114517212] [G loss: 0.5496999621391296]\n",
      "[Epoch 62/1001] [Batch 147/372] [D loss: 0.697722315788269] [G loss: 0.8649287819862366]\n",
      "[Epoch 62/1001] [Batch 148/372] [D loss: 0.701784610748291] [G loss: 0.5311740636825562]\n",
      "[Epoch 62/1001] [Batch 149/372] [D loss: 0.7000867128372192] [G loss: 0.8312245607376099]\n",
      "[Epoch 62/1001] [Batch 150/372] [D loss: 0.6982439756393433] [G loss: 0.580051839351654]\n",
      "[Epoch 62/1001] [Batch 151/372] [D loss: 0.6922115683555603] [G loss: 0.7783362865447998]\n",
      "[Epoch 62/1001] [Batch 152/372] [D loss: 0.694966733455658] [G loss: 0.5986905097961426]\n",
      "[Epoch 62/1001] [Batch 153/372] [D loss: 0.6907351016998291] [G loss: 0.7498809695243835]\n",
      "[Epoch 62/1001] [Batch 154/372] [D loss: 0.686224102973938] [G loss: 0.6470882296562195]\n",
      "[Epoch 62/1001] [Batch 155/372] [D loss: 0.6866568326950073] [G loss: 0.6835483908653259]\n",
      "[Epoch 62/1001] [Batch 156/372] [D loss: 0.6871435642242432] [G loss: 0.7152490615844727]\n",
      "[Epoch 62/1001] [Batch 157/372] [D loss: 0.6967219114303589] [G loss: 0.6302310824394226]\n",
      "[Epoch 62/1001] [Batch 158/372] [D loss: 0.6905805468559265] [G loss: 0.6998531222343445]\n",
      "[Epoch 62/1001] [Batch 159/372] [D loss: 0.6899572610855103] [G loss: 0.6843788027763367]\n",
      "[Epoch 62/1001] [Batch 160/372] [D loss: 0.6845844984054565] [G loss: 0.6342602372169495]\n",
      "[Epoch 62/1001] [Batch 161/372] [D loss: 0.6876896619796753] [G loss: 0.7373160719871521]\n",
      "[Epoch 62/1001] [Batch 162/372] [D loss: 0.6961446404457092] [G loss: 0.5985554456710815]\n",
      "[Epoch 62/1001] [Batch 163/372] [D loss: 0.6975841522216797] [G loss: 0.7959749102592468]\n",
      "[Epoch 62/1001] [Batch 164/372] [D loss: 0.7009543180465698] [G loss: 0.5373124480247498]\n",
      "[Epoch 62/1001] [Batch 165/372] [D loss: 0.7001947164535522] [G loss: 0.8827499151229858]\n",
      "[Epoch 62/1001] [Batch 166/372] [D loss: 0.7093132734298706] [G loss: 0.5062035322189331]\n",
      "[Epoch 62/1001] [Batch 167/372] [D loss: 0.707760214805603] [G loss: 0.8752843141555786]\n",
      "[Epoch 62/1001] [Batch 168/372] [D loss: 0.7052246332168579] [G loss: 0.5531015992164612]\n",
      "[Epoch 62/1001] [Batch 169/372] [D loss: 0.6956207752227783] [G loss: 0.7765909433364868]\n",
      "[Epoch 62/1001] [Batch 170/372] [D loss: 0.6946191191673279] [G loss: 0.6244706511497498]\n",
      "[Epoch 62/1001] [Batch 171/372] [D loss: 0.6921387910842896] [G loss: 0.7155976295471191]\n",
      "[Epoch 62/1001] [Batch 172/372] [D loss: 0.6899703741073608] [G loss: 0.6544124484062195]\n",
      "[Epoch 62/1001] [Batch 173/372] [D loss: 0.6909716129302979] [G loss: 0.6701945066452026]\n",
      "[Epoch 62/1001] [Batch 174/372] [D loss: 0.6857939958572388] [G loss: 0.7238543629646301]\n",
      "[Epoch 62/1001] [Batch 175/372] [D loss: 0.6948992609977722] [G loss: 0.6360524892807007]\n",
      "[Epoch 62/1001] [Batch 176/372] [D loss: 0.6916745901107788] [G loss: 0.7118241190910339]\n",
      "[Epoch 62/1001] [Batch 177/372] [D loss: 0.6885573863983154] [G loss: 0.6518359780311584]\n",
      "[Epoch 62/1001] [Batch 178/372] [D loss: 0.6887531280517578] [G loss: 0.7022392749786377]\n",
      "[Epoch 62/1001] [Batch 179/372] [D loss: 0.6904172897338867] [G loss: 0.6575250029563904]\n",
      "[Epoch 62/1001] [Batch 180/372] [D loss: 0.6890996694564819] [G loss: 0.6948056221008301]\n",
      "[Epoch 62/1001] [Batch 181/372] [D loss: 0.6915848255157471] [G loss: 0.6811168193817139]\n",
      "[Epoch 62/1001] [Batch 182/372] [D loss: 0.689374566078186] [G loss: 0.6581298112869263]\n",
      "[Epoch 62/1001] [Batch 183/372] [D loss: 0.6851992607116699] [G loss: 0.7085316181182861]\n",
      "[Epoch 62/1001] [Batch 184/372] [D loss: 0.6887757778167725] [G loss: 0.6760696768760681]\n",
      "[Epoch 62/1001] [Batch 185/372] [D loss: 0.6909807920455933] [G loss: 0.6660981178283691]\n",
      "[Epoch 62/1001] [Batch 186/372] [D loss: 0.6904733180999756] [G loss: 0.7137240171432495]\n",
      "[Epoch 62/1001] [Batch 187/372] [D loss: 0.6897490620613098] [G loss: 0.6270520091056824]\n",
      "[Epoch 62/1001] [Batch 188/372] [D loss: 0.6946220397949219] [G loss: 0.7348353862762451]\n",
      "[Epoch 62/1001] [Batch 189/372] [D loss: 0.6886677742004395] [G loss: 0.6196223497390747]\n",
      "[Epoch 62/1001] [Batch 190/372] [D loss: 0.6888607740402222] [G loss: 0.749149739742279]\n",
      "[Epoch 62/1001] [Batch 191/372] [D loss: 0.6878325939178467] [G loss: 0.6241509318351746]\n",
      "[Epoch 62/1001] [Batch 192/372] [D loss: 0.6947875022888184] [G loss: 0.7384932637214661]\n",
      "[Epoch 62/1001] [Batch 193/372] [D loss: 0.6869519948959351] [G loss: 0.6238833069801331]\n",
      "[Epoch 62/1001] [Batch 194/372] [D loss: 0.6865753531455994] [G loss: 0.740452766418457]\n",
      "[Epoch 62/1001] [Batch 195/372] [D loss: 0.6932646632194519] [G loss: 0.6377664804458618]\n",
      "[Epoch 62/1001] [Batch 196/372] [D loss: 0.68940269947052] [G loss: 0.7225751280784607]\n",
      "[Epoch 62/1001] [Batch 197/372] [D loss: 0.6927469968795776] [G loss: 0.6492007970809937]\n",
      "[Epoch 62/1001] [Batch 198/372] [D loss: 0.6828623414039612] [G loss: 0.6895607709884644]\n",
      "[Epoch 62/1001] [Batch 199/372] [D loss: 0.6865096688270569] [G loss: 0.6809065341949463]\n",
      "[Epoch 62/1001] [Batch 200/372] [D loss: 0.6937917470932007] [G loss: 0.6846481561660767]\n",
      "[Epoch 62/1001] [Batch 201/372] [D loss: 0.691707968711853] [G loss: 0.6602908372879028]\n",
      "[Epoch 62/1001] [Batch 202/372] [D loss: 0.6932735443115234] [G loss: 0.7169781923294067]\n",
      "[Epoch 62/1001] [Batch 203/372] [D loss: 0.6866003274917603] [G loss: 0.6235989332199097]\n",
      "[Epoch 62/1001] [Batch 204/372] [D loss: 0.6944477558135986] [G loss: 0.7439561486244202]\n",
      "[Epoch 62/1001] [Batch 205/372] [D loss: 0.6905645132064819] [G loss: 0.6146913170814514]\n",
      "[Epoch 62/1001] [Batch 206/372] [D loss: 0.6894586086273193] [G loss: 0.7411609292030334]\n",
      "[Epoch 62/1001] [Batch 207/372] [D loss: 0.6952352523803711] [G loss: 0.6359501481056213]\n",
      "[Epoch 62/1001] [Batch 208/372] [D loss: 0.689087986946106] [G loss: 0.7135897278785706]\n",
      "[Epoch 62/1001] [Batch 209/372] [D loss: 0.6927682161331177] [G loss: 0.659937858581543]\n",
      "[Epoch 62/1001] [Batch 210/372] [D loss: 0.6850807070732117] [G loss: 0.6856079697608948]\n",
      "[Epoch 62/1001] [Batch 211/372] [D loss: 0.6918570399284363] [G loss: 0.6782961487770081]\n",
      "[Epoch 62/1001] [Batch 212/372] [D loss: 0.6823300123214722] [G loss: 0.6833382248878479]\n",
      "[Epoch 62/1001] [Batch 213/372] [D loss: 0.6853775978088379] [G loss: 0.693632185459137]\n",
      "[Epoch 62/1001] [Batch 214/372] [D loss: 0.689315915107727] [G loss: 0.6577104330062866]\n",
      "[Epoch 62/1001] [Batch 215/372] [D loss: 0.6890763640403748] [G loss: 0.6947728991508484]\n",
      "[Epoch 62/1001] [Batch 216/372] [D loss: 0.6910684704780579] [G loss: 0.6562357544898987]\n",
      "[Epoch 62/1001] [Batch 217/372] [D loss: 0.6875648498535156] [G loss: 0.6860591173171997]\n",
      "[Epoch 62/1001] [Batch 218/372] [D loss: 0.692644476890564] [G loss: 0.6943789720535278]\n",
      "[Epoch 62/1001] [Batch 219/372] [D loss: 0.6891697645187378] [G loss: 0.637019157409668]\n",
      "[Epoch 62/1001] [Batch 220/372] [D loss: 0.690058708190918] [G loss: 0.730097234249115]\n",
      "[Epoch 62/1001] [Batch 221/372] [D loss: 0.6946879625320435] [G loss: 0.623755693435669]\n",
      "[Epoch 62/1001] [Batch 222/372] [D loss: 0.6954320669174194] [G loss: 0.7154045701026917]\n",
      "[Epoch 62/1001] [Batch 223/372] [D loss: 0.6911463737487793] [G loss: 0.6467800140380859]\n",
      "[Epoch 62/1001] [Batch 224/372] [D loss: 0.69170743227005] [G loss: 0.7062885761260986]\n",
      "[Epoch 62/1001] [Batch 225/372] [D loss: 0.6901925206184387] [G loss: 0.6709740161895752]\n",
      "[Epoch 62/1001] [Batch 226/372] [D loss: 0.6907814145088196] [G loss: 0.6692954301834106]\n",
      "[Epoch 62/1001] [Batch 227/372] [D loss: 0.6907049417495728] [G loss: 0.6822627782821655]\n",
      "[Epoch 62/1001] [Batch 228/372] [D loss: 0.6849303245544434] [G loss: 0.6691786646842957]\n",
      "[Epoch 62/1001] [Batch 229/372] [D loss: 0.6848410964012146] [G loss: 0.6952489018440247]\n",
      "[Epoch 62/1001] [Batch 230/372] [D loss: 0.6871750354766846] [G loss: 0.688886821269989]\n",
      "[Epoch 62/1001] [Batch 231/372] [D loss: 0.6918280124664307] [G loss: 0.6600887775421143]\n",
      "[Epoch 62/1001] [Batch 232/372] [D loss: 0.6874299049377441] [G loss: 0.6972818970680237]\n",
      "[Epoch 62/1001] [Batch 233/372] [D loss: 0.6960164904594421] [G loss: 0.651504397392273]\n",
      "[Epoch 62/1001] [Batch 234/372] [D loss: 0.6908739805221558] [G loss: 0.7191799879074097]\n",
      "[Epoch 62/1001] [Batch 235/372] [D loss: 0.6896984577178955] [G loss: 0.6312763094902039]\n",
      "[Epoch 62/1001] [Batch 236/372] [D loss: 0.6897714138031006] [G loss: 0.7320369482040405]\n",
      "[Epoch 62/1001] [Batch 237/372] [D loss: 0.6895279884338379] [G loss: 0.6133832931518555]\n",
      "[Epoch 62/1001] [Batch 238/372] [D loss: 0.6916059255599976] [G loss: 0.7530755996704102]\n",
      "[Epoch 62/1001] [Batch 239/372] [D loss: 0.6920088529586792] [G loss: 0.605225145816803]\n",
      "[Epoch 62/1001] [Batch 240/372] [D loss: 0.6902730464935303] [G loss: 0.7885847091674805]\n",
      "[Epoch 62/1001] [Batch 241/372] [D loss: 0.691388726234436] [G loss: 0.5816607475280762]\n",
      "[Epoch 62/1001] [Batch 242/372] [D loss: 0.7000062465667725] [G loss: 0.80455082654953]\n",
      "[Epoch 62/1001] [Batch 243/372] [D loss: 0.699966311454773] [G loss: 0.5577964782714844]\n",
      "[Epoch 62/1001] [Batch 244/372] [D loss: 0.7002764344215393] [G loss: 0.8131732940673828]\n",
      "[Epoch 62/1001] [Batch 245/372] [D loss: 0.6960450410842896] [G loss: 0.5754139423370361]\n",
      "[Epoch 62/1001] [Batch 246/372] [D loss: 0.6997684240341187] [G loss: 0.767856240272522]\n",
      "[Epoch 62/1001] [Batch 247/372] [D loss: 0.69315505027771] [G loss: 0.6297888159751892]\n",
      "[Epoch 62/1001] [Batch 248/372] [D loss: 0.6867842674255371] [G loss: 0.7072294354438782]\n",
      "[Epoch 62/1001] [Batch 249/372] [D loss: 0.6911139488220215] [G loss: 0.6767045855522156]\n",
      "[Epoch 62/1001] [Batch 250/372] [D loss: 0.6923242807388306] [G loss: 0.6716694831848145]\n",
      "[Epoch 62/1001] [Batch 251/372] [D loss: 0.6882618069648743] [G loss: 0.6960707306861877]\n",
      "[Epoch 62/1001] [Batch 252/372] [D loss: 0.6898890733718872] [G loss: 0.6554243564605713]\n",
      "[Epoch 62/1001] [Batch 253/372] [D loss: 0.6909945011138916] [G loss: 0.6840770840644836]\n",
      "[Epoch 62/1001] [Batch 254/372] [D loss: 0.6897250413894653] [G loss: 0.6747952103614807]\n",
      "[Epoch 62/1001] [Batch 255/372] [D loss: 0.6878727674484253] [G loss: 0.6894522309303284]\n",
      "[Epoch 62/1001] [Batch 256/372] [D loss: 0.6884369850158691] [G loss: 0.6834262013435364]\n",
      "[Epoch 62/1001] [Batch 257/372] [D loss: 0.6962180137634277] [G loss: 0.6534242630004883]\n",
      "[Epoch 62/1001] [Batch 258/372] [D loss: 0.6888704299926758] [G loss: 0.7106817364692688]\n",
      "[Epoch 62/1001] [Batch 259/372] [D loss: 0.6869696378707886] [G loss: 0.6473298668861389]\n",
      "[Epoch 62/1001] [Batch 260/372] [D loss: 0.6875139474868774] [G loss: 0.6925121545791626]\n",
      "[Epoch 62/1001] [Batch 261/372] [D loss: 0.6878052949905396] [G loss: 0.6932210922241211]\n",
      "[Epoch 62/1001] [Batch 262/372] [D loss: 0.6896759271621704] [G loss: 0.6286227107048035]\n",
      "[Epoch 62/1001] [Batch 263/372] [D loss: 0.6891968250274658] [G loss: 0.7654081583023071]\n",
      "[Epoch 62/1001] [Batch 264/372] [D loss: 0.6868002414703369] [G loss: 0.5753886699676514]\n",
      "[Epoch 62/1001] [Batch 265/372] [D loss: 0.7024388909339905] [G loss: 0.828122615814209]\n",
      "[Epoch 62/1001] [Batch 266/372] [D loss: 0.70710289478302] [G loss: 0.5292491316795349]\n",
      "[Epoch 62/1001] [Batch 267/372] [D loss: 0.7036101818084717] [G loss: 0.8648409247398376]\n",
      "[Epoch 62/1001] [Batch 268/372] [D loss: 0.7125496864318848] [G loss: 0.5267913341522217]\n",
      "[Epoch 62/1001] [Batch 269/372] [D loss: 0.7051102519035339] [G loss: 0.8205462098121643]\n",
      "[Epoch 62/1001] [Batch 270/372] [D loss: 0.6967048048973083] [G loss: 0.5891789197921753]\n",
      "[Epoch 62/1001] [Batch 271/372] [D loss: 0.6900393962860107] [G loss: 0.7505571246147156]\n",
      "[Epoch 62/1001] [Batch 272/372] [D loss: 0.6853119134902954] [G loss: 0.6422064900398254]\n",
      "[Epoch 62/1001] [Batch 273/372] [D loss: 0.69627845287323] [G loss: 0.7303707003593445]\n",
      "[Epoch 62/1001] [Batch 274/372] [D loss: 0.695658802986145] [G loss: 0.6232559680938721]\n",
      "[Epoch 62/1001] [Batch 275/372] [D loss: 0.6903849244117737] [G loss: 0.7377520203590393]\n",
      "[Epoch 62/1001] [Batch 276/372] [D loss: 0.6891356706619263] [G loss: 0.6529242992401123]\n",
      "[Epoch 62/1001] [Batch 277/372] [D loss: 0.6894304752349854] [G loss: 0.6865139007568359]\n",
      "[Epoch 62/1001] [Batch 278/372] [D loss: 0.6915042400360107] [G loss: 0.6792824268341064]\n",
      "[Epoch 62/1001] [Batch 279/372] [D loss: 0.6929472088813782] [G loss: 0.6677364110946655]\n",
      "[Epoch 62/1001] [Batch 280/372] [D loss: 0.688066840171814] [G loss: 0.6917155981063843]\n",
      "[Epoch 62/1001] [Batch 281/372] [D loss: 0.689803957939148] [G loss: 0.668490469455719]\n",
      "[Epoch 62/1001] [Batch 282/372] [D loss: 0.6930066347122192] [G loss: 0.6961946487426758]\n",
      "[Epoch 62/1001] [Batch 283/372] [D loss: 0.6859197616577148] [G loss: 0.6599946022033691]\n",
      "[Epoch 62/1001] [Batch 284/372] [D loss: 0.689889669418335] [G loss: 0.6816316246986389]\n",
      "[Epoch 62/1001] [Batch 285/372] [D loss: 0.690733790397644] [G loss: 0.6828053593635559]\n",
      "[Epoch 62/1001] [Batch 286/372] [D loss: 0.6897141933441162] [G loss: 0.6588903665542603]\n",
      "[Epoch 62/1001] [Batch 287/372] [D loss: 0.6891277432441711] [G loss: 0.7175750136375427]\n",
      "[Epoch 62/1001] [Batch 288/372] [D loss: 0.6861149072647095] [G loss: 0.6334864497184753]\n",
      "[Epoch 62/1001] [Batch 289/372] [D loss: 0.6870388388633728] [G loss: 0.7487614154815674]\n",
      "[Epoch 62/1001] [Batch 290/372] [D loss: 0.6945509910583496] [G loss: 0.6030848622322083]\n",
      "[Epoch 62/1001] [Batch 291/372] [D loss: 0.686873733997345] [G loss: 0.7616572976112366]\n",
      "[Epoch 62/1001] [Batch 292/372] [D loss: 0.6913919448852539] [G loss: 0.6057994961738586]\n",
      "[Epoch 62/1001] [Batch 293/372] [D loss: 0.6873028874397278] [G loss: 0.7383418679237366]\n",
      "[Epoch 62/1001] [Batch 294/372] [D loss: 0.6887661218643188] [G loss: 0.6354907751083374]\n",
      "[Epoch 62/1001] [Batch 295/372] [D loss: 0.6885173320770264] [G loss: 0.7544752359390259]\n",
      "[Epoch 62/1001] [Batch 296/372] [D loss: 0.6950201988220215] [G loss: 0.5744317173957825]\n",
      "[Epoch 62/1001] [Batch 297/372] [D loss: 0.6997122764587402] [G loss: 0.7764449119567871]\n",
      "[Epoch 62/1001] [Batch 298/372] [D loss: 0.6912398338317871] [G loss: 0.5967233180999756]\n",
      "[Epoch 62/1001] [Batch 299/372] [D loss: 0.6931878328323364] [G loss: 0.7428310513496399]\n",
      "[Epoch 62/1001] [Batch 300/372] [D loss: 0.6924502849578857] [G loss: 0.6336782574653625]\n",
      "[Epoch 62/1001] [Batch 301/372] [D loss: 0.6889656782150269] [G loss: 0.7091067433357239]\n",
      "[Epoch 62/1001] [Batch 302/372] [D loss: 0.691489577293396] [G loss: 0.6630483269691467]\n",
      "[Epoch 62/1001] [Batch 303/372] [D loss: 0.6892789602279663] [G loss: 0.681292712688446]\n",
      "[Epoch 62/1001] [Batch 304/372] [D loss: 0.6918230056762695] [G loss: 0.6746929287910461]\n",
      "[Epoch 62/1001] [Batch 305/372] [D loss: 0.6876030564308167] [G loss: 0.6728507280349731]\n",
      "[Epoch 62/1001] [Batch 306/372] [D loss: 0.6863836050033569] [G loss: 0.698650062084198]\n",
      "[Epoch 62/1001] [Batch 307/372] [D loss: 0.6817706823348999] [G loss: 0.6515620946884155]\n",
      "[Epoch 62/1001] [Batch 308/372] [D loss: 0.688532292842865] [G loss: 0.7162913084030151]\n",
      "[Epoch 62/1001] [Batch 309/372] [D loss: 0.6941815614700317] [G loss: 0.6015841960906982]\n",
      "[Epoch 62/1001] [Batch 310/372] [D loss: 0.6917098760604858] [G loss: 0.8379835486412048]\n",
      "[Epoch 62/1001] [Batch 311/372] [D loss: 0.7011277675628662] [G loss: 0.510704755783081]\n",
      "[Epoch 62/1001] [Batch 312/372] [D loss: 0.7081333994865417] [G loss: 0.8972045183181763]\n",
      "[Epoch 62/1001] [Batch 313/372] [D loss: 0.7093684673309326] [G loss: 0.5213077664375305]\n",
      "[Epoch 62/1001] [Batch 314/372] [D loss: 0.7068384289741516] [G loss: 0.801909327507019]\n",
      "[Epoch 62/1001] [Batch 315/372] [D loss: 0.6963484287261963] [G loss: 0.6078884601593018]\n",
      "[Epoch 62/1001] [Batch 316/372] [D loss: 0.6960480213165283] [G loss: 0.7152921557426453]\n",
      "[Epoch 62/1001] [Batch 317/372] [D loss: 0.6858277320861816] [G loss: 0.6669530272483826]\n",
      "[Epoch 62/1001] [Batch 318/372] [D loss: 0.6839070916175842] [G loss: 0.6849436163902283]\n",
      "[Epoch 62/1001] [Batch 319/372] [D loss: 0.6912242770195007] [G loss: 0.6679269075393677]\n",
      "[Epoch 62/1001] [Batch 320/372] [D loss: 0.6909814476966858] [G loss: 0.7020828723907471]\n",
      "[Epoch 62/1001] [Batch 321/372] [D loss: 0.6852169036865234] [G loss: 0.6549229621887207]\n",
      "[Epoch 62/1001] [Batch 322/372] [D loss: 0.6894708871841431] [G loss: 0.7083412408828735]\n",
      "[Epoch 62/1001] [Batch 323/372] [D loss: 0.6901004314422607] [G loss: 0.6598008871078491]\n",
      "[Epoch 62/1001] [Batch 324/372] [D loss: 0.6894530653953552] [G loss: 0.6895745992660522]\n",
      "[Epoch 62/1001] [Batch 325/372] [D loss: 0.6857137680053711] [G loss: 0.6724157333374023]\n",
      "[Epoch 62/1001] [Batch 326/372] [D loss: 0.6943588256835938] [G loss: 0.6864160895347595]\n",
      "[Epoch 62/1001] [Batch 327/372] [D loss: 0.6940521597862244] [G loss: 0.6457228660583496]\n",
      "[Epoch 62/1001] [Batch 328/372] [D loss: 0.6885645389556885] [G loss: 0.7188448309898376]\n",
      "[Epoch 62/1001] [Batch 329/372] [D loss: 0.6864449381828308] [G loss: 0.6266617178916931]\n",
      "[Epoch 62/1001] [Batch 330/372] [D loss: 0.6872811317443848] [G loss: 0.7324008941650391]\n",
      "[Epoch 62/1001] [Batch 331/372] [D loss: 0.6909906268119812] [G loss: 0.6415597200393677]\n",
      "[Epoch 62/1001] [Batch 332/372] [D loss: 0.6908248662948608] [G loss: 0.7007197141647339]\n",
      "[Epoch 62/1001] [Batch 333/372] [D loss: 0.695013701915741] [G loss: 0.6606035232543945]\n",
      "[Epoch 62/1001] [Batch 334/372] [D loss: 0.6854808330535889] [G loss: 0.6895478963851929]\n",
      "[Epoch 62/1001] [Batch 335/372] [D loss: 0.6892388463020325] [G loss: 0.675287127494812]\n",
      "[Epoch 62/1001] [Batch 336/372] [D loss: 0.6887627840042114] [G loss: 0.6768589019775391]\n",
      "[Epoch 62/1001] [Batch 337/372] [D loss: 0.690827488899231] [G loss: 0.6796280145645142]\n",
      "[Epoch 62/1001] [Batch 338/372] [D loss: 0.6923144459724426] [G loss: 0.6802509427070618]\n",
      "[Epoch 62/1001] [Batch 339/372] [D loss: 0.6883679032325745] [G loss: 0.6687391996383667]\n",
      "[Epoch 62/1001] [Batch 340/372] [D loss: 0.6899888515472412] [G loss: 0.6882728934288025]\n",
      "[Epoch 62/1001] [Batch 341/372] [D loss: 0.6893157958984375] [G loss: 0.663485586643219]\n",
      "[Epoch 62/1001] [Batch 342/372] [D loss: 0.6858035922050476] [G loss: 0.7377071976661682]\n",
      "[Epoch 62/1001] [Batch 343/372] [D loss: 0.6918443441390991] [G loss: 0.6025325655937195]\n",
      "[Epoch 62/1001] [Batch 344/372] [D loss: 0.6935551166534424] [G loss: 0.7759974002838135]\n",
      "[Epoch 62/1001] [Batch 345/372] [D loss: 0.6911388039588928] [G loss: 0.5712966322898865]\n",
      "[Epoch 62/1001] [Batch 346/372] [D loss: 0.6960891485214233] [G loss: 0.8680151700973511]\n",
      "[Epoch 62/1001] [Batch 347/372] [D loss: 0.7070372104644775] [G loss: 0.5109084844589233]\n",
      "[Epoch 62/1001] [Batch 348/372] [D loss: 0.7067252993583679] [G loss: 0.8475719690322876]\n",
      "[Epoch 62/1001] [Batch 349/372] [D loss: 0.7032756805419922] [G loss: 0.573500394821167]\n",
      "[Epoch 62/1001] [Batch 350/372] [D loss: 0.6922003030776978] [G loss: 0.7431844472885132]\n",
      "[Epoch 62/1001] [Batch 351/372] [D loss: 0.6923873424530029] [G loss: 0.6635956764221191]\n",
      "[Epoch 62/1001] [Batch 352/372] [D loss: 0.689812958240509] [G loss: 0.6731781363487244]\n",
      "[Epoch 62/1001] [Batch 353/372] [D loss: 0.6906009912490845] [G loss: 0.6884135007858276]\n",
      "[Epoch 62/1001] [Batch 354/372] [D loss: 0.6921216249465942] [G loss: 0.6627733707427979]\n",
      "[Epoch 62/1001] [Batch 355/372] [D loss: 0.6851303577423096] [G loss: 0.6866178512573242]\n",
      "[Epoch 62/1001] [Batch 356/372] [D loss: 0.6899272799491882] [G loss: 0.6808324456214905]\n",
      "[Epoch 62/1001] [Batch 357/372] [D loss: 0.6876212954521179] [G loss: 0.6699631810188293]\n",
      "[Epoch 62/1001] [Batch 358/372] [D loss: 0.6881948113441467] [G loss: 0.6877074241638184]\n",
      "[Epoch 62/1001] [Batch 359/372] [D loss: 0.6890525817871094] [G loss: 0.6667805314064026]\n",
      "[Epoch 62/1001] [Batch 360/372] [D loss: 0.6852668523788452] [G loss: 0.673180103302002]\n",
      "[Epoch 62/1001] [Batch 361/372] [D loss: 0.6954140663146973] [G loss: 0.6856195330619812]\n",
      "[Epoch 62/1001] [Batch 362/372] [D loss: 0.6824907660484314] [G loss: 0.6323376297950745]\n",
      "[Epoch 62/1001] [Batch 363/372] [D loss: 0.7000086307525635] [G loss: 0.7537515759468079]\n",
      "[Epoch 62/1001] [Batch 364/372] [D loss: 0.6981970071792603] [G loss: 0.5585207939147949]\n",
      "[Epoch 62/1001] [Batch 365/372] [D loss: 0.6984018087387085] [G loss: 0.8427666425704956]\n",
      "[Epoch 62/1001] [Batch 366/372] [D loss: 0.7051295042037964] [G loss: 0.5363902449607849]\n",
      "[Epoch 62/1001] [Batch 367/372] [D loss: 0.6973831653594971] [G loss: 0.8132776618003845]\n",
      "[Epoch 62/1001] [Batch 368/372] [D loss: 0.7017158269882202] [G loss: 0.5914143919944763]\n",
      "[Epoch 62/1001] [Batch 369/372] [D loss: 0.6954545378684998] [G loss: 0.7265812754631042]\n",
      "[Epoch 62/1001] [Batch 370/372] [D loss: 0.6915380954742432] [G loss: 0.6584410667419434]\n",
      "[Epoch 62/1001] [Batch 371/372] [D loss: 0.6889394521713257] [G loss: 0.6768381595611572]\n",
      "[Epoch 63/1001] [Batch 0/372] [D loss: 0.6869663000106812] [G loss: 0.687145471572876]\n",
      "[Epoch 63/1001] [Batch 1/372] [D loss: 0.6836196184158325] [G loss: 0.6699340343475342]\n",
      "[Epoch 63/1001] [Batch 2/372] [D loss: 0.6853257417678833] [G loss: 0.7085350751876831]\n",
      "[Epoch 63/1001] [Batch 3/372] [D loss: 0.6834620237350464] [G loss: 0.6494318246841431]\n",
      "[Epoch 63/1001] [Batch 4/372] [D loss: 0.6938520073890686] [G loss: 0.7063820958137512]\n",
      "[Epoch 63/1001] [Batch 5/372] [D loss: 0.6878571510314941] [G loss: 0.6581435203552246]\n",
      "[Epoch 63/1001] [Batch 6/372] [D loss: 0.6872668862342834] [G loss: 0.716050922870636]\n",
      "[Epoch 63/1001] [Batch 7/372] [D loss: 0.6854751706123352] [G loss: 0.6266089081764221]\n",
      "[Epoch 63/1001] [Batch 8/372] [D loss: 0.6814651489257812] [G loss: 0.7636210322380066]\n",
      "[Epoch 63/1001] [Batch 9/372] [D loss: 0.6838440895080566] [G loss: 0.6030289530754089]\n",
      "[Epoch 63/1001] [Batch 10/372] [D loss: 0.6962350606918335] [G loss: 0.7854816913604736]\n",
      "[Epoch 63/1001] [Batch 11/372] [D loss: 0.689812183380127] [G loss: 0.5601828098297119]\n",
      "[Epoch 63/1001] [Batch 12/372] [D loss: 0.6965391635894775] [G loss: 0.8301531076431274]\n",
      "[Epoch 63/1001] [Batch 13/372] [D loss: 0.7004138231277466] [G loss: 0.5663665533065796]\n",
      "[Epoch 63/1001] [Batch 14/372] [D loss: 0.6951756477355957] [G loss: 0.7575799226760864]\n",
      "[Epoch 63/1001] [Batch 15/372] [D loss: 0.6913225054740906] [G loss: 0.6402871012687683]\n",
      "[Epoch 63/1001] [Batch 16/372] [D loss: 0.6873230934143066] [G loss: 0.6842631697654724]\n",
      "[Epoch 63/1001] [Batch 17/372] [D loss: 0.689315676689148] [G loss: 0.697035551071167]\n",
      "[Epoch 63/1001] [Batch 18/372] [D loss: 0.6871405839920044] [G loss: 0.6417051553726196]\n",
      "[Epoch 63/1001] [Batch 19/372] [D loss: 0.6876971125602722] [G loss: 0.7221775650978088]\n",
      "[Epoch 63/1001] [Batch 20/372] [D loss: 0.6858106255531311] [G loss: 0.6302490234375]\n",
      "[Epoch 63/1001] [Batch 21/372] [D loss: 0.6893137097358704] [G loss: 0.7175440788269043]\n",
      "[Epoch 63/1001] [Batch 22/372] [D loss: 0.6840533018112183] [G loss: 0.6540687084197998]\n",
      "[Epoch 63/1001] [Batch 23/372] [D loss: 0.6932293176651001] [G loss: 0.7017112374305725]\n",
      "[Epoch 63/1001] [Batch 24/372] [D loss: 0.6870671510696411] [G loss: 0.6429712772369385]\n",
      "[Epoch 63/1001] [Batch 25/372] [D loss: 0.6927371621131897] [G loss: 0.7291682362556458]\n",
      "[Epoch 63/1001] [Batch 26/372] [D loss: 0.6934762001037598] [G loss: 0.6407203078269958]\n",
      "[Epoch 63/1001] [Batch 27/372] [D loss: 0.6929742693901062] [G loss: 0.708803653717041]\n",
      "[Epoch 63/1001] [Batch 28/372] [D loss: 0.6909890174865723] [G loss: 0.6448614597320557]\n",
      "[Epoch 63/1001] [Batch 29/372] [D loss: 0.6916207075119019] [G loss: 0.7023604512214661]\n",
      "[Epoch 63/1001] [Batch 30/372] [D loss: 0.6871598958969116] [G loss: 0.678806483745575]\n",
      "[Epoch 63/1001] [Batch 31/372] [D loss: 0.6865995526313782] [G loss: 0.6567094326019287]\n",
      "[Epoch 63/1001] [Batch 32/372] [D loss: 0.6855478882789612] [G loss: 0.7295849323272705]\n",
      "[Epoch 63/1001] [Batch 33/372] [D loss: 0.6876164674758911] [G loss: 0.6246982216835022]\n",
      "[Epoch 63/1001] [Batch 34/372] [D loss: 0.6876854300498962] [G loss: 0.7486411929130554]\n",
      "[Epoch 63/1001] [Batch 35/372] [D loss: 0.6852016448974609] [G loss: 0.6237733960151672]\n",
      "[Epoch 63/1001] [Batch 36/372] [D loss: 0.6865934729576111] [G loss: 0.7367990016937256]\n",
      "[Epoch 63/1001] [Batch 37/372] [D loss: 0.6887801885604858] [G loss: 0.6432635188102722]\n",
      "[Epoch 63/1001] [Batch 38/372] [D loss: 0.6880472302436829] [G loss: 0.7140807509422302]\n",
      "[Epoch 63/1001] [Batch 39/372] [D loss: 0.6955360174179077] [G loss: 0.6491900682449341]\n",
      "[Epoch 63/1001] [Batch 40/372] [D loss: 0.6914920806884766] [G loss: 0.707855761051178]\n",
      "[Epoch 63/1001] [Batch 41/372] [D loss: 0.6857958436012268] [G loss: 0.6277363300323486]\n",
      "[Epoch 63/1001] [Batch 42/372] [D loss: 0.6895703077316284] [G loss: 0.7652958035469055]\n",
      "[Epoch 63/1001] [Batch 43/372] [D loss: 0.6913120746612549] [G loss: 0.5845387578010559]\n",
      "[Epoch 63/1001] [Batch 44/372] [D loss: 0.6959099769592285] [G loss: 0.8193862438201904]\n",
      "[Epoch 63/1001] [Batch 45/372] [D loss: 0.701516330242157] [G loss: 0.5406033396720886]\n",
      "[Epoch 63/1001] [Batch 46/372] [D loss: 0.7018086910247803] [G loss: 0.8345758318901062]\n",
      "[Epoch 63/1001] [Batch 47/372] [D loss: 0.697665810585022] [G loss: 0.5621161460876465]\n",
      "[Epoch 63/1001] [Batch 48/372] [D loss: 0.6966060400009155] [G loss: 0.8040163516998291]\n",
      "[Epoch 63/1001] [Batch 49/372] [D loss: 0.6915544271469116] [G loss: 0.5928924083709717]\n",
      "[Epoch 63/1001] [Batch 50/372] [D loss: 0.6970144510269165] [G loss: 0.7399265766143799]\n",
      "[Epoch 63/1001] [Batch 51/372] [D loss: 0.6936593055725098] [G loss: 0.653455913066864]\n",
      "[Epoch 63/1001] [Batch 52/372] [D loss: 0.6899719834327698] [G loss: 0.6901416778564453]\n",
      "[Epoch 63/1001] [Batch 53/372] [D loss: 0.6965522170066833] [G loss: 0.6712153553962708]\n",
      "[Epoch 63/1001] [Batch 54/372] [D loss: 0.6883617043495178] [G loss: 0.6762917041778564]\n",
      "[Epoch 63/1001] [Batch 55/372] [D loss: 0.6920843124389648] [G loss: 0.6972180604934692]\n",
      "[Epoch 63/1001] [Batch 56/372] [D loss: 0.6907167434692383] [G loss: 0.6544333696365356]\n",
      "[Epoch 63/1001] [Batch 57/372] [D loss: 0.6918431520462036] [G loss: 0.7164252996444702]\n",
      "[Epoch 63/1001] [Batch 58/372] [D loss: 0.6860688924789429] [G loss: 0.6407535076141357]\n",
      "[Epoch 63/1001] [Batch 59/372] [D loss: 0.6872431635856628] [G loss: 0.7146406173706055]\n",
      "[Epoch 63/1001] [Batch 60/372] [D loss: 0.6848196983337402] [G loss: 0.6262478232383728]\n",
      "[Epoch 63/1001] [Batch 61/372] [D loss: 0.6925762891769409] [G loss: 0.7468420267105103]\n",
      "[Epoch 63/1001] [Batch 62/372] [D loss: 0.6856189966201782] [G loss: 0.6053981781005859]\n",
      "[Epoch 63/1001] [Batch 63/372] [D loss: 0.6959831714630127] [G loss: 0.7858402729034424]\n",
      "[Epoch 63/1001] [Batch 64/372] [D loss: 0.693697452545166] [G loss: 0.5638870596885681]\n",
      "[Epoch 63/1001] [Batch 65/372] [D loss: 0.6950628757476807] [G loss: 0.826466977596283]\n",
      "[Epoch 63/1001] [Batch 66/372] [D loss: 0.7013541460037231] [G loss: 0.5410832166671753]\n",
      "[Epoch 63/1001] [Batch 67/372] [D loss: 0.7016127109527588] [G loss: 0.8208645582199097]\n",
      "[Epoch 63/1001] [Batch 68/372] [D loss: 0.6952247619628906] [G loss: 0.5866538882255554]\n",
      "[Epoch 63/1001] [Batch 69/372] [D loss: 0.6874058246612549] [G loss: 0.7515711784362793]\n",
      "[Epoch 63/1001] [Batch 70/372] [D loss: 0.6879534125328064] [G loss: 0.6320299506187439]\n",
      "[Epoch 63/1001] [Batch 71/372] [D loss: 0.686509907245636] [G loss: 0.7256495356559753]\n",
      "[Epoch 63/1001] [Batch 72/372] [D loss: 0.6922288537025452] [G loss: 0.6547484993934631]\n",
      "[Epoch 63/1001] [Batch 73/372] [D loss: 0.6839434504508972] [G loss: 0.7004916071891785]\n",
      "[Epoch 63/1001] [Batch 74/372] [D loss: 0.6842731237411499] [G loss: 0.6681932210922241]\n",
      "[Epoch 63/1001] [Batch 75/372] [D loss: 0.6889525651931763] [G loss: 0.6744630336761475]\n",
      "[Epoch 63/1001] [Batch 76/372] [D loss: 0.6898607015609741] [G loss: 0.6930175423622131]\n",
      "[Epoch 63/1001] [Batch 77/372] [D loss: 0.6930435299873352] [G loss: 0.6461319923400879]\n",
      "[Epoch 63/1001] [Batch 78/372] [D loss: 0.6894762516021729] [G loss: 0.7149372696876526]\n",
      "[Epoch 63/1001] [Batch 79/372] [D loss: 0.6869943141937256] [G loss: 0.6335666179656982]\n",
      "[Epoch 63/1001] [Batch 80/372] [D loss: 0.6890569925308228] [G loss: 0.7190573215484619]\n",
      "[Epoch 63/1001] [Batch 81/372] [D loss: 0.689877986907959] [G loss: 0.6396364569664001]\n",
      "[Epoch 63/1001] [Batch 82/372] [D loss: 0.6879171133041382] [G loss: 0.7353333830833435]\n",
      "[Epoch 63/1001] [Batch 83/372] [D loss: 0.6910017728805542] [G loss: 0.6279166340827942]\n",
      "[Epoch 63/1001] [Batch 84/372] [D loss: 0.6894251108169556] [G loss: 0.7109326124191284]\n",
      "[Epoch 63/1001] [Batch 85/372] [D loss: 0.6910872459411621] [G loss: 0.6552848219871521]\n",
      "[Epoch 63/1001] [Batch 86/372] [D loss: 0.6892724633216858] [G loss: 0.688400149345398]\n",
      "[Epoch 63/1001] [Batch 87/372] [D loss: 0.6892536878585815] [G loss: 0.6710300445556641]\n",
      "[Epoch 63/1001] [Batch 88/372] [D loss: 0.6889380216598511] [G loss: 0.682284951210022]\n",
      "[Epoch 63/1001] [Batch 89/372] [D loss: 0.6893885135650635] [G loss: 0.6615569591522217]\n",
      "[Epoch 63/1001] [Batch 90/372] [D loss: 0.6860710382461548] [G loss: 0.6994718909263611]\n",
      "[Epoch 63/1001] [Batch 91/372] [D loss: 0.6868601441383362] [G loss: 0.6574295163154602]\n",
      "[Epoch 63/1001] [Batch 92/372] [D loss: 0.6936266422271729] [G loss: 0.7124756574630737]\n",
      "[Epoch 63/1001] [Batch 93/372] [D loss: 0.690676212310791] [G loss: 0.6241042613983154]\n",
      "[Epoch 63/1001] [Batch 94/372] [D loss: 0.694520890712738] [G loss: 0.7335622310638428]\n",
      "[Epoch 63/1001] [Batch 95/372] [D loss: 0.6898512840270996] [G loss: 0.6370514035224915]\n",
      "[Epoch 63/1001] [Batch 96/372] [D loss: 0.6890844702720642] [G loss: 0.7026961445808411]\n",
      "[Epoch 63/1001] [Batch 97/372] [D loss: 0.688291072845459] [G loss: 0.6544908285140991]\n",
      "[Epoch 63/1001] [Batch 98/372] [D loss: 0.6850974559783936] [G loss: 0.731304943561554]\n",
      "[Epoch 63/1001] [Batch 99/372] [D loss: 0.693143367767334] [G loss: 0.6264911890029907]\n",
      "[Epoch 63/1001] [Batch 100/372] [D loss: 0.6906577944755554] [G loss: 0.7333933711051941]\n",
      "[Epoch 63/1001] [Batch 101/372] [D loss: 0.6866347789764404] [G loss: 0.6240725517272949]\n",
      "[Epoch 63/1001] [Batch 102/372] [D loss: 0.6903681755065918] [G loss: 0.7283127903938293]\n",
      "[Epoch 63/1001] [Batch 103/372] [D loss: 0.6903502941131592] [G loss: 0.6225506067276001]\n",
      "[Epoch 63/1001] [Batch 104/372] [D loss: 0.6887117624282837] [G loss: 0.7620612978935242]\n",
      "[Epoch 63/1001] [Batch 105/372] [D loss: 0.6906719207763672] [G loss: 0.5842910408973694]\n",
      "[Epoch 63/1001] [Batch 106/372] [D loss: 0.691193699836731] [G loss: 0.8121414184570312]\n",
      "[Epoch 63/1001] [Batch 107/372] [D loss: 0.6993190050125122] [G loss: 0.5482869148254395]\n",
      "[Epoch 63/1001] [Batch 108/372] [D loss: 0.7003077268600464] [G loss: 0.8445907235145569]\n",
      "[Epoch 63/1001] [Batch 109/372] [D loss: 0.7081530094146729] [G loss: 0.5373682975769043]\n",
      "[Epoch 63/1001] [Batch 110/372] [D loss: 0.7055490016937256] [G loss: 0.8045105934143066]\n",
      "[Epoch 63/1001] [Batch 111/372] [D loss: 0.6973628997802734] [G loss: 0.5955880880355835]\n",
      "[Epoch 63/1001] [Batch 112/372] [D loss: 0.6935805678367615] [G loss: 0.7167405486106873]\n",
      "[Epoch 63/1001] [Batch 113/372] [D loss: 0.690207839012146] [G loss: 0.6646137833595276]\n",
      "[Epoch 63/1001] [Batch 114/372] [D loss: 0.6887985467910767] [G loss: 0.6891932487487793]\n",
      "[Epoch 63/1001] [Batch 115/372] [D loss: 0.6813697218894958] [G loss: 0.6734543442726135]\n",
      "[Epoch 63/1001] [Batch 116/372] [D loss: 0.6927068829536438] [G loss: 0.6979824900627136]\n",
      "[Epoch 63/1001] [Batch 117/372] [D loss: 0.6928658485412598] [G loss: 0.6596559286117554]\n",
      "[Epoch 63/1001] [Batch 118/372] [D loss: 0.6899698972702026] [G loss: 0.6682142615318298]\n",
      "[Epoch 63/1001] [Batch 119/372] [D loss: 0.6848102807998657] [G loss: 0.7144458889961243]\n",
      "[Epoch 63/1001] [Batch 120/372] [D loss: 0.692961573600769] [G loss: 0.6404471397399902]\n",
      "[Epoch 63/1001] [Batch 121/372] [D loss: 0.6880846619606018] [G loss: 0.7177609205245972]\n",
      "[Epoch 63/1001] [Batch 122/372] [D loss: 0.6907837390899658] [G loss: 0.6346358060836792]\n",
      "[Epoch 63/1001] [Batch 123/372] [D loss: 0.6945254802703857] [G loss: 0.7405480146408081]\n",
      "[Epoch 63/1001] [Batch 124/372] [D loss: 0.6879161596298218] [G loss: 0.6139659285545349]\n",
      "[Epoch 63/1001] [Batch 125/372] [D loss: 0.6905052661895752] [G loss: 0.7682256102561951]\n",
      "[Epoch 63/1001] [Batch 126/372] [D loss: 0.6881762742996216] [G loss: 0.5984978079795837]\n",
      "[Epoch 63/1001] [Batch 127/372] [D loss: 0.6969693899154663] [G loss: 0.7757399082183838]\n",
      "[Epoch 63/1001] [Batch 128/372] [D loss: 0.6951459646224976] [G loss: 0.5970159769058228]\n",
      "[Epoch 63/1001] [Batch 129/372] [D loss: 0.6913195848464966] [G loss: 0.7545912265777588]\n",
      "[Epoch 63/1001] [Batch 130/372] [D loss: 0.6917150020599365] [G loss: 0.5992943644523621]\n",
      "[Epoch 63/1001] [Batch 131/372] [D loss: 0.6922672986984253] [G loss: 0.7877273559570312]\n",
      "[Epoch 63/1001] [Batch 132/372] [D loss: 0.6930968761444092] [G loss: 0.5643724203109741]\n",
      "[Epoch 63/1001] [Batch 133/372] [D loss: 0.698607325553894] [G loss: 0.8058758974075317]\n",
      "[Epoch 63/1001] [Batch 134/372] [D loss: 0.6977935433387756] [G loss: 0.5600975751876831]\n",
      "[Epoch 63/1001] [Batch 135/372] [D loss: 0.7020341753959656] [G loss: 0.8200465440750122]\n",
      "[Epoch 63/1001] [Batch 136/372] [D loss: 0.6961482763290405] [G loss: 0.5707896947860718]\n",
      "[Epoch 63/1001] [Batch 137/372] [D loss: 0.6980307698249817] [G loss: 0.7645331621170044]\n",
      "[Epoch 63/1001] [Batch 138/372] [D loss: 0.6942227482795715] [G loss: 0.6247561573982239]\n",
      "[Epoch 63/1001] [Batch 139/372] [D loss: 0.6925644874572754] [G loss: 0.7227984070777893]\n",
      "[Epoch 63/1001] [Batch 140/372] [D loss: 0.6932278871536255] [G loss: 0.6458151340484619]\n",
      "[Epoch 63/1001] [Batch 141/372] [D loss: 0.6901130676269531] [G loss: 0.6947304606437683]\n",
      "[Epoch 63/1001] [Batch 142/372] [D loss: 0.6892694234848022] [G loss: 0.6752443313598633]\n",
      "[Epoch 63/1001] [Batch 143/372] [D loss: 0.6887794733047485] [G loss: 0.6709591150283813]\n",
      "[Epoch 63/1001] [Batch 144/372] [D loss: 0.6884070634841919] [G loss: 0.7033618688583374]\n",
      "[Epoch 63/1001] [Batch 145/372] [D loss: 0.6891108751296997] [G loss: 0.6455475687980652]\n",
      "[Epoch 63/1001] [Batch 146/372] [D loss: 0.6928519010543823] [G loss: 0.7094109654426575]\n",
      "[Epoch 63/1001] [Batch 147/372] [D loss: 0.6824575662612915] [G loss: 0.6403453946113586]\n",
      "[Epoch 63/1001] [Batch 148/372] [D loss: 0.6914334297180176] [G loss: 0.7130331993103027]\n",
      "[Epoch 63/1001] [Batch 149/372] [D loss: 0.6894137859344482] [G loss: 0.6576936841011047]\n",
      "[Epoch 63/1001] [Batch 150/372] [D loss: 0.69245445728302] [G loss: 0.6886934041976929]\n",
      "[Epoch 63/1001] [Batch 151/372] [D loss: 0.6870664358139038] [G loss: 0.6591284275054932]\n",
      "[Epoch 63/1001] [Batch 152/372] [D loss: 0.68233323097229] [G loss: 0.6962664127349854]\n",
      "[Epoch 63/1001] [Batch 153/372] [D loss: 0.6914277076721191] [G loss: 0.6885180473327637]\n",
      "[Epoch 63/1001] [Batch 154/372] [D loss: 0.6878547668457031] [G loss: 0.6431751251220703]\n",
      "[Epoch 63/1001] [Batch 155/372] [D loss: 0.6806750297546387] [G loss: 0.7273836135864258]\n",
      "[Epoch 63/1001] [Batch 156/372] [D loss: 0.692406415939331] [G loss: 0.6197823286056519]\n",
      "[Epoch 63/1001] [Batch 157/372] [D loss: 0.6929793357849121] [G loss: 0.7518704533576965]\n",
      "[Epoch 63/1001] [Batch 158/372] [D loss: 0.6861900091171265] [G loss: 0.5811294913291931]\n",
      "[Epoch 63/1001] [Batch 159/372] [D loss: 0.6988712549209595] [G loss: 0.7892593145370483]\n",
      "[Epoch 63/1001] [Batch 160/372] [D loss: 0.698399543762207] [G loss: 0.5677013397216797]\n",
      "[Epoch 63/1001] [Batch 161/372] [D loss: 0.6947749257087708] [G loss: 0.795868992805481]\n",
      "[Epoch 63/1001] [Batch 162/372] [D loss: 0.6937801837921143] [G loss: 0.5927900075912476]\n",
      "[Epoch 63/1001] [Batch 163/372] [D loss: 0.6894451379776001] [G loss: 0.7395933270454407]\n",
      "[Epoch 63/1001] [Batch 164/372] [D loss: 0.693313479423523] [G loss: 0.6214223504066467]\n",
      "[Epoch 63/1001] [Batch 165/372] [D loss: 0.6893959045410156] [G loss: 0.7390287518501282]\n",
      "[Epoch 63/1001] [Batch 166/372] [D loss: 0.6874339580535889] [G loss: 0.6003899574279785]\n",
      "[Epoch 63/1001] [Batch 167/372] [D loss: 0.6934273838996887] [G loss: 0.8199552297592163]\n",
      "[Epoch 63/1001] [Batch 168/372] [D loss: 0.6966602802276611] [G loss: 0.5305142998695374]\n",
      "[Epoch 63/1001] [Batch 169/372] [D loss: 0.7065997123718262] [G loss: 0.8543387055397034]\n",
      "[Epoch 63/1001] [Batch 170/372] [D loss: 0.7018934488296509] [G loss: 0.5435734987258911]\n",
      "[Epoch 63/1001] [Batch 171/372] [D loss: 0.6993653178215027] [G loss: 0.7886419892311096]\n",
      "[Epoch 63/1001] [Batch 172/372] [D loss: 0.693098783493042] [G loss: 0.6402497291564941]\n",
      "[Epoch 63/1001] [Batch 173/372] [D loss: 0.6890279054641724] [G loss: 0.6842533946037292]\n",
      "[Epoch 63/1001] [Batch 174/372] [D loss: 0.6920430660247803] [G loss: 0.6896033883094788]\n",
      "[Epoch 63/1001] [Batch 175/372] [D loss: 0.6868613362312317] [G loss: 0.6566166281700134]\n",
      "[Epoch 63/1001] [Batch 176/372] [D loss: 0.6859732866287231] [G loss: 0.7041841745376587]\n",
      "[Epoch 63/1001] [Batch 177/372] [D loss: 0.6916987895965576] [G loss: 0.6608434915542603]\n",
      "[Epoch 63/1001] [Batch 178/372] [D loss: 0.6913815140724182] [G loss: 0.691085696220398]\n",
      "[Epoch 63/1001] [Batch 179/372] [D loss: 0.6882425546646118] [G loss: 0.6775105595588684]\n",
      "[Epoch 63/1001] [Batch 180/372] [D loss: 0.69012451171875] [G loss: 0.6785838007926941]\n",
      "[Epoch 63/1001] [Batch 181/372] [D loss: 0.6915156841278076] [G loss: 0.6685855388641357]\n",
      "[Epoch 63/1001] [Batch 182/372] [D loss: 0.687652587890625] [G loss: 0.6940099000930786]\n",
      "[Epoch 63/1001] [Batch 183/372] [D loss: 0.689372181892395] [G loss: 0.6547696590423584]\n",
      "[Epoch 63/1001] [Batch 184/372] [D loss: 0.6861817836761475] [G loss: 0.7181939482688904]\n",
      "[Epoch 63/1001] [Batch 185/372] [D loss: 0.6848273277282715] [G loss: 0.6203824281692505]\n",
      "[Epoch 63/1001] [Batch 186/372] [D loss: 0.6972850561141968] [G loss: 0.7612587809562683]\n",
      "[Epoch 63/1001] [Batch 187/372] [D loss: 0.6923490762710571] [G loss: 0.5855066180229187]\n",
      "[Epoch 63/1001] [Batch 188/372] [D loss: 0.6952713131904602] [G loss: 0.8017924427986145]\n",
      "[Epoch 63/1001] [Batch 189/372] [D loss: 0.6944841146469116] [G loss: 0.5651674270629883]\n",
      "[Epoch 63/1001] [Batch 190/372] [D loss: 0.6992897987365723] [G loss: 0.7955303192138672]\n",
      "[Epoch 63/1001] [Batch 191/372] [D loss: 0.6965591907501221] [G loss: 0.5896553993225098]\n",
      "[Epoch 63/1001] [Batch 192/372] [D loss: 0.691619873046875] [G loss: 0.7476831078529358]\n",
      "[Epoch 63/1001] [Batch 193/372] [D loss: 0.6912263631820679] [G loss: 0.6310635209083557]\n",
      "[Epoch 63/1001] [Batch 194/372] [D loss: 0.6863749623298645] [G loss: 0.7068307399749756]\n",
      "[Epoch 63/1001] [Batch 195/372] [D loss: 0.6911126375198364] [G loss: 0.6627132892608643]\n",
      "[Epoch 63/1001] [Batch 196/372] [D loss: 0.6818650960922241] [G loss: 0.676866352558136]\n",
      "[Epoch 63/1001] [Batch 197/372] [D loss: 0.6906569004058838] [G loss: 0.7028017044067383]\n",
      "[Epoch 63/1001] [Batch 198/372] [D loss: 0.694442629814148] [G loss: 0.6486588716506958]\n",
      "[Epoch 63/1001] [Batch 199/372] [D loss: 0.6919828653335571] [G loss: 0.7088878154754639]\n",
      "[Epoch 63/1001] [Batch 200/372] [D loss: 0.6939796209335327] [G loss: 0.641569972038269]\n",
      "[Epoch 63/1001] [Batch 201/372] [D loss: 0.6899155974388123] [G loss: 0.6877295970916748]\n",
      "[Epoch 63/1001] [Batch 202/372] [D loss: 0.6941350698471069] [G loss: 0.6980224847793579]\n",
      "[Epoch 63/1001] [Batch 203/372] [D loss: 0.6964987516403198] [G loss: 0.6363050937652588]\n",
      "[Epoch 63/1001] [Batch 204/372] [D loss: 0.6902869343757629] [G loss: 0.7118029594421387]\n",
      "[Epoch 63/1001] [Batch 205/372] [D loss: 0.6921350359916687] [G loss: 0.642282247543335]\n",
      "[Epoch 63/1001] [Batch 206/372] [D loss: 0.6934048533439636] [G loss: 0.7242233753204346]\n",
      "[Epoch 63/1001] [Batch 207/372] [D loss: 0.6862825155258179] [G loss: 0.6337058544158936]\n",
      "[Epoch 63/1001] [Batch 208/372] [D loss: 0.6906269192695618] [G loss: 0.725231409072876]\n",
      "[Epoch 63/1001] [Batch 209/372] [D loss: 0.688791036605835] [G loss: 0.624758780002594]\n",
      "[Epoch 63/1001] [Batch 210/372] [D loss: 0.6921405792236328] [G loss: 0.764474630355835]\n",
      "[Epoch 63/1001] [Batch 211/372] [D loss: 0.6921696066856384] [G loss: 0.5889037847518921]\n",
      "[Epoch 63/1001] [Batch 212/372] [D loss: 0.6945147514343262] [G loss: 0.7905977368354797]\n",
      "[Epoch 63/1001] [Batch 213/372] [D loss: 0.6973153352737427] [G loss: 0.5749734044075012]\n",
      "[Epoch 63/1001] [Batch 214/372] [D loss: 0.695398211479187] [G loss: 0.7968319654464722]\n",
      "[Epoch 63/1001] [Batch 215/372] [D loss: 0.6978051662445068] [G loss: 0.5676187872886658]\n",
      "[Epoch 63/1001] [Batch 216/372] [D loss: 0.693311333656311] [G loss: 0.8064523935317993]\n",
      "[Epoch 63/1001] [Batch 217/372] [D loss: 0.6991583108901978] [G loss: 0.561744749546051]\n",
      "[Epoch 63/1001] [Batch 218/372] [D loss: 0.7005831003189087] [G loss: 0.799497127532959]\n",
      "[Epoch 63/1001] [Batch 219/372] [D loss: 0.6976968050003052] [G loss: 0.5779651403427124]\n",
      "[Epoch 63/1001] [Batch 220/372] [D loss: 0.696446418762207] [G loss: 0.752475917339325]\n",
      "[Epoch 63/1001] [Batch 221/372] [D loss: 0.6891186237335205] [G loss: 0.6349678039550781]\n",
      "[Epoch 63/1001] [Batch 222/372] [D loss: 0.6922333836555481] [G loss: 0.7079339027404785]\n",
      "[Epoch 63/1001] [Batch 223/372] [D loss: 0.6931952834129333] [G loss: 0.6763583421707153]\n",
      "[Epoch 63/1001] [Batch 224/372] [D loss: 0.6924012899398804] [G loss: 0.6615962386131287]\n",
      "[Epoch 63/1001] [Batch 225/372] [D loss: 0.6934675574302673] [G loss: 0.6961115598678589]\n",
      "[Epoch 63/1001] [Batch 226/372] [D loss: 0.6922234892845154] [G loss: 0.6740270853042603]\n",
      "[Epoch 63/1001] [Batch 227/372] [D loss: 0.6885916590690613] [G loss: 0.6810023188591003]\n",
      "[Epoch 63/1001] [Batch 228/372] [D loss: 0.6941640377044678] [G loss: 0.6723960638046265]\n",
      "[Epoch 63/1001] [Batch 229/372] [D loss: 0.6902798414230347] [G loss: 0.6859389543533325]\n",
      "[Epoch 63/1001] [Batch 230/372] [D loss: 0.6896284222602844] [G loss: 0.6854280233383179]\n",
      "[Epoch 63/1001] [Batch 231/372] [D loss: 0.6873547434806824] [G loss: 0.6641414165496826]\n",
      "[Epoch 63/1001] [Batch 232/372] [D loss: 0.6860477924346924] [G loss: 0.7146348357200623]\n",
      "[Epoch 63/1001] [Batch 233/372] [D loss: 0.6874541640281677] [G loss: 0.6409015655517578]\n",
      "[Epoch 63/1001] [Batch 234/372] [D loss: 0.6905734539031982] [G loss: 0.7057323455810547]\n",
      "[Epoch 63/1001] [Batch 235/372] [D loss: 0.6925447583198547] [G loss: 0.6518906950950623]\n",
      "[Epoch 63/1001] [Batch 236/372] [D loss: 0.6907038688659668] [G loss: 0.6829741597175598]\n",
      "[Epoch 63/1001] [Batch 237/372] [D loss: 0.6859070062637329] [G loss: 0.6893976926803589]\n",
      "[Epoch 63/1001] [Batch 238/372] [D loss: 0.6908618211746216] [G loss: 0.6784628033638]\n",
      "[Epoch 63/1001] [Batch 239/372] [D loss: 0.6871042847633362] [G loss: 0.6612874269485474]\n",
      "[Epoch 63/1001] [Batch 240/372] [D loss: 0.687301754951477] [G loss: 0.7074576616287231]\n",
      "[Epoch 63/1001] [Batch 241/372] [D loss: 0.6906585693359375] [G loss: 0.6331490278244019]\n",
      "[Epoch 63/1001] [Batch 242/372] [D loss: 0.6917629241943359] [G loss: 0.7291061282157898]\n",
      "[Epoch 63/1001] [Batch 243/372] [D loss: 0.6889985203742981] [G loss: 0.608710527420044]\n",
      "[Epoch 63/1001] [Batch 244/372] [D loss: 0.6902903318405151] [G loss: 0.7661438584327698]\n",
      "[Epoch 63/1001] [Batch 245/372] [D loss: 0.6935755610466003] [G loss: 0.5911058187484741]\n",
      "[Epoch 63/1001] [Batch 246/372] [D loss: 0.695608377456665] [G loss: 0.798621416091919]\n",
      "[Epoch 63/1001] [Batch 247/372] [D loss: 0.6964452266693115] [G loss: 0.5399233102798462]\n",
      "[Epoch 63/1001] [Batch 248/372] [D loss: 0.7031224966049194] [G loss: 0.8584193587303162]\n",
      "[Epoch 63/1001] [Batch 249/372] [D loss: 0.7015949487686157] [G loss: 0.524164080619812]\n",
      "[Epoch 63/1001] [Batch 250/372] [D loss: 0.7000446319580078] [G loss: 0.8599739074707031]\n",
      "[Epoch 63/1001] [Batch 251/372] [D loss: 0.7018957138061523] [G loss: 0.5379676818847656]\n",
      "[Epoch 63/1001] [Batch 252/372] [D loss: 0.7012840509414673] [G loss: 0.8141993284225464]\n",
      "[Epoch 63/1001] [Batch 253/372] [D loss: 0.6984077095985413] [G loss: 0.5875027179718018]\n",
      "[Epoch 63/1001] [Batch 254/372] [D loss: 0.6988688707351685] [G loss: 0.7438203692436218]\n",
      "[Epoch 63/1001] [Batch 255/372] [D loss: 0.696143388748169] [G loss: 0.623100757598877]\n",
      "[Epoch 63/1001] [Batch 256/372] [D loss: 0.6872918605804443] [G loss: 0.7186905741691589]\n",
      "[Epoch 63/1001] [Batch 257/372] [D loss: 0.6931289434432983] [G loss: 0.6620081067085266]\n",
      "[Epoch 63/1001] [Batch 258/372] [D loss: 0.6868939399719238] [G loss: 0.6772971749305725]\n",
      "[Epoch 63/1001] [Batch 259/372] [D loss: 0.6877935528755188] [G loss: 0.7006502151489258]\n",
      "[Epoch 63/1001] [Batch 260/372] [D loss: 0.6888492107391357] [G loss: 0.640332043170929]\n",
      "[Epoch 63/1001] [Batch 261/372] [D loss: 0.6965935230255127] [G loss: 0.7199797034263611]\n",
      "[Epoch 63/1001] [Batch 262/372] [D loss: 0.6852191090583801] [G loss: 0.6477186679840088]\n",
      "[Epoch 63/1001] [Batch 263/372] [D loss: 0.6868152022361755] [G loss: 0.6971752643585205]\n",
      "[Epoch 63/1001] [Batch 264/372] [D loss: 0.6901438236236572] [G loss: 0.6774962544441223]\n",
      "[Epoch 63/1001] [Batch 265/372] [D loss: 0.6905938386917114] [G loss: 0.6639618873596191]\n",
      "[Epoch 63/1001] [Batch 266/372] [D loss: 0.6924756169319153] [G loss: 0.691256046295166]\n",
      "[Epoch 63/1001] [Batch 267/372] [D loss: 0.6892197132110596] [G loss: 0.6631465554237366]\n",
      "[Epoch 63/1001] [Batch 268/372] [D loss: 0.6915885210037231] [G loss: 0.6850540041923523]\n",
      "[Epoch 63/1001] [Batch 269/372] [D loss: 0.6932721138000488] [G loss: 0.6789404153823853]\n",
      "[Epoch 63/1001] [Batch 270/372] [D loss: 0.6912449598312378] [G loss: 0.6771658062934875]\n",
      "[Epoch 63/1001] [Batch 271/372] [D loss: 0.684748113155365] [G loss: 0.6829397082328796]\n",
      "[Epoch 63/1001] [Batch 272/372] [D loss: 0.6852632164955139] [G loss: 0.6712952256202698]\n",
      "[Epoch 63/1001] [Batch 273/372] [D loss: 0.6838253140449524] [G loss: 0.6982454061508179]\n",
      "[Epoch 63/1001] [Batch 274/372] [D loss: 0.6836303472518921] [G loss: 0.6574374437332153]\n",
      "[Epoch 63/1001] [Batch 275/372] [D loss: 0.6835232973098755] [G loss: 0.7313154935836792]\n",
      "[Epoch 63/1001] [Batch 276/372] [D loss: 0.6843733191490173] [G loss: 0.6194095015525818]\n",
      "[Epoch 63/1001] [Batch 277/372] [D loss: 0.6932650804519653] [G loss: 0.7339074611663818]\n",
      "[Epoch 63/1001] [Batch 278/372] [D loss: 0.6892725229263306] [G loss: 0.6291207671165466]\n",
      "[Epoch 63/1001] [Batch 279/372] [D loss: 0.687961220741272] [G loss: 0.7315635681152344]\n",
      "[Epoch 63/1001] [Batch 280/372] [D loss: 0.6974125504493713] [G loss: 0.6230409145355225]\n",
      "[Epoch 63/1001] [Batch 281/372] [D loss: 0.6868691444396973] [G loss: 0.7055978775024414]\n",
      "[Epoch 63/1001] [Batch 282/372] [D loss: 0.6867233514785767] [G loss: 0.6534348726272583]\n",
      "[Epoch 63/1001] [Batch 283/372] [D loss: 0.6912872195243835] [G loss: 0.7197574377059937]\n",
      "[Epoch 63/1001] [Batch 284/372] [D loss: 0.6879117488861084] [G loss: 0.6246917843818665]\n",
      "[Epoch 63/1001] [Batch 285/372] [D loss: 0.6887897849082947] [G loss: 0.7159105539321899]\n",
      "[Epoch 63/1001] [Batch 286/372] [D loss: 0.692062497138977] [G loss: 0.6541153192520142]\n",
      "[Epoch 63/1001] [Batch 287/372] [D loss: 0.6898938417434692] [G loss: 0.6781388521194458]\n",
      "[Epoch 63/1001] [Batch 288/372] [D loss: 0.6891318559646606] [G loss: 0.6576386094093323]\n",
      "[Epoch 63/1001] [Batch 289/372] [D loss: 0.6794527769088745] [G loss: 0.6725845336914062]\n",
      "[Epoch 63/1001] [Batch 290/372] [D loss: 0.6859610080718994] [G loss: 0.7110192775726318]\n",
      "[Epoch 63/1001] [Batch 291/372] [D loss: 0.6965382099151611] [G loss: 0.6324349641799927]\n",
      "[Epoch 63/1001] [Batch 292/372] [D loss: 0.6841307878494263] [G loss: 0.7241884469985962]\n",
      "[Epoch 63/1001] [Batch 293/372] [D loss: 0.6878581643104553] [G loss: 0.6228212118148804]\n",
      "[Epoch 63/1001] [Batch 294/372] [D loss: 0.6884995698928833] [G loss: 0.74336838722229]\n",
      "[Epoch 63/1001] [Batch 295/372] [D loss: 0.6872971057891846] [G loss: 0.6193327307701111]\n",
      "[Epoch 63/1001] [Batch 296/372] [D loss: 0.693810224533081] [G loss: 0.7846602201461792]\n",
      "[Epoch 63/1001] [Batch 297/372] [D loss: 0.6868452429771423] [G loss: 0.5573073625564575]\n",
      "[Epoch 63/1001] [Batch 298/372] [D loss: 0.6954374313354492] [G loss: 0.856764554977417]\n",
      "[Epoch 63/1001] [Batch 299/372] [D loss: 0.7023141384124756] [G loss: 0.5069462656974792]\n",
      "[Epoch 63/1001] [Batch 300/372] [D loss: 0.7079450488090515] [G loss: 0.8875250816345215]\n",
      "[Epoch 63/1001] [Batch 301/372] [D loss: 0.7087129354476929] [G loss: 0.5388005971908569]\n",
      "[Epoch 63/1001] [Batch 302/372] [D loss: 0.7002487182617188] [G loss: 0.7715237140655518]\n",
      "[Epoch 63/1001] [Batch 303/372] [D loss: 0.6966570615768433] [G loss: 0.6283541917800903]\n",
      "[Epoch 63/1001] [Batch 304/372] [D loss: 0.6894766092300415] [G loss: 0.7005190253257751]\n",
      "[Epoch 63/1001] [Batch 305/372] [D loss: 0.6894827485084534] [G loss: 0.6786512136459351]\n",
      "[Epoch 63/1001] [Batch 306/372] [D loss: 0.6823616027832031] [G loss: 0.6668189764022827]\n",
      "[Epoch 63/1001] [Batch 307/372] [D loss: 0.6848222017288208] [G loss: 0.7011339664459229]\n",
      "[Epoch 63/1001] [Batch 308/372] [D loss: 0.6917825937271118] [G loss: 0.6454906463623047]\n",
      "[Epoch 63/1001] [Batch 309/372] [D loss: 0.6924541592597961] [G loss: 0.702407956123352]\n",
      "[Epoch 63/1001] [Batch 310/372] [D loss: 0.6881127953529358] [G loss: 0.6574601531028748]\n",
      "[Epoch 63/1001] [Batch 311/372] [D loss: 0.6880906224250793] [G loss: 0.6908363699913025]\n",
      "[Epoch 63/1001] [Batch 312/372] [D loss: 0.6841787099838257] [G loss: 0.6819796562194824]\n",
      "[Epoch 63/1001] [Batch 313/372] [D loss: 0.6959058046340942] [G loss: 0.6491353511810303]\n",
      "[Epoch 63/1001] [Batch 314/372] [D loss: 0.6930460929870605] [G loss: 0.7105456590652466]\n",
      "[Epoch 63/1001] [Batch 315/372] [D loss: 0.685452938079834] [G loss: 0.6232982277870178]\n",
      "[Epoch 63/1001] [Batch 316/372] [D loss: 0.6922184228897095] [G loss: 0.7459968328475952]\n",
      "[Epoch 63/1001] [Batch 317/372] [D loss: 0.6848414540290833] [G loss: 0.6181720495223999]\n",
      "[Epoch 63/1001] [Batch 318/372] [D loss: 0.6879351139068604] [G loss: 0.7295910120010376]\n",
      "[Epoch 63/1001] [Batch 319/372] [D loss: 0.6939808130264282] [G loss: 0.636377215385437]\n",
      "[Epoch 63/1001] [Batch 320/372] [D loss: 0.6952033638954163] [G loss: 0.7085908651351929]\n",
      "[Epoch 63/1001] [Batch 321/372] [D loss: 0.6863760948181152] [G loss: 0.6195063591003418]\n",
      "[Epoch 63/1001] [Batch 322/372] [D loss: 0.6902374029159546] [G loss: 0.7831158638000488]\n",
      "[Epoch 63/1001] [Batch 323/372] [D loss: 0.6974213123321533] [G loss: 0.5604396462440491]\n",
      "[Epoch 63/1001] [Batch 324/372] [D loss: 0.6988248229026794] [G loss: 0.8154346942901611]\n",
      "[Epoch 63/1001] [Batch 325/372] [D loss: 0.6988545060157776] [G loss: 0.5653808116912842]\n",
      "[Epoch 63/1001] [Batch 326/372] [D loss: 0.6955997943878174] [G loss: 0.7841514348983765]\n",
      "[Epoch 63/1001] [Batch 327/372] [D loss: 0.6916354894638062] [G loss: 0.5851097106933594]\n",
      "[Epoch 63/1001] [Batch 328/372] [D loss: 0.6964937448501587] [G loss: 0.7692556381225586]\n",
      "[Epoch 63/1001] [Batch 329/372] [D loss: 0.6923614144325256] [G loss: 0.5927185416221619]\n",
      "[Epoch 63/1001] [Batch 330/372] [D loss: 0.6906946897506714] [G loss: 0.7778530120849609]\n",
      "[Epoch 63/1001] [Batch 331/372] [D loss: 0.691981315612793] [G loss: 0.6067275404930115]\n",
      "[Epoch 63/1001] [Batch 332/372] [D loss: 0.697982668876648] [G loss: 0.7429797649383545]\n",
      "[Epoch 63/1001] [Batch 333/372] [D loss: 0.6937979459762573] [G loss: 0.6205447912216187]\n",
      "[Epoch 63/1001] [Batch 334/372] [D loss: 0.6821180582046509] [G loss: 0.720556378364563]\n",
      "[Epoch 63/1001] [Batch 335/372] [D loss: 0.6889458894729614] [G loss: 0.6305816173553467]\n",
      "[Epoch 63/1001] [Batch 336/372] [D loss: 0.6957834959030151] [G loss: 0.7489245533943176]\n",
      "[Epoch 63/1001] [Batch 337/372] [D loss: 0.6930288076400757] [G loss: 0.5977632403373718]\n",
      "[Epoch 63/1001] [Batch 338/372] [D loss: 0.6955171823501587] [G loss: 0.7742904424667358]\n",
      "[Epoch 63/1001] [Batch 339/372] [D loss: 0.6941366195678711] [G loss: 0.5934237837791443]\n",
      "[Epoch 63/1001] [Batch 340/372] [D loss: 0.6927787065505981] [G loss: 0.7654253244400024]\n",
      "[Epoch 63/1001] [Batch 341/372] [D loss: 0.6888066530227661] [G loss: 0.6062411665916443]\n",
      "[Epoch 63/1001] [Batch 342/372] [D loss: 0.6937706470489502] [G loss: 0.7472910284996033]\n",
      "[Epoch 63/1001] [Batch 343/372] [D loss: 0.6893676519393921] [G loss: 0.6125600337982178]\n",
      "[Epoch 63/1001] [Batch 344/372] [D loss: 0.6882739663124084] [G loss: 0.7496997714042664]\n",
      "[Epoch 63/1001] [Batch 345/372] [D loss: 0.6974072456359863] [G loss: 0.6186915636062622]\n",
      "[Epoch 63/1001] [Batch 346/372] [D loss: 0.6847085952758789] [G loss: 0.7348577976226807]\n",
      "[Epoch 63/1001] [Batch 347/372] [D loss: 0.6891787052154541] [G loss: 0.6332041025161743]\n",
      "[Epoch 63/1001] [Batch 348/372] [D loss: 0.6909438371658325] [G loss: 0.6986318230628967]\n",
      "[Epoch 63/1001] [Batch 349/372] [D loss: 0.6863046884536743] [G loss: 0.6852575540542603]\n",
      "[Epoch 63/1001] [Batch 350/372] [D loss: 0.6883905529975891] [G loss: 0.6506813764572144]\n",
      "[Epoch 63/1001] [Batch 351/372] [D loss: 0.6927839517593384] [G loss: 0.7136873602867126]\n",
      "[Epoch 63/1001] [Batch 352/372] [D loss: 0.6875894069671631] [G loss: 0.623902440071106]\n",
      "[Epoch 63/1001] [Batch 353/372] [D loss: 0.6898403167724609] [G loss: 0.7434254288673401]\n",
      "[Epoch 63/1001] [Batch 354/372] [D loss: 0.6915183067321777] [G loss: 0.6238294243812561]\n",
      "[Epoch 63/1001] [Batch 355/372] [D loss: 0.6887520551681519] [G loss: 0.720413327217102]\n",
      "[Epoch 63/1001] [Batch 356/372] [D loss: 0.6889346241950989] [G loss: 0.6485185623168945]\n",
      "[Epoch 63/1001] [Batch 357/372] [D loss: 0.6877925395965576] [G loss: 0.7108349800109863]\n",
      "[Epoch 63/1001] [Batch 358/372] [D loss: 0.6942995190620422] [G loss: 0.6553044319152832]\n",
      "[Epoch 63/1001] [Batch 359/372] [D loss: 0.6923781633377075] [G loss: 0.6675219535827637]\n",
      "[Epoch 63/1001] [Batch 360/372] [D loss: 0.6924043893814087] [G loss: 0.7034410834312439]\n",
      "[Epoch 63/1001] [Batch 361/372] [D loss: 0.6890494227409363] [G loss: 0.6533116698265076]\n",
      "[Epoch 63/1001] [Batch 362/372] [D loss: 0.6875616312026978] [G loss: 0.6908411979675293]\n",
      "[Epoch 63/1001] [Batch 363/372] [D loss: 0.692385196685791] [G loss: 0.6883045434951782]\n",
      "[Epoch 63/1001] [Batch 364/372] [D loss: 0.6881271600723267] [G loss: 0.6518975496292114]\n",
      "[Epoch 63/1001] [Batch 365/372] [D loss: 0.6833686232566833] [G loss: 0.7087048888206482]\n",
      "[Epoch 63/1001] [Batch 366/372] [D loss: 0.6862083673477173] [G loss: 0.6506942510604858]\n",
      "[Epoch 63/1001] [Batch 367/372] [D loss: 0.6938873529434204] [G loss: 0.7057132124900818]\n",
      "[Epoch 63/1001] [Batch 368/372] [D loss: 0.6857243776321411] [G loss: 0.6352266073226929]\n",
      "[Epoch 63/1001] [Batch 369/372] [D loss: 0.688275158405304] [G loss: 0.7202718257904053]\n",
      "[Epoch 63/1001] [Batch 370/372] [D loss: 0.689994752407074] [G loss: 0.6231014132499695]\n",
      "[Epoch 63/1001] [Batch 371/372] [D loss: 0.6903433799743652] [G loss: 0.7526028752326965]\n",
      "[Epoch 64/1001] [Batch 0/372] [D loss: 0.6889023184776306] [G loss: 0.5703999400138855]\n",
      "[Epoch 64/1001] [Batch 1/372] [D loss: 0.6988179683685303] [G loss: 0.8870656490325928]\n",
      "[Epoch 64/1001] [Batch 2/372] [D loss: 0.7005236148834229] [G loss: 0.46989449858665466]\n",
      "[Epoch 64/1001] [Batch 3/372] [D loss: 0.7123069763183594] [G loss: 0.9598471522331238]\n",
      "[Epoch 64/1001] [Batch 4/372] [D loss: 0.7194406986236572] [G loss: 0.4807594120502472]\n",
      "[Epoch 64/1001] [Batch 5/372] [D loss: 0.712838351726532] [G loss: 0.8707716464996338]\n",
      "[Epoch 64/1001] [Batch 6/372] [D loss: 0.7027888894081116] [G loss: 0.5741565823554993]\n",
      "[Epoch 64/1001] [Batch 7/372] [D loss: 0.6987651586532593] [G loss: 0.713967502117157]\n",
      "[Epoch 64/1001] [Batch 8/372] [D loss: 0.6912720203399658] [G loss: 0.684083878993988]\n",
      "[Epoch 64/1001] [Batch 9/372] [D loss: 0.6795203685760498] [G loss: 0.648034930229187]\n",
      "[Epoch 64/1001] [Batch 10/372] [D loss: 0.6803907155990601] [G loss: 0.744920015335083]\n",
      "[Epoch 64/1001] [Batch 11/372] [D loss: 0.6912298202514648] [G loss: 0.6127815246582031]\n",
      "[Epoch 64/1001] [Batch 12/372] [D loss: 0.6907094717025757] [G loss: 0.7402641773223877]\n",
      "[Epoch 64/1001] [Batch 13/372] [D loss: 0.6887519359588623] [G loss: 0.637505292892456]\n",
      "[Epoch 64/1001] [Batch 14/372] [D loss: 0.6888047456741333] [G loss: 0.7195959091186523]\n",
      "[Epoch 64/1001] [Batch 15/372] [D loss: 0.6840016841888428] [G loss: 0.6434513330459595]\n",
      "[Epoch 64/1001] [Batch 16/372] [D loss: 0.6859380006790161] [G loss: 0.7081699371337891]\n",
      "[Epoch 64/1001] [Batch 17/372] [D loss: 0.6813157796859741] [G loss: 0.6460434198379517]\n",
      "[Epoch 64/1001] [Batch 18/372] [D loss: 0.6906805038452148] [G loss: 0.7815226316452026]\n",
      "[Epoch 64/1001] [Batch 19/372] [D loss: 0.687290608882904] [G loss: 0.5727311968803406]\n",
      "[Epoch 64/1001] [Batch 20/372] [D loss: 0.7016537189483643] [G loss: 0.7598333954811096]\n",
      "[Epoch 64/1001] [Batch 21/372] [D loss: 0.6833551526069641] [G loss: 0.6407602429389954]\n",
      "[Epoch 64/1001] [Batch 22/372] [D loss: 0.6881085634231567] [G loss: 0.690475344657898]\n",
      "[Epoch 64/1001] [Batch 23/372] [D loss: 0.6847717761993408] [G loss: 0.6912450790405273]\n",
      "[Epoch 64/1001] [Batch 24/372] [D loss: 0.6862080097198486] [G loss: 0.6568915843963623]\n",
      "[Epoch 64/1001] [Batch 25/372] [D loss: 0.6888109445571899] [G loss: 0.7044122219085693]\n",
      "[Epoch 64/1001] [Batch 26/372] [D loss: 0.688578188419342] [G loss: 0.6517384648323059]\n",
      "[Epoch 64/1001] [Batch 27/372] [D loss: 0.6898170709609985] [G loss: 0.7063822746276855]\n",
      "[Epoch 64/1001] [Batch 28/372] [D loss: 0.6908775568008423] [G loss: 0.632608950138092]\n",
      "[Epoch 64/1001] [Batch 29/372] [D loss: 0.6917304396629333] [G loss: 0.7331629991531372]\n",
      "[Epoch 64/1001] [Batch 30/372] [D loss: 0.6888547539710999] [G loss: 0.6292943954467773]\n",
      "[Epoch 64/1001] [Batch 31/372] [D loss: 0.6854799389839172] [G loss: 0.7205705046653748]\n",
      "[Epoch 64/1001] [Batch 32/372] [D loss: 0.6947481632232666] [G loss: 0.6256186366081238]\n",
      "[Epoch 64/1001] [Batch 33/372] [D loss: 0.686936616897583] [G loss: 0.7314813137054443]\n",
      "[Epoch 64/1001] [Batch 34/372] [D loss: 0.6877890825271606] [G loss: 0.6437584161758423]\n",
      "[Epoch 64/1001] [Batch 35/372] [D loss: 0.6916241645812988] [G loss: 0.7045009732246399]\n",
      "[Epoch 64/1001] [Batch 36/372] [D loss: 0.6906320452690125] [G loss: 0.6609057784080505]\n",
      "[Epoch 64/1001] [Batch 37/372] [D loss: 0.6891894340515137] [G loss: 0.6853191256523132]\n",
      "[Epoch 64/1001] [Batch 38/372] [D loss: 0.6843909025192261] [G loss: 0.6924169659614563]\n",
      "[Epoch 64/1001] [Batch 39/372] [D loss: 0.6892337799072266] [G loss: 0.6398981213569641]\n",
      "[Epoch 64/1001] [Batch 40/372] [D loss: 0.6860579252243042] [G loss: 0.7193182110786438]\n",
      "[Epoch 64/1001] [Batch 41/372] [D loss: 0.6824071407318115] [G loss: 0.6322999596595764]\n",
      "[Epoch 64/1001] [Batch 42/372] [D loss: 0.6870602965354919] [G loss: 0.7353542447090149]\n",
      "[Epoch 64/1001] [Batch 43/372] [D loss: 0.6853914260864258] [G loss: 0.6280950307846069]\n",
      "[Epoch 64/1001] [Batch 44/372] [D loss: 0.6863868832588196] [G loss: 0.7464863061904907]\n",
      "[Epoch 64/1001] [Batch 45/372] [D loss: 0.6852072477340698] [G loss: 0.6057697534561157]\n",
      "[Epoch 64/1001] [Batch 46/372] [D loss: 0.6925437450408936] [G loss: 0.7623422145843506]\n",
      "[Epoch 64/1001] [Batch 47/372] [D loss: 0.6945369243621826] [G loss: 0.5874311327934265]\n",
      "[Epoch 64/1001] [Batch 48/372] [D loss: 0.6923356056213379] [G loss: 0.7890465259552002]\n",
      "[Epoch 64/1001] [Batch 49/372] [D loss: 0.6932251453399658] [G loss: 0.5880018472671509]\n",
      "[Epoch 64/1001] [Batch 50/372] [D loss: 0.6914544105529785] [G loss: 0.744120180606842]\n",
      "[Epoch 64/1001] [Batch 51/372] [D loss: 0.6848667860031128] [G loss: 0.6233584880828857]\n",
      "[Epoch 64/1001] [Batch 52/372] [D loss: 0.6813518404960632] [G loss: 0.735064685344696]\n",
      "[Epoch 64/1001] [Batch 53/372] [D loss: 0.6947319507598877] [G loss: 0.6266857385635376]\n",
      "[Epoch 64/1001] [Batch 54/372] [D loss: 0.6903914213180542] [G loss: 0.7547115683555603]\n",
      "[Epoch 64/1001] [Batch 55/372] [D loss: 0.6963226199150085] [G loss: 0.5918723344802856]\n",
      "[Epoch 64/1001] [Batch 56/372] [D loss: 0.6889562606811523] [G loss: 0.8021223545074463]\n",
      "[Epoch 64/1001] [Batch 57/372] [D loss: 0.7018893361091614] [G loss: 0.571109414100647]\n",
      "[Epoch 64/1001] [Batch 58/372] [D loss: 0.697028636932373] [G loss: 0.7819627523422241]\n",
      "[Epoch 64/1001] [Batch 59/372] [D loss: 0.6918666958808899] [G loss: 0.6042662262916565]\n",
      "[Epoch 64/1001] [Batch 60/372] [D loss: 0.6883882284164429] [G loss: 0.7321296334266663]\n",
      "[Epoch 64/1001] [Batch 61/372] [D loss: 0.6916463375091553] [G loss: 0.6514396071434021]\n",
      "[Epoch 64/1001] [Batch 62/372] [D loss: 0.6835411787033081] [G loss: 0.6917938590049744]\n",
      "[Epoch 64/1001] [Batch 63/372] [D loss: 0.6830129027366638] [G loss: 0.7014999985694885]\n",
      "[Epoch 64/1001] [Batch 64/372] [D loss: 0.6895875930786133] [G loss: 0.6287596225738525]\n",
      "[Epoch 64/1001] [Batch 65/372] [D loss: 0.6930750608444214] [G loss: 0.737533688545227]\n",
      "[Epoch 64/1001] [Batch 66/372] [D loss: 0.687514066696167] [G loss: 0.6157622337341309]\n",
      "[Epoch 64/1001] [Batch 67/372] [D loss: 0.6918965578079224] [G loss: 0.7430909872055054]\n",
      "[Epoch 64/1001] [Batch 68/372] [D loss: 0.6859626770019531] [G loss: 0.6199284195899963]\n",
      "[Epoch 64/1001] [Batch 69/372] [D loss: 0.6933183670043945] [G loss: 0.7431219816207886]\n",
      "[Epoch 64/1001] [Batch 70/372] [D loss: 0.6883478164672852] [G loss: 0.6186622977256775]\n",
      "[Epoch 64/1001] [Batch 71/372] [D loss: 0.6918027400970459] [G loss: 0.7385963201522827]\n",
      "[Epoch 64/1001] [Batch 72/372] [D loss: 0.689542293548584] [G loss: 0.6201915740966797]\n",
      "[Epoch 64/1001] [Batch 73/372] [D loss: 0.6955556273460388] [G loss: 0.7297738790512085]\n",
      "[Epoch 64/1001] [Batch 74/372] [D loss: 0.6890425682067871] [G loss: 0.6432638764381409]\n",
      "[Epoch 64/1001] [Batch 75/372] [D loss: 0.6884137392044067] [G loss: 0.7046635150909424]\n",
      "[Epoch 64/1001] [Batch 76/372] [D loss: 0.6867163181304932] [G loss: 0.674934446811676]\n",
      "[Epoch 64/1001] [Batch 77/372] [D loss: 0.6837210655212402] [G loss: 0.6732562780380249]\n",
      "[Epoch 64/1001] [Batch 78/372] [D loss: 0.690131664276123] [G loss: 0.6916975378990173]\n",
      "[Epoch 64/1001] [Batch 79/372] [D loss: 0.6844804883003235] [G loss: 0.6644201278686523]\n",
      "[Epoch 64/1001] [Batch 80/372] [D loss: 0.68497234582901] [G loss: 0.682037889957428]\n",
      "[Epoch 64/1001] [Batch 81/372] [D loss: 0.6894209384918213] [G loss: 0.6632310748100281]\n",
      "[Epoch 64/1001] [Batch 82/372] [D loss: 0.692186176776886] [G loss: 0.7119983434677124]\n",
      "[Epoch 64/1001] [Batch 83/372] [D loss: 0.6890246868133545] [G loss: 0.6538119316101074]\n",
      "[Epoch 64/1001] [Batch 84/372] [D loss: 0.6895800828933716] [G loss: 0.7055371999740601]\n",
      "[Epoch 64/1001] [Batch 85/372] [D loss: 0.6858444213867188] [G loss: 0.6433143019676208]\n",
      "[Epoch 64/1001] [Batch 86/372] [D loss: 0.6865581274032593] [G loss: 0.7180915474891663]\n",
      "[Epoch 64/1001] [Batch 87/372] [D loss: 0.6875697374343872] [G loss: 0.6601580381393433]\n",
      "[Epoch 64/1001] [Batch 88/372] [D loss: 0.6900993585586548] [G loss: 0.6647127866744995]\n",
      "[Epoch 64/1001] [Batch 89/372] [D loss: 0.683468222618103] [G loss: 0.6999754309654236]\n",
      "[Epoch 64/1001] [Batch 90/372] [D loss: 0.6919028162956238] [G loss: 0.6545385122299194]\n",
      "[Epoch 64/1001] [Batch 91/372] [D loss: 0.6837323904037476] [G loss: 0.7298563718795776]\n",
      "[Epoch 64/1001] [Batch 92/372] [D loss: 0.6970707774162292] [G loss: 0.5856075882911682]\n",
      "[Epoch 64/1001] [Batch 93/372] [D loss: 0.6926087141036987] [G loss: 0.7991138696670532]\n",
      "[Epoch 64/1001] [Batch 94/372] [D loss: 0.6876281499862671] [G loss: 0.5321288704872131]\n",
      "[Epoch 64/1001] [Batch 95/372] [D loss: 0.699685275554657] [G loss: 0.9555492997169495]\n",
      "[Epoch 64/1001] [Batch 96/372] [D loss: 0.7074662446975708] [G loss: 0.43709471821784973]\n",
      "[Epoch 64/1001] [Batch 97/372] [D loss: 0.732106626033783] [G loss: 0.9991495609283447]\n",
      "[Epoch 64/1001] [Batch 98/372] [D loss: 0.7246092557907104] [G loss: 0.47217637300491333]\n",
      "[Epoch 64/1001] [Batch 99/372] [D loss: 0.7161684632301331] [G loss: 0.823293924331665]\n",
      "[Epoch 64/1001] [Batch 100/372] [D loss: 0.7039822340011597] [G loss: 0.5897330045700073]\n",
      "[Epoch 64/1001] [Batch 101/372] [D loss: 0.7010713815689087] [G loss: 0.7245593667030334]\n",
      "[Epoch 64/1001] [Batch 102/372] [D loss: 0.6923815011978149] [G loss: 0.6502746343612671]\n",
      "[Epoch 64/1001] [Batch 103/372] [D loss: 0.6852849721908569] [G loss: 0.6718978881835938]\n",
      "[Epoch 64/1001] [Batch 104/372] [D loss: 0.6868438720703125] [G loss: 0.7048707008361816]\n",
      "[Epoch 64/1001] [Batch 105/372] [D loss: 0.6932175755500793] [G loss: 0.6416010856628418]\n",
      "[Epoch 64/1001] [Batch 106/372] [D loss: 0.6923884749412537] [G loss: 0.699638307094574]\n",
      "[Epoch 64/1001] [Batch 107/372] [D loss: 0.6949973106384277] [G loss: 0.6525591611862183]\n",
      "[Epoch 64/1001] [Batch 108/372] [D loss: 0.6871441006660461] [G loss: 0.7146490216255188]\n",
      "[Epoch 64/1001] [Batch 109/372] [D loss: 0.6883667707443237] [G loss: 0.6574453115463257]\n",
      "[Epoch 64/1001] [Batch 110/372] [D loss: 0.6835250854492188] [G loss: 0.6789438128471375]\n",
      "[Epoch 64/1001] [Batch 111/372] [D loss: 0.6867826581001282] [G loss: 0.6867765188217163]\n",
      "[Epoch 64/1001] [Batch 112/372] [D loss: 0.6925845146179199] [G loss: 0.6698625683784485]\n",
      "[Epoch 64/1001] [Batch 113/372] [D loss: 0.6887315511703491] [G loss: 0.6697940230369568]\n",
      "[Epoch 64/1001] [Batch 114/372] [D loss: 0.6854879260063171] [G loss: 0.6942534446716309]\n",
      "[Epoch 64/1001] [Batch 115/372] [D loss: 0.6890760660171509] [G loss: 0.6657921075820923]\n",
      "[Epoch 64/1001] [Batch 116/372] [D loss: 0.6923579573631287] [G loss: 0.6922597289085388]\n",
      "[Epoch 64/1001] [Batch 117/372] [D loss: 0.6860730648040771] [G loss: 0.6492471694946289]\n",
      "[Epoch 64/1001] [Batch 118/372] [D loss: 0.687116801738739] [G loss: 0.6965745091438293]\n",
      "[Epoch 64/1001] [Batch 119/372] [D loss: 0.6864392161369324] [G loss: 0.6667906045913696]\n",
      "[Epoch 64/1001] [Batch 120/372] [D loss: 0.681824803352356] [G loss: 0.6742613315582275]\n",
      "[Epoch 64/1001] [Batch 121/372] [D loss: 0.6928603649139404] [G loss: 0.7111236453056335]\n",
      "[Epoch 64/1001] [Batch 122/372] [D loss: 0.6893464922904968] [G loss: 0.6178267598152161]\n",
      "[Epoch 64/1001] [Batch 123/372] [D loss: 0.6932858228683472] [G loss: 0.7275799512863159]\n",
      "[Epoch 64/1001] [Batch 124/372] [D loss: 0.6865760087966919] [G loss: 0.630038857460022]\n",
      "[Epoch 64/1001] [Batch 125/372] [D loss: 0.6839268207550049] [G loss: 0.7372006177902222]\n",
      "[Epoch 64/1001] [Batch 126/372] [D loss: 0.6910461187362671] [G loss: 0.6225106716156006]\n",
      "[Epoch 64/1001] [Batch 127/372] [D loss: 0.69051194190979] [G loss: 0.7394435405731201]\n",
      "[Epoch 64/1001] [Batch 128/372] [D loss: 0.691449761390686] [G loss: 0.6257190108299255]\n",
      "[Epoch 64/1001] [Batch 129/372] [D loss: 0.687545657157898] [G loss: 0.6942314505577087]\n",
      "[Epoch 64/1001] [Batch 130/372] [D loss: 0.686447024345398] [G loss: 0.6734222173690796]\n",
      "[Epoch 64/1001] [Batch 131/372] [D loss: 0.6823691129684448] [G loss: 0.6883745193481445]\n",
      "[Epoch 64/1001] [Batch 132/372] [D loss: 0.6880249977111816] [G loss: 0.6604651808738708]\n",
      "[Epoch 64/1001] [Batch 133/372] [D loss: 0.6830896139144897] [G loss: 0.6886903047561646]\n",
      "[Epoch 64/1001] [Batch 134/372] [D loss: 0.6847836971282959] [G loss: 0.6357616782188416]\n",
      "[Epoch 64/1001] [Batch 135/372] [D loss: 0.6932669878005981] [G loss: 0.7490312457084656]\n",
      "[Epoch 64/1001] [Batch 136/372] [D loss: 0.6908233165740967] [G loss: 0.5938326120376587]\n",
      "[Epoch 64/1001] [Batch 137/372] [D loss: 0.6954954862594604] [G loss: 0.7562052011489868]\n",
      "[Epoch 64/1001] [Batch 138/372] [D loss: 0.6912230849266052] [G loss: 0.6052896976470947]\n",
      "[Epoch 64/1001] [Batch 139/372] [D loss: 0.6885101795196533] [G loss: 0.7359110713005066]\n",
      "[Epoch 64/1001] [Batch 140/372] [D loss: 0.6888875365257263] [G loss: 0.6281571388244629]\n",
      "[Epoch 64/1001] [Batch 141/372] [D loss: 0.6840078830718994] [G loss: 0.7200015187263489]\n",
      "[Epoch 64/1001] [Batch 142/372] [D loss: 0.6898557543754578] [G loss: 0.647927463054657]\n",
      "[Epoch 64/1001] [Batch 143/372] [D loss: 0.6903209686279297] [G loss: 0.6990450620651245]\n",
      "[Epoch 64/1001] [Batch 144/372] [D loss: 0.6885121464729309] [G loss: 0.6567310690879822]\n",
      "[Epoch 64/1001] [Batch 145/372] [D loss: 0.6881440877914429] [G loss: 0.693271279335022]\n",
      "[Epoch 64/1001] [Batch 146/372] [D loss: 0.6887608766555786] [G loss: 0.6828881502151489]\n",
      "[Epoch 64/1001] [Batch 147/372] [D loss: 0.693976640701294] [G loss: 0.6602672338485718]\n",
      "[Epoch 64/1001] [Batch 148/372] [D loss: 0.6814380288124084] [G loss: 0.6911608576774597]\n",
      "[Epoch 64/1001] [Batch 149/372] [D loss: 0.6856767535209656] [G loss: 0.6706867814064026]\n",
      "[Epoch 64/1001] [Batch 150/372] [D loss: 0.6844262480735779] [G loss: 0.6692163348197937]\n",
      "[Epoch 64/1001] [Batch 151/372] [D loss: 0.6852297186851501] [G loss: 0.7005237340927124]\n",
      "[Epoch 64/1001] [Batch 152/372] [D loss: 0.6896232962608337] [G loss: 0.6546607613563538]\n",
      "[Epoch 64/1001] [Batch 153/372] [D loss: 0.6877156496047974] [G loss: 0.7164316177368164]\n",
      "[Epoch 64/1001] [Batch 154/372] [D loss: 0.6891627907752991] [G loss: 0.6405559778213501]\n",
      "[Epoch 64/1001] [Batch 155/372] [D loss: 0.6923110485076904] [G loss: 0.7144322395324707]\n",
      "[Epoch 64/1001] [Batch 156/372] [D loss: 0.6941726803779602] [G loss: 0.640296220779419]\n",
      "[Epoch 64/1001] [Batch 157/372] [D loss: 0.683660089969635] [G loss: 0.6948325037956238]\n",
      "[Epoch 64/1001] [Batch 158/372] [D loss: 0.6903515458106995] [G loss: 0.666679322719574]\n",
      "[Epoch 64/1001] [Batch 159/372] [D loss: 0.6863386034965515] [G loss: 0.6996428966522217]\n",
      "[Epoch 64/1001] [Batch 160/372] [D loss: 0.6831090450286865] [G loss: 0.6600735783576965]\n",
      "[Epoch 64/1001] [Batch 161/372] [D loss: 0.6908292770385742] [G loss: 0.679619550704956]\n",
      "[Epoch 64/1001] [Batch 162/372] [D loss: 0.6828963756561279] [G loss: 0.6768726110458374]\n",
      "[Epoch 64/1001] [Batch 163/372] [D loss: 0.6837464570999146] [G loss: 0.642787754535675]\n",
      "[Epoch 64/1001] [Batch 164/372] [D loss: 0.6877509355545044] [G loss: 0.7923059463500977]\n",
      "[Epoch 64/1001] [Batch 165/372] [D loss: 0.698336660861969] [G loss: 0.533553957939148]\n",
      "[Epoch 64/1001] [Batch 166/372] [D loss: 0.6947339177131653] [G loss: 0.8918640613555908]\n",
      "[Epoch 64/1001] [Batch 167/372] [D loss: 0.7073159217834473] [G loss: 0.506902813911438]\n",
      "[Epoch 64/1001] [Batch 168/372] [D loss: 0.7114531993865967] [G loss: 0.8625810146331787]\n",
      "[Epoch 64/1001] [Batch 169/372] [D loss: 0.7058488130569458] [G loss: 0.5601016879081726]\n",
      "[Epoch 64/1001] [Batch 170/372] [D loss: 0.6982832551002502] [G loss: 0.7578127980232239]\n",
      "[Epoch 64/1001] [Batch 171/372] [D loss: 0.6890455484390259] [G loss: 0.654364824295044]\n",
      "[Epoch 64/1001] [Batch 172/372] [D loss: 0.6830602884292603] [G loss: 0.67335045337677]\n",
      "[Epoch 64/1001] [Batch 173/372] [D loss: 0.6899087429046631] [G loss: 0.7062045335769653]\n",
      "[Epoch 64/1001] [Batch 174/372] [D loss: 0.6866171360015869] [G loss: 0.6451869606971741]\n",
      "[Epoch 64/1001] [Batch 175/372] [D loss: 0.6808944940567017] [G loss: 0.7131593227386475]\n",
      "[Epoch 64/1001] [Batch 176/372] [D loss: 0.6899968385696411] [G loss: 0.6240689158439636]\n",
      "[Epoch 64/1001] [Batch 177/372] [D loss: 0.6834397912025452] [G loss: 0.7377116680145264]\n",
      "[Epoch 64/1001] [Batch 178/372] [D loss: 0.6910042762756348] [G loss: 0.6296827793121338]\n",
      "[Epoch 64/1001] [Batch 179/372] [D loss: 0.6896583437919617] [G loss: 0.6853129267692566]\n",
      "[Epoch 64/1001] [Batch 180/372] [D loss: 0.6848759055137634] [G loss: 0.6944210529327393]\n",
      "[Epoch 64/1001] [Batch 181/372] [D loss: 0.6882106065750122] [G loss: 0.6309382915496826]\n",
      "[Epoch 64/1001] [Batch 182/372] [D loss: 0.6832277774810791] [G loss: 0.7236326932907104]\n",
      "[Epoch 64/1001] [Batch 183/372] [D loss: 0.689637303352356] [G loss: 0.6377749443054199]\n",
      "[Epoch 64/1001] [Batch 184/372] [D loss: 0.6868665218353271] [G loss: 0.6937183737754822]\n",
      "[Epoch 64/1001] [Batch 185/372] [D loss: 0.6871691942214966] [G loss: 0.6708343625068665]\n",
      "[Epoch 64/1001] [Batch 186/372] [D loss: 0.6907414793968201] [G loss: 0.6432128548622131]\n",
      "[Epoch 64/1001] [Batch 187/372] [D loss: 0.6857134103775024] [G loss: 0.7499911189079285]\n",
      "[Epoch 64/1001] [Batch 188/372] [D loss: 0.683417558670044] [G loss: 0.5850314497947693]\n",
      "[Epoch 64/1001] [Batch 189/372] [D loss: 0.6959124803543091] [G loss: 0.8447455167770386]\n",
      "[Epoch 64/1001] [Batch 190/372] [D loss: 0.7067950367927551] [G loss: 0.4781780540943146]\n",
      "[Epoch 64/1001] [Batch 191/372] [D loss: 0.7145640254020691] [G loss: 0.9617736339569092]\n",
      "[Epoch 64/1001] [Batch 192/372] [D loss: 0.7204844951629639] [G loss: 0.49346795678138733]\n",
      "[Epoch 64/1001] [Batch 193/372] [D loss: 0.7122398614883423] [G loss: 0.8166341781616211]\n",
      "[Epoch 64/1001] [Batch 194/372] [D loss: 0.7005251049995422] [G loss: 0.6195334196090698]\n",
      "[Epoch 64/1001] [Batch 195/372] [D loss: 0.6945300102233887] [G loss: 0.6887004375457764]\n",
      "[Epoch 64/1001] [Batch 196/372] [D loss: 0.6925746202468872] [G loss: 0.6668723821640015]\n",
      "[Epoch 64/1001] [Batch 197/372] [D loss: 0.6867811679840088] [G loss: 0.6850495338439941]\n",
      "[Epoch 64/1001] [Batch 198/372] [D loss: 0.6871391534805298] [G loss: 0.6779566407203674]\n",
      "[Epoch 64/1001] [Batch 199/372] [D loss: 0.6934729218482971] [G loss: 0.6836745738983154]\n",
      "[Epoch 64/1001] [Batch 200/372] [D loss: 0.6858259439468384] [G loss: 0.6535850167274475]\n",
      "[Epoch 64/1001] [Batch 201/372] [D loss: 0.6909149885177612] [G loss: 0.732093870639801]\n",
      "[Epoch 64/1001] [Batch 202/372] [D loss: 0.6843256950378418] [G loss: 0.6165491342544556]\n",
      "[Epoch 64/1001] [Batch 203/372] [D loss: 0.6915163397789001] [G loss: 0.7457436919212341]\n",
      "[Epoch 64/1001] [Batch 204/372] [D loss: 0.6879199743270874] [G loss: 0.6303191781044006]\n",
      "[Epoch 64/1001] [Batch 205/372] [D loss: 0.6870604753494263] [G loss: 0.7041857242584229]\n",
      "[Epoch 64/1001] [Batch 206/372] [D loss: 0.6842507123947144] [G loss: 0.6852503418922424]\n",
      "[Epoch 64/1001] [Batch 207/372] [D loss: 0.6934707164764404] [G loss: 0.6540635228157043]\n",
      "[Epoch 64/1001] [Batch 208/372] [D loss: 0.6896464824676514] [G loss: 0.7070021629333496]\n",
      "[Epoch 64/1001] [Batch 209/372] [D loss: 0.6877853870391846] [G loss: 0.6528725624084473]\n",
      "[Epoch 64/1001] [Batch 210/372] [D loss: 0.6887451410293579] [G loss: 0.7029749155044556]\n",
      "[Epoch 64/1001] [Batch 211/372] [D loss: 0.693733811378479] [G loss: 0.6715269088745117]\n",
      "[Epoch 64/1001] [Batch 212/372] [D loss: 0.6939613819122314] [G loss: 0.6806776523590088]\n",
      "[Epoch 64/1001] [Batch 213/372] [D loss: 0.6923561096191406] [G loss: 0.6721599102020264]\n",
      "[Epoch 64/1001] [Batch 214/372] [D loss: 0.6877300143241882] [G loss: 0.6825345754623413]\n",
      "[Epoch 64/1001] [Batch 215/372] [D loss: 0.6895936131477356] [G loss: 0.6863387227058411]\n",
      "[Epoch 64/1001] [Batch 216/372] [D loss: 0.6892117261886597] [G loss: 0.666354238986969]\n",
      "[Epoch 64/1001] [Batch 217/372] [D loss: 0.6907970309257507] [G loss: 0.6822720766067505]\n",
      "[Epoch 64/1001] [Batch 218/372] [D loss: 0.6871511340141296] [G loss: 0.6819398403167725]\n",
      "[Epoch 64/1001] [Batch 219/372] [D loss: 0.6893370151519775] [G loss: 0.65770024061203]\n",
      "[Epoch 64/1001] [Batch 220/372] [D loss: 0.6935127377510071] [G loss: 0.6975533962249756]\n",
      "[Epoch 64/1001] [Batch 221/372] [D loss: 0.6899504661560059] [G loss: 0.6780880093574524]\n",
      "[Epoch 64/1001] [Batch 222/372] [D loss: 0.6847331523895264] [G loss: 0.6695947051048279]\n",
      "[Epoch 64/1001] [Batch 223/372] [D loss: 0.6811991930007935] [G loss: 0.6838976144790649]\n",
      "[Epoch 64/1001] [Batch 224/372] [D loss: 0.6852109432220459] [G loss: 0.6895813941955566]\n",
      "[Epoch 64/1001] [Batch 225/372] [D loss: 0.6887767910957336] [G loss: 0.6466150879859924]\n",
      "[Epoch 64/1001] [Batch 226/372] [D loss: 0.6926989555358887] [G loss: 0.7111217975616455]\n",
      "[Epoch 64/1001] [Batch 227/372] [D loss: 0.6904085874557495] [G loss: 0.6478807330131531]\n",
      "[Epoch 64/1001] [Batch 228/372] [D loss: 0.6849332451820374] [G loss: 0.6979681253433228]\n",
      "[Epoch 64/1001] [Batch 229/372] [D loss: 0.6892266273498535] [G loss: 0.6701658964157104]\n",
      "[Epoch 64/1001] [Batch 230/372] [D loss: 0.6906440258026123] [G loss: 0.6708096861839294]\n",
      "[Epoch 64/1001] [Batch 231/372] [D loss: 0.6921887397766113] [G loss: 0.6847982406616211]\n",
      "[Epoch 64/1001] [Batch 232/372] [D loss: 0.6859756112098694] [G loss: 0.666408360004425]\n",
      "[Epoch 64/1001] [Batch 233/372] [D loss: 0.6947398781776428] [G loss: 0.6892828345298767]\n",
      "[Epoch 64/1001] [Batch 234/372] [D loss: 0.6888738870620728] [G loss: 0.670219898223877]\n",
      "[Epoch 64/1001] [Batch 235/372] [D loss: 0.6939263343811035] [G loss: 0.6629687547683716]\n",
      "[Epoch 64/1001] [Batch 236/372] [D loss: 0.6887822151184082] [G loss: 0.6913864612579346]\n",
      "[Epoch 64/1001] [Batch 237/372] [D loss: 0.6913937330245972] [G loss: 0.6687218546867371]\n",
      "[Epoch 64/1001] [Batch 238/372] [D loss: 0.6922136545181274] [G loss: 0.6684032678604126]\n",
      "[Epoch 64/1001] [Batch 239/372] [D loss: 0.6867501735687256] [G loss: 0.6940792798995972]\n",
      "[Epoch 64/1001] [Batch 240/372] [D loss: 0.6850403547286987] [G loss: 0.6719260215759277]\n",
      "[Epoch 64/1001] [Batch 241/372] [D loss: 0.6877117156982422] [G loss: 0.6781741976737976]\n",
      "[Epoch 64/1001] [Batch 242/372] [D loss: 0.684888482093811] [G loss: 0.6802399158477783]\n",
      "[Epoch 64/1001] [Batch 243/372] [D loss: 0.6822369694709778] [G loss: 0.680600106716156]\n",
      "[Epoch 64/1001] [Batch 244/372] [D loss: 0.68959641456604] [G loss: 0.6633236408233643]\n",
      "[Epoch 64/1001] [Batch 245/372] [D loss: 0.6875730752944946] [G loss: 0.6865456700325012]\n",
      "[Epoch 64/1001] [Batch 246/372] [D loss: 0.6902147531509399] [G loss: 0.6642507314682007]\n",
      "[Epoch 64/1001] [Batch 247/372] [D loss: 0.6903436183929443] [G loss: 0.6708745360374451]\n",
      "[Epoch 64/1001] [Batch 248/372] [D loss: 0.6968457698822021] [G loss: 0.6741235256195068]\n",
      "[Epoch 64/1001] [Batch 249/372] [D loss: 0.6911240220069885] [G loss: 0.6662054657936096]\n",
      "[Epoch 64/1001] [Batch 250/372] [D loss: 0.6923782825469971] [G loss: 0.6844482421875]\n",
      "[Epoch 64/1001] [Batch 251/372] [D loss: 0.6941350102424622] [G loss: 0.658315122127533]\n",
      "[Epoch 64/1001] [Batch 252/372] [D loss: 0.693652868270874] [G loss: 0.7213054895401001]\n",
      "[Epoch 64/1001] [Batch 253/372] [D loss: 0.6880598068237305] [G loss: 0.6173001527786255]\n",
      "[Epoch 64/1001] [Batch 254/372] [D loss: 0.6929144263267517] [G loss: 0.7514716982841492]\n",
      "[Epoch 64/1001] [Batch 255/372] [D loss: 0.6874047517776489] [G loss: 0.6177349090576172]\n",
      "[Epoch 64/1001] [Batch 256/372] [D loss: 0.695580244064331] [G loss: 0.7362269759178162]\n",
      "[Epoch 64/1001] [Batch 257/372] [D loss: 0.6901060342788696] [G loss: 0.6202884912490845]\n",
      "[Epoch 64/1001] [Batch 258/372] [D loss: 0.6919837594032288] [G loss: 0.7407857179641724]\n",
      "[Epoch 64/1001] [Batch 259/372] [D loss: 0.6906308531761169] [G loss: 0.6018389463424683]\n",
      "[Epoch 64/1001] [Batch 260/372] [D loss: 0.689797043800354] [G loss: 0.7881022691726685]\n",
      "[Epoch 64/1001] [Batch 261/372] [D loss: 0.6936637163162231] [G loss: 0.5771479606628418]\n",
      "[Epoch 64/1001] [Batch 262/372] [D loss: 0.6944462060928345] [G loss: 0.7918578386306763]\n",
      "[Epoch 64/1001] [Batch 263/372] [D loss: 0.6898984909057617] [G loss: 0.582154393196106]\n",
      "[Epoch 64/1001] [Batch 264/372] [D loss: 0.6924704313278198] [G loss: 0.7714332342147827]\n",
      "[Epoch 64/1001] [Batch 265/372] [D loss: 0.6935701370239258] [G loss: 0.5895774364471436]\n",
      "[Epoch 64/1001] [Batch 266/372] [D loss: 0.6871000528335571] [G loss: 0.7541362047195435]\n",
      "[Epoch 64/1001] [Batch 267/372] [D loss: 0.6879332065582275] [G loss: 0.6277981400489807]\n",
      "[Epoch 64/1001] [Batch 268/372] [D loss: 0.694693922996521] [G loss: 0.7123128175735474]\n",
      "[Epoch 64/1001] [Batch 269/372] [D loss: 0.6874971389770508] [G loss: 0.6406305432319641]\n",
      "[Epoch 64/1001] [Batch 270/372] [D loss: 0.6933982968330383] [G loss: 0.7342748045921326]\n",
      "[Epoch 64/1001] [Batch 271/372] [D loss: 0.6893185377120972] [G loss: 0.6099548935890198]\n",
      "[Epoch 64/1001] [Batch 272/372] [D loss: 0.6948110461235046] [G loss: 0.7413453459739685]\n",
      "[Epoch 64/1001] [Batch 273/372] [D loss: 0.6923056244850159] [G loss: 0.6165199875831604]\n",
      "[Epoch 64/1001] [Batch 274/372] [D loss: 0.6878811717033386] [G loss: 0.7361764907836914]\n",
      "[Epoch 64/1001] [Batch 275/372] [D loss: 0.6956169605255127] [G loss: 0.6272284388542175]\n",
      "[Epoch 64/1001] [Batch 276/372] [D loss: 0.6857563853263855] [G loss: 0.7431826591491699]\n",
      "[Epoch 64/1001] [Batch 277/372] [D loss: 0.6877502799034119] [G loss: 0.6097121834754944]\n",
      "[Epoch 64/1001] [Batch 278/372] [D loss: 0.6859354972839355] [G loss: 0.8087468147277832]\n",
      "[Epoch 64/1001] [Batch 279/372] [D loss: 0.6974851489067078] [G loss: 0.5519675612449646]\n",
      "[Epoch 64/1001] [Batch 280/372] [D loss: 0.6985276937484741] [G loss: 0.8159103989601135]\n",
      "[Epoch 64/1001] [Batch 281/372] [D loss: 0.6984677314758301] [G loss: 0.5842275619506836]\n",
      "[Epoch 64/1001] [Batch 282/372] [D loss: 0.6882504224777222] [G loss: 0.7540208101272583]\n",
      "[Epoch 64/1001] [Batch 283/372] [D loss: 0.6853768229484558] [G loss: 0.6226025223731995]\n",
      "[Epoch 64/1001] [Batch 284/372] [D loss: 0.689240038394928] [G loss: 0.6920175552368164]\n",
      "[Epoch 64/1001] [Batch 285/372] [D loss: 0.6941075325012207] [G loss: 0.6842019557952881]\n",
      "[Epoch 64/1001] [Batch 286/372] [D loss: 0.6891617774963379] [G loss: 0.6740099191665649]\n",
      "[Epoch 64/1001] [Batch 287/372] [D loss: 0.689700722694397] [G loss: 0.6942988634109497]\n",
      "[Epoch 64/1001] [Batch 288/372] [D loss: 0.6881654262542725] [G loss: 0.6448618173599243]\n",
      "[Epoch 64/1001] [Batch 289/372] [D loss: 0.6899742484092712] [G loss: 0.7127703428268433]\n",
      "[Epoch 64/1001] [Batch 290/372] [D loss: 0.6913308501243591] [G loss: 0.6286734342575073]\n",
      "[Epoch 64/1001] [Batch 291/372] [D loss: 0.6924318671226501] [G loss: 0.759951114654541]\n",
      "[Epoch 64/1001] [Batch 292/372] [D loss: 0.6923218965530396] [G loss: 0.5682307481765747]\n",
      "[Epoch 64/1001] [Batch 293/372] [D loss: 0.6917514801025391] [G loss: 0.8178154230117798]\n",
      "[Epoch 64/1001] [Batch 294/372] [D loss: 0.6955690383911133] [G loss: 0.5475805401802063]\n",
      "[Epoch 64/1001] [Batch 295/372] [D loss: 0.7024903297424316] [G loss: 0.863412082195282]\n",
      "[Epoch 64/1001] [Batch 296/372] [D loss: 0.698421835899353] [G loss: 0.5364415645599365]\n",
      "[Epoch 64/1001] [Batch 297/372] [D loss: 0.7000758051872253] [G loss: 0.8087838292121887]\n",
      "[Epoch 64/1001] [Batch 298/372] [D loss: 0.6924914121627808] [G loss: 0.6167399883270264]\n",
      "[Epoch 64/1001] [Batch 299/372] [D loss: 0.6906896829605103] [G loss: 0.6856265068054199]\n",
      "[Epoch 64/1001] [Batch 300/372] [D loss: 0.6921970844268799] [G loss: 0.6896019577980042]\n",
      "[Epoch 64/1001] [Batch 301/372] [D loss: 0.6883429288864136] [G loss: 0.647195041179657]\n",
      "[Epoch 64/1001] [Batch 302/372] [D loss: 0.6832300424575806] [G loss: 0.7067264914512634]\n",
      "[Epoch 64/1001] [Batch 303/372] [D loss: 0.6880731582641602] [G loss: 0.630111575126648]\n",
      "[Epoch 64/1001] [Batch 304/372] [D loss: 0.6971626281738281] [G loss: 0.7419272661209106]\n",
      "[Epoch 64/1001] [Batch 305/372] [D loss: 0.6905131936073303] [G loss: 0.6061529517173767]\n",
      "[Epoch 64/1001] [Batch 306/372] [D loss: 0.6986973285675049] [G loss: 0.7567176222801208]\n",
      "[Epoch 64/1001] [Batch 307/372] [D loss: 0.6859536170959473] [G loss: 0.6057918071746826]\n",
      "[Epoch 64/1001] [Batch 308/372] [D loss: 0.6915534734725952] [G loss: 0.7468687295913696]\n",
      "[Epoch 64/1001] [Batch 309/372] [D loss: 0.6946866512298584] [G loss: 0.6066098213195801]\n",
      "[Epoch 64/1001] [Batch 310/372] [D loss: 0.6940690279006958] [G loss: 0.7385404706001282]\n",
      "[Epoch 64/1001] [Batch 311/372] [D loss: 0.6889897584915161] [G loss: 0.6353874206542969]\n",
      "[Epoch 64/1001] [Batch 312/372] [D loss: 0.6852717399597168] [G loss: 0.6987354755401611]\n",
      "[Epoch 64/1001] [Batch 313/372] [D loss: 0.688073992729187] [G loss: 0.6473367810249329]\n",
      "[Epoch 64/1001] [Batch 314/372] [D loss: 0.6934022903442383] [G loss: 0.7354846596717834]\n",
      "[Epoch 64/1001] [Batch 315/372] [D loss: 0.6896923780441284] [G loss: 0.6139576435089111]\n",
      "[Epoch 64/1001] [Batch 316/372] [D loss: 0.6938281655311584] [G loss: 0.7640610933303833]\n",
      "[Epoch 64/1001] [Batch 317/372] [D loss: 0.6926449537277222] [G loss: 0.5894858837127686]\n",
      "[Epoch 64/1001] [Batch 318/372] [D loss: 0.6909960508346558] [G loss: 0.7676852941513062]\n",
      "[Epoch 64/1001] [Batch 319/372] [D loss: 0.6964749693870544] [G loss: 0.589357852935791]\n",
      "[Epoch 64/1001] [Batch 320/372] [D loss: 0.6950623989105225] [G loss: 0.762986421585083]\n",
      "[Epoch 64/1001] [Batch 321/372] [D loss: 0.6868626475334167] [G loss: 0.5899451971054077]\n",
      "[Epoch 64/1001] [Batch 322/372] [D loss: 0.6974682807922363] [G loss: 0.8271576166152954]\n",
      "[Epoch 64/1001] [Batch 323/372] [D loss: 0.7007091045379639] [G loss: 0.5489344000816345]\n",
      "[Epoch 64/1001] [Batch 324/372] [D loss: 0.6975574493408203] [G loss: 0.7979405522346497]\n",
      "[Epoch 64/1001] [Batch 325/372] [D loss: 0.6991414427757263] [G loss: 0.593184232711792]\n",
      "[Epoch 64/1001] [Batch 326/372] [D loss: 0.6886255741119385] [G loss: 0.7316845655441284]\n",
      "[Epoch 64/1001] [Batch 327/372] [D loss: 0.6828974485397339] [G loss: 0.6627566814422607]\n",
      "[Epoch 64/1001] [Batch 328/372] [D loss: 0.6939870119094849] [G loss: 0.6818333864212036]\n",
      "[Epoch 64/1001] [Batch 329/372] [D loss: 0.6860175132751465] [G loss: 0.6835834383964539]\n",
      "[Epoch 64/1001] [Batch 330/372] [D loss: 0.6852964162826538] [G loss: 0.6674685478210449]\n",
      "[Epoch 64/1001] [Batch 331/372] [D loss: 0.6896060705184937] [G loss: 0.7005445957183838]\n",
      "[Epoch 64/1001] [Batch 332/372] [D loss: 0.6919277906417847] [G loss: 0.6442205309867859]\n",
      "[Epoch 64/1001] [Batch 333/372] [D loss: 0.6952142715454102] [G loss: 0.7000358700752258]\n",
      "[Epoch 64/1001] [Batch 334/372] [D loss: 0.6886163949966431] [G loss: 0.6660965085029602]\n",
      "[Epoch 64/1001] [Batch 335/372] [D loss: 0.6909789443016052] [G loss: 0.6858458518981934]\n",
      "[Epoch 64/1001] [Batch 336/372] [D loss: 0.6904098391532898] [G loss: 0.6619307398796082]\n",
      "[Epoch 64/1001] [Batch 337/372] [D loss: 0.6909348964691162] [G loss: 0.6684458255767822]\n",
      "[Epoch 64/1001] [Batch 338/372] [D loss: 0.6880208253860474] [G loss: 0.691196084022522]\n",
      "[Epoch 64/1001] [Batch 339/372] [D loss: 0.6902008056640625] [G loss: 0.6829497814178467]\n",
      "[Epoch 64/1001] [Batch 340/372] [D loss: 0.6830666065216064] [G loss: 0.6497018933296204]\n",
      "[Epoch 64/1001] [Batch 341/372] [D loss: 0.6864289045333862] [G loss: 0.7288998961448669]\n",
      "[Epoch 64/1001] [Batch 342/372] [D loss: 0.6926741003990173] [G loss: 0.6181229948997498]\n",
      "[Epoch 64/1001] [Batch 343/372] [D loss: 0.6896368265151978] [G loss: 0.7454153895378113]\n",
      "[Epoch 64/1001] [Batch 344/372] [D loss: 0.6966472268104553] [G loss: 0.6137701272964478]\n",
      "[Epoch 64/1001] [Batch 345/372] [D loss: 0.6933422088623047] [G loss: 0.717804491519928]\n",
      "[Epoch 64/1001] [Batch 346/372] [D loss: 0.6870175004005432] [G loss: 0.6592429876327515]\n",
      "[Epoch 64/1001] [Batch 347/372] [D loss: 0.6871329545974731] [G loss: 0.6895363926887512]\n",
      "[Epoch 64/1001] [Batch 348/372] [D loss: 0.687091588973999] [G loss: 0.6756786108016968]\n",
      "[Epoch 64/1001] [Batch 349/372] [D loss: 0.6942552328109741] [G loss: 0.6588743925094604]\n",
      "[Epoch 64/1001] [Batch 350/372] [D loss: 0.6860740184783936] [G loss: 0.7169694900512695]\n",
      "[Epoch 64/1001] [Batch 351/372] [D loss: 0.6895569562911987] [G loss: 0.6148369312286377]\n",
      "[Epoch 64/1001] [Batch 352/372] [D loss: 0.6979643106460571] [G loss: 0.7596796751022339]\n",
      "[Epoch 64/1001] [Batch 353/372] [D loss: 0.688740074634552] [G loss: 0.570999801158905]\n",
      "[Epoch 64/1001] [Batch 354/372] [D loss: 0.6987800598144531] [G loss: 0.8360373377799988]\n",
      "[Epoch 64/1001] [Batch 355/372] [D loss: 0.7022531032562256] [G loss: 0.5244516134262085]\n",
      "[Epoch 64/1001] [Batch 356/372] [D loss: 0.7039507627487183] [G loss: 0.8561635613441467]\n",
      "[Epoch 64/1001] [Batch 357/372] [D loss: 0.6952495574951172] [G loss: 0.5429036617279053]\n",
      "[Epoch 64/1001] [Batch 358/372] [D loss: 0.7009377479553223] [G loss: 0.8198028206825256]\n",
      "[Epoch 64/1001] [Batch 359/372] [D loss: 0.6954628229141235] [G loss: 0.565262496471405]\n",
      "[Epoch 64/1001] [Batch 360/372] [D loss: 0.6961434483528137] [G loss: 0.7603938579559326]\n",
      "[Epoch 64/1001] [Batch 361/372] [D loss: 0.6951080560684204] [G loss: 0.6218618750572205]\n",
      "[Epoch 64/1001] [Batch 362/372] [D loss: 0.6928724050521851] [G loss: 0.7292079329490662]\n",
      "[Epoch 64/1001] [Batch 363/372] [D loss: 0.6872616410255432] [G loss: 0.6469497680664062]\n",
      "[Epoch 64/1001] [Batch 364/372] [D loss: 0.6795093417167664] [G loss: 0.6766982078552246]\n",
      "[Epoch 64/1001] [Batch 365/372] [D loss: 0.6843166351318359] [G loss: 0.6868959665298462]\n",
      "[Epoch 64/1001] [Batch 366/372] [D loss: 0.6832447052001953] [G loss: 0.6429587602615356]\n",
      "[Epoch 64/1001] [Batch 367/372] [D loss: 0.693996787071228] [G loss: 0.766925036907196]\n",
      "[Epoch 64/1001] [Batch 368/372] [D loss: 0.6885004043579102] [G loss: 0.5808045864105225]\n",
      "[Epoch 64/1001] [Batch 369/372] [D loss: 0.6942105889320374] [G loss: 0.7985846996307373]\n",
      "[Epoch 64/1001] [Batch 370/372] [D loss: 0.6960821747779846] [G loss: 0.5708248615264893]\n",
      "[Epoch 64/1001] [Batch 371/372] [D loss: 0.7009288668632507] [G loss: 0.7612524628639221]\n",
      "[Epoch 65/1001] [Batch 0/372] [D loss: 0.6922634243965149] [G loss: 0.6154121160507202]\n",
      "[Epoch 65/1001] [Batch 1/372] [D loss: 0.692601203918457] [G loss: 0.7201623320579529]\n",
      "[Epoch 65/1001] [Batch 2/372] [D loss: 0.6858195662498474] [G loss: 0.6567348837852478]\n",
      "[Epoch 65/1001] [Batch 3/372] [D loss: 0.6852545738220215] [G loss: 0.7042094469070435]\n",
      "[Epoch 65/1001] [Batch 4/372] [D loss: 0.6844747066497803] [G loss: 0.6672515273094177]\n",
      "[Epoch 65/1001] [Batch 5/372] [D loss: 0.6866729259490967] [G loss: 0.6549592018127441]\n",
      "[Epoch 65/1001] [Batch 6/372] [D loss: 0.687105655670166] [G loss: 0.7232276797294617]\n",
      "[Epoch 65/1001] [Batch 7/372] [D loss: 0.6921595931053162] [G loss: 0.6058530807495117]\n",
      "[Epoch 65/1001] [Batch 8/372] [D loss: 0.6798973083496094] [G loss: 0.7638933658599854]\n",
      "[Epoch 65/1001] [Batch 9/372] [D loss: 0.6828178763389587] [G loss: 0.6102110147476196]\n",
      "[Epoch 65/1001] [Batch 10/372] [D loss: 0.6842799186706543] [G loss: 0.7404331564903259]\n",
      "[Epoch 65/1001] [Batch 11/372] [D loss: 0.6841921210289001] [G loss: 0.625812828540802]\n",
      "[Epoch 65/1001] [Batch 12/372] [D loss: 0.6877341270446777] [G loss: 0.7251137495040894]\n",
      "[Epoch 65/1001] [Batch 13/372] [D loss: 0.6933692693710327] [G loss: 0.6225374937057495]\n",
      "[Epoch 65/1001] [Batch 14/372] [D loss: 0.687088668346405] [G loss: 0.7393149733543396]\n",
      "[Epoch 65/1001] [Batch 15/372] [D loss: 0.6873273849487305] [G loss: 0.6055682301521301]\n",
      "[Epoch 65/1001] [Batch 16/372] [D loss: 0.6883142590522766] [G loss: 0.752138614654541]\n",
      "[Epoch 65/1001] [Batch 17/372] [D loss: 0.6863526105880737] [G loss: 0.6223883628845215]\n",
      "[Epoch 65/1001] [Batch 18/372] [D loss: 0.6864576935768127] [G loss: 0.7264400720596313]\n",
      "[Epoch 65/1001] [Batch 19/372] [D loss: 0.6869933605194092] [G loss: 0.6300320625305176]\n",
      "[Epoch 65/1001] [Batch 20/372] [D loss: 0.6915990114212036] [G loss: 0.7463146448135376]\n",
      "[Epoch 65/1001] [Batch 21/372] [D loss: 0.6873745918273926] [G loss: 0.5942542552947998]\n",
      "[Epoch 65/1001] [Batch 22/372] [D loss: 0.6908246874809265] [G loss: 0.7918958067893982]\n",
      "[Epoch 65/1001] [Batch 23/372] [D loss: 0.6921578645706177] [G loss: 0.5802841186523438]\n",
      "[Epoch 65/1001] [Batch 24/372] [D loss: 0.6919384598731995] [G loss: 0.7690492272377014]\n",
      "[Epoch 65/1001] [Batch 25/372] [D loss: 0.6867938041687012] [G loss: 0.5962755680084229]\n",
      "[Epoch 65/1001] [Batch 26/372] [D loss: 0.6884077787399292] [G loss: 0.7612585425376892]\n",
      "[Epoch 65/1001] [Batch 27/372] [D loss: 0.6867250204086304] [G loss: 0.6212279796600342]\n",
      "[Epoch 65/1001] [Batch 28/372] [D loss: 0.689643919467926] [G loss: 0.7292063236236572]\n",
      "[Epoch 65/1001] [Batch 29/372] [D loss: 0.6891193985939026] [G loss: 0.6287621855735779]\n",
      "[Epoch 65/1001] [Batch 30/372] [D loss: 0.6926107406616211] [G loss: 0.6999445557594299]\n",
      "[Epoch 65/1001] [Batch 31/372] [D loss: 0.6830076575279236] [G loss: 0.6643902063369751]\n",
      "[Epoch 65/1001] [Batch 32/372] [D loss: 0.6841108202934265] [G loss: 0.6948361992835999]\n",
      "[Epoch 65/1001] [Batch 33/372] [D loss: 0.6857967376708984] [G loss: 0.6478512287139893]\n",
      "[Epoch 65/1001] [Batch 34/372] [D loss: 0.6848558187484741] [G loss: 0.7029218077659607]\n",
      "[Epoch 65/1001] [Batch 35/372] [D loss: 0.6808440089225769] [G loss: 0.6281784772872925]\n",
      "[Epoch 65/1001] [Batch 36/372] [D loss: 0.6906002759933472] [G loss: 0.7709881067276001]\n",
      "[Epoch 65/1001] [Batch 37/372] [D loss: 0.6968426704406738] [G loss: 0.5404888391494751]\n",
      "[Epoch 65/1001] [Batch 38/372] [D loss: 0.7009124755859375] [G loss: 0.8658945560455322]\n",
      "[Epoch 65/1001] [Batch 39/372] [D loss: 0.7007310390472412] [G loss: 0.5330027937889099]\n",
      "[Epoch 65/1001] [Batch 40/372] [D loss: 0.6968193054199219] [G loss: 0.8236972093582153]\n",
      "[Epoch 65/1001] [Batch 41/372] [D loss: 0.6907463073730469] [G loss: 0.5922073125839233]\n",
      "[Epoch 65/1001] [Batch 42/372] [D loss: 0.692064642906189] [G loss: 0.7404312491416931]\n",
      "[Epoch 65/1001] [Batch 43/372] [D loss: 0.6892061233520508] [G loss: 0.6532387137413025]\n",
      "[Epoch 65/1001] [Batch 44/372] [D loss: 0.6877148151397705] [G loss: 0.684088945388794]\n",
      "[Epoch 65/1001] [Batch 45/372] [D loss: 0.6852492094039917] [G loss: 0.6580092906951904]\n",
      "[Epoch 65/1001] [Batch 46/372] [D loss: 0.6915491223335266] [G loss: 0.717051088809967]\n",
      "[Epoch 65/1001] [Batch 47/372] [D loss: 0.6893221735954285] [G loss: 0.6418111324310303]\n",
      "[Epoch 65/1001] [Batch 48/372] [D loss: 0.6852001547813416] [G loss: 0.7225736379623413]\n",
      "[Epoch 65/1001] [Batch 49/372] [D loss: 0.6907870769500732] [G loss: 0.6252681016921997]\n",
      "[Epoch 65/1001] [Batch 50/372] [D loss: 0.6818243861198425] [G loss: 0.7482861280441284]\n",
      "[Epoch 65/1001] [Batch 51/372] [D loss: 0.6882017850875854] [G loss: 0.621609628200531]\n",
      "[Epoch 65/1001] [Batch 52/372] [D loss: 0.6926392316818237] [G loss: 0.7162583470344543]\n",
      "[Epoch 65/1001] [Batch 53/372] [D loss: 0.6887546181678772] [G loss: 0.6469051837921143]\n",
      "[Epoch 65/1001] [Batch 54/372] [D loss: 0.6916695237159729] [G loss: 0.7092487812042236]\n",
      "[Epoch 65/1001] [Batch 55/372] [D loss: 0.6928102970123291] [G loss: 0.6284414529800415]\n",
      "[Epoch 65/1001] [Batch 56/372] [D loss: 0.6893466711044312] [G loss: 0.7261908650398254]\n",
      "[Epoch 65/1001] [Batch 57/372] [D loss: 0.6890983581542969] [G loss: 0.6176000833511353]\n",
      "[Epoch 65/1001] [Batch 58/372] [D loss: 0.6873979568481445] [G loss: 0.7549527883529663]\n",
      "[Epoch 65/1001] [Batch 59/372] [D loss: 0.6915275454521179] [G loss: 0.5987837910652161]\n",
      "[Epoch 65/1001] [Batch 60/372] [D loss: 0.6923414468765259] [G loss: 0.7544256448745728]\n",
      "[Epoch 65/1001] [Batch 61/372] [D loss: 0.6921005249023438] [G loss: 0.6210303902626038]\n",
      "[Epoch 65/1001] [Batch 62/372] [D loss: 0.691277027130127] [G loss: 0.7318217158317566]\n",
      "[Epoch 65/1001] [Batch 63/372] [D loss: 0.6924921870231628] [G loss: 0.6439880132675171]\n",
      "[Epoch 65/1001] [Batch 64/372] [D loss: 0.6858606338500977] [G loss: 0.6923454403877258]\n",
      "[Epoch 65/1001] [Batch 65/372] [D loss: 0.6844967603683472] [G loss: 0.673202633857727]\n",
      "[Epoch 65/1001] [Batch 66/372] [D loss: 0.6820803880691528] [G loss: 0.6838448643684387]\n",
      "[Epoch 65/1001] [Batch 67/372] [D loss: 0.6853642463684082] [G loss: 0.6706177592277527]\n",
      "[Epoch 65/1001] [Batch 68/372] [D loss: 0.6904268264770508] [G loss: 0.6792116761207581]\n",
      "[Epoch 65/1001] [Batch 69/372] [D loss: 0.6886880397796631] [G loss: 0.688158392906189]\n",
      "[Epoch 65/1001] [Batch 70/372] [D loss: 0.6863424777984619] [G loss: 0.6548585891723633]\n",
      "[Epoch 65/1001] [Batch 71/372] [D loss: 0.6829195022583008] [G loss: 0.696163535118103]\n",
      "[Epoch 65/1001] [Batch 72/372] [D loss: 0.6842296719551086] [G loss: 0.6536679267883301]\n",
      "[Epoch 65/1001] [Batch 73/372] [D loss: 0.6867138147354126] [G loss: 0.6967775225639343]\n",
      "[Epoch 65/1001] [Batch 74/372] [D loss: 0.6930924654006958] [G loss: 0.6774445176124573]\n",
      "[Epoch 65/1001] [Batch 75/372] [D loss: 0.6914018988609314] [G loss: 0.6631858348846436]\n",
      "[Epoch 65/1001] [Batch 76/372] [D loss: 0.6921395659446716] [G loss: 0.6785789728164673]\n",
      "[Epoch 65/1001] [Batch 77/372] [D loss: 0.6878980398178101] [G loss: 0.6770433783531189]\n",
      "[Epoch 65/1001] [Batch 78/372] [D loss: 0.6846453547477722] [G loss: 0.6732892990112305]\n",
      "[Epoch 65/1001] [Batch 79/372] [D loss: 0.6875231266021729] [G loss: 0.6606590151786804]\n",
      "[Epoch 65/1001] [Batch 80/372] [D loss: 0.6925309896469116] [G loss: 0.7136089205741882]\n",
      "[Epoch 65/1001] [Batch 81/372] [D loss: 0.6845590472221375] [G loss: 0.6152869462966919]\n",
      "[Epoch 65/1001] [Batch 82/372] [D loss: 0.6826969385147095] [G loss: 0.7979298830032349]\n",
      "[Epoch 65/1001] [Batch 83/372] [D loss: 0.6968202590942383] [G loss: 0.5333120822906494]\n",
      "[Epoch 65/1001] [Batch 84/372] [D loss: 0.7041717171669006] [G loss: 0.8777092695236206]\n",
      "[Epoch 65/1001] [Batch 85/372] [D loss: 0.7054182291030884] [G loss: 0.5055619478225708]\n",
      "[Epoch 65/1001] [Batch 86/372] [D loss: 0.7119133472442627] [G loss: 0.8827540874481201]\n",
      "[Epoch 65/1001] [Batch 87/372] [D loss: 0.7100474834442139] [G loss: 0.5406357645988464]\n",
      "[Epoch 65/1001] [Batch 88/372] [D loss: 0.6992580890655518] [G loss: 0.764728307723999]\n",
      "[Epoch 65/1001] [Batch 89/372] [D loss: 0.6869205236434937] [G loss: 0.654867947101593]\n",
      "[Epoch 65/1001] [Batch 90/372] [D loss: 0.6875755786895752] [G loss: 0.6652154326438904]\n",
      "[Epoch 65/1001] [Batch 91/372] [D loss: 0.682951807975769] [G loss: 0.6817872524261475]\n",
      "[Epoch 65/1001] [Batch 92/372] [D loss: 0.6953222751617432] [G loss: 0.6888056993484497]\n",
      "[Epoch 65/1001] [Batch 93/372] [D loss: 0.6858932375907898] [G loss: 0.654050350189209]\n",
      "[Epoch 65/1001] [Batch 94/372] [D loss: 0.6859674453735352] [G loss: 0.7311812043190002]\n",
      "[Epoch 65/1001] [Batch 95/372] [D loss: 0.6860860586166382] [G loss: 0.5989753007888794]\n",
      "[Epoch 65/1001] [Batch 96/372] [D loss: 0.6875220537185669] [G loss: 0.7553209066390991]\n",
      "[Epoch 65/1001] [Batch 97/372] [D loss: 0.6914288997650146] [G loss: 0.6416827440261841]\n",
      "[Epoch 65/1001] [Batch 98/372] [D loss: 0.6894609928131104] [G loss: 0.6888003349304199]\n",
      "[Epoch 65/1001] [Batch 99/372] [D loss: 0.6873885989189148] [G loss: 0.6775571703910828]\n",
      "[Epoch 65/1001] [Batch 100/372] [D loss: 0.6903002262115479] [G loss: 0.663861095905304]\n",
      "[Epoch 65/1001] [Batch 101/372] [D loss: 0.6871320009231567] [G loss: 0.6890223622322083]\n",
      "[Epoch 65/1001] [Batch 102/372] [D loss: 0.6875481009483337] [G loss: 0.6699234843254089]\n",
      "[Epoch 65/1001] [Batch 103/372] [D loss: 0.6844174861907959] [G loss: 0.696733832359314]\n",
      "[Epoch 65/1001] [Batch 104/372] [D loss: 0.6851388812065125] [G loss: 0.6455770134925842]\n",
      "[Epoch 65/1001] [Batch 105/372] [D loss: 0.6926735639572144] [G loss: 0.7154843807220459]\n",
      "[Epoch 65/1001] [Batch 106/372] [D loss: 0.678748607635498] [G loss: 0.6439011693000793]\n",
      "[Epoch 65/1001] [Batch 107/372] [D loss: 0.6962392330169678] [G loss: 0.7269920110702515]\n",
      "[Epoch 65/1001] [Batch 108/372] [D loss: 0.6854653358459473] [G loss: 0.634046733379364]\n",
      "[Epoch 65/1001] [Batch 109/372] [D loss: 0.6926923990249634] [G loss: 0.7170292735099792]\n",
      "[Epoch 65/1001] [Batch 110/372] [D loss: 0.6918624639511108] [G loss: 0.6397243738174438]\n",
      "[Epoch 65/1001] [Batch 111/372] [D loss: 0.6975483894348145] [G loss: 0.7123861312866211]\n",
      "[Epoch 65/1001] [Batch 112/372] [D loss: 0.6887145042419434] [G loss: 0.6464946269989014]\n",
      "[Epoch 65/1001] [Batch 113/372] [D loss: 0.6925697326660156] [G loss: 0.7222163081169128]\n",
      "[Epoch 65/1001] [Batch 114/372] [D loss: 0.6810335516929626] [G loss: 0.6125672459602356]\n",
      "[Epoch 65/1001] [Batch 115/372] [D loss: 0.6832415461540222] [G loss: 0.7704601287841797]\n",
      "[Epoch 65/1001] [Batch 116/372] [D loss: 0.68657386302948] [G loss: 0.6097629070281982]\n",
      "[Epoch 65/1001] [Batch 117/372] [D loss: 0.6880297064781189] [G loss: 0.7615524530410767]\n",
      "[Epoch 65/1001] [Batch 118/372] [D loss: 0.6893109083175659] [G loss: 0.5836009979248047]\n",
      "[Epoch 65/1001] [Batch 119/372] [D loss: 0.6892550587654114] [G loss: 0.8086790442466736]\n",
      "[Epoch 65/1001] [Batch 120/372] [D loss: 0.6946776509284973] [G loss: 0.5239983797073364]\n",
      "[Epoch 65/1001] [Batch 121/372] [D loss: 0.6947009563446045] [G loss: 0.9330530166625977]\n",
      "[Epoch 65/1001] [Batch 122/372] [D loss: 0.7125197052955627] [G loss: 0.4792945981025696]\n",
      "[Epoch 65/1001] [Batch 123/372] [D loss: 0.714606761932373] [G loss: 0.9187606573104858]\n",
      "[Epoch 65/1001] [Batch 124/372] [D loss: 0.7128797769546509] [G loss: 0.5129035711288452]\n",
      "[Epoch 65/1001] [Batch 125/372] [D loss: 0.7080563306808472] [G loss: 0.7977865934371948]\n",
      "[Epoch 65/1001] [Batch 126/372] [D loss: 0.6946210861206055] [G loss: 0.6480920314788818]\n",
      "[Epoch 65/1001] [Batch 127/372] [D loss: 0.6885474920272827] [G loss: 0.6713880300521851]\n",
      "[Epoch 65/1001] [Batch 128/372] [D loss: 0.6901754140853882] [G loss: 0.6978557109832764]\n",
      "[Epoch 65/1001] [Batch 129/372] [D loss: 0.6931164264678955] [G loss: 0.6340175867080688]\n",
      "[Epoch 65/1001] [Batch 130/372] [D loss: 0.6906384825706482] [G loss: 0.7234205603599548]\n",
      "[Epoch 65/1001] [Batch 131/372] [D loss: 0.6941500902175903] [G loss: 0.648726224899292]\n",
      "[Epoch 65/1001] [Batch 132/372] [D loss: 0.6912470459938049] [G loss: 0.6929511427879333]\n",
      "[Epoch 65/1001] [Batch 133/372] [D loss: 0.687882125377655] [G loss: 0.6654976010322571]\n",
      "[Epoch 65/1001] [Batch 134/372] [D loss: 0.6915110349655151] [G loss: 0.6809301972389221]\n",
      "[Epoch 65/1001] [Batch 135/372] [D loss: 0.6889171004295349] [G loss: 0.679347038269043]\n",
      "[Epoch 65/1001] [Batch 136/372] [D loss: 0.6916502118110657] [G loss: 0.6548803448677063]\n",
      "[Epoch 65/1001] [Batch 137/372] [D loss: 0.6845805644989014] [G loss: 0.703379213809967]\n",
      "[Epoch 65/1001] [Batch 138/372] [D loss: 0.6899424195289612] [G loss: 0.6504898071289062]\n",
      "[Epoch 65/1001] [Batch 139/372] [D loss: 0.6943691968917847] [G loss: 0.69483882188797]\n",
      "[Epoch 65/1001] [Batch 140/372] [D loss: 0.6896742582321167] [G loss: 0.6410418152809143]\n",
      "[Epoch 65/1001] [Batch 141/372] [D loss: 0.6885599493980408] [G loss: 0.6960610747337341]\n",
      "[Epoch 65/1001] [Batch 142/372] [D loss: 0.6937403678894043] [G loss: 0.6572428345680237]\n",
      "[Epoch 65/1001] [Batch 143/372] [D loss: 0.6911215782165527] [G loss: 0.696770191192627]\n",
      "[Epoch 65/1001] [Batch 144/372] [D loss: 0.6867483854293823] [G loss: 0.6632713079452515]\n",
      "[Epoch 65/1001] [Batch 145/372] [D loss: 0.6855412125587463] [G loss: 0.6787693500518799]\n",
      "[Epoch 65/1001] [Batch 146/372] [D loss: 0.6866773366928101] [G loss: 0.6996049284934998]\n",
      "[Epoch 65/1001] [Batch 147/372] [D loss: 0.6883127689361572] [G loss: 0.6473596096038818]\n",
      "[Epoch 65/1001] [Batch 148/372] [D loss: 0.685107946395874] [G loss: 0.7008454203605652]\n",
      "[Epoch 65/1001] [Batch 149/372] [D loss: 0.6855306625366211] [G loss: 0.6593946218490601]\n",
      "[Epoch 65/1001] [Batch 150/372] [D loss: 0.6878739595413208] [G loss: 0.699059009552002]\n",
      "[Epoch 65/1001] [Batch 151/372] [D loss: 0.6934274435043335] [G loss: 0.659455418586731]\n",
      "[Epoch 65/1001] [Batch 152/372] [D loss: 0.690231204032898] [G loss: 0.6884230971336365]\n",
      "[Epoch 65/1001] [Batch 153/372] [D loss: 0.6881587505340576] [G loss: 0.6927928328514099]\n",
      "[Epoch 65/1001] [Batch 154/372] [D loss: 0.6918914318084717] [G loss: 0.6500996947288513]\n",
      "[Epoch 65/1001] [Batch 155/372] [D loss: 0.6865981817245483] [G loss: 0.7086783647537231]\n",
      "[Epoch 65/1001] [Batch 156/372] [D loss: 0.6938089728355408] [G loss: 0.6378849744796753]\n",
      "[Epoch 65/1001] [Batch 157/372] [D loss: 0.6849377751350403] [G loss: 0.7318572402000427]\n",
      "[Epoch 65/1001] [Batch 158/372] [D loss: 0.6885098218917847] [G loss: 0.6238877177238464]\n",
      "[Epoch 65/1001] [Batch 159/372] [D loss: 0.6897701025009155] [G loss: 0.7319114804267883]\n",
      "[Epoch 65/1001] [Batch 160/372] [D loss: 0.6948952078819275] [G loss: 0.6471188068389893]\n",
      "[Epoch 65/1001] [Batch 161/372] [D loss: 0.6876517534255981] [G loss: 0.7084877490997314]\n",
      "[Epoch 65/1001] [Batch 162/372] [D loss: 0.68732088804245] [G loss: 0.6365346908569336]\n",
      "[Epoch 65/1001] [Batch 163/372] [D loss: 0.6825313568115234] [G loss: 0.7214354872703552]\n",
      "[Epoch 65/1001] [Batch 164/372] [D loss: 0.6902295351028442] [G loss: 0.6465479731559753]\n",
      "[Epoch 65/1001] [Batch 165/372] [D loss: 0.6902567148208618] [G loss: 0.7121110558509827]\n",
      "[Epoch 65/1001] [Batch 166/372] [D loss: 0.6910245418548584] [G loss: 0.658795952796936]\n",
      "[Epoch 65/1001] [Batch 167/372] [D loss: 0.6889830827713013] [G loss: 0.6878644824028015]\n",
      "[Epoch 65/1001] [Batch 168/372] [D loss: 0.69424968957901] [G loss: 0.6371474266052246]\n",
      "[Epoch 65/1001] [Batch 169/372] [D loss: 0.6894913911819458] [G loss: 0.7171111702919006]\n",
      "[Epoch 65/1001] [Batch 170/372] [D loss: 0.6897347569465637] [G loss: 0.6493256092071533]\n",
      "[Epoch 65/1001] [Batch 171/372] [D loss: 0.6887266635894775] [G loss: 0.7008424401283264]\n",
      "[Epoch 65/1001] [Batch 172/372] [D loss: 0.6948444247245789] [G loss: 0.6546590924263]\n",
      "[Epoch 65/1001] [Batch 173/372] [D loss: 0.6918318271636963] [G loss: 0.6933824419975281]\n",
      "[Epoch 65/1001] [Batch 174/372] [D loss: 0.6860620379447937] [G loss: 0.6825331449508667]\n",
      "[Epoch 65/1001] [Batch 175/372] [D loss: 0.6886777877807617] [G loss: 0.6451809406280518]\n",
      "[Epoch 65/1001] [Batch 176/372] [D loss: 0.6884902715682983] [G loss: 0.7406988739967346]\n",
      "[Epoch 65/1001] [Batch 177/372] [D loss: 0.6883242130279541] [G loss: 0.595477283000946]\n",
      "[Epoch 65/1001] [Batch 178/372] [D loss: 0.6915345191955566] [G loss: 0.8207176923751831]\n",
      "[Epoch 65/1001] [Batch 179/372] [D loss: 0.6946234703063965] [G loss: 0.5532119870185852]\n",
      "[Epoch 65/1001] [Batch 180/372] [D loss: 0.696064829826355] [G loss: 0.777708113193512]\n",
      "[Epoch 65/1001] [Batch 181/372] [D loss: 0.6923635601997375] [G loss: 0.6235129833221436]\n",
      "[Epoch 65/1001] [Batch 182/372] [D loss: 0.689590334892273] [G loss: 0.7159774899482727]\n",
      "[Epoch 65/1001] [Batch 183/372] [D loss: 0.6867083311080933] [G loss: 0.6554819345474243]\n",
      "[Epoch 65/1001] [Batch 184/372] [D loss: 0.6883279085159302] [G loss: 0.6785721778869629]\n",
      "[Epoch 65/1001] [Batch 185/372] [D loss: 0.6892133951187134] [G loss: 0.6773242950439453]\n",
      "[Epoch 65/1001] [Batch 186/372] [D loss: 0.6886214017868042] [G loss: 0.6816506385803223]\n",
      "[Epoch 65/1001] [Batch 187/372] [D loss: 0.6872316598892212] [G loss: 0.7050443887710571]\n",
      "[Epoch 65/1001] [Batch 188/372] [D loss: 0.684135377407074] [G loss: 0.6267796158790588]\n",
      "[Epoch 65/1001] [Batch 189/372] [D loss: 0.6878352165222168] [G loss: 0.7856966257095337]\n",
      "[Epoch 65/1001] [Batch 190/372] [D loss: 0.6916099786758423] [G loss: 0.5853826403617859]\n",
      "[Epoch 65/1001] [Batch 191/372] [D loss: 0.6962584257125854] [G loss: 0.7565699815750122]\n",
      "[Epoch 65/1001] [Batch 192/372] [D loss: 0.6893352270126343] [G loss: 0.6128616333007812]\n",
      "[Epoch 65/1001] [Batch 193/372] [D loss: 0.6937597990036011] [G loss: 0.7399752140045166]\n",
      "[Epoch 65/1001] [Batch 194/372] [D loss: 0.6931319832801819] [G loss: 0.6366125345230103]\n",
      "[Epoch 65/1001] [Batch 195/372] [D loss: 0.6910355091094971] [G loss: 0.7151653170585632]\n",
      "[Epoch 65/1001] [Batch 196/372] [D loss: 0.6885237693786621] [G loss: 0.6405335664749146]\n",
      "[Epoch 65/1001] [Batch 197/372] [D loss: 0.6917215585708618] [G loss: 0.7071335911750793]\n",
      "[Epoch 65/1001] [Batch 198/372] [D loss: 0.6821715235710144] [G loss: 0.6620088815689087]\n",
      "[Epoch 65/1001] [Batch 199/372] [D loss: 0.6884959936141968] [G loss: 0.6828768253326416]\n",
      "[Epoch 65/1001] [Batch 200/372] [D loss: 0.6917009353637695] [G loss: 0.678561806678772]\n",
      "[Epoch 65/1001] [Batch 201/372] [D loss: 0.6937758922576904] [G loss: 0.6721033453941345]\n",
      "[Epoch 65/1001] [Batch 202/372] [D loss: 0.691519021987915] [G loss: 0.6667458415031433]\n",
      "[Epoch 65/1001] [Batch 203/372] [D loss: 0.6860339045524597] [G loss: 0.6952412128448486]\n",
      "[Epoch 65/1001] [Batch 204/372] [D loss: 0.6901093125343323] [G loss: 0.6382595300674438]\n",
      "[Epoch 65/1001] [Batch 205/372] [D loss: 0.6860677003860474] [G loss: 0.7495057582855225]\n",
      "[Epoch 65/1001] [Batch 206/372] [D loss: 0.688294529914856] [G loss: 0.6034532785415649]\n",
      "[Epoch 65/1001] [Batch 207/372] [D loss: 0.6903952360153198] [G loss: 0.8036208152770996]\n",
      "[Epoch 65/1001] [Batch 208/372] [D loss: 0.6924965381622314] [G loss: 0.567780613899231]\n",
      "[Epoch 65/1001] [Batch 209/372] [D loss: 0.6972780227661133] [G loss: 0.7929413318634033]\n",
      "[Epoch 65/1001] [Batch 210/372] [D loss: 0.6930494904518127] [G loss: 0.5834504961967468]\n",
      "[Epoch 65/1001] [Batch 211/372] [D loss: 0.6920474767684937] [G loss: 0.7717445492744446]\n",
      "[Epoch 65/1001] [Batch 212/372] [D loss: 0.6873798370361328] [G loss: 0.5871216058731079]\n",
      "[Epoch 65/1001] [Batch 213/372] [D loss: 0.6956321001052856] [G loss: 0.771963357925415]\n",
      "[Epoch 65/1001] [Batch 214/372] [D loss: 0.6950549483299255] [G loss: 0.6136422157287598]\n",
      "[Epoch 65/1001] [Batch 215/372] [D loss: 0.6843435764312744] [G loss: 0.723200261592865]\n",
      "[Epoch 65/1001] [Batch 216/372] [D loss: 0.6877224445343018] [G loss: 0.6348282694816589]\n",
      "[Epoch 65/1001] [Batch 217/372] [D loss: 0.6941208243370056] [G loss: 0.7019351124763489]\n",
      "[Epoch 65/1001] [Batch 218/372] [D loss: 0.6870536804199219] [G loss: 0.6524714231491089]\n",
      "[Epoch 65/1001] [Batch 219/372] [D loss: 0.6982066631317139] [G loss: 0.6942018270492554]\n",
      "[Epoch 65/1001] [Batch 220/372] [D loss: 0.686392068862915] [G loss: 0.6355226039886475]\n",
      "[Epoch 65/1001] [Batch 221/372] [D loss: 0.6901851892471313] [G loss: 0.731652021408081]\n",
      "[Epoch 65/1001] [Batch 222/372] [D loss: 0.7011207342147827] [G loss: 0.5982757806777954]\n",
      "[Epoch 65/1001] [Batch 223/372] [D loss: 0.6827533841133118] [G loss: 0.7779773473739624]\n",
      "[Epoch 65/1001] [Batch 224/372] [D loss: 0.6862937211990356] [G loss: 0.562169075012207]\n",
      "[Epoch 65/1001] [Batch 225/372] [D loss: 0.7055603265762329] [G loss: 0.8171828985214233]\n",
      "[Epoch 65/1001] [Batch 226/372] [D loss: 0.7026628255844116] [G loss: 0.5291277766227722]\n",
      "[Epoch 65/1001] [Batch 227/372] [D loss: 0.7011294960975647] [G loss: 0.87428218126297]\n",
      "[Epoch 65/1001] [Batch 228/372] [D loss: 0.706619143486023] [G loss: 0.540334165096283]\n",
      "[Epoch 65/1001] [Batch 229/372] [D loss: 0.6984431743621826] [G loss: 0.7759338617324829]\n",
      "[Epoch 65/1001] [Batch 230/372] [D loss: 0.6855659484863281] [G loss: 0.6391883492469788]\n",
      "[Epoch 65/1001] [Batch 231/372] [D loss: 0.6913624405860901] [G loss: 0.6811220645904541]\n",
      "[Epoch 65/1001] [Batch 232/372] [D loss: 0.6955262422561646] [G loss: 0.6769598722457886]\n",
      "[Epoch 65/1001] [Batch 233/372] [D loss: 0.6809167861938477] [G loss: 0.6739494800567627]\n",
      "[Epoch 65/1001] [Batch 234/372] [D loss: 0.6838908791542053] [G loss: 0.6806795001029968]\n",
      "[Epoch 65/1001] [Batch 235/372] [D loss: 0.6833435297012329] [G loss: 0.6816343665122986]\n",
      "[Epoch 65/1001] [Batch 236/372] [D loss: 0.6854373216629028] [G loss: 0.682744562625885]\n",
      "[Epoch 65/1001] [Batch 237/372] [D loss: 0.6868569254875183] [G loss: 0.6652022004127502]\n",
      "[Epoch 65/1001] [Batch 238/372] [D loss: 0.6825668811798096] [G loss: 0.6851931214332581]\n",
      "[Epoch 65/1001] [Batch 239/372] [D loss: 0.6755719184875488] [G loss: 0.6654964089393616]\n",
      "[Epoch 65/1001] [Batch 240/372] [D loss: 0.6881927251815796] [G loss: 0.6820549964904785]\n",
      "[Epoch 65/1001] [Batch 241/372] [D loss: 0.6818152666091919] [G loss: 0.6739210486412048]\n",
      "[Epoch 65/1001] [Batch 242/372] [D loss: 0.691561758518219] [G loss: 0.6849013566970825]\n",
      "[Epoch 65/1001] [Batch 243/372] [D loss: 0.6855850219726562] [G loss: 0.6582210659980774]\n",
      "[Epoch 65/1001] [Batch 244/372] [D loss: 0.6942735910415649] [G loss: 0.6910470724105835]\n",
      "[Epoch 65/1001] [Batch 245/372] [D loss: 0.6872784495353699] [G loss: 0.6502187848091125]\n",
      "[Epoch 65/1001] [Batch 246/372] [D loss: 0.685802161693573] [G loss: 0.7371618151664734]\n",
      "[Epoch 65/1001] [Batch 247/372] [D loss: 0.687015175819397] [G loss: 0.5915859937667847]\n",
      "[Epoch 65/1001] [Batch 248/372] [D loss: 0.6975219249725342] [G loss: 0.7768529653549194]\n",
      "[Epoch 65/1001] [Batch 249/372] [D loss: 0.69563227891922] [G loss: 0.5762157440185547]\n",
      "[Epoch 65/1001] [Batch 250/372] [D loss: 0.6984372138977051] [G loss: 0.7914939522743225]\n",
      "[Epoch 65/1001] [Batch 251/372] [D loss: 0.6936919689178467] [G loss: 0.5739244222640991]\n",
      "[Epoch 65/1001] [Batch 252/372] [D loss: 0.6964097023010254] [G loss: 0.7691051959991455]\n",
      "[Epoch 65/1001] [Batch 253/372] [D loss: 0.6903302073478699] [G loss: 0.6054142117500305]\n",
      "[Epoch 65/1001] [Batch 254/372] [D loss: 0.6912767887115479] [G loss: 0.7519909143447876]\n",
      "[Epoch 65/1001] [Batch 255/372] [D loss: 0.6856975555419922] [G loss: 0.6202085018157959]\n",
      "[Epoch 65/1001] [Batch 256/372] [D loss: 0.6893929243087769] [G loss: 0.7391414046287537]\n",
      "[Epoch 65/1001] [Batch 257/372] [D loss: 0.688535213470459] [G loss: 0.6357753872871399]\n",
      "[Epoch 65/1001] [Batch 258/372] [D loss: 0.6931031942367554] [G loss: 0.69794762134552]\n",
      "[Epoch 65/1001] [Batch 259/372] [D loss: 0.6834373474121094] [G loss: 0.6733316779136658]\n",
      "[Epoch 65/1001] [Batch 260/372] [D loss: 0.6807750463485718] [G loss: 0.6922082901000977]\n",
      "[Epoch 65/1001] [Batch 261/372] [D loss: 0.6990654468536377] [G loss: 0.6678429841995239]\n",
      "[Epoch 65/1001] [Batch 262/372] [D loss: 0.6918100118637085] [G loss: 0.6605904698371887]\n",
      "[Epoch 65/1001] [Batch 263/372] [D loss: 0.6939767003059387] [G loss: 0.7149836421012878]\n",
      "[Epoch 65/1001] [Batch 264/372] [D loss: 0.6898822784423828] [G loss: 0.6099296808242798]\n",
      "[Epoch 65/1001] [Batch 265/372] [D loss: 0.6906419396400452] [G loss: 0.7570475339889526]\n",
      "[Epoch 65/1001] [Batch 266/372] [D loss: 0.6858578324317932] [G loss: 0.5937765836715698]\n",
      "[Epoch 65/1001] [Batch 267/372] [D loss: 0.6849617958068848] [G loss: 0.8017798662185669]\n",
      "[Epoch 65/1001] [Batch 268/372] [D loss: 0.6955875754356384] [G loss: 0.5544827580451965]\n",
      "[Epoch 65/1001] [Batch 269/372] [D loss: 0.69635009765625] [G loss: 0.7950634360313416]\n",
      "[Epoch 65/1001] [Batch 270/372] [D loss: 0.7005331516265869] [G loss: 0.5746011734008789]\n",
      "[Epoch 65/1001] [Batch 271/372] [D loss: 0.6922374963760376] [G loss: 0.7638106346130371]\n",
      "[Epoch 65/1001] [Batch 272/372] [D loss: 0.6935760974884033] [G loss: 0.6000745296478271]\n",
      "[Epoch 65/1001] [Batch 273/372] [D loss: 0.6868858337402344] [G loss: 0.7461999654769897]\n",
      "[Epoch 65/1001] [Batch 274/372] [D loss: 0.6852284073829651] [G loss: 0.5948367714881897]\n",
      "[Epoch 65/1001] [Batch 275/372] [D loss: 0.6937062740325928] [G loss: 0.7659419178962708]\n",
      "[Epoch 65/1001] [Batch 276/372] [D loss: 0.6902575492858887] [G loss: 0.5733060240745544]\n",
      "[Epoch 65/1001] [Batch 277/372] [D loss: 0.6928434371948242] [G loss: 0.8338727355003357]\n",
      "[Epoch 65/1001] [Batch 278/372] [D loss: 0.6963818073272705] [G loss: 0.5483996272087097]\n",
      "[Epoch 65/1001] [Batch 279/372] [D loss: 0.6990092992782593] [G loss: 0.7859283685684204]\n",
      "[Epoch 65/1001] [Batch 280/372] [D loss: 0.6958751678466797] [G loss: 0.6034128665924072]\n",
      "[Epoch 65/1001] [Batch 281/372] [D loss: 0.6893972158432007] [G loss: 0.7162796258926392]\n",
      "[Epoch 65/1001] [Batch 282/372] [D loss: 0.6868925094604492] [G loss: 0.6670401692390442]\n",
      "[Epoch 65/1001] [Batch 283/372] [D loss: 0.6837904453277588] [G loss: 0.6724205613136292]\n",
      "[Epoch 65/1001] [Batch 284/372] [D loss: 0.6844474077224731] [G loss: 0.6826543211936951]\n",
      "[Epoch 65/1001] [Batch 285/372] [D loss: 0.6859711408615112] [G loss: 0.6855248212814331]\n",
      "[Epoch 65/1001] [Batch 286/372] [D loss: 0.6823144555091858] [G loss: 0.6445333957672119]\n",
      "[Epoch 65/1001] [Batch 287/372] [D loss: 0.7025371193885803] [G loss: 0.7244929671287537]\n",
      "[Epoch 65/1001] [Batch 288/372] [D loss: 0.6900514364242554] [G loss: 0.6095147728919983]\n",
      "[Epoch 65/1001] [Batch 289/372] [D loss: 0.6903965473175049] [G loss: 0.7611058950424194]\n",
      "[Epoch 65/1001] [Batch 290/372] [D loss: 0.6885213851928711] [G loss: 0.6059625744819641]\n",
      "[Epoch 65/1001] [Batch 291/372] [D loss: 0.6878609657287598] [G loss: 0.7645789384841919]\n",
      "[Epoch 65/1001] [Batch 292/372] [D loss: 0.6890281438827515] [G loss: 0.5930305123329163]\n",
      "[Epoch 65/1001] [Batch 293/372] [D loss: 0.6875402927398682] [G loss: 0.7571175694465637]\n",
      "[Epoch 65/1001] [Batch 294/372] [D loss: 0.6929949522018433] [G loss: 0.6341519951820374]\n",
      "[Epoch 65/1001] [Batch 295/372] [D loss: 0.6927061676979065] [G loss: 0.694036602973938]\n",
      "[Epoch 65/1001] [Batch 296/372] [D loss: 0.6908679008483887] [G loss: 0.6737862229347229]\n",
      "[Epoch 65/1001] [Batch 297/372] [D loss: 0.6863071322441101] [G loss: 0.6639991998672485]\n",
      "[Epoch 65/1001] [Batch 298/372] [D loss: 0.6837750673294067] [G loss: 0.6910874843597412]\n",
      "[Epoch 65/1001] [Batch 299/372] [D loss: 0.687667965888977] [G loss: 0.6800752878189087]\n",
      "[Epoch 65/1001] [Batch 300/372] [D loss: 0.6830040216445923] [G loss: 0.6835648417472839]\n",
      "[Epoch 65/1001] [Batch 301/372] [D loss: 0.6835063695907593] [G loss: 0.6770598292350769]\n",
      "[Epoch 65/1001] [Batch 302/372] [D loss: 0.6882508397102356] [G loss: 0.672134280204773]\n",
      "[Epoch 65/1001] [Batch 303/372] [D loss: 0.6894913911819458] [G loss: 0.6718814969062805]\n",
      "[Epoch 65/1001] [Batch 304/372] [D loss: 0.6889330148696899] [G loss: 0.6878987550735474]\n",
      "[Epoch 65/1001] [Batch 305/372] [D loss: 0.691273033618927] [G loss: 0.6585056781768799]\n",
      "[Epoch 65/1001] [Batch 306/372] [D loss: 0.6920015811920166] [G loss: 0.710303008556366]\n",
      "[Epoch 65/1001] [Batch 307/372] [D loss: 0.6881991624832153] [G loss: 0.6153026819229126]\n",
      "[Epoch 65/1001] [Batch 308/372] [D loss: 0.6889784336090088] [G loss: 0.7556626200675964]\n",
      "[Epoch 65/1001] [Batch 309/372] [D loss: 0.689618706703186] [G loss: 0.6167385578155518]\n",
      "[Epoch 65/1001] [Batch 310/372] [D loss: 0.6913824081420898] [G loss: 0.7220075130462646]\n",
      "[Epoch 65/1001] [Batch 311/372] [D loss: 0.6856073141098022] [G loss: 0.645668625831604]\n",
      "[Epoch 65/1001] [Batch 312/372] [D loss: 0.6901648044586182] [G loss: 0.715674877166748]\n",
      "[Epoch 65/1001] [Batch 313/372] [D loss: 0.686651349067688] [G loss: 0.6174293160438538]\n",
      "[Epoch 65/1001] [Batch 314/372] [D loss: 0.689583420753479] [G loss: 0.7706414461135864]\n",
      "[Epoch 65/1001] [Batch 315/372] [D loss: 0.6987244486808777] [G loss: 0.5682660937309265]\n",
      "[Epoch 65/1001] [Batch 316/372] [D loss: 0.6941628456115723] [G loss: 0.8194482922554016]\n",
      "[Epoch 65/1001] [Batch 317/372] [D loss: 0.6980480551719666] [G loss: 0.5434668660163879]\n",
      "[Epoch 65/1001] [Batch 318/372] [D loss: 0.7025762796401978] [G loss: 0.8045080900192261]\n",
      "[Epoch 65/1001] [Batch 319/372] [D loss: 0.6889450550079346] [G loss: 0.5915697813034058]\n",
      "[Epoch 65/1001] [Batch 320/372] [D loss: 0.6979649066925049] [G loss: 0.7606011033058167]\n",
      "[Epoch 65/1001] [Batch 321/372] [D loss: 0.6951315402984619] [G loss: 0.6023207902908325]\n",
      "[Epoch 65/1001] [Batch 322/372] [D loss: 0.6920487880706787] [G loss: 0.7406559586524963]\n",
      "[Epoch 65/1001] [Batch 323/372] [D loss: 0.6843562722206116] [G loss: 0.6198900938034058]\n",
      "[Epoch 65/1001] [Batch 324/372] [D loss: 0.6918163299560547] [G loss: 0.7185150384902954]\n",
      "[Epoch 65/1001] [Batch 325/372] [D loss: 0.6870385408401489] [G loss: 0.6432962417602539]\n",
      "[Epoch 65/1001] [Batch 326/372] [D loss: 0.6824532747268677] [G loss: 0.7129158973693848]\n",
      "[Epoch 65/1001] [Batch 327/372] [D loss: 0.6990423798561096] [G loss: 0.6414086818695068]\n",
      "[Epoch 65/1001] [Batch 328/372] [D loss: 0.6914154887199402] [G loss: 0.6869640350341797]\n",
      "[Epoch 65/1001] [Batch 329/372] [D loss: 0.6887232065200806] [G loss: 0.6733575463294983]\n",
      "[Epoch 65/1001] [Batch 330/372] [D loss: 0.6961619853973389] [G loss: 0.6542713046073914]\n",
      "[Epoch 65/1001] [Batch 331/372] [D loss: 0.6952974796295166] [G loss: 0.6862831115722656]\n",
      "[Epoch 65/1001] [Batch 332/372] [D loss: 0.6876424551010132] [G loss: 0.6669081449508667]\n",
      "[Epoch 65/1001] [Batch 333/372] [D loss: 0.6842308044433594] [G loss: 0.6896941661834717]\n",
      "[Epoch 65/1001] [Batch 334/372] [D loss: 0.6897785663604736] [G loss: 0.6723127961158752]\n",
      "[Epoch 65/1001] [Batch 335/372] [D loss: 0.6891931295394897] [G loss: 0.671921968460083]\n",
      "[Epoch 65/1001] [Batch 336/372] [D loss: 0.6835412979125977] [G loss: 0.6808444857597351]\n",
      "[Epoch 65/1001] [Batch 337/372] [D loss: 0.694394588470459] [G loss: 0.6810150146484375]\n",
      "[Epoch 65/1001] [Batch 338/372] [D loss: 0.6870687007904053] [G loss: 0.6600719690322876]\n",
      "[Epoch 65/1001] [Batch 339/372] [D loss: 0.6914712190628052] [G loss: 0.6925235986709595]\n",
      "[Epoch 65/1001] [Batch 340/372] [D loss: 0.6869466304779053] [G loss: 0.6419504880905151]\n",
      "[Epoch 65/1001] [Batch 341/372] [D loss: 0.6910752058029175] [G loss: 0.7449811697006226]\n",
      "[Epoch 65/1001] [Batch 342/372] [D loss: 0.6810430288314819] [G loss: 0.5857283473014832]\n",
      "[Epoch 65/1001] [Batch 343/372] [D loss: 0.6954956650733948] [G loss: 0.816425621509552]\n",
      "[Epoch 65/1001] [Batch 344/372] [D loss: 0.6942425966262817] [G loss: 0.5284335017204285]\n",
      "[Epoch 65/1001] [Batch 345/372] [D loss: 0.694023847579956] [G loss: 0.8219766020774841]\n",
      "[Epoch 65/1001] [Batch 346/372] [D loss: 0.6977242827415466] [G loss: 0.5639216303825378]\n",
      "[Epoch 65/1001] [Batch 347/372] [D loss: 0.6891234517097473] [G loss: 0.866483211517334]\n",
      "[Epoch 65/1001] [Batch 348/372] [D loss: 0.6960356831550598] [G loss: 0.5057994723320007]\n",
      "[Epoch 65/1001] [Batch 349/372] [D loss: 0.7024964094161987] [G loss: 0.9318978190422058]\n",
      "[Epoch 65/1001] [Batch 350/372] [D loss: 0.7079730033874512] [G loss: 0.49441978335380554]\n",
      "[Epoch 65/1001] [Batch 351/372] [D loss: 0.7099511623382568] [G loss: 0.8360025882720947]\n",
      "[Epoch 65/1001] [Batch 352/372] [D loss: 0.7031817436218262] [G loss: 0.5929665565490723]\n",
      "[Epoch 65/1001] [Batch 353/372] [D loss: 0.695223867893219] [G loss: 0.6982112526893616]\n",
      "[Epoch 65/1001] [Batch 354/372] [D loss: 0.6891063451766968] [G loss: 0.6719908118247986]\n",
      "[Epoch 65/1001] [Batch 355/372] [D loss: 0.6875745058059692] [G loss: 0.6527085900306702]\n",
      "[Epoch 65/1001] [Batch 356/372] [D loss: 0.6902468204498291] [G loss: 0.6944964528083801]\n",
      "[Epoch 65/1001] [Batch 357/372] [D loss: 0.6886337995529175] [G loss: 0.6534749865531921]\n",
      "[Epoch 65/1001] [Batch 358/372] [D loss: 0.6917514801025391] [G loss: 0.6953239440917969]\n",
      "[Epoch 65/1001] [Batch 359/372] [D loss: 0.693321943283081] [G loss: 0.6480669379234314]\n",
      "[Epoch 65/1001] [Batch 360/372] [D loss: 0.6926466822624207] [G loss: 0.6834273934364319]\n",
      "[Epoch 65/1001] [Batch 361/372] [D loss: 0.6878139972686768] [G loss: 0.681542158126831]\n",
      "[Epoch 65/1001] [Batch 362/372] [D loss: 0.6907719969749451] [G loss: 0.6709976196289062]\n",
      "[Epoch 65/1001] [Batch 363/372] [D loss: 0.6826274991035461] [G loss: 0.6921982765197754]\n",
      "[Epoch 65/1001] [Batch 364/372] [D loss: 0.690619707107544] [G loss: 0.6532590389251709]\n",
      "[Epoch 65/1001] [Batch 365/372] [D loss: 0.6877964735031128] [G loss: 0.7031530141830444]\n",
      "[Epoch 65/1001] [Batch 366/372] [D loss: 0.6901400089263916] [G loss: 0.649091362953186]\n",
      "[Epoch 65/1001] [Batch 367/372] [D loss: 0.6823263168334961] [G loss: 0.6809452176094055]\n",
      "[Epoch 65/1001] [Batch 368/372] [D loss: 0.6857884526252747] [G loss: 0.7050212025642395]\n",
      "[Epoch 65/1001] [Batch 369/372] [D loss: 0.6921601295471191] [G loss: 0.6512365937232971]\n",
      "[Epoch 65/1001] [Batch 370/372] [D loss: 0.6866371631622314] [G loss: 0.6947355270385742]\n",
      "[Epoch 65/1001] [Batch 371/372] [D loss: 0.6874651908874512] [G loss: 0.6463021636009216]\n",
      "[Epoch 66/1001] [Batch 0/372] [D loss: 0.6766588091850281] [G loss: 0.7340407967567444]\n",
      "[Epoch 66/1001] [Batch 1/372] [D loss: 0.6758708953857422] [G loss: 0.6178640723228455]\n",
      "[Epoch 66/1001] [Batch 2/372] [D loss: 0.6898233890533447] [G loss: 0.7767688632011414]\n",
      "[Epoch 66/1001] [Batch 3/372] [D loss: 0.6810258030891418] [G loss: 0.5580207705497742]\n",
      "[Epoch 66/1001] [Batch 4/372] [D loss: 0.6964818239212036] [G loss: 0.8647233843803406]\n",
      "[Epoch 66/1001] [Batch 5/372] [D loss: 0.7024277448654175] [G loss: 0.4999220669269562]\n",
      "[Epoch 66/1001] [Batch 6/372] [D loss: 0.7076135873794556] [G loss: 0.8962999582290649]\n",
      "[Epoch 66/1001] [Batch 7/372] [D loss: 0.7058359384536743] [G loss: 0.538892924785614]\n",
      "[Epoch 66/1001] [Batch 8/372] [D loss: 0.7007436752319336] [G loss: 0.7714606523513794]\n",
      "[Epoch 66/1001] [Batch 9/372] [D loss: 0.6954551339149475] [G loss: 0.6466022729873657]\n",
      "[Epoch 66/1001] [Batch 10/372] [D loss: 0.6876577138900757] [G loss: 0.6775287389755249]\n",
      "[Epoch 66/1001] [Batch 11/372] [D loss: 0.6821825504302979] [G loss: 0.6877163648605347]\n",
      "[Epoch 66/1001] [Batch 12/372] [D loss: 0.685076117515564] [G loss: 0.6643778085708618]\n",
      "[Epoch 66/1001] [Batch 13/372] [D loss: 0.6794371604919434] [G loss: 0.6923990249633789]\n",
      "[Epoch 66/1001] [Batch 14/372] [D loss: 0.6828702092170715] [G loss: 0.6818675994873047]\n",
      "[Epoch 66/1001] [Batch 15/372] [D loss: 0.6850429773330688] [G loss: 0.664216160774231]\n",
      "[Epoch 66/1001] [Batch 16/372] [D loss: 0.6904029846191406] [G loss: 0.6782519817352295]\n",
      "[Epoch 66/1001] [Batch 17/372] [D loss: 0.6904714107513428] [G loss: 0.6934448480606079]\n",
      "[Epoch 66/1001] [Batch 18/372] [D loss: 0.685384213924408] [G loss: 0.6572070121765137]\n",
      "[Epoch 66/1001] [Batch 19/372] [D loss: 0.6933721899986267] [G loss: 0.6833416819572449]\n",
      "[Epoch 66/1001] [Batch 20/372] [D loss: 0.6810076236724854] [G loss: 0.6742839813232422]\n",
      "[Epoch 66/1001] [Batch 21/372] [D loss: 0.6851537227630615] [G loss: 0.6836444735527039]\n",
      "[Epoch 66/1001] [Batch 22/372] [D loss: 0.6855069398880005] [G loss: 0.6774299144744873]\n",
      "[Epoch 66/1001] [Batch 23/372] [D loss: 0.6805870532989502] [G loss: 0.6738349795341492]\n",
      "[Epoch 66/1001] [Batch 24/372] [D loss: 0.6849937438964844] [G loss: 0.6916190385818481]\n",
      "[Epoch 66/1001] [Batch 25/372] [D loss: 0.6859184503555298] [G loss: 0.6493575572967529]\n",
      "[Epoch 66/1001] [Batch 26/372] [D loss: 0.682926595211029] [G loss: 0.7356841564178467]\n",
      "[Epoch 66/1001] [Batch 27/372] [D loss: 0.6878670454025269] [G loss: 0.6147568225860596]\n",
      "[Epoch 66/1001] [Batch 28/372] [D loss: 0.690954327583313] [G loss: 0.7367520928382874]\n",
      "[Epoch 66/1001] [Batch 29/372] [D loss: 0.6879841089248657] [G loss: 0.6250703930854797]\n",
      "[Epoch 66/1001] [Batch 30/372] [D loss: 0.6909574866294861] [G loss: 0.7242108583450317]\n",
      "[Epoch 66/1001] [Batch 31/372] [D loss: 0.6852982640266418] [G loss: 0.6328392624855042]\n",
      "[Epoch 66/1001] [Batch 32/372] [D loss: 0.6917279958724976] [G loss: 0.7133665084838867]\n",
      "[Epoch 66/1001] [Batch 33/372] [D loss: 0.6886416673660278] [G loss: 0.630804181098938]\n",
      "[Epoch 66/1001] [Batch 34/372] [D loss: 0.6809621453285217] [G loss: 0.7635259032249451]\n",
      "[Epoch 66/1001] [Batch 35/372] [D loss: 0.680708110332489] [G loss: 0.5862285494804382]\n",
      "[Epoch 66/1001] [Batch 36/372] [D loss: 0.6885637044906616] [G loss: 0.8042148351669312]\n",
      "[Epoch 66/1001] [Batch 37/372] [D loss: 0.6938966512680054] [G loss: 0.5505255460739136]\n",
      "[Epoch 66/1001] [Batch 38/372] [D loss: 0.7003105878829956] [G loss: 0.8349788188934326]\n",
      "[Epoch 66/1001] [Batch 39/372] [D loss: 0.6980737447738647] [G loss: 0.5410584807395935]\n",
      "[Epoch 66/1001] [Batch 40/372] [D loss: 0.6968510150909424] [G loss: 0.8038154244422913]\n",
      "[Epoch 66/1001] [Batch 41/372] [D loss: 0.6926885843276978] [G loss: 0.6161927580833435]\n",
      "[Epoch 66/1001] [Batch 42/372] [D loss: 0.6934032440185547] [G loss: 0.7030364274978638]\n",
      "[Epoch 66/1001] [Batch 43/372] [D loss: 0.6847115755081177] [G loss: 0.6758386492729187]\n",
      "[Epoch 66/1001] [Batch 44/372] [D loss: 0.6884817481040955] [G loss: 0.6706140637397766]\n",
      "[Epoch 66/1001] [Batch 45/372] [D loss: 0.6919472217559814] [G loss: 0.6794189810752869]\n",
      "[Epoch 66/1001] [Batch 46/372] [D loss: 0.6847500801086426] [G loss: 0.6710749864578247]\n",
      "[Epoch 66/1001] [Batch 47/372] [D loss: 0.6872929930686951] [G loss: 0.6713984608650208]\n",
      "[Epoch 66/1001] [Batch 48/372] [D loss: 0.6887439489364624] [G loss: 0.6983680129051208]\n",
      "[Epoch 66/1001] [Batch 49/372] [D loss: 0.6891878247261047] [G loss: 0.64761883020401]\n",
      "[Epoch 66/1001] [Batch 50/372] [D loss: 0.6861251592636108] [G loss: 0.6991788148880005]\n",
      "[Epoch 66/1001] [Batch 51/372] [D loss: 0.6864945888519287] [G loss: 0.6648964881896973]\n",
      "[Epoch 66/1001] [Batch 52/372] [D loss: 0.6894083023071289] [G loss: 0.690330445766449]\n",
      "[Epoch 66/1001] [Batch 53/372] [D loss: 0.6826516389846802] [G loss: 0.6640677452087402]\n",
      "[Epoch 66/1001] [Batch 54/372] [D loss: 0.6890283823013306] [G loss: 0.7211673259735107]\n",
      "[Epoch 66/1001] [Batch 55/372] [D loss: 0.6869391202926636] [G loss: 0.6239678263664246]\n",
      "[Epoch 66/1001] [Batch 56/372] [D loss: 0.6868866682052612] [G loss: 0.7140753269195557]\n",
      "[Epoch 66/1001] [Batch 57/372] [D loss: 0.6878819465637207] [G loss: 0.6433088779449463]\n",
      "[Epoch 66/1001] [Batch 58/372] [D loss: 0.6841423511505127] [G loss: 0.715387225151062]\n",
      "[Epoch 66/1001] [Batch 59/372] [D loss: 0.6839025020599365] [G loss: 0.6630688309669495]\n",
      "[Epoch 66/1001] [Batch 60/372] [D loss: 0.6900004744529724] [G loss: 0.6989138126373291]\n",
      "[Epoch 66/1001] [Batch 61/372] [D loss: 0.6857612133026123] [G loss: 0.6293702125549316]\n",
      "[Epoch 66/1001] [Batch 62/372] [D loss: 0.6937928199768066] [G loss: 0.7751112580299377]\n",
      "[Epoch 66/1001] [Batch 63/372] [D loss: 0.6858341693878174] [G loss: 0.5499509572982788]\n",
      "[Epoch 66/1001] [Batch 64/372] [D loss: 0.7083830833435059] [G loss: 0.8434122204780579]\n",
      "[Epoch 66/1001] [Batch 65/372] [D loss: 0.6994184255599976] [G loss: 0.5237439274787903]\n",
      "[Epoch 66/1001] [Batch 66/372] [D loss: 0.6988708972930908] [G loss: 0.8588802814483643]\n",
      "[Epoch 66/1001] [Batch 67/372] [D loss: 0.6955892443656921] [G loss: 0.5382441282272339]\n",
      "[Epoch 66/1001] [Batch 68/372] [D loss: 0.7064205408096313] [G loss: 0.848572850227356]\n",
      "[Epoch 66/1001] [Batch 69/372] [D loss: 0.6965692043304443] [G loss: 0.5615272521972656]\n",
      "[Epoch 66/1001] [Batch 70/372] [D loss: 0.6992006301879883] [G loss: 0.7794296741485596]\n",
      "[Epoch 66/1001] [Batch 71/372] [D loss: 0.688927412033081] [G loss: 0.6068685054779053]\n",
      "[Epoch 66/1001] [Batch 72/372] [D loss: 0.6866389513015747] [G loss: 0.7277283668518066]\n",
      "[Epoch 66/1001] [Batch 73/372] [D loss: 0.6883226633071899] [G loss: 0.6393550634384155]\n",
      "[Epoch 66/1001] [Batch 74/372] [D loss: 0.6803159713745117] [G loss: 0.7340934872627258]\n",
      "[Epoch 66/1001] [Batch 75/372] [D loss: 0.6896114945411682] [G loss: 0.5953465104103088]\n",
      "[Epoch 66/1001] [Batch 76/372] [D loss: 0.6881311535835266] [G loss: 0.786937415599823]\n",
      "[Epoch 66/1001] [Batch 77/372] [D loss: 0.6937776207923889] [G loss: 0.5828049182891846]\n",
      "[Epoch 66/1001] [Batch 78/372] [D loss: 0.6924418210983276] [G loss: 0.7655487656593323]\n",
      "[Epoch 66/1001] [Batch 79/372] [D loss: 0.6917686462402344] [G loss: 0.6065393686294556]\n",
      "[Epoch 66/1001] [Batch 80/372] [D loss: 0.6870752573013306] [G loss: 0.7376495003700256]\n",
      "[Epoch 66/1001] [Batch 81/372] [D loss: 0.6839907765388489] [G loss: 0.6438632011413574]\n",
      "[Epoch 66/1001] [Batch 82/372] [D loss: 0.6856730580329895] [G loss: 0.6876094341278076]\n",
      "[Epoch 66/1001] [Batch 83/372] [D loss: 0.6864660382270813] [G loss: 0.6734171509742737]\n",
      "[Epoch 66/1001] [Batch 84/372] [D loss: 0.6858930587768555] [G loss: 0.6575148105621338]\n",
      "[Epoch 66/1001] [Batch 85/372] [D loss: 0.6899129748344421] [G loss: 0.7077597975730896]\n",
      "[Epoch 66/1001] [Batch 86/372] [D loss: 0.6862235069274902] [G loss: 0.6676182150840759]\n",
      "[Epoch 66/1001] [Batch 87/372] [D loss: 0.679193377494812] [G loss: 0.6773535013198853]\n",
      "[Epoch 66/1001] [Batch 88/372] [D loss: 0.6903243660926819] [G loss: 0.6805938482284546]\n",
      "[Epoch 66/1001] [Batch 89/372] [D loss: 0.679597020149231] [G loss: 0.6486220955848694]\n",
      "[Epoch 66/1001] [Batch 90/372] [D loss: 0.6850754022598267] [G loss: 0.7257627844810486]\n",
      "[Epoch 66/1001] [Batch 91/372] [D loss: 0.6867316365242004] [G loss: 0.6060956716537476]\n",
      "[Epoch 66/1001] [Batch 92/372] [D loss: 0.6845093965530396] [G loss: 0.7977811694145203]\n",
      "[Epoch 66/1001] [Batch 93/372] [D loss: 0.7019188404083252] [G loss: 0.5121725797653198]\n",
      "[Epoch 66/1001] [Batch 94/372] [D loss: 0.7058395743370056] [G loss: 0.9646435976028442]\n",
      "[Epoch 66/1001] [Batch 95/372] [D loss: 0.733121395111084] [G loss: 0.4541388154029846]\n",
      "[Epoch 66/1001] [Batch 96/372] [D loss: 0.71866774559021] [G loss: 0.862284779548645]\n",
      "[Epoch 66/1001] [Batch 97/372] [D loss: 0.7090492248535156] [G loss: 0.613884687423706]\n",
      "[Epoch 66/1001] [Batch 98/372] [D loss: 0.6927000880241394] [G loss: 0.6446185111999512]\n",
      "[Epoch 66/1001] [Batch 99/372] [D loss: 0.6883101463317871] [G loss: 0.7489205598831177]\n",
      "[Epoch 66/1001] [Batch 100/372] [D loss: 0.6930093169212341] [G loss: 0.6284441351890564]\n",
      "[Epoch 66/1001] [Batch 101/372] [D loss: 0.6807997226715088] [G loss: 0.7028383612632751]\n",
      "[Epoch 66/1001] [Batch 102/372] [D loss: 0.6852765679359436] [G loss: 0.67585688829422]\n",
      "[Epoch 66/1001] [Batch 103/372] [D loss: 0.6890863180160522] [G loss: 0.6791598200798035]\n",
      "[Epoch 66/1001] [Batch 104/372] [D loss: 0.6836477518081665] [G loss: 0.6864194273948669]\n",
      "[Epoch 66/1001] [Batch 105/372] [D loss: 0.6831163167953491] [G loss: 0.6544663310050964]\n",
      "[Epoch 66/1001] [Batch 106/372] [D loss: 0.685492753982544] [G loss: 0.6888362169265747]\n",
      "[Epoch 66/1001] [Batch 107/372] [D loss: 0.6824477910995483] [G loss: 0.6942647695541382]\n",
      "[Epoch 66/1001] [Batch 108/372] [D loss: 0.6872721910476685] [G loss: 0.6444196105003357]\n",
      "[Epoch 66/1001] [Batch 109/372] [D loss: 0.6856109499931335] [G loss: 0.7013365030288696]\n",
      "[Epoch 66/1001] [Batch 110/372] [D loss: 0.6871814727783203] [G loss: 0.6772494912147522]\n",
      "[Epoch 66/1001] [Batch 111/372] [D loss: 0.6851043105125427] [G loss: 0.6514952182769775]\n",
      "[Epoch 66/1001] [Batch 112/372] [D loss: 0.6858553886413574] [G loss: 0.7208155989646912]\n",
      "[Epoch 66/1001] [Batch 113/372] [D loss: 0.6888167858123779] [G loss: 0.6039829254150391]\n",
      "[Epoch 66/1001] [Batch 114/372] [D loss: 0.6928285360336304] [G loss: 0.7533994317054749]\n",
      "[Epoch 66/1001] [Batch 115/372] [D loss: 0.6889595985412598] [G loss: 0.6282836198806763]\n",
      "[Epoch 66/1001] [Batch 116/372] [D loss: 0.6903619766235352] [G loss: 0.6989084482192993]\n",
      "[Epoch 66/1001] [Batch 117/372] [D loss: 0.6980630159378052] [G loss: 0.6600885391235352]\n",
      "[Epoch 66/1001] [Batch 118/372] [D loss: 0.68533855676651] [G loss: 0.6871021389961243]\n",
      "[Epoch 66/1001] [Batch 119/372] [D loss: 0.68904048204422] [G loss: 0.6460515856742859]\n",
      "[Epoch 66/1001] [Batch 120/372] [D loss: 0.6883316040039062] [G loss: 0.7371962070465088]\n",
      "[Epoch 66/1001] [Batch 121/372] [D loss: 0.6890231370925903] [G loss: 0.6054482460021973]\n",
      "[Epoch 66/1001] [Batch 122/372] [D loss: 0.6857606172561646] [G loss: 0.7667651772499084]\n",
      "[Epoch 66/1001] [Batch 123/372] [D loss: 0.6919543743133545] [G loss: 0.575160026550293]\n",
      "[Epoch 66/1001] [Batch 124/372] [D loss: 0.6985560655593872] [G loss: 0.7769694328308105]\n",
      "[Epoch 66/1001] [Batch 125/372] [D loss: 0.6911959648132324] [G loss: 0.6101239323616028]\n",
      "[Epoch 66/1001] [Batch 126/372] [D loss: 0.6906633377075195] [G loss: 0.7579030394554138]\n",
      "[Epoch 66/1001] [Batch 127/372] [D loss: 0.6945123076438904] [G loss: 0.5700528025627136]\n",
      "[Epoch 66/1001] [Batch 128/372] [D loss: 0.7006545066833496] [G loss: 0.8276138305664062]\n",
      "[Epoch 66/1001] [Batch 129/372] [D loss: 0.7005122900009155] [G loss: 0.5474454164505005]\n",
      "[Epoch 66/1001] [Batch 130/372] [D loss: 0.695452094078064] [G loss: 0.7903587818145752]\n",
      "[Epoch 66/1001] [Batch 131/372] [D loss: 0.6977932453155518] [G loss: 0.620495080947876]\n",
      "[Epoch 66/1001] [Batch 132/372] [D loss: 0.6925619840621948] [G loss: 0.6915196180343628]\n",
      "[Epoch 66/1001] [Batch 133/372] [D loss: 0.6891120076179504] [G loss: 0.6855335235595703]\n",
      "[Epoch 66/1001] [Batch 134/372] [D loss: 0.6849169135093689] [G loss: 0.6345492601394653]\n",
      "[Epoch 66/1001] [Batch 135/372] [D loss: 0.6879463195800781] [G loss: 0.7465274930000305]\n",
      "[Epoch 66/1001] [Batch 136/372] [D loss: 0.6895127296447754] [G loss: 0.6138578653335571]\n",
      "[Epoch 66/1001] [Batch 137/372] [D loss: 0.6900017261505127] [G loss: 0.730769157409668]\n",
      "[Epoch 66/1001] [Batch 138/372] [D loss: 0.692827582359314] [G loss: 0.6393178105354309]\n",
      "[Epoch 66/1001] [Batch 139/372] [D loss: 0.691809892654419] [G loss: 0.7086098194122314]\n",
      "[Epoch 66/1001] [Batch 140/372] [D loss: 0.6916967034339905] [G loss: 0.6454915404319763]\n",
      "[Epoch 66/1001] [Batch 141/372] [D loss: 0.6878838539123535] [G loss: 0.7187062501907349]\n",
      "[Epoch 66/1001] [Batch 142/372] [D loss: 0.6879123449325562] [G loss: 0.6277821660041809]\n",
      "[Epoch 66/1001] [Batch 143/372] [D loss: 0.6929268836975098] [G loss: 0.7277501821517944]\n",
      "[Epoch 66/1001] [Batch 144/372] [D loss: 0.6848477721214294] [G loss: 0.62593013048172]\n",
      "[Epoch 66/1001] [Batch 145/372] [D loss: 0.6975304484367371] [G loss: 0.7377728819847107]\n",
      "[Epoch 66/1001] [Batch 146/372] [D loss: 0.6893861889839172] [G loss: 0.605279266834259]\n",
      "[Epoch 66/1001] [Batch 147/372] [D loss: 0.6919335126876831] [G loss: 0.7585363388061523]\n",
      "[Epoch 66/1001] [Batch 148/372] [D loss: 0.6979501843452454] [G loss: 0.604338526725769]\n",
      "[Epoch 66/1001] [Batch 149/372] [D loss: 0.6906506419181824] [G loss: 0.741935133934021]\n",
      "[Epoch 66/1001] [Batch 150/372] [D loss: 0.6877039670944214] [G loss: 0.6349824666976929]\n",
      "[Epoch 66/1001] [Batch 151/372] [D loss: 0.6877204179763794] [G loss: 0.7015828490257263]\n",
      "[Epoch 66/1001] [Batch 152/372] [D loss: 0.6918219327926636] [G loss: 0.6639403700828552]\n",
      "[Epoch 66/1001] [Batch 153/372] [D loss: 0.6867567300796509] [G loss: 0.6778786778450012]\n",
      "[Epoch 66/1001] [Batch 154/372] [D loss: 0.6924816966056824] [G loss: 0.7081410884857178]\n",
      "[Epoch 66/1001] [Batch 155/372] [D loss: 0.6888755559921265] [G loss: 0.6789503693580627]\n",
      "[Epoch 66/1001] [Batch 156/372] [D loss: 0.6916826963424683] [G loss: 0.6524871587753296]\n",
      "[Epoch 66/1001] [Batch 157/372] [D loss: 0.6894295811653137] [G loss: 0.7062085270881653]\n",
      "[Epoch 66/1001] [Batch 158/372] [D loss: 0.676139235496521] [G loss: 0.6435092687606812]\n",
      "[Epoch 66/1001] [Batch 159/372] [D loss: 0.683491587638855] [G loss: 0.6994996070861816]\n",
      "[Epoch 66/1001] [Batch 160/372] [D loss: 0.6957176327705383] [G loss: 0.6502606868743896]\n",
      "[Epoch 66/1001] [Batch 161/372] [D loss: 0.6864267587661743] [G loss: 0.7008806467056274]\n",
      "[Epoch 66/1001] [Batch 162/372] [D loss: 0.6933156251907349] [G loss: 0.652967095375061]\n",
      "[Epoch 66/1001] [Batch 163/372] [D loss: 0.6918383836746216] [G loss: 0.6778541207313538]\n",
      "[Epoch 66/1001] [Batch 164/372] [D loss: 0.6879057884216309] [G loss: 0.6849813461303711]\n",
      "[Epoch 66/1001] [Batch 165/372] [D loss: 0.6897330284118652] [G loss: 0.6450857520103455]\n",
      "[Epoch 66/1001] [Batch 166/372] [D loss: 0.6871845722198486] [G loss: 0.7129278182983398]\n",
      "[Epoch 66/1001] [Batch 167/372] [D loss: 0.6772902011871338] [G loss: 0.5966154932975769]\n",
      "[Epoch 66/1001] [Batch 168/372] [D loss: 0.6981074810028076] [G loss: 0.8707423806190491]\n",
      "[Epoch 66/1001] [Batch 169/372] [D loss: 0.7019743919372559] [G loss: 0.46298912167549133]\n",
      "[Epoch 66/1001] [Batch 170/372] [D loss: 0.7165021896362305] [G loss: 0.9918814897537231]\n",
      "[Epoch 66/1001] [Batch 171/372] [D loss: 0.7253236770629883] [G loss: 0.48170357942581177]\n",
      "[Epoch 66/1001] [Batch 172/372] [D loss: 0.7132380604743958] [G loss: 0.8216111063957214]\n",
      "[Epoch 66/1001] [Batch 173/372] [D loss: 0.6961351633071899] [G loss: 0.6333425045013428]\n",
      "[Epoch 66/1001] [Batch 174/372] [D loss: 0.689995527267456] [G loss: 0.674637496471405]\n",
      "[Epoch 66/1001] [Batch 175/372] [D loss: 0.6822798252105713] [G loss: 0.705540120601654]\n",
      "[Epoch 66/1001] [Batch 176/372] [D loss: 0.6917714476585388] [G loss: 0.6470020413398743]\n",
      "[Epoch 66/1001] [Batch 177/372] [D loss: 0.6868675947189331] [G loss: 0.6922189593315125]\n",
      "[Epoch 66/1001] [Batch 178/372] [D loss: 0.6824313402175903] [G loss: 0.6768324375152588]\n",
      "[Epoch 66/1001] [Batch 179/372] [D loss: 0.6855584383010864] [G loss: 0.6609446406364441]\n",
      "[Epoch 66/1001] [Batch 180/372] [D loss: 0.6898019313812256] [G loss: 0.7222611308097839]\n",
      "[Epoch 66/1001] [Batch 181/372] [D loss: 0.6876312494277954] [G loss: 0.6258912086486816]\n",
      "[Epoch 66/1001] [Batch 182/372] [D loss: 0.6886125802993774] [G loss: 0.7283160090446472]\n",
      "[Epoch 66/1001] [Batch 183/372] [D loss: 0.6964595317840576] [G loss: 0.6369316577911377]\n",
      "[Epoch 66/1001] [Batch 184/372] [D loss: 0.6910932064056396] [G loss: 0.7057623863220215]\n",
      "[Epoch 66/1001] [Batch 185/372] [D loss: 0.6898453235626221] [G loss: 0.6423937082290649]\n",
      "[Epoch 66/1001] [Batch 186/372] [D loss: 0.6888688802719116] [G loss: 0.7145909070968628]\n",
      "[Epoch 66/1001] [Batch 187/372] [D loss: 0.6873788833618164] [G loss: 0.6359831094741821]\n",
      "[Epoch 66/1001] [Batch 188/372] [D loss: 0.6870607137680054] [G loss: 0.7112992405891418]\n",
      "[Epoch 66/1001] [Batch 189/372] [D loss: 0.6993969678878784] [G loss: 0.6318573951721191]\n",
      "[Epoch 66/1001] [Batch 190/372] [D loss: 0.684290885925293] [G loss: 0.7119173407554626]\n",
      "[Epoch 66/1001] [Batch 191/372] [D loss: 0.6927682161331177] [G loss: 0.6319434642791748]\n",
      "[Epoch 66/1001] [Batch 192/372] [D loss: 0.6852943897247314] [G loss: 0.7080006003379822]\n",
      "[Epoch 66/1001] [Batch 193/372] [D loss: 0.6864804029464722] [G loss: 0.6408327221870422]\n",
      "[Epoch 66/1001] [Batch 194/372] [D loss: 0.6831989884376526] [G loss: 0.7126902937889099]\n",
      "[Epoch 66/1001] [Batch 195/372] [D loss: 0.6870166659355164] [G loss: 0.6213238835334778]\n",
      "[Epoch 66/1001] [Batch 196/372] [D loss: 0.693202018737793] [G loss: 0.7927159667015076]\n",
      "[Epoch 66/1001] [Batch 197/372] [D loss: 0.6914497017860413] [G loss: 0.5559542775154114]\n",
      "[Epoch 66/1001] [Batch 198/372] [D loss: 0.6979775428771973] [G loss: 0.8018739819526672]\n",
      "[Epoch 66/1001] [Batch 199/372] [D loss: 0.6938918232917786] [G loss: 0.5902955532073975]\n",
      "[Epoch 66/1001] [Batch 200/372] [D loss: 0.6900306940078735] [G loss: 0.7472366094589233]\n",
      "[Epoch 66/1001] [Batch 201/372] [D loss: 0.691743016242981] [G loss: 0.6332308650016785]\n",
      "[Epoch 66/1001] [Batch 202/372] [D loss: 0.6929999589920044] [G loss: 0.7042107582092285]\n",
      "[Epoch 66/1001] [Batch 203/372] [D loss: 0.6842615604400635] [G loss: 0.658501148223877]\n",
      "[Epoch 66/1001] [Batch 204/372] [D loss: 0.6828798651695251] [G loss: 0.6686366200447083]\n",
      "[Epoch 66/1001] [Batch 205/372] [D loss: 0.6816498041152954] [G loss: 0.7007920145988464]\n",
      "[Epoch 66/1001] [Batch 206/372] [D loss: 0.6842185258865356] [G loss: 0.6408705711364746]\n",
      "[Epoch 66/1001] [Batch 207/372] [D loss: 0.6939195394515991] [G loss: 0.7043173313140869]\n",
      "[Epoch 66/1001] [Batch 208/372] [D loss: 0.6858184933662415] [G loss: 0.6452350616455078]\n",
      "[Epoch 66/1001] [Batch 209/372] [D loss: 0.6910656690597534] [G loss: 0.7220536470413208]\n",
      "[Epoch 66/1001] [Batch 210/372] [D loss: 0.6880367398262024] [G loss: 0.624916672706604]\n",
      "[Epoch 66/1001] [Batch 211/372] [D loss: 0.6869940757751465] [G loss: 0.7141482830047607]\n",
      "[Epoch 66/1001] [Batch 212/372] [D loss: 0.6918196082115173] [G loss: 0.6386347413063049]\n",
      "[Epoch 66/1001] [Batch 213/372] [D loss: 0.6909109950065613] [G loss: 0.6967235803604126]\n",
      "[Epoch 66/1001] [Batch 214/372] [D loss: 0.6946348547935486] [G loss: 0.6436267495155334]\n",
      "[Epoch 66/1001] [Batch 215/372] [D loss: 0.6842900514602661] [G loss: 0.7175054550170898]\n",
      "[Epoch 66/1001] [Batch 216/372] [D loss: 0.6868112087249756] [G loss: 0.6419923901557922]\n",
      "[Epoch 66/1001] [Batch 217/372] [D loss: 0.6900354027748108] [G loss: 0.6823431849479675]\n",
      "[Epoch 66/1001] [Batch 218/372] [D loss: 0.6925883293151855] [G loss: 0.6872282028198242]\n",
      "[Epoch 66/1001] [Batch 219/372] [D loss: 0.6897696852684021] [G loss: 0.6584171652793884]\n",
      "[Epoch 66/1001] [Batch 220/372] [D loss: 0.6772329807281494] [G loss: 0.7084368467330933]\n",
      "[Epoch 66/1001] [Batch 221/372] [D loss: 0.6833591461181641] [G loss: 0.6295196413993835]\n",
      "[Epoch 66/1001] [Batch 222/372] [D loss: 0.6887292861938477] [G loss: 0.7301791906356812]\n",
      "[Epoch 66/1001] [Batch 223/372] [D loss: 0.6907912492752075] [G loss: 0.6098345518112183]\n",
      "[Epoch 66/1001] [Batch 224/372] [D loss: 0.6873629093170166] [G loss: 0.7343441843986511]\n",
      "[Epoch 66/1001] [Batch 225/372] [D loss: 0.6917564272880554] [G loss: 0.6111314296722412]\n",
      "[Epoch 66/1001] [Batch 226/372] [D loss: 0.6909189224243164] [G loss: 0.7895992994308472]\n",
      "[Epoch 66/1001] [Batch 227/372] [D loss: 0.6948722004890442] [G loss: 0.5453712344169617]\n",
      "[Epoch 66/1001] [Batch 228/372] [D loss: 0.6982002258300781] [G loss: 0.86403489112854]\n",
      "[Epoch 66/1001] [Batch 229/372] [D loss: 0.7000282406806946] [G loss: 0.526494026184082]\n",
      "[Epoch 66/1001] [Batch 230/372] [D loss: 0.7024508714675903] [G loss: 0.8092085123062134]\n",
      "[Epoch 66/1001] [Batch 231/372] [D loss: 0.6943383812904358] [G loss: 0.6024404764175415]\n",
      "[Epoch 66/1001] [Batch 232/372] [D loss: 0.6964249610900879] [G loss: 0.7187010049819946]\n",
      "[Epoch 66/1001] [Batch 233/372] [D loss: 0.6841079592704773] [G loss: 0.6694644093513489]\n",
      "[Epoch 66/1001] [Batch 234/372] [D loss: 0.6934031248092651] [G loss: 0.6604870557785034]\n",
      "[Epoch 66/1001] [Batch 235/372] [D loss: 0.6867144107818604] [G loss: 0.6849247217178345]\n",
      "[Epoch 66/1001] [Batch 236/372] [D loss: 0.6874884963035583] [G loss: 0.6590914130210876]\n",
      "[Epoch 66/1001] [Batch 237/372] [D loss: 0.6842105388641357] [G loss: 0.6898757219314575]\n",
      "[Epoch 66/1001] [Batch 238/372] [D loss: 0.6904828548431396] [G loss: 0.6513053178787231]\n",
      "[Epoch 66/1001] [Batch 239/372] [D loss: 0.692612886428833] [G loss: 0.72928386926651]\n",
      "[Epoch 66/1001] [Batch 240/372] [D loss: 0.6870859861373901] [G loss: 0.6269429922103882]\n",
      "[Epoch 66/1001] [Batch 241/372] [D loss: 0.6895818710327148] [G loss: 0.7119152545928955]\n",
      "[Epoch 66/1001] [Batch 242/372] [D loss: 0.6968148946762085] [G loss: 0.6383190751075745]\n",
      "[Epoch 66/1001] [Batch 243/372] [D loss: 0.6859927773475647] [G loss: 0.6886674761772156]\n",
      "[Epoch 66/1001] [Batch 244/372] [D loss: 0.6952472925186157] [G loss: 0.6701861619949341]\n",
      "[Epoch 66/1001] [Batch 245/372] [D loss: 0.6832791566848755] [G loss: 0.660457968711853]\n",
      "[Epoch 66/1001] [Batch 246/372] [D loss: 0.6959882974624634] [G loss: 0.7712582945823669]\n",
      "[Epoch 66/1001] [Batch 247/372] [D loss: 0.6912504434585571] [G loss: 0.5734266638755798]\n",
      "[Epoch 66/1001] [Batch 248/372] [D loss: 0.6926788091659546] [G loss: 0.7919446229934692]\n",
      "[Epoch 66/1001] [Batch 249/372] [D loss: 0.6937904357910156] [G loss: 0.5906814932823181]\n",
      "[Epoch 66/1001] [Batch 250/372] [D loss: 0.6926499605178833] [G loss: 0.7546766400337219]\n",
      "[Epoch 66/1001] [Batch 251/372] [D loss: 0.6881675720214844] [G loss: 0.6156408190727234]\n",
      "[Epoch 66/1001] [Batch 252/372] [D loss: 0.688339114189148] [G loss: 0.7372780442237854]\n",
      "[Epoch 66/1001] [Batch 253/372] [D loss: 0.6968178749084473] [G loss: 0.6211216449737549]\n",
      "[Epoch 66/1001] [Batch 254/372] [D loss: 0.6859265565872192] [G loss: 0.7103554606437683]\n",
      "[Epoch 66/1001] [Batch 255/372] [D loss: 0.6954311728477478] [G loss: 0.6712263822555542]\n",
      "[Epoch 66/1001] [Batch 256/372] [D loss: 0.6883808374404907] [G loss: 0.6645921468734741]\n",
      "[Epoch 66/1001] [Batch 257/372] [D loss: 0.6872825622558594] [G loss: 0.7088597416877747]\n",
      "[Epoch 66/1001] [Batch 258/372] [D loss: 0.6901452541351318] [G loss: 0.6571192145347595]\n",
      "[Epoch 66/1001] [Batch 259/372] [D loss: 0.6940134167671204] [G loss: 0.6793773174285889]\n",
      "[Epoch 66/1001] [Batch 260/372] [D loss: 0.6863723397254944] [G loss: 0.6609433889389038]\n",
      "[Epoch 66/1001] [Batch 261/372] [D loss: 0.6858935356140137] [G loss: 0.7065828442573547]\n",
      "[Epoch 66/1001] [Batch 262/372] [D loss: 0.6818604469299316] [G loss: 0.6388469934463501]\n",
      "[Epoch 66/1001] [Batch 263/372] [D loss: 0.6931219100952148] [G loss: 0.7213633060455322]\n",
      "[Epoch 66/1001] [Batch 264/372] [D loss: 0.6872637271881104] [G loss: 0.6335265636444092]\n",
      "[Epoch 66/1001] [Batch 265/372] [D loss: 0.6863510608673096] [G loss: 0.7371149659156799]\n",
      "[Epoch 66/1001] [Batch 266/372] [D loss: 0.6860659122467041] [G loss: 0.6197038888931274]\n",
      "[Epoch 66/1001] [Batch 267/372] [D loss: 0.681960940361023] [G loss: 0.7261291742324829]\n",
      "[Epoch 66/1001] [Batch 268/372] [D loss: 0.6870124340057373] [G loss: 0.6503445506095886]\n",
      "[Epoch 66/1001] [Batch 269/372] [D loss: 0.6950926780700684] [G loss: 0.7016068696975708]\n",
      "[Epoch 66/1001] [Batch 270/372] [D loss: 0.6807604432106018] [G loss: 0.6605669856071472]\n",
      "[Epoch 66/1001] [Batch 271/372] [D loss: 0.6901266574859619] [G loss: 0.6875357627868652]\n",
      "[Epoch 66/1001] [Batch 272/372] [D loss: 0.694836437702179] [G loss: 0.6610917448997498]\n",
      "[Epoch 66/1001] [Batch 273/372] [D loss: 0.6844596266746521] [G loss: 0.7035345435142517]\n",
      "[Epoch 66/1001] [Batch 274/372] [D loss: 0.6753682494163513] [G loss: 0.6410492658615112]\n",
      "[Epoch 66/1001] [Batch 275/372] [D loss: 0.6889747381210327] [G loss: 0.7493814826011658]\n",
      "[Epoch 66/1001] [Batch 276/372] [D loss: 0.6881239414215088] [G loss: 0.5736294984817505]\n",
      "[Epoch 66/1001] [Batch 277/372] [D loss: 0.695166289806366] [G loss: 0.8016640543937683]\n",
      "[Epoch 66/1001] [Batch 278/372] [D loss: 0.6941347122192383] [G loss: 0.5685574412345886]\n",
      "[Epoch 66/1001] [Batch 279/372] [D loss: 0.6906408667564392] [G loss: 0.837952733039856]\n",
      "[Epoch 66/1001] [Batch 280/372] [D loss: 0.6914012432098389] [G loss: 0.5333802700042725]\n",
      "[Epoch 66/1001] [Batch 281/372] [D loss: 0.7045409679412842] [G loss: 0.8515393733978271]\n",
      "[Epoch 66/1001] [Batch 282/372] [D loss: 0.7012835144996643] [G loss: 0.5366020202636719]\n",
      "[Epoch 66/1001] [Batch 283/372] [D loss: 0.6966996192932129] [G loss: 0.8332627415657043]\n",
      "[Epoch 66/1001] [Batch 284/372] [D loss: 0.7011903524398804] [G loss: 0.5951884388923645]\n",
      "[Epoch 66/1001] [Batch 285/372] [D loss: 0.6872425079345703] [G loss: 0.7459076046943665]\n",
      "[Epoch 66/1001] [Batch 286/372] [D loss: 0.6979411840438843] [G loss: 0.6227903962135315]\n",
      "[Epoch 66/1001] [Batch 287/372] [D loss: 0.6988591551780701] [G loss: 0.6930800676345825]\n",
      "[Epoch 66/1001] [Batch 288/372] [D loss: 0.6870817542076111] [G loss: 0.6518851518630981]\n",
      "[Epoch 66/1001] [Batch 289/372] [D loss: 0.6876174807548523] [G loss: 0.6983764171600342]\n",
      "[Epoch 66/1001] [Batch 290/372] [D loss: 0.6882150173187256] [G loss: 0.6727099418640137]\n",
      "[Epoch 66/1001] [Batch 291/372] [D loss: 0.6979360580444336] [G loss: 0.6855640411376953]\n",
      "[Epoch 66/1001] [Batch 292/372] [D loss: 0.6888695955276489] [G loss: 0.6669880151748657]\n",
      "[Epoch 66/1001] [Batch 293/372] [D loss: 0.6887353658676147] [G loss: 0.6676872968673706]\n",
      "[Epoch 66/1001] [Batch 294/372] [D loss: 0.6899455785751343] [G loss: 0.6894559264183044]\n",
      "[Epoch 66/1001] [Batch 295/372] [D loss: 0.6876611709594727] [G loss: 0.6599095463752747]\n",
      "[Epoch 66/1001] [Batch 296/372] [D loss: 0.6873469352722168] [G loss: 0.7038201093673706]\n",
      "[Epoch 66/1001] [Batch 297/372] [D loss: 0.6854922771453857] [G loss: 0.6469553112983704]\n",
      "[Epoch 66/1001] [Batch 298/372] [D loss: 0.6858072280883789] [G loss: 0.6934433579444885]\n",
      "[Epoch 66/1001] [Batch 299/372] [D loss: 0.6919913291931152] [G loss: 0.6703336834907532]\n",
      "[Epoch 66/1001] [Batch 300/372] [D loss: 0.6865932941436768] [G loss: 0.6696509122848511]\n",
      "[Epoch 66/1001] [Batch 301/372] [D loss: 0.6837040781974792] [G loss: 0.6825990676879883]\n",
      "[Epoch 66/1001] [Batch 302/372] [D loss: 0.690655529499054] [G loss: 0.6816432476043701]\n",
      "[Epoch 66/1001] [Batch 303/372] [D loss: 0.6879532337188721] [G loss: 0.664795994758606]\n",
      "[Epoch 66/1001] [Batch 304/372] [D loss: 0.6905874013900757] [G loss: 0.6965016722679138]\n",
      "[Epoch 66/1001] [Batch 305/372] [D loss: 0.6904623508453369] [G loss: 0.6575680375099182]\n",
      "[Epoch 66/1001] [Batch 306/372] [D loss: 0.6888427138328552] [G loss: 0.7012954950332642]\n",
      "[Epoch 66/1001] [Batch 307/372] [D loss: 0.6871607303619385] [G loss: 0.6309802532196045]\n",
      "[Epoch 66/1001] [Batch 308/372] [D loss: 0.6911063194274902] [G loss: 0.729341447353363]\n",
      "[Epoch 66/1001] [Batch 309/372] [D loss: 0.6870148181915283] [G loss: 0.6184678077697754]\n",
      "[Epoch 66/1001] [Batch 310/372] [D loss: 0.6920138597488403] [G loss: 0.7250041365623474]\n",
      "[Epoch 66/1001] [Batch 311/372] [D loss: 0.6883504390716553] [G loss: 0.6522520184516907]\n",
      "[Epoch 66/1001] [Batch 312/372] [D loss: 0.6899534463882446] [G loss: 0.7159396409988403]\n",
      "[Epoch 66/1001] [Batch 313/372] [D loss: 0.6876718401908875] [G loss: 0.6338698267936707]\n",
      "[Epoch 66/1001] [Batch 314/372] [D loss: 0.6868489384651184] [G loss: 0.7080328464508057]\n",
      "[Epoch 66/1001] [Batch 315/372] [D loss: 0.6885899305343628] [G loss: 0.6429845690727234]\n",
      "[Epoch 66/1001] [Batch 316/372] [D loss: 0.692129373550415] [G loss: 0.7253806591033936]\n",
      "[Epoch 66/1001] [Batch 317/372] [D loss: 0.6925557851791382] [G loss: 0.6134029030799866]\n",
      "[Epoch 66/1001] [Batch 318/372] [D loss: 0.6891688108444214] [G loss: 0.7586984634399414]\n",
      "[Epoch 66/1001] [Batch 319/372] [D loss: 0.6902621984481812] [G loss: 0.587344765663147]\n",
      "[Epoch 66/1001] [Batch 320/372] [D loss: 0.6900965571403503] [G loss: 0.8095401525497437]\n",
      "[Epoch 66/1001] [Batch 321/372] [D loss: 0.6938266754150391] [G loss: 0.5809984803199768]\n",
      "[Epoch 66/1001] [Batch 322/372] [D loss: 0.6906511187553406] [G loss: 0.7523679137229919]\n",
      "[Epoch 66/1001] [Batch 323/372] [D loss: 0.6868524551391602] [G loss: 0.6226475834846497]\n",
      "[Epoch 66/1001] [Batch 324/372] [D loss: 0.6870681047439575] [G loss: 0.7307016849517822]\n",
      "[Epoch 66/1001] [Batch 325/372] [D loss: 0.6908721923828125] [G loss: 0.620547354221344]\n",
      "[Epoch 66/1001] [Batch 326/372] [D loss: 0.6861420273780823] [G loss: 0.7304830551147461]\n",
      "[Epoch 66/1001] [Batch 327/372] [D loss: 0.6869432926177979] [G loss: 0.6125928163528442]\n",
      "[Epoch 66/1001] [Batch 328/372] [D loss: 0.6906554698944092] [G loss: 0.7469034194946289]\n",
      "[Epoch 66/1001] [Batch 329/372] [D loss: 0.6972808241844177] [G loss: 0.5831145644187927]\n",
      "[Epoch 66/1001] [Batch 330/372] [D loss: 0.6904350519180298] [G loss: 0.809841513633728]\n",
      "[Epoch 66/1001] [Batch 331/372] [D loss: 0.6917146444320679] [G loss: 0.5445833206176758]\n",
      "[Epoch 66/1001] [Batch 332/372] [D loss: 0.700149416923523] [G loss: 0.8633003234863281]\n",
      "[Epoch 66/1001] [Batch 333/372] [D loss: 0.7013417482376099] [G loss: 0.5340567231178284]\n",
      "[Epoch 66/1001] [Batch 334/372] [D loss: 0.6997483968734741] [G loss: 0.8094640970230103]\n",
      "[Epoch 66/1001] [Batch 335/372] [D loss: 0.7005608677864075] [G loss: 0.6028705835342407]\n",
      "[Epoch 66/1001] [Batch 336/372] [D loss: 0.6936382055282593] [G loss: 0.7334234714508057]\n",
      "[Epoch 66/1001] [Batch 337/372] [D loss: 0.6909103393554688] [G loss: 0.6416932344436646]\n",
      "[Epoch 66/1001] [Batch 338/372] [D loss: 0.6927188634872437] [G loss: 0.6998106837272644]\n",
      "[Epoch 66/1001] [Batch 339/372] [D loss: 0.6925260424613953] [G loss: 0.6682388782501221]\n",
      "[Epoch 66/1001] [Batch 340/372] [D loss: 0.690444827079773] [G loss: 0.6523325443267822]\n",
      "[Epoch 66/1001] [Batch 341/372] [D loss: 0.6906465291976929] [G loss: 0.7272411584854126]\n",
      "[Epoch 66/1001] [Batch 342/372] [D loss: 0.687645435333252] [G loss: 0.6268812417984009]\n",
      "[Epoch 66/1001] [Batch 343/372] [D loss: 0.6894468665122986] [G loss: 0.725106418132782]\n",
      "[Epoch 66/1001] [Batch 344/372] [D loss: 0.6948176622390747] [G loss: 0.6475716829299927]\n",
      "[Epoch 66/1001] [Batch 345/372] [D loss: 0.6908869743347168] [G loss: 0.7002376317977905]\n",
      "[Epoch 66/1001] [Batch 346/372] [D loss: 0.6891366839408875] [G loss: 0.6708369851112366]\n",
      "[Epoch 66/1001] [Batch 347/372] [D loss: 0.6836106181144714] [G loss: 0.6860515475273132]\n",
      "[Epoch 66/1001] [Batch 348/372] [D loss: 0.6891942024230957] [G loss: 0.6656805276870728]\n",
      "[Epoch 66/1001] [Batch 349/372] [D loss: 0.6944085359573364] [G loss: 0.6798767447471619]\n",
      "[Epoch 66/1001] [Batch 350/372] [D loss: 0.6887269616127014] [G loss: 0.6760309934616089]\n",
      "[Epoch 66/1001] [Batch 351/372] [D loss: 0.6944630742073059] [G loss: 0.6680968403816223]\n",
      "[Epoch 66/1001] [Batch 352/372] [D loss: 0.6893389225006104] [G loss: 0.69300377368927]\n",
      "[Epoch 66/1001] [Batch 353/372] [D loss: 0.6821922063827515] [G loss: 0.6409793496131897]\n",
      "[Epoch 66/1001] [Batch 354/372] [D loss: 0.6873877048492432] [G loss: 0.7411255836486816]\n",
      "[Epoch 66/1001] [Batch 355/372] [D loss: 0.6939280033111572] [G loss: 0.5956993699073792]\n",
      "[Epoch 66/1001] [Batch 356/372] [D loss: 0.6874200701713562] [G loss: 0.7736703753471375]\n",
      "[Epoch 66/1001] [Batch 357/372] [D loss: 0.69484943151474] [G loss: 0.5812508463859558]\n",
      "[Epoch 66/1001] [Batch 358/372] [D loss: 0.6958940029144287] [G loss: 0.7652416825294495]\n",
      "[Epoch 66/1001] [Batch 359/372] [D loss: 0.6958888173103333] [G loss: 0.6110007762908936]\n",
      "[Epoch 66/1001] [Batch 360/372] [D loss: 0.6885979175567627] [G loss: 0.7308236360549927]\n",
      "[Epoch 66/1001] [Batch 361/372] [D loss: 0.6872879266738892] [G loss: 0.6427149176597595]\n",
      "[Epoch 66/1001] [Batch 362/372] [D loss: 0.6897048950195312] [G loss: 0.6895079612731934]\n",
      "[Epoch 66/1001] [Batch 363/372] [D loss: 0.6882960796356201] [G loss: 0.6664084196090698]\n",
      "[Epoch 66/1001] [Batch 364/372] [D loss: 0.681280255317688] [G loss: 0.6848809719085693]\n",
      "[Epoch 66/1001] [Batch 365/372] [D loss: 0.6876053810119629] [G loss: 0.6706258654594421]\n",
      "[Epoch 66/1001] [Batch 366/372] [D loss: 0.685175359249115] [G loss: 0.6771643757820129]\n",
      "[Epoch 66/1001] [Batch 367/372] [D loss: 0.6932785511016846] [G loss: 0.6729443669319153]\n",
      "[Epoch 66/1001] [Batch 368/372] [D loss: 0.6823979616165161] [G loss: 0.6681881546974182]\n",
      "[Epoch 66/1001] [Batch 369/372] [D loss: 0.6861112713813782] [G loss: 0.706969678401947]\n",
      "[Epoch 66/1001] [Batch 370/372] [D loss: 0.6848214864730835] [G loss: 0.6538411378860474]\n",
      "[Epoch 66/1001] [Batch 371/372] [D loss: 0.6868836879730225] [G loss: 0.7211145758628845]\n",
      "[Epoch 67/1001] [Batch 0/372] [D loss: 0.6875425577163696] [G loss: 0.6328088641166687]\n",
      "[Epoch 67/1001] [Batch 1/372] [D loss: 0.6908785104751587] [G loss: 0.7286465167999268]\n",
      "[Epoch 67/1001] [Batch 2/372] [D loss: 0.6812829375267029] [G loss: 0.6511874198913574]\n",
      "[Epoch 67/1001] [Batch 3/372] [D loss: 0.6870457530021667] [G loss: 0.6837760210037231]\n",
      "[Epoch 67/1001] [Batch 4/372] [D loss: 0.6838916540145874] [G loss: 0.7017820477485657]\n",
      "[Epoch 67/1001] [Batch 5/372] [D loss: 0.6847048997879028] [G loss: 0.6349642276763916]\n",
      "[Epoch 67/1001] [Batch 6/372] [D loss: 0.6935111880302429] [G loss: 0.7481711506843567]\n",
      "[Epoch 67/1001] [Batch 7/372] [D loss: 0.6841340065002441] [G loss: 0.6015836596488953]\n",
      "[Epoch 67/1001] [Batch 8/372] [D loss: 0.6842049360275269] [G loss: 0.754160463809967]\n",
      "[Epoch 67/1001] [Batch 9/372] [D loss: 0.6899653673171997] [G loss: 0.6274999380111694]\n",
      "[Epoch 67/1001] [Batch 10/372] [D loss: 0.6864922642707825] [G loss: 0.7074647545814514]\n",
      "[Epoch 67/1001] [Batch 11/372] [D loss: 0.6898084878921509] [G loss: 0.6284260749816895]\n",
      "[Epoch 67/1001] [Batch 12/372] [D loss: 0.6840316653251648] [G loss: 0.7251637578010559]\n",
      "[Epoch 67/1001] [Batch 13/372] [D loss: 0.6833347678184509] [G loss: 0.6538814902305603]\n",
      "[Epoch 67/1001] [Batch 14/372] [D loss: 0.6853078007698059] [G loss: 0.7363365888595581]\n",
      "[Epoch 67/1001] [Batch 15/372] [D loss: 0.6849695444107056] [G loss: 0.6020251512527466]\n",
      "[Epoch 67/1001] [Batch 16/372] [D loss: 0.6954492926597595] [G loss: 0.7639302611351013]\n",
      "[Epoch 67/1001] [Batch 17/372] [D loss: 0.6962120532989502] [G loss: 0.5696531534194946]\n",
      "[Epoch 67/1001] [Batch 18/372] [D loss: 0.6933368444442749] [G loss: 0.8233861923217773]\n",
      "[Epoch 67/1001] [Batch 19/372] [D loss: 0.6942129135131836] [G loss: 0.5603645443916321]\n",
      "[Epoch 67/1001] [Batch 20/372] [D loss: 0.6923999190330505] [G loss: 0.7862410545349121]\n",
      "[Epoch 67/1001] [Batch 21/372] [D loss: 0.6904322504997253] [G loss: 0.6113412380218506]\n",
      "[Epoch 67/1001] [Batch 22/372] [D loss: 0.6923362016677856] [G loss: 0.7140811085700989]\n",
      "[Epoch 67/1001] [Batch 23/372] [D loss: 0.6921005845069885] [G loss: 0.6533522009849548]\n",
      "[Epoch 67/1001] [Batch 24/372] [D loss: 0.691174328327179] [G loss: 0.6912029981613159]\n",
      "[Epoch 67/1001] [Batch 25/372] [D loss: 0.6851147413253784] [G loss: 0.645046591758728]\n",
      "[Epoch 67/1001] [Batch 26/372] [D loss: 0.6903218030929565] [G loss: 0.7277730703353882]\n",
      "[Epoch 67/1001] [Batch 27/372] [D loss: 0.6889906525611877] [G loss: 0.6478025317192078]\n",
      "[Epoch 67/1001] [Batch 28/372] [D loss: 0.6918100118637085] [G loss: 0.7070631980895996]\n",
      "[Epoch 67/1001] [Batch 29/372] [D loss: 0.6948120594024658] [G loss: 0.6411796808242798]\n",
      "[Epoch 67/1001] [Batch 30/372] [D loss: 0.6851911544799805] [G loss: 0.6856863498687744]\n",
      "[Epoch 67/1001] [Batch 31/372] [D loss: 0.6853278875350952] [G loss: 0.6863957047462463]\n",
      "[Epoch 67/1001] [Batch 32/372] [D loss: 0.6827129125595093] [G loss: 0.6710128784179688]\n",
      "[Epoch 67/1001] [Batch 33/372] [D loss: 0.6839978694915771] [G loss: 0.6497694253921509]\n",
      "[Epoch 67/1001] [Batch 34/372] [D loss: 0.6909087896347046] [G loss: 0.7160062193870544]\n",
      "[Epoch 67/1001] [Batch 35/372] [D loss: 0.6883059740066528] [G loss: 0.6607018113136292]\n",
      "[Epoch 67/1001] [Batch 36/372] [D loss: 0.6843578815460205] [G loss: 0.6871179342269897]\n",
      "[Epoch 67/1001] [Batch 37/372] [D loss: 0.6841574907302856] [G loss: 0.6699178814888]\n",
      "[Epoch 67/1001] [Batch 38/372] [D loss: 0.6830167174339294] [G loss: 0.6966956257820129]\n",
      "[Epoch 67/1001] [Batch 39/372] [D loss: 0.6884387135505676] [G loss: 0.6449880599975586]\n",
      "[Epoch 67/1001] [Batch 40/372] [D loss: 0.686555027961731] [G loss: 0.6997842192649841]\n",
      "[Epoch 67/1001] [Batch 41/372] [D loss: 0.6831406354904175] [G loss: 0.6581335067749023]\n",
      "[Epoch 67/1001] [Batch 42/372] [D loss: 0.6786333918571472] [G loss: 0.6972364187240601]\n",
      "[Epoch 67/1001] [Batch 43/372] [D loss: 0.681287407875061] [G loss: 0.6674338579177856]\n",
      "[Epoch 67/1001] [Batch 44/372] [D loss: 0.6866328716278076] [G loss: 0.6870570778846741]\n",
      "[Epoch 67/1001] [Batch 45/372] [D loss: 0.692764401435852] [G loss: 0.6493545770645142]\n",
      "[Epoch 67/1001] [Batch 46/372] [D loss: 0.6850194931030273] [G loss: 0.6954158544540405]\n",
      "[Epoch 67/1001] [Batch 47/372] [D loss: 0.6841961145401001] [G loss: 0.6722289323806763]\n",
      "[Epoch 67/1001] [Batch 48/372] [D loss: 0.6864299774169922] [G loss: 0.6723893284797668]\n",
      "[Epoch 67/1001] [Batch 49/372] [D loss: 0.6774088144302368] [G loss: 0.6746031045913696]\n",
      "[Epoch 67/1001] [Batch 50/372] [D loss: 0.6856322288513184] [G loss: 0.6848397254943848]\n",
      "[Epoch 67/1001] [Batch 51/372] [D loss: 0.6939882040023804] [G loss: 0.6229492425918579]\n",
      "[Epoch 67/1001] [Batch 52/372] [D loss: 0.687171220779419] [G loss: 0.7872849702835083]\n",
      "[Epoch 67/1001] [Batch 53/372] [D loss: 0.688804030418396] [G loss: 0.5820321440696716]\n",
      "[Epoch 67/1001] [Batch 54/372] [D loss: 0.6822186708450317] [G loss: 0.8155346512794495]\n",
      "[Epoch 67/1001] [Batch 55/372] [D loss: 0.6918683648109436] [G loss: 0.5175263285636902]\n",
      "[Epoch 67/1001] [Batch 56/372] [D loss: 0.6955116391181946] [G loss: 0.9307345151901245]\n",
      "[Epoch 67/1001] [Batch 57/372] [D loss: 0.7182455658912659] [G loss: 0.46918177604675293]\n",
      "[Epoch 67/1001] [Batch 58/372] [D loss: 0.720116376876831] [G loss: 0.9383765459060669]\n",
      "[Epoch 67/1001] [Batch 59/372] [D loss: 0.7113101482391357] [G loss: 0.5380973219871521]\n",
      "[Epoch 67/1001] [Batch 60/372] [D loss: 0.6956275701522827] [G loss: 0.7611543536186218]\n",
      "[Epoch 67/1001] [Batch 61/372] [D loss: 0.6923661231994629] [G loss: 0.6573129892349243]\n",
      "[Epoch 67/1001] [Batch 62/372] [D loss: 0.6856244802474976] [G loss: 0.6443450450897217]\n",
      "[Epoch 67/1001] [Batch 63/372] [D loss: 0.6775346994400024] [G loss: 0.728698194026947]\n",
      "[Epoch 67/1001] [Batch 64/372] [D loss: 0.6945697665214539] [G loss: 0.6533641219139099]\n",
      "[Epoch 67/1001] [Batch 65/372] [D loss: 0.6955224871635437] [G loss: 0.6895067691802979]\n",
      "[Epoch 67/1001] [Batch 66/372] [D loss: 0.6831846237182617] [G loss: 0.645207941532135]\n",
      "[Epoch 67/1001] [Batch 67/372] [D loss: 0.6880511045455933] [G loss: 0.7078825831413269]\n",
      "[Epoch 67/1001] [Batch 68/372] [D loss: 0.6863465905189514] [G loss: 0.6226901412010193]\n",
      "[Epoch 67/1001] [Batch 69/372] [D loss: 0.6893255710601807] [G loss: 0.7618247866630554]\n",
      "[Epoch 67/1001] [Batch 70/372] [D loss: 0.6866214871406555] [G loss: 0.597325325012207]\n",
      "[Epoch 67/1001] [Batch 71/372] [D loss: 0.6969904899597168] [G loss: 0.7614694237709045]\n",
      "[Epoch 67/1001] [Batch 72/372] [D loss: 0.694229006767273] [G loss: 0.5854619741439819]\n",
      "[Epoch 67/1001] [Batch 73/372] [D loss: 0.6861007809638977] [G loss: 0.790459930896759]\n",
      "[Epoch 67/1001] [Batch 74/372] [D loss: 0.6943780183792114] [G loss: 0.5763518810272217]\n",
      "[Epoch 67/1001] [Batch 75/372] [D loss: 0.6946196556091309] [G loss: 0.7755427360534668]\n",
      "[Epoch 67/1001] [Batch 76/372] [D loss: 0.6868641972541809] [G loss: 0.6146676540374756]\n",
      "[Epoch 67/1001] [Batch 77/372] [D loss: 0.6855217218399048] [G loss: 0.7024050951004028]\n",
      "[Epoch 67/1001] [Batch 78/372] [D loss: 0.6891859173774719] [G loss: 0.6797788143157959]\n",
      "[Epoch 67/1001] [Batch 79/372] [D loss: 0.6879619359970093] [G loss: 0.6656997203826904]\n",
      "[Epoch 67/1001] [Batch 80/372] [D loss: 0.6860542297363281] [G loss: 0.6985988020896912]\n",
      "[Epoch 67/1001] [Batch 81/372] [D loss: 0.6866262555122375] [G loss: 0.6535040736198425]\n",
      "[Epoch 67/1001] [Batch 82/372] [D loss: 0.692719578742981] [G loss: 0.6871881484985352]\n",
      "[Epoch 67/1001] [Batch 83/372] [D loss: 0.6868739128112793] [G loss: 0.6662337779998779]\n",
      "[Epoch 67/1001] [Batch 84/372] [D loss: 0.6876043081283569] [G loss: 0.6754677295684814]\n",
      "[Epoch 67/1001] [Batch 85/372] [D loss: 0.6819049119949341] [G loss: 0.7030150890350342]\n",
      "[Epoch 67/1001] [Batch 86/372] [D loss: 0.6895607709884644] [G loss: 0.6373115181922913]\n",
      "[Epoch 67/1001] [Batch 87/372] [D loss: 0.6864959597587585] [G loss: 0.7126489877700806]\n",
      "[Epoch 67/1001] [Batch 88/372] [D loss: 0.6883938312530518] [G loss: 0.6509172916412354]\n",
      "[Epoch 67/1001] [Batch 89/372] [D loss: 0.6840367317199707] [G loss: 0.7088402509689331]\n",
      "[Epoch 67/1001] [Batch 90/372] [D loss: 0.6824507713317871] [G loss: 0.6415356397628784]\n",
      "[Epoch 67/1001] [Batch 91/372] [D loss: 0.6898061037063599] [G loss: 0.7127187252044678]\n",
      "[Epoch 67/1001] [Batch 92/372] [D loss: 0.6865600347518921] [G loss: 0.6356921195983887]\n",
      "[Epoch 67/1001] [Batch 93/372] [D loss: 0.6866120100021362] [G loss: 0.736183226108551]\n",
      "[Epoch 67/1001] [Batch 94/372] [D loss: 0.6796998977661133] [G loss: 0.6308335065841675]\n",
      "[Epoch 67/1001] [Batch 95/372] [D loss: 0.6791216135025024] [G loss: 0.7357128858566284]\n",
      "[Epoch 67/1001] [Batch 96/372] [D loss: 0.687800943851471] [G loss: 0.6214612722396851]\n",
      "[Epoch 67/1001] [Batch 97/372] [D loss: 0.6894861459732056] [G loss: 0.776436984539032]\n",
      "[Epoch 67/1001] [Batch 98/372] [D loss: 0.6808294057846069] [G loss: 0.579168975353241]\n",
      "[Epoch 67/1001] [Batch 99/372] [D loss: 0.6895102262496948] [G loss: 0.7829869985580444]\n",
      "[Epoch 67/1001] [Batch 100/372] [D loss: 0.6910284757614136] [G loss: 0.6004348397254944]\n",
      "[Epoch 67/1001] [Batch 101/372] [D loss: 0.6872727870941162] [G loss: 0.7261096239089966]\n",
      "[Epoch 67/1001] [Batch 102/372] [D loss: 0.6894127726554871] [G loss: 0.6445133090019226]\n",
      "[Epoch 67/1001] [Batch 103/372] [D loss: 0.6840131282806396] [G loss: 0.7157688140869141]\n",
      "[Epoch 67/1001] [Batch 104/372] [D loss: 0.6830030083656311] [G loss: 0.6343873739242554]\n",
      "[Epoch 67/1001] [Batch 105/372] [D loss: 0.689074695110321] [G loss: 0.7111443877220154]\n",
      "[Epoch 67/1001] [Batch 106/372] [D loss: 0.6907552480697632] [G loss: 0.6473925113677979]\n",
      "[Epoch 67/1001] [Batch 107/372] [D loss: 0.6910747289657593] [G loss: 0.7037245631217957]\n",
      "[Epoch 67/1001] [Batch 108/372] [D loss: 0.6859006881713867] [G loss: 0.6425986289978027]\n",
      "[Epoch 67/1001] [Batch 109/372] [D loss: 0.689371645450592] [G loss: 0.6921166181564331]\n",
      "[Epoch 67/1001] [Batch 110/372] [D loss: 0.6859650611877441] [G loss: 0.6303161382675171]\n",
      "[Epoch 67/1001] [Batch 111/372] [D loss: 0.6834744215011597] [G loss: 0.7507644891738892]\n",
      "[Epoch 67/1001] [Batch 112/372] [D loss: 0.6853042840957642] [G loss: 0.5526223182678223]\n",
      "[Epoch 67/1001] [Batch 113/372] [D loss: 0.7025763988494873] [G loss: 0.8859673738479614]\n",
      "[Epoch 67/1001] [Batch 114/372] [D loss: 0.7038258910179138] [G loss: 0.4724463224411011]\n",
      "[Epoch 67/1001] [Batch 115/372] [D loss: 0.7097536325454712] [G loss: 0.898729145526886]\n",
      "[Epoch 67/1001] [Batch 116/372] [D loss: 0.7054675817489624] [G loss: 0.5538410544395447]\n",
      "[Epoch 67/1001] [Batch 117/372] [D loss: 0.6923426389694214] [G loss: 0.7407699823379517]\n",
      "[Epoch 67/1001] [Batch 118/372] [D loss: 0.6884325742721558] [G loss: 0.6755417585372925]\n",
      "[Epoch 67/1001] [Batch 119/372] [D loss: 0.6940833330154419] [G loss: 0.6347858905792236]\n",
      "[Epoch 67/1001] [Batch 120/372] [D loss: 0.6876790523529053] [G loss: 0.7297483682632446]\n",
      "[Epoch 67/1001] [Batch 121/372] [D loss: 0.6866014003753662] [G loss: 0.6379390954971313]\n",
      "[Epoch 67/1001] [Batch 122/372] [D loss: 0.6858035326004028] [G loss: 0.6969797611236572]\n",
      "[Epoch 67/1001] [Batch 123/372] [D loss: 0.6838999390602112] [G loss: 0.6690787672996521]\n",
      "[Epoch 67/1001] [Batch 124/372] [D loss: 0.6880919933319092] [G loss: 0.6917663812637329]\n",
      "[Epoch 67/1001] [Batch 125/372] [D loss: 0.6845456957817078] [G loss: 0.6437726616859436]\n",
      "[Epoch 67/1001] [Batch 126/372] [D loss: 0.6874157786369324] [G loss: 0.7058559656143188]\n",
      "[Epoch 67/1001] [Batch 127/372] [D loss: 0.6858311891555786] [G loss: 0.6309736371040344]\n",
      "[Epoch 67/1001] [Batch 128/372] [D loss: 0.6834477782249451] [G loss: 0.722257673740387]\n",
      "[Epoch 67/1001] [Batch 129/372] [D loss: 0.6773271560668945] [G loss: 0.6060481071472168]\n",
      "[Epoch 67/1001] [Batch 130/372] [D loss: 0.6977264881134033] [G loss: 0.7789844274520874]\n",
      "[Epoch 67/1001] [Batch 131/372] [D loss: 0.6826355457305908] [G loss: 0.5706515312194824]\n",
      "[Epoch 67/1001] [Batch 132/372] [D loss: 0.6834174990653992] [G loss: 0.798550546169281]\n",
      "[Epoch 67/1001] [Batch 133/372] [D loss: 0.6875066757202148] [G loss: 0.5535539984703064]\n",
      "[Epoch 67/1001] [Batch 134/372] [D loss: 0.7000254392623901] [G loss: 0.8524885177612305]\n",
      "[Epoch 67/1001] [Batch 135/372] [D loss: 0.7038381099700928] [G loss: 0.5217469334602356]\n",
      "[Epoch 67/1001] [Batch 136/372] [D loss: 0.6964783072471619] [G loss: 0.8443827629089355]\n",
      "[Epoch 67/1001] [Batch 137/372] [D loss: 0.6967391967773438] [G loss: 0.5401060581207275]\n",
      "[Epoch 67/1001] [Batch 138/372] [D loss: 0.699710488319397] [G loss: 0.8152197599411011]\n",
      "[Epoch 67/1001] [Batch 139/372] [D loss: 0.6943323612213135] [G loss: 0.5767183899879456]\n",
      "[Epoch 67/1001] [Batch 140/372] [D loss: 0.6943957209587097] [G loss: 0.7840534448623657]\n",
      "[Epoch 67/1001] [Batch 141/372] [D loss: 0.6906051635742188] [G loss: 0.5706378221511841]\n",
      "[Epoch 67/1001] [Batch 142/372] [D loss: 0.697144627571106] [G loss: 0.7712088227272034]\n",
      "[Epoch 67/1001] [Batch 143/372] [D loss: 0.6898648142814636] [G loss: 0.6279039978981018]\n",
      "[Epoch 67/1001] [Batch 144/372] [D loss: 0.686689019203186] [G loss: 0.6932822465896606]\n",
      "[Epoch 67/1001] [Batch 145/372] [D loss: 0.6886924505233765] [G loss: 0.6744140982627869]\n",
      "[Epoch 67/1001] [Batch 146/372] [D loss: 0.6817003488540649] [G loss: 0.6715366840362549]\n",
      "[Epoch 67/1001] [Batch 147/372] [D loss: 0.6857337355613708] [G loss: 0.6786153316497803]\n",
      "[Epoch 67/1001] [Batch 148/372] [D loss: 0.6816691160202026] [G loss: 0.6317920684814453]\n",
      "[Epoch 67/1001] [Batch 149/372] [D loss: 0.6859683990478516] [G loss: 0.8198274970054626]\n",
      "[Epoch 67/1001] [Batch 150/372] [D loss: 0.7008283138275146] [G loss: 0.491868257522583]\n",
      "[Epoch 67/1001] [Batch 151/372] [D loss: 0.7063060998916626] [G loss: 0.9459289312362671]\n",
      "[Epoch 67/1001] [Batch 152/372] [D loss: 0.7059088349342346] [G loss: 0.48901498317718506]\n",
      "[Epoch 67/1001] [Batch 153/372] [D loss: 0.704562783241272] [G loss: 0.8768718242645264]\n",
      "[Epoch 67/1001] [Batch 154/372] [D loss: 0.7060102224349976] [G loss: 0.5400731563568115]\n",
      "[Epoch 67/1001] [Batch 155/372] [D loss: 0.7012735605239868] [G loss: 0.772430419921875]\n",
      "[Epoch 67/1001] [Batch 156/372] [D loss: 0.6893142461776733] [G loss: 0.6360675096511841]\n",
      "[Epoch 67/1001] [Batch 157/372] [D loss: 0.6875979900360107] [G loss: 0.6902607679367065]\n",
      "[Epoch 67/1001] [Batch 158/372] [D loss: 0.6872608661651611] [G loss: 0.6761264204978943]\n",
      "[Epoch 67/1001] [Batch 159/372] [D loss: 0.6843509674072266] [G loss: 0.6861897706985474]\n",
      "[Epoch 67/1001] [Batch 160/372] [D loss: 0.6867986917495728] [G loss: 0.6545071601867676]\n",
      "[Epoch 67/1001] [Batch 161/372] [D loss: 0.687644362449646] [G loss: 0.6989577412605286]\n",
      "[Epoch 67/1001] [Batch 162/372] [D loss: 0.6946386098861694] [G loss: 0.6590428948402405]\n",
      "[Epoch 67/1001] [Batch 163/372] [D loss: 0.6953099370002747] [G loss: 0.6733430624008179]\n",
      "[Epoch 67/1001] [Batch 164/372] [D loss: 0.6881342530250549] [G loss: 0.6750565767288208]\n",
      "[Epoch 67/1001] [Batch 165/372] [D loss: 0.6863359212875366] [G loss: 0.6660365462303162]\n",
      "[Epoch 67/1001] [Batch 166/372] [D loss: 0.6840908527374268] [G loss: 0.6795583367347717]\n",
      "[Epoch 67/1001] [Batch 167/372] [D loss: 0.6932281255722046] [G loss: 0.6716507077217102]\n",
      "[Epoch 67/1001] [Batch 168/372] [D loss: 0.6916552782058716] [G loss: 0.6546074748039246]\n",
      "[Epoch 67/1001] [Batch 169/372] [D loss: 0.691038966178894] [G loss: 0.7062129974365234]\n",
      "[Epoch 67/1001] [Batch 170/372] [D loss: 0.6868314743041992] [G loss: 0.6328749656677246]\n",
      "[Epoch 67/1001] [Batch 171/372] [D loss: 0.6874401569366455] [G loss: 0.689832329750061]\n",
      "[Epoch 67/1001] [Batch 172/372] [D loss: 0.68767911195755] [G loss: 0.7012832164764404]\n",
      "[Epoch 67/1001] [Batch 173/372] [D loss: 0.6815353035926819] [G loss: 0.6265671253204346]\n",
      "[Epoch 67/1001] [Batch 174/372] [D loss: 0.6934915781021118] [G loss: 0.7669997215270996]\n",
      "[Epoch 67/1001] [Batch 175/372] [D loss: 0.6885844469070435] [G loss: 0.5778257846832275]\n",
      "[Epoch 67/1001] [Batch 176/372] [D loss: 0.6940377950668335] [G loss: 0.7774115800857544]\n",
      "[Epoch 67/1001] [Batch 177/372] [D loss: 0.6879198551177979] [G loss: 0.6034663319587708]\n",
      "[Epoch 67/1001] [Batch 178/372] [D loss: 0.6940035820007324] [G loss: 0.7249103784561157]\n",
      "[Epoch 67/1001] [Batch 179/372] [D loss: 0.6870616674423218] [G loss: 0.6460901498794556]\n",
      "[Epoch 67/1001] [Batch 180/372] [D loss: 0.6848333477973938] [G loss: 0.6797571182250977]\n",
      "[Epoch 67/1001] [Batch 181/372] [D loss: 0.6861299276351929] [G loss: 0.6937096118927002]\n",
      "[Epoch 67/1001] [Batch 182/372] [D loss: 0.6863820552825928] [G loss: 0.6731537580490112]\n",
      "[Epoch 67/1001] [Batch 183/372] [D loss: 0.6899008750915527] [G loss: 0.6715047955513]\n",
      "[Epoch 67/1001] [Batch 184/372] [D loss: 0.6862645745277405] [G loss: 0.6477017402648926]\n",
      "[Epoch 67/1001] [Batch 185/372] [D loss: 0.6903516054153442] [G loss: 0.7185168266296387]\n",
      "[Epoch 67/1001] [Batch 186/372] [D loss: 0.6932939291000366] [G loss: 0.6153782606124878]\n",
      "[Epoch 67/1001] [Batch 187/372] [D loss: 0.6812582015991211] [G loss: 0.735806405544281]\n",
      "[Epoch 67/1001] [Batch 188/372] [D loss: 0.6814247369766235] [G loss: 0.6405375599861145]\n",
      "[Epoch 67/1001] [Batch 189/372] [D loss: 0.6909127235412598] [G loss: 0.7155311107635498]\n",
      "[Epoch 67/1001] [Batch 190/372] [D loss: 0.691558837890625] [G loss: 0.6549216508865356]\n",
      "[Epoch 67/1001] [Batch 191/372] [D loss: 0.6832797527313232] [G loss: 0.6754207015037537]\n",
      "[Epoch 67/1001] [Batch 192/372] [D loss: 0.6798778772354126] [G loss: 0.6937891244888306]\n",
      "[Epoch 67/1001] [Batch 193/372] [D loss: 0.6837954521179199] [G loss: 0.6472764015197754]\n",
      "[Epoch 67/1001] [Batch 194/372] [D loss: 0.6978163123130798] [G loss: 0.7363583445549011]\n",
      "[Epoch 67/1001] [Batch 195/372] [D loss: 0.6922032833099365] [G loss: 0.583053708076477]\n",
      "[Epoch 67/1001] [Batch 196/372] [D loss: 0.6926575899124146] [G loss: 0.7917441129684448]\n",
      "[Epoch 67/1001] [Batch 197/372] [D loss: 0.6960537433624268] [G loss: 0.5758762359619141]\n",
      "[Epoch 67/1001] [Batch 198/372] [D loss: 0.695301353931427] [G loss: 0.7659906148910522]\n",
      "[Epoch 67/1001] [Batch 199/372] [D loss: 0.6930911540985107] [G loss: 0.6054670214653015]\n",
      "[Epoch 67/1001] [Batch 200/372] [D loss: 0.6858114004135132] [G loss: 0.7274115085601807]\n",
      "[Epoch 67/1001] [Batch 201/372] [D loss: 0.6842496395111084] [G loss: 0.6477550268173218]\n",
      "[Epoch 67/1001] [Batch 202/372] [D loss: 0.6924474239349365] [G loss: 0.7138217687606812]\n",
      "[Epoch 67/1001] [Batch 203/372] [D loss: 0.6951882839202881] [G loss: 0.63730788230896]\n",
      "[Epoch 67/1001] [Batch 204/372] [D loss: 0.6910752058029175] [G loss: 0.6997771263122559]\n",
      "[Epoch 67/1001] [Batch 205/372] [D loss: 0.6891791820526123] [G loss: 0.655876636505127]\n",
      "[Epoch 67/1001] [Batch 206/372] [D loss: 0.6901543140411377] [G loss: 0.6852253079414368]\n",
      "[Epoch 67/1001] [Batch 207/372] [D loss: 0.6907514333724976] [G loss: 0.6605220437049866]\n",
      "[Epoch 67/1001] [Batch 208/372] [D loss: 0.6873722076416016] [G loss: 0.6987664103507996]\n",
      "[Epoch 67/1001] [Batch 209/372] [D loss: 0.6903159022331238] [G loss: 0.6456539630889893]\n",
      "[Epoch 67/1001] [Batch 210/372] [D loss: 0.6847211122512817] [G loss: 0.6944400072097778]\n",
      "[Epoch 67/1001] [Batch 211/372] [D loss: 0.6840589046478271] [G loss: 0.6398769617080688]\n",
      "[Epoch 67/1001] [Batch 212/372] [D loss: 0.6935300230979919] [G loss: 0.7909703254699707]\n",
      "[Epoch 67/1001] [Batch 213/372] [D loss: 0.6945019364356995] [G loss: 0.5198933482170105]\n",
      "[Epoch 67/1001] [Batch 214/372] [D loss: 0.6984741687774658] [G loss: 0.8843441605567932]\n",
      "[Epoch 67/1001] [Batch 215/372] [D loss: 0.7071691751480103] [G loss: 0.5335991978645325]\n",
      "[Epoch 67/1001] [Batch 216/372] [D loss: 0.7017369270324707] [G loss: 0.7847414016723633]\n",
      "[Epoch 67/1001] [Batch 217/372] [D loss: 0.6898510456085205] [G loss: 0.6266129612922668]\n",
      "[Epoch 67/1001] [Batch 218/372] [D loss: 0.6899583339691162] [G loss: 0.6866199374198914]\n",
      "[Epoch 67/1001] [Batch 219/372] [D loss: 0.6856578588485718] [G loss: 0.6828149557113647]\n",
      "[Epoch 67/1001] [Batch 220/372] [D loss: 0.6856372356414795] [G loss: 0.6730082631111145]\n",
      "[Epoch 67/1001] [Batch 221/372] [D loss: 0.6884902715682983] [G loss: 0.684843897819519]\n",
      "[Epoch 67/1001] [Batch 222/372] [D loss: 0.6855706572532654] [G loss: 0.6692633628845215]\n",
      "[Epoch 67/1001] [Batch 223/372] [D loss: 0.6863290071487427] [G loss: 0.6723084449768066]\n",
      "[Epoch 67/1001] [Batch 224/372] [D loss: 0.6877420544624329] [G loss: 0.6889294981956482]\n",
      "[Epoch 67/1001] [Batch 225/372] [D loss: 0.6878789067268372] [G loss: 0.6561756134033203]\n",
      "[Epoch 67/1001] [Batch 226/372] [D loss: 0.687893271446228] [G loss: 0.6987261772155762]\n",
      "[Epoch 67/1001] [Batch 227/372] [D loss: 0.6912845373153687] [G loss: 0.650310218334198]\n",
      "[Epoch 67/1001] [Batch 228/372] [D loss: 0.6851036548614502] [G loss: 0.702623724937439]\n",
      "[Epoch 67/1001] [Batch 229/372] [D loss: 0.6846656203269958] [G loss: 0.6652580499649048]\n",
      "[Epoch 67/1001] [Batch 230/372] [D loss: 0.6885381937026978] [G loss: 0.6730982065200806]\n",
      "[Epoch 67/1001] [Batch 231/372] [D loss: 0.6848604679107666] [G loss: 0.6929396986961365]\n",
      "[Epoch 67/1001] [Batch 232/372] [D loss: 0.6906337738037109] [G loss: 0.6286743879318237]\n",
      "[Epoch 67/1001] [Batch 233/372] [D loss: 0.6885157823562622] [G loss: 0.7652462720870972]\n",
      "[Epoch 67/1001] [Batch 234/372] [D loss: 0.691040575504303] [G loss: 0.5807666778564453]\n",
      "[Epoch 67/1001] [Batch 235/372] [D loss: 0.6872348189353943] [G loss: 0.7758921384811401]\n",
      "[Epoch 67/1001] [Batch 236/372] [D loss: 0.6830569505691528] [G loss: 0.5886639356613159]\n",
      "[Epoch 67/1001] [Batch 237/372] [D loss: 0.6988899111747742] [G loss: 0.7555792331695557]\n",
      "[Epoch 67/1001] [Batch 238/372] [D loss: 0.6942477226257324] [G loss: 0.5940307378768921]\n",
      "[Epoch 67/1001] [Batch 239/372] [D loss: 0.6875690817832947] [G loss: 0.7400063872337341]\n",
      "[Epoch 67/1001] [Batch 240/372] [D loss: 0.6919615268707275] [G loss: 0.6140788793563843]\n",
      "[Epoch 67/1001] [Batch 241/372] [D loss: 0.6889902353286743] [G loss: 0.7632024884223938]\n",
      "[Epoch 67/1001] [Batch 242/372] [D loss: 0.6914429664611816] [G loss: 0.5609276294708252]\n",
      "[Epoch 67/1001] [Batch 243/372] [D loss: 0.6971002817153931] [G loss: 0.8424866199493408]\n",
      "[Epoch 67/1001] [Batch 244/372] [D loss: 0.7082637548446655] [G loss: 0.5164313912391663]\n",
      "[Epoch 67/1001] [Batch 245/372] [D loss: 0.696042537689209] [G loss: 0.8668254017829895]\n",
      "[Epoch 67/1001] [Batch 246/372] [D loss: 0.7040581107139587] [G loss: 0.5479434728622437]\n",
      "[Epoch 67/1001] [Batch 247/372] [D loss: 0.6940453052520752] [G loss: 0.7589052319526672]\n",
      "[Epoch 67/1001] [Batch 248/372] [D loss: 0.6907131671905518] [G loss: 0.6410847306251526]\n",
      "[Epoch 67/1001] [Batch 249/372] [D loss: 0.6929396390914917] [G loss: 0.678485631942749]\n",
      "[Epoch 67/1001] [Batch 250/372] [D loss: 0.6831503510475159] [G loss: 0.6849819421768188]\n",
      "[Epoch 67/1001] [Batch 251/372] [D loss: 0.6804214715957642] [G loss: 0.6675841808319092]\n",
      "[Epoch 67/1001] [Batch 252/372] [D loss: 0.6854379177093506] [G loss: 0.6755962371826172]\n",
      "[Epoch 67/1001] [Batch 253/372] [D loss: 0.6876696348190308] [G loss: 0.6821321845054626]\n",
      "[Epoch 67/1001] [Batch 254/372] [D loss: 0.6770412921905518] [G loss: 0.6703948974609375]\n",
      "[Epoch 67/1001] [Batch 255/372] [D loss: 0.686214804649353] [G loss: 0.661095917224884]\n",
      "[Epoch 67/1001] [Batch 256/372] [D loss: 0.6929875016212463] [G loss: 0.7063125967979431]\n",
      "[Epoch 67/1001] [Batch 257/372] [D loss: 0.685141921043396] [G loss: 0.6234942674636841]\n",
      "[Epoch 67/1001] [Batch 258/372] [D loss: 0.6805856227874756] [G loss: 0.7327179908752441]\n",
      "[Epoch 67/1001] [Batch 259/372] [D loss: 0.6798921823501587] [G loss: 0.6080764532089233]\n",
      "[Epoch 67/1001] [Batch 260/372] [D loss: 0.6982322931289673] [G loss: 0.8179308176040649]\n",
      "[Epoch 67/1001] [Batch 261/372] [D loss: 0.7009150385856628] [G loss: 0.5217259526252747]\n",
      "[Epoch 67/1001] [Batch 262/372] [D loss: 0.7066754102706909] [G loss: 0.8631260991096497]\n",
      "[Epoch 67/1001] [Batch 263/372] [D loss: 0.7073528170585632] [G loss: 0.5113736391067505]\n",
      "[Epoch 67/1001] [Batch 264/372] [D loss: 0.7096492648124695] [G loss: 0.8326455354690552]\n",
      "[Epoch 67/1001] [Batch 265/372] [D loss: 0.7019246816635132] [G loss: 0.5783758163452148]\n",
      "[Epoch 67/1001] [Batch 266/372] [D loss: 0.6921615600585938] [G loss: 0.7201536893844604]\n",
      "[Epoch 67/1001] [Batch 267/372] [D loss: 0.6848286390304565] [G loss: 0.6612196564674377]\n",
      "[Epoch 67/1001] [Batch 268/372] [D loss: 0.6946455836296082] [G loss: 0.6766936779022217]\n",
      "[Epoch 67/1001] [Batch 269/372] [D loss: 0.6939787268638611] [G loss: 0.6504815816879272]\n",
      "[Epoch 67/1001] [Batch 270/372] [D loss: 0.6853376626968384] [G loss: 0.7160407304763794]\n",
      "[Epoch 67/1001] [Batch 271/372] [D loss: 0.6872438788414001] [G loss: 0.6220878958702087]\n",
      "[Epoch 67/1001] [Batch 272/372] [D loss: 0.6883735060691833] [G loss: 0.71077561378479]\n",
      "[Epoch 67/1001] [Batch 273/372] [D loss: 0.6898164749145508] [G loss: 0.647473931312561]\n",
      "[Epoch 67/1001] [Batch 274/372] [D loss: 0.6838910579681396] [G loss: 0.6932722330093384]\n",
      "[Epoch 67/1001] [Batch 275/372] [D loss: 0.6886899471282959] [G loss: 0.6756595373153687]\n",
      "[Epoch 67/1001] [Batch 276/372] [D loss: 0.6866515874862671] [G loss: 0.6596918106079102]\n",
      "[Epoch 67/1001] [Batch 277/372] [D loss: 0.6945304274559021] [G loss: 0.6943354606628418]\n",
      "[Epoch 67/1001] [Batch 278/372] [D loss: 0.6879599094390869] [G loss: 0.6544249653816223]\n",
      "[Epoch 67/1001] [Batch 279/372] [D loss: 0.6891056895256042] [G loss: 0.6979683041572571]\n",
      "[Epoch 67/1001] [Batch 280/372] [D loss: 0.6837738752365112] [G loss: 0.6288368105888367]\n",
      "[Epoch 67/1001] [Batch 281/372] [D loss: 0.684578537940979] [G loss: 0.7359488606452942]\n",
      "[Epoch 67/1001] [Batch 282/372] [D loss: 0.6917668581008911] [G loss: 0.6178238391876221]\n",
      "[Epoch 67/1001] [Batch 283/372] [D loss: 0.6792216300964355] [G loss: 0.7192767262458801]\n",
      "[Epoch 67/1001] [Batch 284/372] [D loss: 0.6896756887435913] [G loss: 0.6435309052467346]\n",
      "[Epoch 67/1001] [Batch 285/372] [D loss: 0.6871780157089233] [G loss: 0.7067561149597168]\n",
      "[Epoch 67/1001] [Batch 286/372] [D loss: 0.6893686056137085] [G loss: 0.6525464653968811]\n",
      "[Epoch 67/1001] [Batch 287/372] [D loss: 0.6907864212989807] [G loss: 0.7103912830352783]\n",
      "[Epoch 67/1001] [Batch 288/372] [D loss: 0.6885712742805481] [G loss: 0.626339316368103]\n",
      "[Epoch 67/1001] [Batch 289/372] [D loss: 0.6859117150306702] [G loss: 0.733428418636322]\n",
      "[Epoch 67/1001] [Batch 290/372] [D loss: 0.6877017021179199] [G loss: 0.6376380324363708]\n",
      "[Epoch 67/1001] [Batch 291/372] [D loss: 0.6812422275543213] [G loss: 0.6806366443634033]\n",
      "[Epoch 67/1001] [Batch 292/372] [D loss: 0.6895885467529297] [G loss: 0.6887401938438416]\n",
      "[Epoch 67/1001] [Batch 293/372] [D loss: 0.6909713745117188] [G loss: 0.668594479560852]\n",
      "[Epoch 67/1001] [Batch 294/372] [D loss: 0.6874287724494934] [G loss: 0.6599946022033691]\n",
      "[Epoch 67/1001] [Batch 295/372] [D loss: 0.683659553527832] [G loss: 0.7103427052497864]\n",
      "[Epoch 67/1001] [Batch 296/372] [D loss: 0.6844932436943054] [G loss: 0.6389287114143372]\n",
      "[Epoch 67/1001] [Batch 297/372] [D loss: 0.6944941878318787] [G loss: 0.7512133717536926]\n",
      "[Epoch 67/1001] [Batch 298/372] [D loss: 0.6837766170501709] [G loss: 0.5706139802932739]\n",
      "[Epoch 67/1001] [Batch 299/372] [D loss: 0.6892547607421875] [G loss: 0.8243513107299805]\n",
      "[Epoch 67/1001] [Batch 300/372] [D loss: 0.6995349526405334] [G loss: 0.5532070398330688]\n",
      "[Epoch 67/1001] [Batch 301/372] [D loss: 0.6903104782104492] [G loss: 0.822126030921936]\n",
      "[Epoch 67/1001] [Batch 302/372] [D loss: 0.6954346895217896] [G loss: 0.5596553087234497]\n",
      "[Epoch 67/1001] [Batch 303/372] [D loss: 0.6885828971862793] [G loss: 0.7707079648971558]\n",
      "[Epoch 67/1001] [Batch 304/372] [D loss: 0.6830100417137146] [G loss: 0.6449885368347168]\n",
      "[Epoch 67/1001] [Batch 305/372] [D loss: 0.6809823513031006] [G loss: 0.6773742437362671]\n",
      "[Epoch 67/1001] [Batch 306/372] [D loss: 0.6941706538200378] [G loss: 0.6878869533538818]\n",
      "[Epoch 67/1001] [Batch 307/372] [D loss: 0.6838914155960083] [G loss: 0.6628512740135193]\n",
      "[Epoch 67/1001] [Batch 308/372] [D loss: 0.6920360922813416] [G loss: 0.6707010865211487]\n",
      "[Epoch 67/1001] [Batch 309/372] [D loss: 0.6864878535270691] [G loss: 0.669154167175293]\n",
      "[Epoch 67/1001] [Batch 310/372] [D loss: 0.6998354196548462] [G loss: 0.7058618664741516]\n",
      "[Epoch 67/1001] [Batch 311/372] [D loss: 0.6898983120918274] [G loss: 0.6083457469940186]\n",
      "[Epoch 67/1001] [Batch 312/372] [D loss: 0.686814546585083] [G loss: 0.7782690525054932]\n",
      "[Epoch 67/1001] [Batch 313/372] [D loss: 0.6969007849693298] [G loss: 0.557003378868103]\n",
      "[Epoch 67/1001] [Batch 314/372] [D loss: 0.7035007476806641] [G loss: 0.7995376586914062]\n",
      "[Epoch 67/1001] [Batch 315/372] [D loss: 0.6961630582809448] [G loss: 0.5534675121307373]\n",
      "[Epoch 67/1001] [Batch 316/372] [D loss: 0.694668173789978] [G loss: 0.8178606629371643]\n",
      "[Epoch 67/1001] [Batch 317/372] [D loss: 0.689129650592804] [G loss: 0.5553988218307495]\n",
      "[Epoch 67/1001] [Batch 318/372] [D loss: 0.7053271532058716] [G loss: 0.8130441308021545]\n",
      "[Epoch 67/1001] [Batch 319/372] [D loss: 0.6936295032501221] [G loss: 0.5708833336830139]\n",
      "[Epoch 67/1001] [Batch 320/372] [D loss: 0.698502779006958] [G loss: 0.7636460065841675]\n",
      "[Epoch 67/1001] [Batch 321/372] [D loss: 0.6991103291511536] [G loss: 0.6083180904388428]\n",
      "[Epoch 67/1001] [Batch 322/372] [D loss: 0.6909914612770081] [G loss: 0.7193710803985596]\n",
      "[Epoch 67/1001] [Batch 323/372] [D loss: 0.6832431554794312] [G loss: 0.6738768219947815]\n",
      "[Epoch 67/1001] [Batch 324/372] [D loss: 0.6866371631622314] [G loss: 0.680709719657898]\n",
      "[Epoch 67/1001] [Batch 325/372] [D loss: 0.6775897741317749] [G loss: 0.6843965649604797]\n",
      "[Epoch 67/1001] [Batch 326/372] [D loss: 0.6836209893226624] [G loss: 0.6595118641853333]\n",
      "[Epoch 67/1001] [Batch 327/372] [D loss: 0.6859574317932129] [G loss: 0.6947715282440186]\n",
      "[Epoch 67/1001] [Batch 328/372] [D loss: 0.6864530444145203] [G loss: 0.6547948718070984]\n",
      "[Epoch 67/1001] [Batch 329/372] [D loss: 0.689566433429718] [G loss: 0.6805134415626526]\n",
      "[Epoch 67/1001] [Batch 330/372] [D loss: 0.6882913112640381] [G loss: 0.6808325052261353]\n",
      "[Epoch 67/1001] [Batch 331/372] [D loss: 0.6902260780334473] [G loss: 0.6699256300926208]\n",
      "[Epoch 67/1001] [Batch 332/372] [D loss: 0.6832253932952881] [G loss: 0.6742061972618103]\n",
      "[Epoch 67/1001] [Batch 333/372] [D loss: 0.6844406127929688] [G loss: 0.6761598587036133]\n",
      "[Epoch 67/1001] [Batch 334/372] [D loss: 0.6920369267463684] [G loss: 0.6933683753013611]\n",
      "[Epoch 67/1001] [Batch 335/372] [D loss: 0.6938260793685913] [G loss: 0.6270418167114258]\n",
      "[Epoch 67/1001] [Batch 336/372] [D loss: 0.689077615737915] [G loss: 0.7445498108863831]\n",
      "[Epoch 67/1001] [Batch 337/372] [D loss: 0.6911767721176147] [G loss: 0.6273427605628967]\n",
      "[Epoch 67/1001] [Batch 338/372] [D loss: 0.6897306442260742] [G loss: 0.6857175230979919]\n",
      "[Epoch 67/1001] [Batch 339/372] [D loss: 0.691159725189209] [G loss: 0.6755362153053284]\n",
      "[Epoch 67/1001] [Batch 340/372] [D loss: 0.6902225017547607] [G loss: 0.6901897192001343]\n",
      "[Epoch 67/1001] [Batch 341/372] [D loss: 0.6884896755218506] [G loss: 0.6633543372154236]\n",
      "[Epoch 67/1001] [Batch 342/372] [D loss: 0.6924440860748291] [G loss: 0.6737335920333862]\n",
      "[Epoch 67/1001] [Batch 343/372] [D loss: 0.6865856647491455] [G loss: 0.6611689925193787]\n",
      "[Epoch 67/1001] [Batch 344/372] [D loss: 0.6901570558547974] [G loss: 0.7016382217407227]\n",
      "[Epoch 67/1001] [Batch 345/372] [D loss: 0.6841186285018921] [G loss: 0.6401993036270142]\n",
      "[Epoch 67/1001] [Batch 346/372] [D loss: 0.6847693920135498] [G loss: 0.7097442746162415]\n",
      "[Epoch 67/1001] [Batch 347/372] [D loss: 0.693805992603302] [G loss: 0.6279526948928833]\n",
      "[Epoch 67/1001] [Batch 348/372] [D loss: 0.688890814781189] [G loss: 0.7248860001564026]\n",
      "[Epoch 67/1001] [Batch 349/372] [D loss: 0.6920541524887085] [G loss: 0.6214491128921509]\n",
      "[Epoch 67/1001] [Batch 350/372] [D loss: 0.6862189769744873] [G loss: 0.7210866212844849]\n",
      "[Epoch 67/1001] [Batch 351/372] [D loss: 0.6896137595176697] [G loss: 0.631752610206604]\n",
      "[Epoch 67/1001] [Batch 352/372] [D loss: 0.6918045282363892] [G loss: 0.7258908748626709]\n",
      "[Epoch 67/1001] [Batch 353/372] [D loss: 0.6849473714828491] [G loss: 0.6090720891952515]\n",
      "[Epoch 67/1001] [Batch 354/372] [D loss: 0.6908743977546692] [G loss: 0.7522414922714233]\n",
      "[Epoch 67/1001] [Batch 355/372] [D loss: 0.6963546276092529] [G loss: 0.5898426175117493]\n",
      "[Epoch 67/1001] [Batch 356/372] [D loss: 0.6879473924636841] [G loss: 0.7718230485916138]\n",
      "[Epoch 67/1001] [Batch 357/372] [D loss: 0.697025716304779] [G loss: 0.5860477685928345]\n",
      "[Epoch 67/1001] [Batch 358/372] [D loss: 0.6935826539993286] [G loss: 0.7712021470069885]\n",
      "[Epoch 67/1001] [Batch 359/372] [D loss: 0.7007705569267273] [G loss: 0.5868589282035828]\n",
      "[Epoch 67/1001] [Batch 360/372] [D loss: 0.6896120309829712] [G loss: 0.7610129714012146]\n",
      "[Epoch 67/1001] [Batch 361/372] [D loss: 0.6903818845748901] [G loss: 0.608269453048706]\n",
      "[Epoch 67/1001] [Batch 362/372] [D loss: 0.6961508989334106] [G loss: 0.735882043838501]\n",
      "[Epoch 67/1001] [Batch 363/372] [D loss: 0.6911313533782959] [G loss: 0.6133825182914734]\n",
      "[Epoch 67/1001] [Batch 364/372] [D loss: 0.6942976713180542] [G loss: 0.7173768877983093]\n",
      "[Epoch 67/1001] [Batch 365/372] [D loss: 0.6912598609924316] [G loss: 0.6469190120697021]\n",
      "[Epoch 67/1001] [Batch 366/372] [D loss: 0.6912720203399658] [G loss: 0.6911695599555969]\n",
      "[Epoch 67/1001] [Batch 367/372] [D loss: 0.6848679780960083] [G loss: 0.6845874786376953]\n",
      "[Epoch 67/1001] [Batch 368/372] [D loss: 0.6840133666992188] [G loss: 0.6570262908935547]\n",
      "[Epoch 67/1001] [Batch 369/372] [D loss: 0.6737960577011108] [G loss: 0.6874496936798096]\n",
      "[Epoch 67/1001] [Batch 370/372] [D loss: 0.6937761306762695] [G loss: 0.6855769157409668]\n",
      "[Epoch 67/1001] [Batch 371/372] [D loss: 0.6906273365020752] [G loss: 0.690601110458374]\n",
      "[Epoch 68/1001] [Batch 0/372] [D loss: 0.6869784593582153] [G loss: 0.6480857133865356]\n",
      "[Epoch 68/1001] [Batch 1/372] [D loss: 0.6882227659225464] [G loss: 0.7203435897827148]\n",
      "[Epoch 68/1001] [Batch 2/372] [D loss: 0.6835770606994629] [G loss: 0.6412680149078369]\n",
      "[Epoch 68/1001] [Batch 3/372] [D loss: 0.6922921538352966] [G loss: 0.7048168182373047]\n",
      "[Epoch 68/1001] [Batch 4/372] [D loss: 0.6822665333747864] [G loss: 0.6510159373283386]\n",
      "[Epoch 68/1001] [Batch 5/372] [D loss: 0.6846939325332642] [G loss: 0.7017322778701782]\n",
      "[Epoch 68/1001] [Batch 6/372] [D loss: 0.6834123134613037] [G loss: 0.6630219221115112]\n",
      "[Epoch 68/1001] [Batch 7/372] [D loss: 0.6822163462638855] [G loss: 0.6906334161758423]\n",
      "[Epoch 68/1001] [Batch 8/372] [D loss: 0.6801051497459412] [G loss: 0.6814226508140564]\n",
      "[Epoch 68/1001] [Batch 9/372] [D loss: 0.691839873790741] [G loss: 0.6380182504653931]\n",
      "[Epoch 68/1001] [Batch 10/372] [D loss: 0.6787334680557251] [G loss: 0.7608449459075928]\n",
      "[Epoch 68/1001] [Batch 11/372] [D loss: 0.6904284954071045] [G loss: 0.5786833763122559]\n",
      "[Epoch 68/1001] [Batch 12/372] [D loss: 0.6857642531394958] [G loss: 0.7944427132606506]\n",
      "[Epoch 68/1001] [Batch 13/372] [D loss: 0.6852623820304871] [G loss: 0.5407000780105591]\n",
      "[Epoch 68/1001] [Batch 14/372] [D loss: 0.6931110620498657] [G loss: 0.9350478649139404]\n",
      "[Epoch 68/1001] [Batch 15/372] [D loss: 0.715117871761322] [G loss: 0.4101208746433258]\n",
      "[Epoch 68/1001] [Batch 16/372] [D loss: 0.721055805683136] [G loss: 1.1355913877487183]\n",
      "[Epoch 68/1001] [Batch 17/372] [D loss: 0.7547802925109863] [G loss: 0.41574132442474365]\n",
      "[Epoch 68/1001] [Batch 18/372] [D loss: 0.732208788394928] [G loss: 0.8621305823326111]\n",
      "[Epoch 68/1001] [Batch 19/372] [D loss: 0.7054507732391357] [G loss: 0.6521238088607788]\n",
      "[Epoch 68/1001] [Batch 20/372] [D loss: 0.6905312538146973] [G loss: 0.6134082674980164]\n",
      "[Epoch 68/1001] [Batch 21/372] [D loss: 0.694170355796814] [G loss: 0.7476254105567932]\n",
      "[Epoch 68/1001] [Batch 22/372] [D loss: 0.6841220855712891] [G loss: 0.6479267477989197]\n",
      "[Epoch 68/1001] [Batch 23/372] [D loss: 0.6905964612960815] [G loss: 0.6710749864578247]\n",
      "[Epoch 68/1001] [Batch 24/372] [D loss: 0.6848282814025879] [G loss: 0.691556453704834]\n",
      "[Epoch 68/1001] [Batch 25/372] [D loss: 0.6857043504714966] [G loss: 0.6779064536094666]\n",
      "[Epoch 68/1001] [Batch 26/372] [D loss: 0.6871476173400879] [G loss: 0.6892379522323608]\n",
      "[Epoch 68/1001] [Batch 27/372] [D loss: 0.6897114515304565] [G loss: 0.6479636430740356]\n",
      "[Epoch 68/1001] [Batch 28/372] [D loss: 0.6856659054756165] [G loss: 0.7008028030395508]\n",
      "[Epoch 68/1001] [Batch 29/372] [D loss: 0.6806080937385559] [G loss: 0.6616065502166748]\n",
      "[Epoch 68/1001] [Batch 30/372] [D loss: 0.6926590204238892] [G loss: 0.7146173715591431]\n",
      "[Epoch 68/1001] [Batch 31/372] [D loss: 0.6852825880050659] [G loss: 0.6438573002815247]\n",
      "[Epoch 68/1001] [Batch 32/372] [D loss: 0.6789917945861816] [G loss: 0.7029119729995728]\n",
      "[Epoch 68/1001] [Batch 33/372] [D loss: 0.6871408224105835] [G loss: 0.6855598092079163]\n",
      "[Epoch 68/1001] [Batch 34/372] [D loss: 0.6854721307754517] [G loss: 0.6443004608154297]\n",
      "[Epoch 68/1001] [Batch 35/372] [D loss: 0.6842151880264282] [G loss: 0.710170567035675]\n",
      "[Epoch 68/1001] [Batch 36/372] [D loss: 0.6926454305648804] [G loss: 0.6678236722946167]\n",
      "[Epoch 68/1001] [Batch 37/372] [D loss: 0.6864036321640015] [G loss: 0.6906664371490479]\n",
      "[Epoch 68/1001] [Batch 38/372] [D loss: 0.68177729845047] [G loss: 0.6573786735534668]\n",
      "[Epoch 68/1001] [Batch 39/372] [D loss: 0.685636043548584] [G loss: 0.7015879154205322]\n",
      "[Epoch 68/1001] [Batch 40/372] [D loss: 0.694277286529541] [G loss: 0.6408442258834839]\n",
      "[Epoch 68/1001] [Batch 41/372] [D loss: 0.6857640743255615] [G loss: 0.7170045375823975]\n",
      "[Epoch 68/1001] [Batch 42/372] [D loss: 0.6871840953826904] [G loss: 0.6696072220802307]\n",
      "[Epoch 68/1001] [Batch 43/372] [D loss: 0.6865043044090271] [G loss: 0.6823225021362305]\n",
      "[Epoch 68/1001] [Batch 44/372] [D loss: 0.6828427314758301] [G loss: 0.6795252561569214]\n",
      "[Epoch 68/1001] [Batch 45/372] [D loss: 0.6877937316894531] [G loss: 0.6710813641548157]\n",
      "[Epoch 68/1001] [Batch 46/372] [D loss: 0.6848536729812622] [G loss: 0.6715853810310364]\n",
      "[Epoch 68/1001] [Batch 47/372] [D loss: 0.682492733001709] [G loss: 0.681967556476593]\n",
      "[Epoch 68/1001] [Batch 48/372] [D loss: 0.6822673082351685] [G loss: 0.6836636662483215]\n",
      "[Epoch 68/1001] [Batch 49/372] [D loss: 0.6748766899108887] [G loss: 0.6622269153594971]\n",
      "[Epoch 68/1001] [Batch 50/372] [D loss: 0.6852015256881714] [G loss: 0.7236082553863525]\n",
      "[Epoch 68/1001] [Batch 51/372] [D loss: 0.6864179372787476] [G loss: 0.627444863319397]\n",
      "[Epoch 68/1001] [Batch 52/372] [D loss: 0.6875678896903992] [G loss: 0.7339053153991699]\n",
      "[Epoch 68/1001] [Batch 53/372] [D loss: 0.6940476298332214] [G loss: 0.6109393835067749]\n",
      "[Epoch 68/1001] [Batch 54/372] [D loss: 0.6912548542022705] [G loss: 0.7431473135948181]\n",
      "[Epoch 68/1001] [Batch 55/372] [D loss: 0.6857136487960815] [G loss: 0.6270317435264587]\n",
      "[Epoch 68/1001] [Batch 56/372] [D loss: 0.6909608840942383] [G loss: 0.7069493532180786]\n",
      "[Epoch 68/1001] [Batch 57/372] [D loss: 0.6996769905090332] [G loss: 0.6439327001571655]\n",
      "[Epoch 68/1001] [Batch 58/372] [D loss: 0.6815255284309387] [G loss: 0.7305685877799988]\n",
      "[Epoch 68/1001] [Batch 59/372] [D loss: 0.6907716989517212] [G loss: 0.6329295635223389]\n",
      "[Epoch 68/1001] [Batch 60/372] [D loss: 0.6929311156272888] [G loss: 0.6841427087783813]\n",
      "[Epoch 68/1001] [Batch 61/372] [D loss: 0.6868305206298828] [G loss: 0.6695026159286499]\n",
      "[Epoch 68/1001] [Batch 62/372] [D loss: 0.6858657598495483] [G loss: 0.6808364391326904]\n",
      "[Epoch 68/1001] [Batch 63/372] [D loss: 0.6817057132720947] [G loss: 0.6508106589317322]\n",
      "[Epoch 68/1001] [Batch 64/372] [D loss: 0.6889722347259521] [G loss: 0.7338454723358154]\n",
      "[Epoch 68/1001] [Batch 65/372] [D loss: 0.6927076578140259] [G loss: 0.6105571985244751]\n",
      "[Epoch 68/1001] [Batch 66/372] [D loss: 0.693320095539093] [G loss: 0.7324396371841431]\n",
      "[Epoch 68/1001] [Batch 67/372] [D loss: 0.6913173198699951] [G loss: 0.63467937707901]\n",
      "[Epoch 68/1001] [Batch 68/372] [D loss: 0.683895468711853] [G loss: 0.7166221737861633]\n",
      "[Epoch 68/1001] [Batch 69/372] [D loss: 0.686617374420166] [G loss: 0.6257827281951904]\n",
      "[Epoch 68/1001] [Batch 70/372] [D loss: 0.693102240562439] [G loss: 0.738405168056488]\n",
      "[Epoch 68/1001] [Batch 71/372] [D loss: 0.6904894113540649] [G loss: 0.6435600519180298]\n",
      "[Epoch 68/1001] [Batch 72/372] [D loss: 0.6797334551811218] [G loss: 0.6906284093856812]\n",
      "[Epoch 68/1001] [Batch 73/372] [D loss: 0.6889724731445312] [G loss: 0.6778126358985901]\n",
      "[Epoch 68/1001] [Batch 74/372] [D loss: 0.6878350377082825] [G loss: 0.6682080626487732]\n",
      "[Epoch 68/1001] [Batch 75/372] [D loss: 0.6832165122032166] [G loss: 0.6789063215255737]\n",
      "[Epoch 68/1001] [Batch 76/372] [D loss: 0.6874421834945679] [G loss: 0.6840260624885559]\n",
      "[Epoch 68/1001] [Batch 77/372] [D loss: 0.6834242343902588] [G loss: 0.6737101674079895]\n",
      "[Epoch 68/1001] [Batch 78/372] [D loss: 0.6911369562149048] [G loss: 0.6870023012161255]\n",
      "[Epoch 68/1001] [Batch 79/372] [D loss: 0.6877210140228271] [G loss: 0.6552514433860779]\n",
      "[Epoch 68/1001] [Batch 80/372] [D loss: 0.6845245361328125] [G loss: 0.6876811385154724]\n",
      "[Epoch 68/1001] [Batch 81/372] [D loss: 0.6854026317596436] [G loss: 0.6836125254631042]\n",
      "[Epoch 68/1001] [Batch 82/372] [D loss: 0.6811741590499878] [G loss: 0.6720647215843201]\n",
      "[Epoch 68/1001] [Batch 83/372] [D loss: 0.6824198961257935] [G loss: 0.6888128519058228]\n",
      "[Epoch 68/1001] [Batch 84/372] [D loss: 0.6864181756973267] [G loss: 0.6512592434883118]\n",
      "[Epoch 68/1001] [Batch 85/372] [D loss: 0.6960017085075378] [G loss: 0.6731584668159485]\n",
      "[Epoch 68/1001] [Batch 86/372] [D loss: 0.6856377124786377] [G loss: 0.6907288432121277]\n",
      "[Epoch 68/1001] [Batch 87/372] [D loss: 0.6843029260635376] [G loss: 0.6839988827705383]\n",
      "[Epoch 68/1001] [Batch 88/372] [D loss: 0.6797306537628174] [G loss: 0.6549875736236572]\n",
      "[Epoch 68/1001] [Batch 89/372] [D loss: 0.6833943128585815] [G loss: 0.7035641074180603]\n",
      "[Epoch 68/1001] [Batch 90/372] [D loss: 0.6891198754310608] [G loss: 0.6347665786743164]\n",
      "[Epoch 68/1001] [Batch 91/372] [D loss: 0.6908971071243286] [G loss: 0.7264609932899475]\n",
      "[Epoch 68/1001] [Batch 92/372] [D loss: 0.6857647895812988] [G loss: 0.6182281374931335]\n",
      "[Epoch 68/1001] [Batch 93/372] [D loss: 0.6896936297416687] [G loss: 0.7710261940956116]\n",
      "[Epoch 68/1001] [Batch 94/372] [D loss: 0.6960674524307251] [G loss: 0.5790428519248962]\n",
      "[Epoch 68/1001] [Batch 95/372] [D loss: 0.6972635984420776] [G loss: 0.7952301502227783]\n",
      "[Epoch 68/1001] [Batch 96/372] [D loss: 0.6907986402511597] [G loss: 0.5632123351097107]\n",
      "[Epoch 68/1001] [Batch 97/372] [D loss: 0.6950885057449341] [G loss: 0.7994614243507385]\n",
      "[Epoch 68/1001] [Batch 98/372] [D loss: 0.6916763186454773] [G loss: 0.596716582775116]\n",
      "[Epoch 68/1001] [Batch 99/372] [D loss: 0.69045490026474] [G loss: 0.7480077147483826]\n",
      "[Epoch 68/1001] [Batch 100/372] [D loss: 0.6903959512710571] [G loss: 0.6254663467407227]\n",
      "[Epoch 68/1001] [Batch 101/372] [D loss: 0.6885370016098022] [G loss: 0.7187879681587219]\n",
      "[Epoch 68/1001] [Batch 102/372] [D loss: 0.6866044998168945] [G loss: 0.6259805560112]\n",
      "[Epoch 68/1001] [Batch 103/372] [D loss: 0.6857671141624451] [G loss: 0.7196574807167053]\n",
      "[Epoch 68/1001] [Batch 104/372] [D loss: 0.6882504820823669] [G loss: 0.6472305655479431]\n",
      "[Epoch 68/1001] [Batch 105/372] [D loss: 0.6845722794532776] [G loss: 0.707877516746521]\n",
      "[Epoch 68/1001] [Batch 106/372] [D loss: 0.6915887594223022] [G loss: 0.6621862053871155]\n",
      "[Epoch 68/1001] [Batch 107/372] [D loss: 0.6864288449287415] [G loss: 0.6756480932235718]\n",
      "[Epoch 68/1001] [Batch 108/372] [D loss: 0.6895306706428528] [G loss: 0.6677070260047913]\n",
      "[Epoch 68/1001] [Batch 109/372] [D loss: 0.6926282644271851] [G loss: 0.6793936491012573]\n",
      "[Epoch 68/1001] [Batch 110/372] [D loss: 0.6835940480232239] [G loss: 0.6644608378410339]\n",
      "[Epoch 68/1001] [Batch 111/372] [D loss: 0.6820720434188843] [G loss: 0.668749213218689]\n",
      "[Epoch 68/1001] [Batch 112/372] [D loss: 0.6823405027389526] [G loss: 0.6968540549278259]\n",
      "[Epoch 68/1001] [Batch 113/372] [D loss: 0.6855567693710327] [G loss: 0.6454405784606934]\n",
      "[Epoch 68/1001] [Batch 114/372] [D loss: 0.6815273761749268] [G loss: 0.7041978240013123]\n",
      "[Epoch 68/1001] [Batch 115/372] [D loss: 0.6835750937461853] [G loss: 0.6564877033233643]\n",
      "[Epoch 68/1001] [Batch 116/372] [D loss: 0.6870511770248413] [G loss: 0.6648573279380798]\n",
      "[Epoch 68/1001] [Batch 117/372] [D loss: 0.6872068643569946] [G loss: 0.7120123505592346]\n",
      "[Epoch 68/1001] [Batch 118/372] [D loss: 0.6929415464401245] [G loss: 0.5748080015182495]\n",
      "[Epoch 68/1001] [Batch 119/372] [D loss: 0.6922891139984131] [G loss: 0.8808259963989258]\n",
      "[Epoch 68/1001] [Batch 120/372] [D loss: 0.7030899524688721] [G loss: 0.4459490478038788]\n",
      "[Epoch 68/1001] [Batch 121/372] [D loss: 0.7237917184829712] [G loss: 0.9943525791168213]\n",
      "[Epoch 68/1001] [Batch 122/372] [D loss: 0.7303246855735779] [G loss: 0.45644664764404297]\n",
      "[Epoch 68/1001] [Batch 123/372] [D loss: 0.713572084903717] [G loss: 0.865804135799408]\n",
      "[Epoch 68/1001] [Batch 124/372] [D loss: 0.7036091685295105] [G loss: 0.6047427654266357]\n",
      "[Epoch 68/1001] [Batch 125/372] [D loss: 0.6884781718254089] [G loss: 0.6889475584030151]\n",
      "[Epoch 68/1001] [Batch 126/372] [D loss: 0.6811851859092712] [G loss: 0.6876330375671387]\n",
      "[Epoch 68/1001] [Batch 127/372] [D loss: 0.6907811164855957] [G loss: 0.633074939250946]\n",
      "[Epoch 68/1001] [Batch 128/372] [D loss: 0.687552809715271] [G loss: 0.7145949602127075]\n",
      "[Epoch 68/1001] [Batch 129/372] [D loss: 0.67708420753479] [G loss: 0.6575382351875305]\n",
      "[Epoch 68/1001] [Batch 130/372] [D loss: 0.6945844888687134] [G loss: 0.6804976463317871]\n",
      "[Epoch 68/1001] [Batch 131/372] [D loss: 0.6840230226516724] [G loss: 0.650128960609436]\n",
      "[Epoch 68/1001] [Batch 132/372] [D loss: 0.6855692863464355] [G loss: 0.7363696694374084]\n",
      "[Epoch 68/1001] [Batch 133/372] [D loss: 0.6968752145767212] [G loss: 0.5590456128120422]\n",
      "[Epoch 68/1001] [Batch 134/372] [D loss: 0.6960490942001343] [G loss: 0.820354700088501]\n",
      "[Epoch 68/1001] [Batch 135/372] [D loss: 0.6921910643577576] [G loss: 0.5311277508735657]\n",
      "[Epoch 68/1001] [Batch 136/372] [D loss: 0.6948798298835754] [G loss: 0.8335959315299988]\n",
      "[Epoch 68/1001] [Batch 137/372] [D loss: 0.6922365427017212] [G loss: 0.5725092887878418]\n",
      "[Epoch 68/1001] [Batch 138/372] [D loss: 0.6937408447265625] [G loss: 0.7495163679122925]\n",
      "[Epoch 68/1001] [Batch 139/372] [D loss: 0.6967180967330933] [G loss: 0.6308240294456482]\n",
      "[Epoch 68/1001] [Batch 140/372] [D loss: 0.6862917542457581] [G loss: 0.6858288645744324]\n",
      "[Epoch 68/1001] [Batch 141/372] [D loss: 0.6821331977844238] [G loss: 0.6892309188842773]\n",
      "[Epoch 68/1001] [Batch 142/372] [D loss: 0.6834207773208618] [G loss: 0.6471419930458069]\n",
      "[Epoch 68/1001] [Batch 143/372] [D loss: 0.6793307065963745] [G loss: 0.7065981030464172]\n",
      "[Epoch 68/1001] [Batch 144/372] [D loss: 0.6794953346252441] [G loss: 0.6724969148635864]\n",
      "[Epoch 68/1001] [Batch 145/372] [D loss: 0.6911013722419739] [G loss: 0.6721556186676025]\n",
      "[Epoch 68/1001] [Batch 146/372] [D loss: 0.6862808465957642] [G loss: 0.677335262298584]\n",
      "[Epoch 68/1001] [Batch 147/372] [D loss: 0.6916989088058472] [G loss: 0.6661787629127502]\n",
      "[Epoch 68/1001] [Batch 148/372] [D loss: 0.68538498878479] [G loss: 0.6654715538024902]\n",
      "[Epoch 68/1001] [Batch 149/372] [D loss: 0.6863875985145569] [G loss: 0.6928498148918152]\n",
      "[Epoch 68/1001] [Batch 150/372] [D loss: 0.6902931332588196] [G loss: 0.6729326248168945]\n",
      "[Epoch 68/1001] [Batch 151/372] [D loss: 0.6852315664291382] [G loss: 0.6735256314277649]\n",
      "[Epoch 68/1001] [Batch 152/372] [D loss: 0.6837608814239502] [G loss: 0.7030718326568604]\n",
      "[Epoch 68/1001] [Batch 153/372] [D loss: 0.685032844543457] [G loss: 0.6531492471694946]\n",
      "[Epoch 68/1001] [Batch 154/372] [D loss: 0.6935359835624695] [G loss: 0.68695068359375]\n",
      "[Epoch 68/1001] [Batch 155/372] [D loss: 0.6820470690727234] [G loss: 0.6575728058815002]\n",
      "[Epoch 68/1001] [Batch 156/372] [D loss: 0.6893194317817688] [G loss: 0.6979629397392273]\n",
      "[Epoch 68/1001] [Batch 157/372] [D loss: 0.6822508573532104] [G loss: 0.6693544387817383]\n",
      "[Epoch 68/1001] [Batch 158/372] [D loss: 0.690815806388855] [G loss: 0.678278923034668]\n",
      "[Epoch 68/1001] [Batch 159/372] [D loss: 0.6826801300048828] [G loss: 0.6755946278572083]\n",
      "[Epoch 68/1001] [Batch 160/372] [D loss: 0.6823489665985107] [G loss: 0.6502739191055298]\n",
      "[Epoch 68/1001] [Batch 161/372] [D loss: 0.6841741800308228] [G loss: 0.7366533279418945]\n",
      "[Epoch 68/1001] [Batch 162/372] [D loss: 0.6822075843811035] [G loss: 0.6136651039123535]\n",
      "[Epoch 68/1001] [Batch 163/372] [D loss: 0.690932035446167] [G loss: 0.7548216581344604]\n",
      "[Epoch 68/1001] [Batch 164/372] [D loss: 0.6915222406387329] [G loss: 0.5964769124984741]\n",
      "[Epoch 68/1001] [Batch 165/372] [D loss: 0.696749746799469] [G loss: 0.7670620679855347]\n",
      "[Epoch 68/1001] [Batch 166/372] [D loss: 0.690401017665863] [G loss: 0.5957669019699097]\n",
      "[Epoch 68/1001] [Batch 167/372] [D loss: 0.6907681226730347] [G loss: 0.7235267162322998]\n",
      "[Epoch 68/1001] [Batch 168/372] [D loss: 0.6919249296188354] [G loss: 0.663640558719635]\n",
      "[Epoch 68/1001] [Batch 169/372] [D loss: 0.6857259273529053] [G loss: 0.6685996651649475]\n",
      "[Epoch 68/1001] [Batch 170/372] [D loss: 0.6819044947624207] [G loss: 0.7056301832199097]\n",
      "[Epoch 68/1001] [Batch 171/372] [D loss: 0.6871499419212341] [G loss: 0.6283158659934998]\n",
      "[Epoch 68/1001] [Batch 172/372] [D loss: 0.6871131062507629] [G loss: 0.7255581617355347]\n",
      "[Epoch 68/1001] [Batch 173/372] [D loss: 0.6881506443023682] [G loss: 0.6223393082618713]\n",
      "[Epoch 68/1001] [Batch 174/372] [D loss: 0.6929981708526611] [G loss: 0.765826940536499]\n",
      "[Epoch 68/1001] [Batch 175/372] [D loss: 0.6985197067260742] [G loss: 0.5785343647003174]\n",
      "[Epoch 68/1001] [Batch 176/372] [D loss: 0.6951902508735657] [G loss: 0.7822971940040588]\n",
      "[Epoch 68/1001] [Batch 177/372] [D loss: 0.6898399591445923] [G loss: 0.6145272254943848]\n",
      "[Epoch 68/1001] [Batch 178/372] [D loss: 0.6925144195556641] [G loss: 0.7081990242004395]\n",
      "[Epoch 68/1001] [Batch 179/372] [D loss: 0.6904406547546387] [G loss: 0.6698669195175171]\n",
      "[Epoch 68/1001] [Batch 180/372] [D loss: 0.6866292953491211] [G loss: 0.6508690118789673]\n",
      "[Epoch 68/1001] [Batch 181/372] [D loss: 0.6878475546836853] [G loss: 0.7319179773330688]\n",
      "[Epoch 68/1001] [Batch 182/372] [D loss: 0.6907591819763184] [G loss: 0.620936930179596]\n",
      "[Epoch 68/1001] [Batch 183/372] [D loss: 0.6911871433258057] [G loss: 0.7180408239364624]\n",
      "[Epoch 68/1001] [Batch 184/372] [D loss: 0.6794909238815308] [G loss: 0.6390578746795654]\n",
      "[Epoch 68/1001] [Batch 185/372] [D loss: 0.6867822408676147] [G loss: 0.728102445602417]\n",
      "[Epoch 68/1001] [Batch 186/372] [D loss: 0.6883583068847656] [G loss: 0.6296952962875366]\n",
      "[Epoch 68/1001] [Batch 187/372] [D loss: 0.6866915225982666] [G loss: 0.7204174399375916]\n",
      "[Epoch 68/1001] [Batch 188/372] [D loss: 0.6899954080581665] [G loss: 0.628024697303772]\n",
      "[Epoch 68/1001] [Batch 189/372] [D loss: 0.6856950521469116] [G loss: 0.732065737247467]\n",
      "[Epoch 68/1001] [Batch 190/372] [D loss: 0.6898530721664429] [G loss: 0.6143510341644287]\n",
      "[Epoch 68/1001] [Batch 191/372] [D loss: 0.6822991371154785] [G loss: 0.7384624481201172]\n",
      "[Epoch 68/1001] [Batch 192/372] [D loss: 0.6934770345687866] [G loss: 0.6011273264884949]\n",
      "[Epoch 68/1001] [Batch 193/372] [D loss: 0.6935454607009888] [G loss: 0.8014609813690186]\n",
      "[Epoch 68/1001] [Batch 194/372] [D loss: 0.6982302665710449] [G loss: 0.5553188920021057]\n",
      "[Epoch 68/1001] [Batch 195/372] [D loss: 0.6936488151550293] [G loss: 0.7854439616203308]\n",
      "[Epoch 68/1001] [Batch 196/372] [D loss: 0.6899164915084839] [G loss: 0.5865920782089233]\n",
      "[Epoch 68/1001] [Batch 197/372] [D loss: 0.6897265911102295] [G loss: 0.7542505264282227]\n",
      "[Epoch 68/1001] [Batch 198/372] [D loss: 0.697460412979126] [G loss: 0.5969131588935852]\n",
      "[Epoch 68/1001] [Batch 199/372] [D loss: 0.6895183324813843] [G loss: 0.7959895133972168]\n",
      "[Epoch 68/1001] [Batch 200/372] [D loss: 0.6866922378540039] [G loss: 0.5398771166801453]\n",
      "[Epoch 68/1001] [Batch 201/372] [D loss: 0.6952974796295166] [G loss: 0.8689039349555969]\n",
      "[Epoch 68/1001] [Batch 202/372] [D loss: 0.7086848616600037] [G loss: 0.524946928024292]\n",
      "[Epoch 68/1001] [Batch 203/372] [D loss: 0.692920446395874] [G loss: 0.8311625719070435]\n",
      "[Epoch 68/1001] [Batch 204/372] [D loss: 0.6964436769485474] [G loss: 0.582382082939148]\n",
      "[Epoch 68/1001] [Batch 205/372] [D loss: 0.6983791589736938] [G loss: 0.748373806476593]\n",
      "[Epoch 68/1001] [Batch 206/372] [D loss: 0.688614010810852] [G loss: 0.6389704942703247]\n",
      "[Epoch 68/1001] [Batch 207/372] [D loss: 0.6877185702323914] [G loss: 0.6994175314903259]\n",
      "[Epoch 68/1001] [Batch 208/372] [D loss: 0.6890168786048889] [G loss: 0.668384313583374]\n",
      "[Epoch 68/1001] [Batch 209/372] [D loss: 0.683213472366333] [G loss: 0.6661067605018616]\n",
      "[Epoch 68/1001] [Batch 210/372] [D loss: 0.6855654716491699] [G loss: 0.6988922357559204]\n",
      "[Epoch 68/1001] [Batch 211/372] [D loss: 0.687629759311676] [G loss: 0.6731103658676147]\n",
      "[Epoch 68/1001] [Batch 212/372] [D loss: 0.695325493812561] [G loss: 0.6580861806869507]\n",
      "[Epoch 68/1001] [Batch 213/372] [D loss: 0.6856955289840698] [G loss: 0.6987929344177246]\n",
      "[Epoch 68/1001] [Batch 214/372] [D loss: 0.6777451634407043] [G loss: 0.6787692308425903]\n",
      "[Epoch 68/1001] [Batch 215/372] [D loss: 0.6801880598068237] [G loss: 0.6600828766822815]\n",
      "[Epoch 68/1001] [Batch 216/372] [D loss: 0.6952192783355713] [G loss: 0.6896654367446899]\n",
      "[Epoch 68/1001] [Batch 217/372] [D loss: 0.6887233257293701] [G loss: 0.6827265024185181]\n",
      "[Epoch 68/1001] [Batch 218/372] [D loss: 0.6877331733703613] [G loss: 0.6203994154930115]\n",
      "[Epoch 68/1001] [Batch 219/372] [D loss: 0.6898061633110046] [G loss: 0.7333354949951172]\n",
      "[Epoch 68/1001] [Batch 220/372] [D loss: 0.6846842169761658] [G loss: 0.6183803677558899]\n",
      "[Epoch 68/1001] [Batch 221/372] [D loss: 0.688478946685791] [G loss: 0.7335001230239868]\n",
      "[Epoch 68/1001] [Batch 222/372] [D loss: 0.6897279024124146] [G loss: 0.6191511750221252]\n",
      "[Epoch 68/1001] [Batch 223/372] [D loss: 0.6902556419372559] [G loss: 0.7622824907302856]\n",
      "[Epoch 68/1001] [Batch 224/372] [D loss: 0.6917031407356262] [G loss: 0.5952610969543457]\n",
      "[Epoch 68/1001] [Batch 225/372] [D loss: 0.6899039149284363] [G loss: 0.7177006006240845]\n",
      "[Epoch 68/1001] [Batch 226/372] [D loss: 0.6831598281860352] [G loss: 0.6630765199661255]\n",
      "[Epoch 68/1001] [Batch 227/372] [D loss: 0.6952548027038574] [G loss: 0.6754859089851379]\n",
      "[Epoch 68/1001] [Batch 228/372] [D loss: 0.6769503355026245] [G loss: 0.657482922077179]\n",
      "[Epoch 68/1001] [Batch 229/372] [D loss: 0.6843925714492798] [G loss: 0.6891211867332458]\n",
      "[Epoch 68/1001] [Batch 230/372] [D loss: 0.6814391613006592] [G loss: 0.6724437475204468]\n",
      "[Epoch 68/1001] [Batch 231/372] [D loss: 0.6807456016540527] [G loss: 0.6655521988868713]\n",
      "[Epoch 68/1001] [Batch 232/372] [D loss: 0.6892930269241333] [G loss: 0.6678885817527771]\n",
      "[Epoch 68/1001] [Batch 233/372] [D loss: 0.6801455020904541] [G loss: 0.671648383140564]\n",
      "[Epoch 68/1001] [Batch 234/372] [D loss: 0.6885011196136475] [G loss: 0.6856523156166077]\n",
      "[Epoch 68/1001] [Batch 235/372] [D loss: 0.6930920481681824] [G loss: 0.6306714415550232]\n",
      "[Epoch 68/1001] [Batch 236/372] [D loss: 0.683867871761322] [G loss: 0.7112149596214294]\n",
      "[Epoch 68/1001] [Batch 237/372] [D loss: 0.6905171871185303] [G loss: 0.6207987070083618]\n",
      "[Epoch 68/1001] [Batch 238/372] [D loss: 0.6901674270629883] [G loss: 0.7282228469848633]\n",
      "[Epoch 68/1001] [Batch 239/372] [D loss: 0.6846175193786621] [G loss: 0.6023510098457336]\n",
      "[Epoch 68/1001] [Batch 240/372] [D loss: 0.6848804950714111] [G loss: 0.7975075244903564]\n",
      "[Epoch 68/1001] [Batch 241/372] [D loss: 0.688103437423706] [G loss: 0.5382526516914368]\n",
      "[Epoch 68/1001] [Batch 242/372] [D loss: 0.6898689270019531] [G loss: 0.9389863610267639]\n",
      "[Epoch 68/1001] [Batch 243/372] [D loss: 0.7093907594680786] [G loss: 0.45344024896621704]\n",
      "[Epoch 68/1001] [Batch 244/372] [D loss: 0.7166613340377808] [G loss: 0.9604681134223938]\n",
      "[Epoch 68/1001] [Batch 245/372] [D loss: 0.7152647972106934] [G loss: 0.49195945262908936]\n",
      "[Epoch 68/1001] [Batch 246/372] [D loss: 0.7091672420501709] [G loss: 0.8177611827850342]\n",
      "[Epoch 68/1001] [Batch 247/372] [D loss: 0.6970781087875366] [G loss: 0.6120901107788086]\n",
      "[Epoch 68/1001] [Batch 248/372] [D loss: 0.6910350322723389] [G loss: 0.6718094944953918]\n",
      "[Epoch 68/1001] [Batch 249/372] [D loss: 0.6934352517127991] [G loss: 0.6770829558372498]\n",
      "[Epoch 68/1001] [Batch 250/372] [D loss: 0.6906651854515076] [G loss: 0.676034152507782]\n",
      "[Epoch 68/1001] [Batch 251/372] [D loss: 0.684596061706543] [G loss: 0.6669459939002991]\n",
      "[Epoch 68/1001] [Batch 252/372] [D loss: 0.6915405988693237] [G loss: 0.6638870239257812]\n",
      "[Epoch 68/1001] [Batch 253/372] [D loss: 0.6829032897949219] [G loss: 0.6769866943359375]\n",
      "[Epoch 68/1001] [Batch 254/372] [D loss: 0.6843959093093872] [G loss: 0.6834807991981506]\n",
      "[Epoch 68/1001] [Batch 255/372] [D loss: 0.685482919216156] [G loss: 0.6222224235534668]\n",
      "[Epoch 68/1001] [Batch 256/372] [D loss: 0.688334047794342] [G loss: 0.7422064542770386]\n",
      "[Epoch 68/1001] [Batch 257/372] [D loss: 0.6947076320648193] [G loss: 0.6013002991676331]\n",
      "[Epoch 68/1001] [Batch 258/372] [D loss: 0.6964156627655029] [G loss: 0.7384713292121887]\n",
      "[Epoch 68/1001] [Batch 259/372] [D loss: 0.6866607666015625] [G loss: 0.5859440565109253]\n",
      "[Epoch 68/1001] [Batch 260/372] [D loss: 0.6905249357223511] [G loss: 0.7786465287208557]\n",
      "[Epoch 68/1001] [Batch 261/372] [D loss: 0.6876298189163208] [G loss: 0.5928521156311035]\n",
      "[Epoch 68/1001] [Batch 262/372] [D loss: 0.6850229501724243] [G loss: 0.75803542137146]\n",
      "[Epoch 68/1001] [Batch 263/372] [D loss: 0.6875091791152954] [G loss: 0.581672728061676]\n",
      "[Epoch 68/1001] [Batch 264/372] [D loss: 0.6781589984893799] [G loss: 0.7894960641860962]\n",
      "[Epoch 68/1001] [Batch 265/372] [D loss: 0.694726824760437] [G loss: 0.5579333305358887]\n",
      "[Epoch 68/1001] [Batch 266/372] [D loss: 0.6926320791244507] [G loss: 0.8194888830184937]\n",
      "[Epoch 68/1001] [Batch 267/372] [D loss: 0.690543532371521] [G loss: 0.562906801700592]\n",
      "[Epoch 68/1001] [Batch 268/372] [D loss: 0.6970981955528259] [G loss: 0.7897921800613403]\n",
      "[Epoch 68/1001] [Batch 269/372] [D loss: 0.6895882487297058] [G loss: 0.5878069400787354]\n",
      "[Epoch 68/1001] [Batch 270/372] [D loss: 0.6927356719970703] [G loss: 0.7304292917251587]\n",
      "[Epoch 68/1001] [Batch 271/372] [D loss: 0.6890139579772949] [G loss: 0.6450884938240051]\n",
      "[Epoch 68/1001] [Batch 272/372] [D loss: 0.6909199953079224] [G loss: 0.6778042316436768]\n",
      "[Epoch 68/1001] [Batch 273/372] [D loss: 0.6928600072860718] [G loss: 0.6865635514259338]\n",
      "[Epoch 68/1001] [Batch 274/372] [D loss: 0.6908293962478638] [G loss: 0.6485283970832825]\n",
      "[Epoch 68/1001] [Batch 275/372] [D loss: 0.692042350769043] [G loss: 0.6988736391067505]\n",
      "[Epoch 68/1001] [Batch 276/372] [D loss: 0.6893861293792725] [G loss: 0.6486926078796387]\n",
      "[Epoch 68/1001] [Batch 277/372] [D loss: 0.6889711022377014] [G loss: 0.6988098621368408]\n",
      "[Epoch 68/1001] [Batch 278/372] [D loss: 0.6772875785827637] [G loss: 0.6419546008110046]\n",
      "[Epoch 68/1001] [Batch 279/372] [D loss: 0.6904280185699463] [G loss: 0.7204259037971497]\n",
      "[Epoch 68/1001] [Batch 280/372] [D loss: 0.6860281229019165] [G loss: 0.6502494812011719]\n",
      "[Epoch 68/1001] [Batch 281/372] [D loss: 0.6857895851135254] [G loss: 0.7029511332511902]\n",
      "[Epoch 68/1001] [Batch 282/372] [D loss: 0.6867198944091797] [G loss: 0.644961416721344]\n",
      "[Epoch 68/1001] [Batch 283/372] [D loss: 0.6766111850738525] [G loss: 0.7035290002822876]\n",
      "[Epoch 68/1001] [Batch 284/372] [D loss: 0.6843698024749756] [G loss: 0.6799138784408569]\n",
      "[Epoch 68/1001] [Batch 285/372] [D loss: 0.6831116676330566] [G loss: 0.6698516607284546]\n",
      "[Epoch 68/1001] [Batch 286/372] [D loss: 0.6938896179199219] [G loss: 0.6724938154220581]\n",
      "[Epoch 68/1001] [Batch 287/372] [D loss: 0.6770164370536804] [G loss: 0.66578608751297]\n",
      "[Epoch 68/1001] [Batch 288/372] [D loss: 0.6922863721847534] [G loss: 0.7051200270652771]\n",
      "[Epoch 68/1001] [Batch 289/372] [D loss: 0.6835969686508179] [G loss: 0.6276606321334839]\n",
      "[Epoch 68/1001] [Batch 290/372] [D loss: 0.6913639307022095] [G loss: 0.717985212802887]\n",
      "[Epoch 68/1001] [Batch 291/372] [D loss: 0.6827924251556396] [G loss: 0.6238771677017212]\n",
      "[Epoch 68/1001] [Batch 292/372] [D loss: 0.6898512840270996] [G loss: 0.733803927898407]\n",
      "[Epoch 68/1001] [Batch 293/372] [D loss: 0.6867423057556152] [G loss: 0.5943750739097595]\n",
      "[Epoch 68/1001] [Batch 294/372] [D loss: 0.6982152462005615] [G loss: 0.8087261915206909]\n",
      "[Epoch 68/1001] [Batch 295/372] [D loss: 0.6965935230255127] [G loss: 0.5408024787902832]\n",
      "[Epoch 68/1001] [Batch 296/372] [D loss: 0.7034645676612854] [G loss: 0.8338668346405029]\n",
      "[Epoch 68/1001] [Batch 297/372] [D loss: 0.6993736028671265] [G loss: 0.54539954662323]\n",
      "[Epoch 68/1001] [Batch 298/372] [D loss: 0.6990014910697937] [G loss: 0.8226389288902283]\n",
      "[Epoch 68/1001] [Batch 299/372] [D loss: 0.6959980130195618] [G loss: 0.5738288164138794]\n",
      "[Epoch 68/1001] [Batch 300/372] [D loss: 0.6960378885269165] [G loss: 0.7454781532287598]\n",
      "[Epoch 68/1001] [Batch 301/372] [D loss: 0.6940039396286011] [G loss: 0.628910481929779]\n",
      "[Epoch 68/1001] [Batch 302/372] [D loss: 0.6849370002746582] [G loss: 0.6802379488945007]\n",
      "[Epoch 68/1001] [Batch 303/372] [D loss: 0.6810619831085205] [G loss: 0.6911643147468567]\n",
      "[Epoch 68/1001] [Batch 304/372] [D loss: 0.6835917234420776] [G loss: 0.6608577370643616]\n",
      "[Epoch 68/1001] [Batch 305/372] [D loss: 0.6889623999595642] [G loss: 0.6843480467796326]\n",
      "[Epoch 68/1001] [Batch 306/372] [D loss: 0.6865130662918091] [G loss: 0.6606084704399109]\n",
      "[Epoch 68/1001] [Batch 307/372] [D loss: 0.6844122409820557] [G loss: 0.6779792308807373]\n",
      "[Epoch 68/1001] [Batch 308/372] [D loss: 0.6865890026092529] [G loss: 0.7050613164901733]\n",
      "[Epoch 68/1001] [Batch 309/372] [D loss: 0.6891428828239441] [G loss: 0.6238982081413269]\n",
      "[Epoch 68/1001] [Batch 310/372] [D loss: 0.6836429834365845] [G loss: 0.740861177444458]\n",
      "[Epoch 68/1001] [Batch 311/372] [D loss: 0.686496376991272] [G loss: 0.62203049659729]\n",
      "[Epoch 68/1001] [Batch 312/372] [D loss: 0.6879683136940002] [G loss: 0.7154189348220825]\n",
      "[Epoch 68/1001] [Batch 313/372] [D loss: 0.6825666427612305] [G loss: 0.6329654455184937]\n",
      "[Epoch 68/1001] [Batch 314/372] [D loss: 0.6901777982711792] [G loss: 0.745026707649231]\n",
      "[Epoch 68/1001] [Batch 315/372] [D loss: 0.6822792291641235] [G loss: 0.5940307974815369]\n",
      "[Epoch 68/1001] [Batch 316/372] [D loss: 0.6885390877723694] [G loss: 0.7675174474716187]\n",
      "[Epoch 68/1001] [Batch 317/372] [D loss: 0.6848976612091064] [G loss: 0.6039101481437683]\n",
      "[Epoch 68/1001] [Batch 318/372] [D loss: 0.6892348527908325] [G loss: 0.7411279082298279]\n",
      "[Epoch 68/1001] [Batch 319/372] [D loss: 0.6959561109542847] [G loss: 0.6105086207389832]\n",
      "[Epoch 68/1001] [Batch 320/372] [D loss: 0.6739120483398438] [G loss: 0.7445552945137024]\n",
      "[Epoch 68/1001] [Batch 321/372] [D loss: 0.6788182854652405] [G loss: 0.6309933662414551]\n",
      "[Epoch 68/1001] [Batch 322/372] [D loss: 0.6954551339149475] [G loss: 0.7094964981079102]\n",
      "[Epoch 68/1001] [Batch 323/372] [D loss: 0.6780703067779541] [G loss: 0.6225806474685669]\n",
      "[Epoch 68/1001] [Batch 324/372] [D loss: 0.678909420967102] [G loss: 0.7353028059005737]\n",
      "[Epoch 68/1001] [Batch 325/372] [D loss: 0.6899780035018921] [G loss: 0.5844621062278748]\n",
      "[Epoch 68/1001] [Batch 326/372] [D loss: 0.6871090531349182] [G loss: 0.8114714026451111]\n",
      "[Epoch 68/1001] [Batch 327/372] [D loss: 0.6984922289848328] [G loss: 0.5191165804862976]\n",
      "[Epoch 68/1001] [Batch 328/372] [D loss: 0.7144979238510132] [G loss: 0.8795085549354553]\n",
      "[Epoch 68/1001] [Batch 329/372] [D loss: 0.7006266117095947] [G loss: 0.5311459898948669]\n",
      "[Epoch 68/1001] [Batch 330/372] [D loss: 0.7020630836486816] [G loss: 0.7703386545181274]\n",
      "[Epoch 68/1001] [Batch 331/372] [D loss: 0.6881970167160034] [G loss: 0.6304312348365784]\n",
      "[Epoch 68/1001] [Batch 332/372] [D loss: 0.6831761598587036] [G loss: 0.6806555986404419]\n",
      "[Epoch 68/1001] [Batch 333/372] [D loss: 0.6811701059341431] [G loss: 0.7173312306404114]\n",
      "[Epoch 68/1001] [Batch 334/372] [D loss: 0.6932034492492676] [G loss: 0.6332416534423828]\n",
      "[Epoch 68/1001] [Batch 335/372] [D loss: 0.6898980140686035] [G loss: 0.6971629858016968]\n",
      "[Epoch 68/1001] [Batch 336/372] [D loss: 0.6754587292671204] [G loss: 0.66184002161026]\n",
      "[Epoch 68/1001] [Batch 337/372] [D loss: 0.6831854581832886] [G loss: 0.6963316202163696]\n",
      "[Epoch 68/1001] [Batch 338/372] [D loss: 0.6894427537918091] [G loss: 0.6539396643638611]\n",
      "[Epoch 68/1001] [Batch 339/372] [D loss: 0.6852833032608032] [G loss: 0.6868969798088074]\n",
      "[Epoch 68/1001] [Batch 340/372] [D loss: 0.6816589832305908] [G loss: 0.6809277534484863]\n",
      "[Epoch 68/1001] [Batch 341/372] [D loss: 0.683892011642456] [G loss: 0.6488969922065735]\n",
      "[Epoch 68/1001] [Batch 342/372] [D loss: 0.6913497447967529] [G loss: 0.6989789009094238]\n",
      "[Epoch 68/1001] [Batch 343/372] [D loss: 0.6916516423225403] [G loss: 0.6419182419776917]\n",
      "[Epoch 68/1001] [Batch 344/372] [D loss: 0.6894351243972778] [G loss: 0.6808075308799744]\n",
      "[Epoch 68/1001] [Batch 345/372] [D loss: 0.6819742918014526] [G loss: 0.6799735426902771]\n",
      "[Epoch 68/1001] [Batch 346/372] [D loss: 0.6866892576217651] [G loss: 0.6719918847084045]\n",
      "[Epoch 68/1001] [Batch 347/372] [D loss: 0.6886898279190063] [G loss: 0.6490647196769714]\n",
      "[Epoch 68/1001] [Batch 348/372] [D loss: 0.6862770318984985] [G loss: 0.6931589841842651]\n",
      "[Epoch 68/1001] [Batch 349/372] [D loss: 0.6854224801063538] [G loss: 0.620176374912262]\n",
      "[Epoch 68/1001] [Batch 350/372] [D loss: 0.6982415914535522] [G loss: 0.8225228786468506]\n",
      "[Epoch 68/1001] [Batch 351/372] [D loss: 0.7018814086914062] [G loss: 0.5251428484916687]\n",
      "[Epoch 68/1001] [Batch 352/372] [D loss: 0.6952242851257324] [G loss: 0.8348639607429504]\n",
      "[Epoch 68/1001] [Batch 353/372] [D loss: 0.69626784324646] [G loss: 0.5849090814590454]\n",
      "[Epoch 68/1001] [Batch 354/372] [D loss: 0.6926438808441162] [G loss: 0.717329204082489]\n",
      "[Epoch 68/1001] [Batch 355/372] [D loss: 0.6860719323158264] [G loss: 0.663770854473114]\n",
      "[Epoch 68/1001] [Batch 356/372] [D loss: 0.6885091066360474] [G loss: 0.6673154830932617]\n",
      "[Epoch 68/1001] [Batch 357/372] [D loss: 0.6940402984619141] [G loss: 0.704707145690918]\n",
      "[Epoch 68/1001] [Batch 358/372] [D loss: 0.6926004886627197] [G loss: 0.6229450702667236]\n",
      "[Epoch 68/1001] [Batch 359/372] [D loss: 0.6898898482322693] [G loss: 0.7085942029953003]\n",
      "[Epoch 68/1001] [Batch 360/372] [D loss: 0.6837114691734314] [G loss: 0.6653660535812378]\n",
      "[Epoch 68/1001] [Batch 361/372] [D loss: 0.686820387840271] [G loss: 0.6629565358161926]\n",
      "[Epoch 68/1001] [Batch 362/372] [D loss: 0.6815037727355957] [G loss: 0.6872512102127075]\n",
      "[Epoch 68/1001] [Batch 363/372] [D loss: 0.6884168386459351] [G loss: 0.6590434312820435]\n",
      "[Epoch 68/1001] [Batch 364/372] [D loss: 0.6894596219062805] [G loss: 0.7063202261924744]\n",
      "[Epoch 68/1001] [Batch 365/372] [D loss: 0.6858816742897034] [G loss: 0.6473128795623779]\n",
      "[Epoch 68/1001] [Batch 366/372] [D loss: 0.6834194660186768] [G loss: 0.672135591506958]\n",
      "[Epoch 68/1001] [Batch 367/372] [D loss: 0.6881101131439209] [G loss: 0.6864798665046692]\n",
      "[Epoch 68/1001] [Batch 368/372] [D loss: 0.6796189546585083] [G loss: 0.6443740129470825]\n",
      "[Epoch 68/1001] [Batch 369/372] [D loss: 0.6907484531402588] [G loss: 0.710625410079956]\n",
      "[Epoch 68/1001] [Batch 370/372] [D loss: 0.6876310110092163] [G loss: 0.6175237894058228]\n",
      "[Epoch 68/1001] [Batch 371/372] [D loss: 0.6994047164916992] [G loss: 0.7920204997062683]\n",
      "[Epoch 69/1001] [Batch 0/372] [D loss: 0.68187415599823] [G loss: 0.5511754751205444]\n",
      "[Epoch 69/1001] [Batch 1/372] [D loss: 0.6923799514770508] [G loss: 0.8507044911384583]\n",
      "[Epoch 69/1001] [Batch 2/372] [D loss: 0.6976965665817261] [G loss: 0.5351689457893372]\n",
      "[Epoch 69/1001] [Batch 3/372] [D loss: 0.69880211353302] [G loss: 0.856258749961853]\n",
      "[Epoch 69/1001] [Batch 4/372] [D loss: 0.6997279524803162] [G loss: 0.5384843945503235]\n",
      "[Epoch 69/1001] [Batch 5/372] [D loss: 0.7003405094146729] [G loss: 0.7906331419944763]\n",
      "[Epoch 69/1001] [Batch 6/372] [D loss: 0.690897524356842] [G loss: 0.6068475246429443]\n",
      "[Epoch 69/1001] [Batch 7/372] [D loss: 0.688704252243042] [G loss: 0.7103147506713867]\n",
      "[Epoch 69/1001] [Batch 8/372] [D loss: 0.687989354133606] [G loss: 0.6793937683105469]\n",
      "[Epoch 69/1001] [Batch 9/372] [D loss: 0.6906280517578125] [G loss: 0.6505208015441895]\n",
      "[Epoch 69/1001] [Batch 10/372] [D loss: 0.6904617547988892] [G loss: 0.7181833386421204]\n",
      "[Epoch 69/1001] [Batch 11/372] [D loss: 0.6843079924583435] [G loss: 0.638853907585144]\n",
      "[Epoch 69/1001] [Batch 12/372] [D loss: 0.6799724698066711] [G loss: 0.7237104177474976]\n",
      "[Epoch 69/1001] [Batch 13/372] [D loss: 0.6849650740623474] [G loss: 0.6762004494667053]\n",
      "[Epoch 69/1001] [Batch 14/372] [D loss: 0.6862083673477173] [G loss: 0.6407232284545898]\n",
      "[Epoch 69/1001] [Batch 15/372] [D loss: 0.6886960864067078] [G loss: 0.7446854114532471]\n",
      "[Epoch 69/1001] [Batch 16/372] [D loss: 0.6828088760375977] [G loss: 0.6050190329551697]\n",
      "[Epoch 69/1001] [Batch 17/372] [D loss: 0.6948327422142029] [G loss: 0.7645208239555359]\n",
      "[Epoch 69/1001] [Batch 18/372] [D loss: 0.6809758543968201] [G loss: 0.5919113159179688]\n",
      "[Epoch 69/1001] [Batch 19/372] [D loss: 0.6891365051269531] [G loss: 0.7586004734039307]\n",
      "[Epoch 69/1001] [Batch 20/372] [D loss: 0.6866658926010132] [G loss: 0.5957757234573364]\n",
      "[Epoch 69/1001] [Batch 21/372] [D loss: 0.6927462816238403] [G loss: 0.8192369937896729]\n",
      "[Epoch 69/1001] [Batch 22/372] [D loss: 0.6870839595794678] [G loss: 0.5362675189971924]\n",
      "[Epoch 69/1001] [Batch 23/372] [D loss: 0.6965490579605103] [G loss: 0.8551108837127686]\n",
      "[Epoch 69/1001] [Batch 24/372] [D loss: 0.7011232376098633] [G loss: 0.5378910303115845]\n",
      "[Epoch 69/1001] [Batch 25/372] [D loss: 0.6884433031082153] [G loss: 0.7784067392349243]\n",
      "[Epoch 69/1001] [Batch 26/372] [D loss: 0.6843115091323853] [G loss: 0.642477810382843]\n",
      "[Epoch 69/1001] [Batch 27/372] [D loss: 0.6836715936660767] [G loss: 0.6547612547874451]\n",
      "[Epoch 69/1001] [Batch 28/372] [D loss: 0.6809093952178955] [G loss: 0.7118991017341614]\n",
      "[Epoch 69/1001] [Batch 29/372] [D loss: 0.6780672073364258] [G loss: 0.644926905632019]\n",
      "[Epoch 69/1001] [Batch 30/372] [D loss: 0.6847279071807861] [G loss: 0.6831275820732117]\n",
      "[Epoch 69/1001] [Batch 31/372] [D loss: 0.6874387264251709] [G loss: 0.6808676719665527]\n",
      "[Epoch 69/1001] [Batch 32/372] [D loss: 0.6803184151649475] [G loss: 0.6609036922454834]\n",
      "[Epoch 69/1001] [Batch 33/372] [D loss: 0.6884069442749023] [G loss: 0.7051739692687988]\n",
      "[Epoch 69/1001] [Batch 34/372] [D loss: 0.6847250461578369] [G loss: 0.6845216155052185]\n",
      "[Epoch 69/1001] [Batch 35/372] [D loss: 0.694121241569519] [G loss: 0.629149317741394]\n",
      "[Epoch 69/1001] [Batch 36/372] [D loss: 0.6831222772598267] [G loss: 0.7378479838371277]\n",
      "[Epoch 69/1001] [Batch 37/372] [D loss: 0.6903113126754761] [G loss: 0.6199991106987]\n",
      "[Epoch 69/1001] [Batch 38/372] [D loss: 0.6915842294692993] [G loss: 0.7396427989006042]\n",
      "[Epoch 69/1001] [Batch 39/372] [D loss: 0.6858431100845337] [G loss: 0.6340147256851196]\n",
      "[Epoch 69/1001] [Batch 40/372] [D loss: 0.6814939379692078] [G loss: 0.7082109451293945]\n",
      "[Epoch 69/1001] [Batch 41/372] [D loss: 0.682101845741272] [G loss: 0.6586133241653442]\n",
      "[Epoch 69/1001] [Batch 42/372] [D loss: 0.6838975548744202] [G loss: 0.7089048624038696]\n",
      "[Epoch 69/1001] [Batch 43/372] [D loss: 0.6866905689239502] [G loss: 0.6294006109237671]\n",
      "[Epoch 69/1001] [Batch 44/372] [D loss: 0.6825231909751892] [G loss: 0.7546462416648865]\n",
      "[Epoch 69/1001] [Batch 45/372] [D loss: 0.6944557428359985] [G loss: 0.5832058787345886]\n",
      "[Epoch 69/1001] [Batch 46/372] [D loss: 0.6896072626113892] [G loss: 0.7933313846588135]\n",
      "[Epoch 69/1001] [Batch 47/372] [D loss: 0.6831954121589661] [G loss: 0.5805920958518982]\n",
      "[Epoch 69/1001] [Batch 48/372] [D loss: 0.6895889043807983] [G loss: 0.7477063536643982]\n",
      "[Epoch 69/1001] [Batch 49/372] [D loss: 0.6857051253318787] [G loss: 0.6366081833839417]\n",
      "[Epoch 69/1001] [Batch 50/372] [D loss: 0.6844041347503662] [G loss: 0.746242105960846]\n",
      "[Epoch 69/1001] [Batch 51/372] [D loss: 0.6829695701599121] [G loss: 0.608558714389801]\n",
      "[Epoch 69/1001] [Batch 52/372] [D loss: 0.692456841468811] [G loss: 0.7446157932281494]\n",
      "[Epoch 69/1001] [Batch 53/372] [D loss: 0.6796385049819946] [G loss: 0.6273993253707886]\n",
      "[Epoch 69/1001] [Batch 54/372] [D loss: 0.6871246099472046] [G loss: 0.7266456484794617]\n",
      "[Epoch 69/1001] [Batch 55/372] [D loss: 0.6758110523223877] [G loss: 0.61796635389328]\n",
      "[Epoch 69/1001] [Batch 56/372] [D loss: 0.6824626922607422] [G loss: 0.7933483123779297]\n",
      "[Epoch 69/1001] [Batch 57/372] [D loss: 0.6855990290641785] [G loss: 0.5394572615623474]\n",
      "[Epoch 69/1001] [Batch 58/372] [D loss: 0.6959307193756104] [G loss: 0.8715174198150635]\n",
      "[Epoch 69/1001] [Batch 59/372] [D loss: 0.7027583718299866] [G loss: 0.49151238799095154]\n",
      "[Epoch 69/1001] [Batch 60/372] [D loss: 0.7063111066818237] [G loss: 0.8926980495452881]\n",
      "[Epoch 69/1001] [Batch 61/372] [D loss: 0.7039580345153809] [G loss: 0.5422412753105164]\n",
      "[Epoch 69/1001] [Batch 62/372] [D loss: 0.6941518783569336] [G loss: 0.7791991233825684]\n",
      "[Epoch 69/1001] [Batch 63/372] [D loss: 0.6958373785018921] [G loss: 0.5950567722320557]\n",
      "[Epoch 69/1001] [Batch 64/372] [D loss: 0.6978186368942261] [G loss: 0.7188448905944824]\n",
      "[Epoch 69/1001] [Batch 65/372] [D loss: 0.6982874870300293] [G loss: 0.6381036043167114]\n",
      "[Epoch 69/1001] [Batch 66/372] [D loss: 0.6788057684898376] [G loss: 0.6905570030212402]\n",
      "[Epoch 69/1001] [Batch 67/372] [D loss: 0.6924586296081543] [G loss: 0.6793134212493896]\n",
      "[Epoch 69/1001] [Batch 68/372] [D loss: 0.6889423131942749] [G loss: 0.6573585271835327]\n",
      "[Epoch 69/1001] [Batch 69/372] [D loss: 0.682756781578064] [G loss: 0.6858773231506348]\n",
      "[Epoch 69/1001] [Batch 70/372] [D loss: 0.6864914894104004] [G loss: 0.6746965646743774]\n",
      "[Epoch 69/1001] [Batch 71/372] [D loss: 0.6820223331451416] [G loss: 0.6630773544311523]\n",
      "[Epoch 69/1001] [Batch 72/372] [D loss: 0.6812903881072998] [G loss: 0.6853352785110474]\n",
      "[Epoch 69/1001] [Batch 73/372] [D loss: 0.6826654076576233] [G loss: 0.6982167363166809]\n",
      "[Epoch 69/1001] [Batch 74/372] [D loss: 0.6815328001976013] [G loss: 0.6270805597305298]\n",
      "[Epoch 69/1001] [Batch 75/372] [D loss: 0.6906824707984924] [G loss: 0.7359873056411743]\n",
      "[Epoch 69/1001] [Batch 76/372] [D loss: 0.6850770711898804] [G loss: 0.6176372766494751]\n",
      "[Epoch 69/1001] [Batch 77/372] [D loss: 0.6886745691299438] [G loss: 0.7580658197402954]\n",
      "[Epoch 69/1001] [Batch 78/372] [D loss: 0.6901800632476807] [G loss: 0.5945807099342346]\n",
      "[Epoch 69/1001] [Batch 79/372] [D loss: 0.697404146194458] [G loss: 0.7275892496109009]\n",
      "[Epoch 69/1001] [Batch 80/372] [D loss: 0.6871779561042786] [G loss: 0.6212068796157837]\n",
      "[Epoch 69/1001] [Batch 81/372] [D loss: 0.6893723011016846] [G loss: 0.7331139445304871]\n",
      "[Epoch 69/1001] [Batch 82/372] [D loss: 0.6840941905975342] [G loss: 0.6223685145378113]\n",
      "[Epoch 69/1001] [Batch 83/372] [D loss: 0.6728620529174805] [G loss: 0.7003645896911621]\n",
      "[Epoch 69/1001] [Batch 84/372] [D loss: 0.6939501762390137] [G loss: 0.6697813868522644]\n",
      "[Epoch 69/1001] [Batch 85/372] [D loss: 0.6771643161773682] [G loss: 0.6755145192146301]\n",
      "[Epoch 69/1001] [Batch 86/372] [D loss: 0.6835221648216248] [G loss: 0.6850807070732117]\n",
      "[Epoch 69/1001] [Batch 87/372] [D loss: 0.6906877756118774] [G loss: 0.6560609936714172]\n",
      "[Epoch 69/1001] [Batch 88/372] [D loss: 0.6884545683860779] [G loss: 0.6695926189422607]\n",
      "[Epoch 69/1001] [Batch 89/372] [D loss: 0.6914775371551514] [G loss: 0.6897850036621094]\n",
      "[Epoch 69/1001] [Batch 90/372] [D loss: 0.678061842918396] [G loss: 0.6569721698760986]\n",
      "[Epoch 69/1001] [Batch 91/372] [D loss: 0.6866683959960938] [G loss: 0.7462621331214905]\n",
      "[Epoch 69/1001] [Batch 92/372] [D loss: 0.6878567934036255] [G loss: 0.5830221176147461]\n",
      "[Epoch 69/1001] [Batch 93/372] [D loss: 0.6848498582839966] [G loss: 0.8265217542648315]\n",
      "[Epoch 69/1001] [Batch 94/372] [D loss: 0.7044762372970581] [G loss: 0.5333711504936218]\n",
      "[Epoch 69/1001] [Batch 95/372] [D loss: 0.6914228200912476] [G loss: 0.8290976285934448]\n",
      "[Epoch 69/1001] [Batch 96/372] [D loss: 0.6945043802261353] [G loss: 0.5749723315238953]\n",
      "[Epoch 69/1001] [Batch 97/372] [D loss: 0.6944597959518433] [G loss: 0.7248023152351379]\n",
      "[Epoch 69/1001] [Batch 98/372] [D loss: 0.6891388893127441] [G loss: 0.649764358997345]\n",
      "[Epoch 69/1001] [Batch 99/372] [D loss: 0.6943174600601196] [G loss: 0.6494068503379822]\n",
      "[Epoch 69/1001] [Batch 100/372] [D loss: 0.6875104904174805] [G loss: 0.7301502823829651]\n",
      "[Epoch 69/1001] [Batch 101/372] [D loss: 0.680849015712738] [G loss: 0.6126213073730469]\n",
      "[Epoch 69/1001] [Batch 102/372] [D loss: 0.6886430382728577] [G loss: 0.7425665855407715]\n",
      "[Epoch 69/1001] [Batch 103/372] [D loss: 0.6920063495635986] [G loss: 0.6474495530128479]\n",
      "[Epoch 69/1001] [Batch 104/372] [D loss: 0.6908304691314697] [G loss: 0.6887356042861938]\n",
      "[Epoch 69/1001] [Batch 105/372] [D loss: 0.6906447410583496] [G loss: 0.6766835451126099]\n",
      "[Epoch 69/1001] [Batch 106/372] [D loss: 0.6869028806686401] [G loss: 0.6437011361122131]\n",
      "[Epoch 69/1001] [Batch 107/372] [D loss: 0.6984272003173828] [G loss: 0.7034736275672913]\n",
      "[Epoch 69/1001] [Batch 108/372] [D loss: 0.6843891143798828] [G loss: 0.6503063440322876]\n",
      "[Epoch 69/1001] [Batch 109/372] [D loss: 0.6856184005737305] [G loss: 0.6986790299415588]\n",
      "[Epoch 69/1001] [Batch 110/372] [D loss: 0.6841001510620117] [G loss: 0.6573337316513062]\n",
      "[Epoch 69/1001] [Batch 111/372] [D loss: 0.691907525062561] [G loss: 0.6660008430480957]\n",
      "[Epoch 69/1001] [Batch 112/372] [D loss: 0.6887450814247131] [G loss: 0.6857797503471375]\n",
      "[Epoch 69/1001] [Batch 113/372] [D loss: 0.6869632601737976] [G loss: 0.663516640663147]\n",
      "[Epoch 69/1001] [Batch 114/372] [D loss: 0.68382328748703] [G loss: 0.6789486408233643]\n",
      "[Epoch 69/1001] [Batch 115/372] [D loss: 0.6836891174316406] [G loss: 0.6776854991912842]\n",
      "[Epoch 69/1001] [Batch 116/372] [D loss: 0.6767100095748901] [G loss: 0.682912290096283]\n",
      "[Epoch 69/1001] [Batch 117/372] [D loss: 0.6848301887512207] [G loss: 0.6428104639053345]\n",
      "[Epoch 69/1001] [Batch 118/372] [D loss: 0.6879066228866577] [G loss: 0.7383058071136475]\n",
      "[Epoch 69/1001] [Batch 119/372] [D loss: 0.6874010562896729] [G loss: 0.5896926522254944]\n",
      "[Epoch 69/1001] [Batch 120/372] [D loss: 0.6844936013221741] [G loss: 0.7831711769104004]\n",
      "[Epoch 69/1001] [Batch 121/372] [D loss: 0.6866253614425659] [G loss: 0.5412024259567261]\n",
      "[Epoch 69/1001] [Batch 122/372] [D loss: 0.6990863084793091] [G loss: 0.9142432808876038]\n",
      "[Epoch 69/1001] [Batch 123/372] [D loss: 0.7047566771507263] [G loss: 0.47810888290405273]\n",
      "[Epoch 69/1001] [Batch 124/372] [D loss: 0.7059574723243713] [G loss: 0.897781491279602]\n",
      "[Epoch 69/1001] [Batch 125/372] [D loss: 0.7063372135162354] [G loss: 0.5682114958763123]\n",
      "[Epoch 69/1001] [Batch 126/372] [D loss: 0.6944297552108765] [G loss: 0.7322107553482056]\n",
      "[Epoch 69/1001] [Batch 127/372] [D loss: 0.6925537586212158] [G loss: 0.652961015701294]\n",
      "[Epoch 69/1001] [Batch 128/372] [D loss: 0.6950404644012451] [G loss: 0.6626660227775574]\n",
      "[Epoch 69/1001] [Batch 129/372] [D loss: 0.6774085164070129] [G loss: 0.6984969973564148]\n",
      "[Epoch 69/1001] [Batch 130/372] [D loss: 0.6899917125701904] [G loss: 0.6498897075653076]\n",
      "[Epoch 69/1001] [Batch 131/372] [D loss: 0.6810935735702515] [G loss: 0.7010394930839539]\n",
      "[Epoch 69/1001] [Batch 132/372] [D loss: 0.6867331266403198] [G loss: 0.6574968099594116]\n",
      "[Epoch 69/1001] [Batch 133/372] [D loss: 0.683510422706604] [G loss: 0.6809184551239014]\n",
      "[Epoch 69/1001] [Batch 134/372] [D loss: 0.6895413994789124] [G loss: 0.6699323058128357]\n",
      "[Epoch 69/1001] [Batch 135/372] [D loss: 0.694534182548523] [G loss: 0.6894588470458984]\n",
      "[Epoch 69/1001] [Batch 136/372] [D loss: 0.6887681484222412] [G loss: 0.648351788520813]\n",
      "[Epoch 69/1001] [Batch 137/372] [D loss: 0.6859989166259766] [G loss: 0.6888977289199829]\n",
      "[Epoch 69/1001] [Batch 138/372] [D loss: 0.6873697638511658] [G loss: 0.6569331884384155]\n",
      "[Epoch 69/1001] [Batch 139/372] [D loss: 0.6819037199020386] [G loss: 0.6878654956817627]\n",
      "[Epoch 69/1001] [Batch 140/372] [D loss: 0.6875791549682617] [G loss: 0.6612231135368347]\n",
      "[Epoch 69/1001] [Batch 141/372] [D loss: 0.6873608231544495] [G loss: 0.6786947846412659]\n",
      "[Epoch 69/1001] [Batch 142/372] [D loss: 0.6825186014175415] [G loss: 0.6591147184371948]\n",
      "[Epoch 69/1001] [Batch 143/372] [D loss: 0.6877236366271973] [G loss: 0.7156471014022827]\n",
      "[Epoch 69/1001] [Batch 144/372] [D loss: 0.6852651238441467] [G loss: 0.6255435347557068]\n",
      "[Epoch 69/1001] [Batch 145/372] [D loss: 0.6806595325469971] [G loss: 0.7237903475761414]\n",
      "[Epoch 69/1001] [Batch 146/372] [D loss: 0.6717299222946167] [G loss: 0.6488197445869446]\n",
      "[Epoch 69/1001] [Batch 147/372] [D loss: 0.6838208436965942] [G loss: 0.7002454996109009]\n",
      "[Epoch 69/1001] [Batch 148/372] [D loss: 0.6921149492263794] [G loss: 0.637862503528595]\n",
      "[Epoch 69/1001] [Batch 149/372] [D loss: 0.6815297603607178] [G loss: 0.711910605430603]\n",
      "[Epoch 69/1001] [Batch 150/372] [D loss: 0.6755421757698059] [G loss: 0.6438777446746826]\n",
      "[Epoch 69/1001] [Batch 151/372] [D loss: 0.6917604804039001] [G loss: 0.689704179763794]\n",
      "[Epoch 69/1001] [Batch 152/372] [D loss: 0.6929596662521362] [G loss: 0.6456330418586731]\n",
      "[Epoch 69/1001] [Batch 153/372] [D loss: 0.6851340532302856] [G loss: 0.7367962002754211]\n",
      "[Epoch 69/1001] [Batch 154/372] [D loss: 0.6935603022575378] [G loss: 0.5970024466514587]\n",
      "[Epoch 69/1001] [Batch 155/372] [D loss: 0.6917349100112915] [G loss: 0.7625271677970886]\n",
      "[Epoch 69/1001] [Batch 156/372] [D loss: 0.690512478351593] [G loss: 0.5708093047142029]\n",
      "[Epoch 69/1001] [Batch 157/372] [D loss: 0.6953823566436768] [G loss: 0.8185490965843201]\n",
      "[Epoch 69/1001] [Batch 158/372] [D loss: 0.6903396844863892] [G loss: 0.537457287311554]\n",
      "[Epoch 69/1001] [Batch 159/372] [D loss: 0.7045286893844604] [G loss: 0.8635063767433167]\n",
      "[Epoch 69/1001] [Batch 160/372] [D loss: 0.7051966190338135] [G loss: 0.5341380834579468]\n",
      "[Epoch 69/1001] [Batch 161/372] [D loss: 0.6863418221473694] [G loss: 0.8255221247673035]\n",
      "[Epoch 69/1001] [Batch 162/372] [D loss: 0.690639317035675] [G loss: 0.5921321511268616]\n",
      "[Epoch 69/1001] [Batch 163/372] [D loss: 0.6956201195716858] [G loss: 0.7307533621788025]\n",
      "[Epoch 69/1001] [Batch 164/372] [D loss: 0.6878279447555542] [G loss: 0.6208827495574951]\n",
      "[Epoch 69/1001] [Batch 165/372] [D loss: 0.6801239252090454] [G loss: 0.7475742697715759]\n",
      "[Epoch 69/1001] [Batch 166/372] [D loss: 0.6765464544296265] [G loss: 0.6263251304626465]\n",
      "[Epoch 69/1001] [Batch 167/372] [D loss: 0.6873018741607666] [G loss: 0.7249662280082703]\n",
      "[Epoch 69/1001] [Batch 168/372] [D loss: 0.6868307590484619] [G loss: 0.6420098543167114]\n",
      "[Epoch 69/1001] [Batch 169/372] [D loss: 0.682868480682373] [G loss: 0.706028938293457]\n",
      "[Epoch 69/1001] [Batch 170/372] [D loss: 0.6884771585464478] [G loss: 0.632469892501831]\n",
      "[Epoch 69/1001] [Batch 171/372] [D loss: 0.6891557574272156] [G loss: 0.7216187119483948]\n",
      "[Epoch 69/1001] [Batch 172/372] [D loss: 0.6830973029136658] [G loss: 0.6368655562400818]\n",
      "[Epoch 69/1001] [Batch 173/372] [D loss: 0.6818941831588745] [G loss: 0.7173126935958862]\n",
      "[Epoch 69/1001] [Batch 174/372] [D loss: 0.6909542679786682] [G loss: 0.64629065990448]\n",
      "[Epoch 69/1001] [Batch 175/372] [D loss: 0.6882879734039307] [G loss: 0.6862500905990601]\n",
      "[Epoch 69/1001] [Batch 176/372] [D loss: 0.6887812614440918] [G loss: 0.6511068344116211]\n",
      "[Epoch 69/1001] [Batch 177/372] [D loss: 0.6840282678604126] [G loss: 0.6787772178649902]\n",
      "[Epoch 69/1001] [Batch 178/372] [D loss: 0.6785812973976135] [G loss: 0.6773031949996948]\n",
      "[Epoch 69/1001] [Batch 179/372] [D loss: 0.6868167519569397] [G loss: 0.6336561441421509]\n",
      "[Epoch 69/1001] [Batch 180/372] [D loss: 0.6849544048309326] [G loss: 0.7726216912269592]\n",
      "[Epoch 69/1001] [Batch 181/372] [D loss: 0.684914231300354] [G loss: 0.5136151909828186]\n",
      "[Epoch 69/1001] [Batch 182/372] [D loss: 0.7108949422836304] [G loss: 1.031908631324768]\n",
      "[Epoch 69/1001] [Batch 183/372] [D loss: 0.734410285949707] [G loss: 0.41141337156295776]\n",
      "[Epoch 69/1001] [Batch 184/372] [D loss: 0.7459385395050049] [G loss: 0.935914158821106]\n",
      "[Epoch 69/1001] [Batch 185/372] [D loss: 0.7183117270469666] [G loss: 0.5743649005889893]\n",
      "[Epoch 69/1001] [Batch 186/372] [D loss: 0.7044403553009033] [G loss: 0.6841458082199097]\n",
      "[Epoch 69/1001] [Batch 187/372] [D loss: 0.6946442127227783] [G loss: 0.6974008679389954]\n",
      "[Epoch 69/1001] [Batch 188/372] [D loss: 0.6867539286613464] [G loss: 0.6197528839111328]\n",
      "[Epoch 69/1001] [Batch 189/372] [D loss: 0.692720890045166] [G loss: 0.7294958829879761]\n",
      "[Epoch 69/1001] [Batch 190/372] [D loss: 0.6899657249450684] [G loss: 0.6334148645401001]\n",
      "[Epoch 69/1001] [Batch 191/372] [D loss: 0.6869494915008545] [G loss: 0.6894295811653137]\n",
      "[Epoch 69/1001] [Batch 192/372] [D loss: 0.6837087869644165] [G loss: 0.6644144654273987]\n",
      "[Epoch 69/1001] [Batch 193/372] [D loss: 0.6771527528762817] [G loss: 0.697519063949585]\n",
      "[Epoch 69/1001] [Batch 194/372] [D loss: 0.6845285296440125] [G loss: 0.6613597869873047]\n",
      "[Epoch 69/1001] [Batch 195/372] [D loss: 0.6881653666496277] [G loss: 0.6704781651496887]\n",
      "[Epoch 69/1001] [Batch 196/372] [D loss: 0.6793915033340454] [G loss: 0.6963337063789368]\n",
      "[Epoch 69/1001] [Batch 197/372] [D loss: 0.6850438117980957] [G loss: 0.6578409075737]\n",
      "[Epoch 69/1001] [Batch 198/372] [D loss: 0.6877644658088684] [G loss: 0.6754860281944275]\n",
      "[Epoch 69/1001] [Batch 199/372] [D loss: 0.6855269074440002] [G loss: 0.6663729548454285]\n",
      "[Epoch 69/1001] [Batch 200/372] [D loss: 0.6820621490478516] [G loss: 0.6848160028457642]\n",
      "[Epoch 69/1001] [Batch 201/372] [D loss: 0.683883786201477] [G loss: 0.674825131893158]\n",
      "[Epoch 69/1001] [Batch 202/372] [D loss: 0.6905426979064941] [G loss: 0.6609565019607544]\n",
      "[Epoch 69/1001] [Batch 203/372] [D loss: 0.6904348134994507] [G loss: 0.6895377039909363]\n",
      "[Epoch 69/1001] [Batch 204/372] [D loss: 0.6876440644264221] [G loss: 0.6680883765220642]\n",
      "[Epoch 69/1001] [Batch 205/372] [D loss: 0.6791695356369019] [G loss: 0.6862439513206482]\n",
      "[Epoch 69/1001] [Batch 206/372] [D loss: 0.6842082142829895] [G loss: 0.6448432207107544]\n",
      "[Epoch 69/1001] [Batch 207/372] [D loss: 0.684644877910614] [G loss: 0.7221276760101318]\n",
      "[Epoch 69/1001] [Batch 208/372] [D loss: 0.6831275820732117] [G loss: 0.6274093389511108]\n",
      "[Epoch 69/1001] [Batch 209/372] [D loss: 0.6961873173713684] [G loss: 0.763630211353302]\n",
      "[Epoch 69/1001] [Batch 210/372] [D loss: 0.6788032054901123] [G loss: 0.5956636071205139]\n",
      "[Epoch 69/1001] [Batch 211/372] [D loss: 0.6910969018936157] [G loss: 0.7765036821365356]\n",
      "[Epoch 69/1001] [Batch 212/372] [D loss: 0.6874756217002869] [G loss: 0.580089271068573]\n",
      "[Epoch 69/1001] [Batch 213/372] [D loss: 0.6906204223632812] [G loss: 0.7561978101730347]\n",
      "[Epoch 69/1001] [Batch 214/372] [D loss: 0.6835500001907349] [G loss: 0.6128591299057007]\n",
      "[Epoch 69/1001] [Batch 215/372] [D loss: 0.6869168877601624] [G loss: 0.7518726587295532]\n",
      "[Epoch 69/1001] [Batch 216/372] [D loss: 0.6842109560966492] [G loss: 0.6119840145111084]\n",
      "[Epoch 69/1001] [Batch 217/372] [D loss: 0.679720401763916] [G loss: 0.7592727541923523]\n",
      "[Epoch 69/1001] [Batch 218/372] [D loss: 0.6903148293495178] [G loss: 0.5880261063575745]\n",
      "[Epoch 69/1001] [Batch 219/372] [D loss: 0.6897321343421936] [G loss: 0.7968912124633789]\n",
      "[Epoch 69/1001] [Batch 220/372] [D loss: 0.6939623355865479] [G loss: 0.569304883480072]\n",
      "[Epoch 69/1001] [Batch 221/372] [D loss: 0.6917484998703003] [G loss: 0.7717574834823608]\n",
      "[Epoch 69/1001] [Batch 222/372] [D loss: 0.6903402805328369] [G loss: 0.6213691234588623]\n",
      "[Epoch 69/1001] [Batch 223/372] [D loss: 0.6847207546234131] [G loss: 0.6850010752677917]\n",
      "[Epoch 69/1001] [Batch 224/372] [D loss: 0.6890192627906799] [G loss: 0.6918357610702515]\n",
      "[Epoch 69/1001] [Batch 225/372] [D loss: 0.6831768751144409] [G loss: 0.6269386410713196]\n",
      "[Epoch 69/1001] [Batch 226/372] [D loss: 0.6827635169029236] [G loss: 0.734035074710846]\n",
      "[Epoch 69/1001] [Batch 227/372] [D loss: 0.6842881441116333] [G loss: 0.6060800552368164]\n",
      "[Epoch 69/1001] [Batch 228/372] [D loss: 0.6887765526771545] [G loss: 0.7558809518814087]\n",
      "[Epoch 69/1001] [Batch 229/372] [D loss: 0.6858406066894531] [G loss: 0.6018227338790894]\n",
      "[Epoch 69/1001] [Batch 230/372] [D loss: 0.6903691291809082] [G loss: 0.7283068299293518]\n",
      "[Epoch 69/1001] [Batch 231/372] [D loss: 0.689602255821228] [G loss: 0.6296740174293518]\n",
      "[Epoch 69/1001] [Batch 232/372] [D loss: 0.6871579885482788] [G loss: 0.6897790431976318]\n",
      "[Epoch 69/1001] [Batch 233/372] [D loss: 0.6892373561859131] [G loss: 0.6889253854751587]\n",
      "[Epoch 69/1001] [Batch 234/372] [D loss: 0.6839920282363892] [G loss: 0.6264402866363525]\n",
      "[Epoch 69/1001] [Batch 235/372] [D loss: 0.6829014420509338] [G loss: 0.725019097328186]\n",
      "[Epoch 69/1001] [Batch 236/372] [D loss: 0.6795806288719177] [G loss: 0.6379317045211792]\n",
      "[Epoch 69/1001] [Batch 237/372] [D loss: 0.6849686503410339] [G loss: 0.7393770813941956]\n",
      "[Epoch 69/1001] [Batch 238/372] [D loss: 0.6822282671928406] [G loss: 0.6008802652359009]\n",
      "[Epoch 69/1001] [Batch 239/372] [D loss: 0.6911522150039673] [G loss: 0.8034716248512268]\n",
      "[Epoch 69/1001] [Batch 240/372] [D loss: 0.6865164041519165] [G loss: 0.5371350049972534]\n",
      "[Epoch 69/1001] [Batch 241/372] [D loss: 0.6933636665344238] [G loss: 0.8423088788986206]\n",
      "[Epoch 69/1001] [Batch 242/372] [D loss: 0.7001098394393921] [G loss: 0.5347486734390259]\n",
      "[Epoch 69/1001] [Batch 243/372] [D loss: 0.7029529809951782] [G loss: 0.8771401643753052]\n",
      "[Epoch 69/1001] [Batch 244/372] [D loss: 0.6999802589416504] [G loss: 0.5267561078071594]\n",
      "[Epoch 69/1001] [Batch 245/372] [D loss: 0.7006901502609253] [G loss: 0.7845087647438049]\n",
      "[Epoch 69/1001] [Batch 246/372] [D loss: 0.6898378133773804] [G loss: 0.6371810436248779]\n",
      "[Epoch 69/1001] [Batch 247/372] [D loss: 0.6857324838638306] [G loss: 0.6726178526878357]\n",
      "[Epoch 69/1001] [Batch 248/372] [D loss: 0.6896399259567261] [G loss: 0.7097461223602295]\n",
      "[Epoch 69/1001] [Batch 249/372] [D loss: 0.6973028779029846] [G loss: 0.6175130605697632]\n",
      "[Epoch 69/1001] [Batch 250/372] [D loss: 0.6883804202079773] [G loss: 0.7220971584320068]\n",
      "[Epoch 69/1001] [Batch 251/372] [D loss: 0.6835368871688843] [G loss: 0.667247474193573]\n",
      "[Epoch 69/1001] [Batch 252/372] [D loss: 0.6837743520736694] [G loss: 0.6797086596488953]\n",
      "[Epoch 69/1001] [Batch 253/372] [D loss: 0.6896933317184448] [G loss: 0.6589623689651489]\n",
      "[Epoch 69/1001] [Batch 254/372] [D loss: 0.6866750717163086] [G loss: 0.6838626265525818]\n",
      "[Epoch 69/1001] [Batch 255/372] [D loss: 0.6881369352340698] [G loss: 0.6783932447433472]\n",
      "[Epoch 69/1001] [Batch 256/372] [D loss: 0.6894404292106628] [G loss: 0.6591858267784119]\n",
      "[Epoch 69/1001] [Batch 257/372] [D loss: 0.6851537227630615] [G loss: 0.6807858943939209]\n",
      "[Epoch 69/1001] [Batch 258/372] [D loss: 0.6800107955932617] [G loss: 0.6861659288406372]\n",
      "[Epoch 69/1001] [Batch 259/372] [D loss: 0.6892614960670471] [G loss: 0.6535038948059082]\n",
      "[Epoch 69/1001] [Batch 260/372] [D loss: 0.684005856513977] [G loss: 0.704510509967804]\n",
      "[Epoch 69/1001] [Batch 261/372] [D loss: 0.6853041648864746] [G loss: 0.6606825590133667]\n",
      "[Epoch 69/1001] [Batch 262/372] [D loss: 0.686937689781189] [G loss: 0.6327100992202759]\n",
      "[Epoch 69/1001] [Batch 263/372] [D loss: 0.6956851482391357] [G loss: 0.7419700026512146]\n",
      "[Epoch 69/1001] [Batch 264/372] [D loss: 0.68375563621521] [G loss: 0.6168345212936401]\n",
      "[Epoch 69/1001] [Batch 265/372] [D loss: 0.6889275312423706] [G loss: 0.7266685962677002]\n",
      "[Epoch 69/1001] [Batch 266/372] [D loss: 0.6843371391296387] [G loss: 0.6438431739807129]\n",
      "[Epoch 69/1001] [Batch 267/372] [D loss: 0.6883983612060547] [G loss: 0.6815862059593201]\n",
      "[Epoch 69/1001] [Batch 268/372] [D loss: 0.6856071949005127] [G loss: 0.6971694827079773]\n",
      "[Epoch 69/1001] [Batch 269/372] [D loss: 0.678170919418335] [G loss: 0.6216012239456177]\n",
      "[Epoch 69/1001] [Batch 270/372] [D loss: 0.6887407898902893] [G loss: 0.7392752170562744]\n",
      "[Epoch 69/1001] [Batch 271/372] [D loss: 0.6854923963546753] [G loss: 0.6099313497543335]\n",
      "[Epoch 69/1001] [Batch 272/372] [D loss: 0.6842817068099976] [G loss: 0.7636407017707825]\n",
      "[Epoch 69/1001] [Batch 273/372] [D loss: 0.6892461776733398] [G loss: 0.587680459022522]\n",
      "[Epoch 69/1001] [Batch 274/372] [D loss: 0.6902003288269043] [G loss: 0.797942042350769]\n",
      "[Epoch 69/1001] [Batch 275/372] [D loss: 0.6928689479827881] [G loss: 0.5628824234008789]\n",
      "[Epoch 69/1001] [Batch 276/372] [D loss: 0.6983386278152466] [G loss: 0.7828705310821533]\n",
      "[Epoch 69/1001] [Batch 277/372] [D loss: 0.6910122036933899] [G loss: 0.604555606842041]\n",
      "[Epoch 69/1001] [Batch 278/372] [D loss: 0.6882543563842773] [G loss: 0.7473044395446777]\n",
      "[Epoch 69/1001] [Batch 279/372] [D loss: 0.6929725408554077] [G loss: 0.6125478148460388]\n",
      "[Epoch 69/1001] [Batch 280/372] [D loss: 0.6898878812789917] [G loss: 0.7044143676757812]\n",
      "[Epoch 69/1001] [Batch 281/372] [D loss: 0.6839406490325928] [G loss: 0.6666886806488037]\n",
      "[Epoch 69/1001] [Batch 282/372] [D loss: 0.681036651134491] [G loss: 0.6615533828735352]\n",
      "[Epoch 69/1001] [Batch 283/372] [D loss: 0.6899726390838623] [G loss: 0.718713641166687]\n",
      "[Epoch 69/1001] [Batch 284/372] [D loss: 0.6883633136749268] [G loss: 0.6151997447013855]\n",
      "[Epoch 69/1001] [Batch 285/372] [D loss: 0.6885497570037842] [G loss: 0.7662058472633362]\n",
      "[Epoch 69/1001] [Batch 286/372] [D loss: 0.7001802325248718] [G loss: 0.5818167924880981]\n",
      "[Epoch 69/1001] [Batch 287/372] [D loss: 0.6956309080123901] [G loss: 0.7632070183753967]\n",
      "[Epoch 69/1001] [Batch 288/372] [D loss: 0.693958044052124] [G loss: 0.5941213965415955]\n",
      "[Epoch 69/1001] [Batch 289/372] [D loss: 0.6921905875205994] [G loss: 0.7429935932159424]\n",
      "[Epoch 69/1001] [Batch 290/372] [D loss: 0.6891078352928162] [G loss: 0.6318564414978027]\n",
      "[Epoch 69/1001] [Batch 291/372] [D loss: 0.6873078346252441] [G loss: 0.7163420915603638]\n",
      "[Epoch 69/1001] [Batch 292/372] [D loss: 0.691604495048523] [G loss: 0.6434261798858643]\n",
      "[Epoch 69/1001] [Batch 293/372] [D loss: 0.6830747127532959] [G loss: 0.6961612701416016]\n",
      "[Epoch 69/1001] [Batch 294/372] [D loss: 0.6896325349807739] [G loss: 0.6517354846000671]\n",
      "[Epoch 69/1001] [Batch 295/372] [D loss: 0.6807730197906494] [G loss: 0.6912469267845154]\n",
      "[Epoch 69/1001] [Batch 296/372] [D loss: 0.6817561388015747] [G loss: 0.6647016406059265]\n",
      "[Epoch 69/1001] [Batch 297/372] [D loss: 0.6751950979232788] [G loss: 0.6930412650108337]\n",
      "[Epoch 69/1001] [Batch 298/372] [D loss: 0.6934845447540283] [G loss: 0.6824025511741638]\n",
      "[Epoch 69/1001] [Batch 299/372] [D loss: 0.6863669753074646] [G loss: 0.6414077877998352]\n",
      "[Epoch 69/1001] [Batch 300/372] [D loss: 0.6889587640762329] [G loss: 0.7116987109184265]\n",
      "[Epoch 69/1001] [Batch 301/372] [D loss: 0.6890021562576294] [G loss: 0.6393065452575684]\n",
      "[Epoch 69/1001] [Batch 302/372] [D loss: 0.6863950490951538] [G loss: 0.6983124017715454]\n",
      "[Epoch 69/1001] [Batch 303/372] [D loss: 0.6918026208877563] [G loss: 0.6651642322540283]\n",
      "[Epoch 69/1001] [Batch 304/372] [D loss: 0.6872968673706055] [G loss: 0.6639531254768372]\n",
      "[Epoch 69/1001] [Batch 305/372] [D loss: 0.6775516271591187] [G loss: 0.6885780096054077]\n",
      "[Epoch 69/1001] [Batch 306/372] [D loss: 0.6822603940963745] [G loss: 0.6720107197761536]\n",
      "[Epoch 69/1001] [Batch 307/372] [D loss: 0.68352872133255] [G loss: 0.6796779632568359]\n",
      "[Epoch 69/1001] [Batch 308/372] [D loss: 0.694421648979187] [G loss: 0.6579263806343079]\n",
      "[Epoch 69/1001] [Batch 309/372] [D loss: 0.6854732036590576] [G loss: 0.6820103526115417]\n",
      "[Epoch 69/1001] [Batch 310/372] [D loss: 0.6967380046844482] [G loss: 0.6590572595596313]\n",
      "[Epoch 69/1001] [Batch 311/372] [D loss: 0.6953884363174438] [G loss: 0.6925535202026367]\n",
      "[Epoch 69/1001] [Batch 312/372] [D loss: 0.6907880306243896] [G loss: 0.6340540051460266]\n",
      "[Epoch 69/1001] [Batch 313/372] [D loss: 0.6843971610069275] [G loss: 0.7039526104927063]\n",
      "[Epoch 69/1001] [Batch 314/372] [D loss: 0.6838987469673157] [G loss: 0.671134889125824]\n",
      "[Epoch 69/1001] [Batch 315/372] [D loss: 0.6903636455535889] [G loss: 0.6554867029190063]\n",
      "[Epoch 69/1001] [Batch 316/372] [D loss: 0.6789839863777161] [G loss: 0.701924741268158]\n",
      "[Epoch 69/1001] [Batch 317/372] [D loss: 0.6886810064315796] [G loss: 0.6326457262039185]\n",
      "[Epoch 69/1001] [Batch 318/372] [D loss: 0.691325306892395] [G loss: 0.7336477637290955]\n",
      "[Epoch 69/1001] [Batch 319/372] [D loss: 0.6934636831283569] [G loss: 0.6267220377922058]\n",
      "[Epoch 69/1001] [Batch 320/372] [D loss: 0.6899846792221069] [G loss: 0.7110782265663147]\n",
      "[Epoch 69/1001] [Batch 321/372] [D loss: 0.6897099614143372] [G loss: 0.6185061931610107]\n",
      "[Epoch 69/1001] [Batch 322/372] [D loss: 0.6921423673629761] [G loss: 0.7403385639190674]\n",
      "[Epoch 69/1001] [Batch 323/372] [D loss: 0.6907215118408203] [G loss: 0.6159863471984863]\n",
      "[Epoch 69/1001] [Batch 324/372] [D loss: 0.6912596225738525] [G loss: 0.763251543045044]\n",
      "[Epoch 69/1001] [Batch 325/372] [D loss: 0.6951348781585693] [G loss: 0.5628045797348022]\n",
      "[Epoch 69/1001] [Batch 326/372] [D loss: 0.6966919898986816] [G loss: 0.8393665552139282]\n",
      "[Epoch 69/1001] [Batch 327/372] [D loss: 0.6987557411193848] [G loss: 0.5363973379135132]\n",
      "[Epoch 69/1001] [Batch 328/372] [D loss: 0.69260573387146] [G loss: 0.8461630344390869]\n",
      "[Epoch 69/1001] [Batch 329/372] [D loss: 0.6963486075401306] [G loss: 0.5366335511207581]\n",
      "[Epoch 69/1001] [Batch 330/372] [D loss: 0.6997543573379517] [G loss: 0.8301874995231628]\n",
      "[Epoch 69/1001] [Batch 331/372] [D loss: 0.694025993347168] [G loss: 0.5800437927246094]\n",
      "[Epoch 69/1001] [Batch 332/372] [D loss: 0.693388819694519] [G loss: 0.7341816425323486]\n",
      "[Epoch 69/1001] [Batch 333/372] [D loss: 0.6911800503730774] [G loss: 0.6546130180358887]\n",
      "[Epoch 69/1001] [Batch 334/372] [D loss: 0.6866951584815979] [G loss: 0.6585682034492493]\n",
      "[Epoch 69/1001] [Batch 335/372] [D loss: 0.6823145151138306] [G loss: 0.6922667026519775]\n",
      "[Epoch 69/1001] [Batch 336/372] [D loss: 0.6797704696655273] [G loss: 0.6720108389854431]\n",
      "[Epoch 69/1001] [Batch 337/372] [D loss: 0.6886776685714722] [G loss: 0.6522061228752136]\n",
      "[Epoch 69/1001] [Batch 338/372] [D loss: 0.6827505826950073] [G loss: 0.7015656232833862]\n",
      "[Epoch 69/1001] [Batch 339/372] [D loss: 0.6911138296127319] [G loss: 0.6422109603881836]\n",
      "[Epoch 69/1001] [Batch 340/372] [D loss: 0.6862343549728394] [G loss: 0.7022702097892761]\n",
      "[Epoch 69/1001] [Batch 341/372] [D loss: 0.6870853900909424] [G loss: 0.6302893757820129]\n",
      "[Epoch 69/1001] [Batch 342/372] [D loss: 0.6939343214035034] [G loss: 0.7066162824630737]\n",
      "[Epoch 69/1001] [Batch 343/372] [D loss: 0.6807843446731567] [G loss: 0.6170511245727539]\n",
      "[Epoch 69/1001] [Batch 344/372] [D loss: 0.6820980310440063] [G loss: 0.7460260391235352]\n",
      "[Epoch 69/1001] [Batch 345/372] [D loss: 0.6905739307403564] [G loss: 0.6078977584838867]\n",
      "[Epoch 69/1001] [Batch 346/372] [D loss: 0.6841518878936768] [G loss: 0.8315359354019165]\n",
      "[Epoch 69/1001] [Batch 347/372] [D loss: 0.7026236057281494] [G loss: 0.4794315993785858]\n",
      "[Epoch 69/1001] [Batch 348/372] [D loss: 0.7120707631111145] [G loss: 0.9530206322669983]\n",
      "[Epoch 69/1001] [Batch 349/372] [D loss: 0.7225127816200256] [G loss: 0.4893129765987396]\n",
      "[Epoch 69/1001] [Batch 350/372] [D loss: 0.7089976072311401] [G loss: 0.8107048273086548]\n",
      "[Epoch 69/1001] [Batch 351/372] [D loss: 0.7019121646881104] [G loss: 0.6186156272888184]\n",
      "[Epoch 69/1001] [Batch 352/372] [D loss: 0.6863474249839783] [G loss: 0.6654289364814758]\n",
      "[Epoch 69/1001] [Batch 353/372] [D loss: 0.6875897645950317] [G loss: 0.7059649229049683]\n",
      "[Epoch 69/1001] [Batch 354/372] [D loss: 0.6992049217224121] [G loss: 0.6383077502250671]\n",
      "[Epoch 69/1001] [Batch 355/372] [D loss: 0.692924976348877] [G loss: 0.6844099164009094]\n",
      "[Epoch 69/1001] [Batch 356/372] [D loss: 0.6862608790397644] [G loss: 0.6732581853866577]\n",
      "[Epoch 69/1001] [Batch 357/372] [D loss: 0.6788676977157593] [G loss: 0.6749182343482971]\n",
      "[Epoch 69/1001] [Batch 358/372] [D loss: 0.6875882148742676] [G loss: 0.6833548545837402]\n",
      "[Epoch 69/1001] [Batch 359/372] [D loss: 0.6844605207443237] [G loss: 0.6777201890945435]\n",
      "[Epoch 69/1001] [Batch 360/372] [D loss: 0.6860689520835876] [G loss: 0.6600888967514038]\n",
      "[Epoch 69/1001] [Batch 361/372] [D loss: 0.6816461086273193] [G loss: 0.690695583820343]\n",
      "[Epoch 69/1001] [Batch 362/372] [D loss: 0.6889323592185974] [G loss: 0.6754586696624756]\n",
      "[Epoch 69/1001] [Batch 363/372] [D loss: 0.6870942115783691] [G loss: 0.6670398712158203]\n",
      "[Epoch 69/1001] [Batch 364/372] [D loss: 0.6886060833930969] [G loss: 0.7028262615203857]\n",
      "[Epoch 69/1001] [Batch 365/372] [D loss: 0.6861150860786438] [G loss: 0.6274805068969727]\n",
      "[Epoch 69/1001] [Batch 366/372] [D loss: 0.6852995157241821] [G loss: 0.7460705041885376]\n",
      "[Epoch 69/1001] [Batch 367/372] [D loss: 0.6887564659118652] [G loss: 0.6235226392745972]\n",
      "[Epoch 69/1001] [Batch 368/372] [D loss: 0.6873858571052551] [G loss: 0.7228911519050598]\n",
      "[Epoch 69/1001] [Batch 369/372] [D loss: 0.6871541738510132] [G loss: 0.6082120537757874]\n",
      "[Epoch 69/1001] [Batch 370/372] [D loss: 0.6847082376480103] [G loss: 0.745090126991272]\n",
      "[Epoch 69/1001] [Batch 371/372] [D loss: 0.6914096474647522] [G loss: 0.6232012510299683]\n",
      "[Epoch 70/1001] [Batch 0/372] [D loss: 0.6845979690551758] [G loss: 0.7305403351783752]\n",
      "[Epoch 70/1001] [Batch 1/372] [D loss: 0.6789238452911377] [G loss: 0.6298214197158813]\n",
      "[Epoch 70/1001] [Batch 2/372] [D loss: 0.6834135055541992] [G loss: 0.7088819742202759]\n",
      "[Epoch 70/1001] [Batch 3/372] [D loss: 0.6786807775497437] [G loss: 0.6691195964813232]\n",
      "[Epoch 70/1001] [Batch 4/372] [D loss: 0.6831592321395874] [G loss: 0.6852981448173523]\n",
      "[Epoch 70/1001] [Batch 5/372] [D loss: 0.6817111372947693] [G loss: 0.7020689845085144]\n",
      "[Epoch 70/1001] [Batch 6/372] [D loss: 0.674260139465332] [G loss: 0.6340761184692383]\n",
      "[Epoch 70/1001] [Batch 7/372] [D loss: 0.6908303499221802] [G loss: 0.74669349193573]\n",
      "[Epoch 70/1001] [Batch 8/372] [D loss: 0.6920750141143799] [G loss: 0.6032052636146545]\n",
      "[Epoch 70/1001] [Batch 9/372] [D loss: 0.6928234100341797] [G loss: 0.7203177809715271]\n",
      "[Epoch 70/1001] [Batch 10/372] [D loss: 0.6735739707946777] [G loss: 0.6386412382125854]\n",
      "[Epoch 70/1001] [Batch 11/372] [D loss: 0.6839015483856201] [G loss: 0.7676402926445007]\n",
      "[Epoch 70/1001] [Batch 12/372] [D loss: 0.6966294646263123] [G loss: 0.5608052015304565]\n",
      "[Epoch 70/1001] [Batch 13/372] [D loss: 0.6895930171012878] [G loss: 0.8256926536560059]\n",
      "[Epoch 70/1001] [Batch 14/372] [D loss: 0.6992162466049194] [G loss: 0.5523061156272888]\n",
      "[Epoch 70/1001] [Batch 15/372] [D loss: 0.6942849159240723] [G loss: 0.7962296009063721]\n",
      "[Epoch 70/1001] [Batch 16/372] [D loss: 0.6894654035568237] [G loss: 0.6008921265602112]\n",
      "[Epoch 70/1001] [Batch 17/372] [D loss: 0.6872364282608032] [G loss: 0.7415051460266113]\n",
      "[Epoch 70/1001] [Batch 18/372] [D loss: 0.6896640658378601] [G loss: 0.6432810425758362]\n",
      "[Epoch 70/1001] [Batch 19/372] [D loss: 0.6901394724845886] [G loss: 0.6837171912193298]\n",
      "[Epoch 70/1001] [Batch 20/372] [D loss: 0.6823428869247437] [G loss: 0.6763445138931274]\n",
      "[Epoch 70/1001] [Batch 21/372] [D loss: 0.6869330406188965] [G loss: 0.6622533202171326]\n",
      "[Epoch 70/1001] [Batch 22/372] [D loss: 0.6796771287918091] [G loss: 0.7127699255943298]\n",
      "[Epoch 70/1001] [Batch 23/372] [D loss: 0.6898258924484253] [G loss: 0.6146589517593384]\n",
      "[Epoch 70/1001] [Batch 24/372] [D loss: 0.6857000589370728] [G loss: 0.7470424175262451]\n",
      "[Epoch 70/1001] [Batch 25/372] [D loss: 0.6886423826217651] [G loss: 0.5752663612365723]\n",
      "[Epoch 70/1001] [Batch 26/372] [D loss: 0.6856495141983032] [G loss: 0.8189255595207214]\n",
      "[Epoch 70/1001] [Batch 27/372] [D loss: 0.7001887559890747] [G loss: 0.5429016351699829]\n",
      "[Epoch 70/1001] [Batch 28/372] [D loss: 0.6881120800971985] [G loss: 0.7935033440589905]\n",
      "[Epoch 70/1001] [Batch 29/372] [D loss: 0.68415766954422] [G loss: 0.6126973032951355]\n",
      "[Epoch 70/1001] [Batch 30/372] [D loss: 0.6837674379348755] [G loss: 0.7518145442008972]\n",
      "[Epoch 70/1001] [Batch 31/372] [D loss: 0.6833590269088745] [G loss: 0.5993627905845642]\n",
      "[Epoch 70/1001] [Batch 32/372] [D loss: 0.6855231523513794] [G loss: 0.7666144371032715]\n",
      "[Epoch 70/1001] [Batch 33/372] [D loss: 0.6937515139579773] [G loss: 0.5881323218345642]\n",
      "[Epoch 70/1001] [Batch 34/372] [D loss: 0.6893118619918823] [G loss: 0.7543298006057739]\n",
      "[Epoch 70/1001] [Batch 35/372] [D loss: 0.6900467872619629] [G loss: 0.6171897649765015]\n",
      "[Epoch 70/1001] [Batch 36/372] [D loss: 0.6895216703414917] [G loss: 0.6995384693145752]\n",
      "[Epoch 70/1001] [Batch 37/372] [D loss: 0.685518741607666] [G loss: 0.6611157059669495]\n",
      "[Epoch 70/1001] [Batch 38/372] [D loss: 0.6862237453460693] [G loss: 0.678480863571167]\n",
      "[Epoch 70/1001] [Batch 39/372] [D loss: 0.6750953197479248] [G loss: 0.6553301215171814]\n",
      "[Epoch 70/1001] [Batch 40/372] [D loss: 0.6922475099563599] [G loss: 0.7527039051055908]\n",
      "[Epoch 70/1001] [Batch 41/372] [D loss: 0.6871579885482788] [G loss: 0.5909155607223511]\n",
      "[Epoch 70/1001] [Batch 42/372] [D loss: 0.6889955997467041] [G loss: 0.7733912467956543]\n",
      "[Epoch 70/1001] [Batch 43/372] [D loss: 0.6846925020217896] [G loss: 0.5818666219711304]\n",
      "[Epoch 70/1001] [Batch 44/372] [D loss: 0.6840017437934875] [G loss: 0.7841356992721558]\n",
      "[Epoch 70/1001] [Batch 45/372] [D loss: 0.6813164353370667] [G loss: 0.5878930687904358]\n",
      "[Epoch 70/1001] [Batch 46/372] [D loss: 0.6896570324897766] [G loss: 0.7576369047164917]\n",
      "[Epoch 70/1001] [Batch 47/372] [D loss: 0.6876993775367737] [G loss: 0.5981010794639587]\n",
      "[Epoch 70/1001] [Batch 48/372] [D loss: 0.6838802695274353] [G loss: 0.7658998370170593]\n",
      "[Epoch 70/1001] [Batch 49/372] [D loss: 0.6903743147850037] [G loss: 0.5784293413162231]\n",
      "[Epoch 70/1001] [Batch 50/372] [D loss: 0.6946190595626831] [G loss: 0.750770092010498]\n",
      "[Epoch 70/1001] [Batch 51/372] [D loss: 0.683296799659729] [G loss: 0.6211704015731812]\n",
      "[Epoch 70/1001] [Batch 52/372] [D loss: 0.6819484233856201] [G loss: 0.7207052707672119]\n",
      "[Epoch 70/1001] [Batch 53/372] [D loss: 0.6848975419998169] [G loss: 0.5986080169677734]\n",
      "[Epoch 70/1001] [Batch 54/372] [D loss: 0.6943302750587463] [G loss: 0.8744166493415833]\n",
      "[Epoch 70/1001] [Batch 55/372] [D loss: 0.7060728669166565] [G loss: 0.467975378036499]\n",
      "[Epoch 70/1001] [Batch 56/372] [D loss: 0.7112895250320435] [G loss: 0.9616312980651855]\n",
      "[Epoch 70/1001] [Batch 57/372] [D loss: 0.7212033271789551] [G loss: 0.5119917988777161]\n",
      "[Epoch 70/1001] [Batch 58/372] [D loss: 0.7075774073600769] [G loss: 0.757585883140564]\n",
      "[Epoch 70/1001] [Batch 59/372] [D loss: 0.6910214424133301] [G loss: 0.6720676422119141]\n",
      "[Epoch 70/1001] [Batch 60/372] [D loss: 0.6884526014328003] [G loss: 0.648684561252594]\n",
      "[Epoch 70/1001] [Batch 61/372] [D loss: 0.6876485347747803] [G loss: 0.6945739984512329]\n",
      "[Epoch 70/1001] [Batch 62/372] [D loss: 0.6800746917724609] [G loss: 0.6859182715415955]\n",
      "[Epoch 70/1001] [Batch 63/372] [D loss: 0.6825336217880249] [G loss: 0.6535726189613342]\n",
      "[Epoch 70/1001] [Batch 64/372] [D loss: 0.6796391010284424] [G loss: 0.716917097568512]\n",
      "[Epoch 70/1001] [Batch 65/372] [D loss: 0.6790934801101685] [G loss: 0.6449857950210571]\n",
      "[Epoch 70/1001] [Batch 66/372] [D loss: 0.6846693158149719] [G loss: 0.7012929320335388]\n",
      "[Epoch 70/1001] [Batch 67/372] [D loss: 0.6823939085006714] [G loss: 0.6567529439926147]\n",
      "[Epoch 70/1001] [Batch 68/372] [D loss: 0.6807740330696106] [G loss: 0.7056452035903931]\n",
      "[Epoch 70/1001] [Batch 69/372] [D loss: 0.684141993522644] [G loss: 0.6411905288696289]\n",
      "[Epoch 70/1001] [Batch 70/372] [D loss: 0.6813778877258301] [G loss: 0.7000428438186646]\n",
      "[Epoch 70/1001] [Batch 71/372] [D loss: 0.6889996528625488] [G loss: 0.6640759706497192]\n",
      "[Epoch 70/1001] [Batch 72/372] [D loss: 0.690593957901001] [G loss: 0.6596806645393372]\n",
      "[Epoch 70/1001] [Batch 73/372] [D loss: 0.6820214986801147] [G loss: 0.6928119659423828]\n",
      "[Epoch 70/1001] [Batch 74/372] [D loss: 0.6922370195388794] [G loss: 0.6732075214385986]\n",
      "[Epoch 70/1001] [Batch 75/372] [D loss: 0.6833875775337219] [G loss: 0.6675781011581421]\n",
      "[Epoch 70/1001] [Batch 76/372] [D loss: 0.6883519887924194] [G loss: 0.6653667092323303]\n",
      "[Epoch 70/1001] [Batch 77/372] [D loss: 0.6785227060317993] [G loss: 0.7210302352905273]\n",
      "[Epoch 70/1001] [Batch 78/372] [D loss: 0.6882860660552979] [G loss: 0.5978591442108154]\n",
      "[Epoch 70/1001] [Batch 79/372] [D loss: 0.6832813620567322] [G loss: 0.7743363976478577]\n",
      "[Epoch 70/1001] [Batch 80/372] [D loss: 0.6897796392440796] [G loss: 0.5753333568572998]\n",
      "[Epoch 70/1001] [Batch 81/372] [D loss: 0.699155867099762] [G loss: 0.8048890233039856]\n",
      "[Epoch 70/1001] [Batch 82/372] [D loss: 0.6914936304092407] [G loss: 0.5692519545555115]\n",
      "[Epoch 70/1001] [Batch 83/372] [D loss: 0.6834129095077515] [G loss: 0.7700037360191345]\n",
      "[Epoch 70/1001] [Batch 84/372] [D loss: 0.68507981300354] [G loss: 0.6226476430892944]\n",
      "[Epoch 70/1001] [Batch 85/372] [D loss: 0.6932647824287415] [G loss: 0.6891466379165649]\n",
      "[Epoch 70/1001] [Batch 86/372] [D loss: 0.6865558624267578] [G loss: 0.6850123405456543]\n",
      "[Epoch 70/1001] [Batch 87/372] [D loss: 0.6944642066955566] [G loss: 0.6189192533493042]\n",
      "[Epoch 70/1001] [Batch 88/372] [D loss: 0.6920405626296997] [G loss: 0.7435638308525085]\n",
      "[Epoch 70/1001] [Batch 89/372] [D loss: 0.6884986162185669] [G loss: 0.6047536730766296]\n",
      "[Epoch 70/1001] [Batch 90/372] [D loss: 0.6882283687591553] [G loss: 0.7347465753555298]\n",
      "[Epoch 70/1001] [Batch 91/372] [D loss: 0.6894345283508301] [G loss: 0.6049770712852478]\n",
      "[Epoch 70/1001] [Batch 92/372] [D loss: 0.6865981817245483] [G loss: 0.7317657470703125]\n",
      "[Epoch 70/1001] [Batch 93/372] [D loss: 0.6865051984786987] [G loss: 0.6384940147399902]\n",
      "[Epoch 70/1001] [Batch 94/372] [D loss: 0.6922099590301514] [G loss: 0.6940793395042419]\n",
      "[Epoch 70/1001] [Batch 95/372] [D loss: 0.6816580295562744] [G loss: 0.644349992275238]\n",
      "[Epoch 70/1001] [Batch 96/372] [D loss: 0.6883388757705688] [G loss: 0.7357813715934753]\n",
      "[Epoch 70/1001] [Batch 97/372] [D loss: 0.6832610368728638] [G loss: 0.6030650734901428]\n",
      "[Epoch 70/1001] [Batch 98/372] [D loss: 0.6939195394515991] [G loss: 0.7756522297859192]\n",
      "[Epoch 70/1001] [Batch 99/372] [D loss: 0.6914482116699219] [G loss: 0.5726576447486877]\n",
      "[Epoch 70/1001] [Batch 100/372] [D loss: 0.6903072595596313] [G loss: 0.7719739079475403]\n",
      "[Epoch 70/1001] [Batch 101/372] [D loss: 0.6873133182525635] [G loss: 0.6197728514671326]\n",
      "[Epoch 70/1001] [Batch 102/372] [D loss: 0.6839265823364258] [G loss: 0.7028141021728516]\n",
      "[Epoch 70/1001] [Batch 103/372] [D loss: 0.6917939186096191] [G loss: 0.6631702780723572]\n",
      "[Epoch 70/1001] [Batch 104/372] [D loss: 0.6821306943893433] [G loss: 0.6953149437904358]\n",
      "[Epoch 70/1001] [Batch 105/372] [D loss: 0.6822521686553955] [G loss: 0.6533334255218506]\n",
      "[Epoch 70/1001] [Batch 106/372] [D loss: 0.6820732355117798] [G loss: 0.6854270696640015]\n",
      "[Epoch 70/1001] [Batch 107/372] [D loss: 0.6798575520515442] [G loss: 0.6392298340797424]\n",
      "[Epoch 70/1001] [Batch 108/372] [D loss: 0.6855868101119995] [G loss: 0.7175156474113464]\n",
      "[Epoch 70/1001] [Batch 109/372] [D loss: 0.6930350065231323] [G loss: 0.6220400333404541]\n",
      "[Epoch 70/1001] [Batch 110/372] [D loss: 0.6868396401405334] [G loss: 0.7583385705947876]\n",
      "[Epoch 70/1001] [Batch 111/372] [D loss: 0.6809428930282593] [G loss: 0.5953260064125061]\n",
      "[Epoch 70/1001] [Batch 112/372] [D loss: 0.6847694516181946] [G loss: 0.775577187538147]\n",
      "[Epoch 70/1001] [Batch 113/372] [D loss: 0.6881741285324097] [G loss: 0.5995000600814819]\n",
      "[Epoch 70/1001] [Batch 114/372] [D loss: 0.6886670589447021] [G loss: 0.7180542349815369]\n",
      "[Epoch 70/1001] [Batch 115/372] [D loss: 0.6899113655090332] [G loss: 0.6458234786987305]\n",
      "[Epoch 70/1001] [Batch 116/372] [D loss: 0.6867645978927612] [G loss: 0.7124021053314209]\n",
      "[Epoch 70/1001] [Batch 117/372] [D loss: 0.6808816194534302] [G loss: 0.6530748605728149]\n",
      "[Epoch 70/1001] [Batch 118/372] [D loss: 0.679382860660553] [G loss: 0.6905653476715088]\n",
      "[Epoch 70/1001] [Batch 119/372] [D loss: 0.6871793270111084] [G loss: 0.6692638397216797]\n",
      "[Epoch 70/1001] [Batch 120/372] [D loss: 0.6838119626045227] [G loss: 0.705540657043457]\n",
      "[Epoch 70/1001] [Batch 121/372] [D loss: 0.6909778714179993] [G loss: 0.6189053654670715]\n",
      "[Epoch 70/1001] [Batch 122/372] [D loss: 0.6846299767494202] [G loss: 0.716263473033905]\n",
      "[Epoch 70/1001] [Batch 123/372] [D loss: 0.6863062381744385] [G loss: 0.6551013588905334]\n",
      "[Epoch 70/1001] [Batch 124/372] [D loss: 0.688800036907196] [G loss: 0.7023078203201294]\n",
      "[Epoch 70/1001] [Batch 125/372] [D loss: 0.6831161975860596] [G loss: 0.6379557251930237]\n",
      "[Epoch 70/1001] [Batch 126/372] [D loss: 0.6869010925292969] [G loss: 0.7135040760040283]\n",
      "[Epoch 70/1001] [Batch 127/372] [D loss: 0.6801433563232422] [G loss: 0.6138763427734375]\n",
      "[Epoch 70/1001] [Batch 128/372] [D loss: 0.6978716850280762] [G loss: 0.8085043430328369]\n",
      "[Epoch 70/1001] [Batch 129/372] [D loss: 0.6874439716339111] [G loss: 0.5476998090744019]\n",
      "[Epoch 70/1001] [Batch 130/372] [D loss: 0.6967329382896423] [G loss: 0.8666360378265381]\n",
      "[Epoch 70/1001] [Batch 131/372] [D loss: 0.6873210668563843] [G loss: 0.5326741933822632]\n",
      "[Epoch 70/1001] [Batch 132/372] [D loss: 0.706472635269165] [G loss: 0.8133299946784973]\n",
      "[Epoch 70/1001] [Batch 133/372] [D loss: 0.6941537857055664] [G loss: 0.5625123381614685]\n",
      "[Epoch 70/1001] [Batch 134/372] [D loss: 0.6895445585250854] [G loss: 0.7803662419319153]\n",
      "[Epoch 70/1001] [Batch 135/372] [D loss: 0.6902859210968018] [G loss: 0.583736002445221]\n",
      "[Epoch 70/1001] [Batch 136/372] [D loss: 0.6914498805999756] [G loss: 0.7883106470108032]\n",
      "[Epoch 70/1001] [Batch 137/372] [D loss: 0.6947083473205566] [G loss: 0.5701361298561096]\n",
      "[Epoch 70/1001] [Batch 138/372] [D loss: 0.6946461796760559] [G loss: 0.7609409093856812]\n",
      "[Epoch 70/1001] [Batch 139/372] [D loss: 0.6912417411804199] [G loss: 0.6411271095275879]\n",
      "[Epoch 70/1001] [Batch 140/372] [D loss: 0.6907071471214294] [G loss: 0.6831247210502625]\n",
      "[Epoch 70/1001] [Batch 141/372] [D loss: 0.688906729221344] [G loss: 0.6801866292953491]\n",
      "[Epoch 70/1001] [Batch 142/372] [D loss: 0.6885626316070557] [G loss: 0.670335054397583]\n",
      "[Epoch 70/1001] [Batch 143/372] [D loss: 0.685213029384613] [G loss: 0.680161714553833]\n",
      "[Epoch 70/1001] [Batch 144/372] [D loss: 0.6837115287780762] [G loss: 0.6764479875564575]\n",
      "[Epoch 70/1001] [Batch 145/372] [D loss: 0.6777945756912231] [G loss: 0.68817138671875]\n",
      "[Epoch 70/1001] [Batch 146/372] [D loss: 0.6861112117767334] [G loss: 0.6980486512184143]\n",
      "[Epoch 70/1001] [Batch 147/372] [D loss: 0.685218870639801] [G loss: 0.6470672488212585]\n",
      "[Epoch 70/1001] [Batch 148/372] [D loss: 0.6891090869903564] [G loss: 0.696251392364502]\n",
      "[Epoch 70/1001] [Batch 149/372] [D loss: 0.6878597736358643] [G loss: 0.6643913388252258]\n",
      "[Epoch 70/1001] [Batch 150/372] [D loss: 0.6859129071235657] [G loss: 0.6746652126312256]\n",
      "[Epoch 70/1001] [Batch 151/372] [D loss: 0.6804769039154053] [G loss: 0.6627433896064758]\n",
      "[Epoch 70/1001] [Batch 152/372] [D loss: 0.6843157410621643] [G loss: 0.6705464124679565]\n",
      "[Epoch 70/1001] [Batch 153/372] [D loss: 0.6784757375717163] [G loss: 0.7069293856620789]\n",
      "[Epoch 70/1001] [Batch 154/372] [D loss: 0.6816257238388062] [G loss: 0.6332114934921265]\n",
      "[Epoch 70/1001] [Batch 155/372] [D loss: 0.6797119975090027] [G loss: 0.6923156380653381]\n",
      "[Epoch 70/1001] [Batch 156/372] [D loss: 0.6838254332542419] [G loss: 0.650964617729187]\n",
      "[Epoch 70/1001] [Batch 157/372] [D loss: 0.6868971586227417] [G loss: 0.7304404377937317]\n",
      "[Epoch 70/1001] [Batch 158/372] [D loss: 0.6938385963439941] [G loss: 0.5708993673324585]\n",
      "[Epoch 70/1001] [Batch 159/372] [D loss: 0.685850977897644] [G loss: 0.8208569288253784]\n",
      "[Epoch 70/1001] [Batch 160/372] [D loss: 0.6889070272445679] [G loss: 0.5392417907714844]\n",
      "[Epoch 70/1001] [Batch 161/372] [D loss: 0.7035454511642456] [G loss: 0.8722480535507202]\n",
      "[Epoch 70/1001] [Batch 162/372] [D loss: 0.7025407552719116] [G loss: 0.4683445990085602]\n",
      "[Epoch 70/1001] [Batch 163/372] [D loss: 0.70778888463974] [G loss: 0.9451857805252075]\n",
      "[Epoch 70/1001] [Batch 164/372] [D loss: 0.7150170207023621] [G loss: 0.4729115962982178]\n",
      "[Epoch 70/1001] [Batch 165/372] [D loss: 0.7141324877738953] [G loss: 0.9160245656967163]\n",
      "[Epoch 70/1001] [Batch 166/372] [D loss: 0.7026060819625854] [G loss: 0.5564904808998108]\n",
      "[Epoch 70/1001] [Batch 167/372] [D loss: 0.7029626965522766] [G loss: 0.7135190963745117]\n",
      "[Epoch 70/1001] [Batch 168/372] [D loss: 0.6907411813735962] [G loss: 0.688911497592926]\n",
      "[Epoch 70/1001] [Batch 169/372] [D loss: 0.6901790499687195] [G loss: 0.6291930675506592]\n",
      "[Epoch 70/1001] [Batch 170/372] [D loss: 0.6867044568061829] [G loss: 0.6981205940246582]\n",
      "[Epoch 70/1001] [Batch 171/372] [D loss: 0.6877516508102417] [G loss: 0.6582964062690735]\n",
      "[Epoch 70/1001] [Batch 172/372] [D loss: 0.6798621416091919] [G loss: 0.6587110757827759]\n",
      "[Epoch 70/1001] [Batch 173/372] [D loss: 0.6820564270019531] [G loss: 0.6866036653518677]\n",
      "[Epoch 70/1001] [Batch 174/372] [D loss: 0.6842370629310608] [G loss: 0.6837090849876404]\n",
      "[Epoch 70/1001] [Batch 175/372] [D loss: 0.6935293078422546] [G loss: 0.6546489596366882]\n",
      "[Epoch 70/1001] [Batch 176/372] [D loss: 0.6898759603500366] [G loss: 0.6587057113647461]\n",
      "[Epoch 70/1001] [Batch 177/372] [D loss: 0.685849666595459] [G loss: 0.7214381694793701]\n",
      "[Epoch 70/1001] [Batch 178/372] [D loss: 0.6938858032226562] [G loss: 0.6270461082458496]\n",
      "[Epoch 70/1001] [Batch 179/372] [D loss: 0.6928601264953613] [G loss: 0.7042930126190186]\n",
      "[Epoch 70/1001] [Batch 180/372] [D loss: 0.6808376312255859] [G loss: 0.6452049016952515]\n",
      "[Epoch 70/1001] [Batch 181/372] [D loss: 0.6807101964950562] [G loss: 0.7093005180358887]\n",
      "[Epoch 70/1001] [Batch 182/372] [D loss: 0.682744026184082] [G loss: 0.6466111540794373]\n",
      "[Epoch 70/1001] [Batch 183/372] [D loss: 0.680992603302002] [G loss: 0.7142655253410339]\n",
      "[Epoch 70/1001] [Batch 184/372] [D loss: 0.6823249459266663] [G loss: 0.6273497343063354]\n",
      "[Epoch 70/1001] [Batch 185/372] [D loss: 0.6869356632232666] [G loss: 0.7433050274848938]\n",
      "[Epoch 70/1001] [Batch 186/372] [D loss: 0.6918317675590515] [G loss: 0.6124522686004639]\n",
      "[Epoch 70/1001] [Batch 187/372] [D loss: 0.692209005355835] [G loss: 0.7161963582038879]\n",
      "[Epoch 70/1001] [Batch 188/372] [D loss: 0.6948399543762207] [G loss: 0.6420578360557556]\n",
      "[Epoch 70/1001] [Batch 189/372] [D loss: 0.6857528686523438] [G loss: 0.6836991310119629]\n",
      "[Epoch 70/1001] [Batch 190/372] [D loss: 0.6844532489776611] [G loss: 0.6678161025047302]\n",
      "[Epoch 70/1001] [Batch 191/372] [D loss: 0.6841446161270142] [G loss: 0.6995424628257751]\n",
      "[Epoch 70/1001] [Batch 192/372] [D loss: 0.6808526515960693] [G loss: 0.6483820676803589]\n",
      "[Epoch 70/1001] [Batch 193/372] [D loss: 0.690191388130188] [G loss: 0.7188748121261597]\n",
      "[Epoch 70/1001] [Batch 194/372] [D loss: 0.685596227645874] [G loss: 0.6245831847190857]\n",
      "[Epoch 70/1001] [Batch 195/372] [D loss: 0.6919873952865601] [G loss: 0.7116422057151794]\n",
      "[Epoch 70/1001] [Batch 196/372] [D loss: 0.6846932768821716] [G loss: 0.6470398902893066]\n",
      "[Epoch 70/1001] [Batch 197/372] [D loss: 0.6821929216384888] [G loss: 0.6891835927963257]\n",
      "[Epoch 70/1001] [Batch 198/372] [D loss: 0.6836891174316406] [G loss: 0.6600611209869385]\n",
      "[Epoch 70/1001] [Batch 199/372] [D loss: 0.6921753883361816] [G loss: 0.669343888759613]\n",
      "[Epoch 70/1001] [Batch 200/372] [D loss: 0.6898254156112671] [G loss: 0.6669424772262573]\n",
      "[Epoch 70/1001] [Batch 201/372] [D loss: 0.6868735551834106] [G loss: 0.6746662259101868]\n",
      "[Epoch 70/1001] [Batch 202/372] [D loss: 0.6825839281082153] [G loss: 0.6692553162574768]\n",
      "[Epoch 70/1001] [Batch 203/372] [D loss: 0.688156247138977] [G loss: 0.6615822911262512]\n",
      "[Epoch 70/1001] [Batch 204/372] [D loss: 0.68047034740448] [G loss: 0.6949869394302368]\n",
      "[Epoch 70/1001] [Batch 205/372] [D loss: 0.6822469234466553] [G loss: 0.6544392108917236]\n",
      "[Epoch 70/1001] [Batch 206/372] [D loss: 0.677033007144928] [G loss: 0.7370800971984863]\n",
      "[Epoch 70/1001] [Batch 207/372] [D loss: 0.6846809983253479] [G loss: 0.599513590335846]\n",
      "[Epoch 70/1001] [Batch 208/372] [D loss: 0.6921910643577576] [G loss: 0.7783187031745911]\n",
      "[Epoch 70/1001] [Batch 209/372] [D loss: 0.6916232109069824] [G loss: 0.5585771799087524]\n",
      "[Epoch 70/1001] [Batch 210/372] [D loss: 0.6934444308280945] [G loss: 0.8395185470581055]\n",
      "[Epoch 70/1001] [Batch 211/372] [D loss: 0.6920380592346191] [G loss: 0.5354246497154236]\n",
      "[Epoch 70/1001] [Batch 212/372] [D loss: 0.7024091482162476] [G loss: 0.8721718192100525]\n",
      "[Epoch 70/1001] [Batch 213/372] [D loss: 0.7088460326194763] [G loss: 0.4935672879219055]\n",
      "[Epoch 70/1001] [Batch 214/372] [D loss: 0.7069627046585083] [G loss: 0.8993930816650391]\n",
      "[Epoch 70/1001] [Batch 215/372] [D loss: 0.7102267742156982] [G loss: 0.5440722703933716]\n",
      "[Epoch 70/1001] [Batch 216/372] [D loss: 0.7000710964202881] [G loss: 0.7742831707000732]\n",
      "[Epoch 70/1001] [Batch 217/372] [D loss: 0.6912753582000732] [G loss: 0.6191586256027222]\n",
      "[Epoch 70/1001] [Batch 218/372] [D loss: 0.692493200302124] [G loss: 0.7148987650871277]\n",
      "[Epoch 70/1001] [Batch 219/372] [D loss: 0.6782417297363281] [G loss: 0.6443995833396912]\n",
      "[Epoch 70/1001] [Batch 220/372] [D loss: 0.6910054087638855] [G loss: 0.7053098082542419]\n",
      "[Epoch 70/1001] [Batch 221/372] [D loss: 0.684982419013977] [G loss: 0.6540091633796692]\n",
      "[Epoch 70/1001] [Batch 222/372] [D loss: 0.6891968250274658] [G loss: 0.6810590028762817]\n",
      "[Epoch 70/1001] [Batch 223/372] [D loss: 0.6875545978546143] [G loss: 0.6768271327018738]\n",
      "[Epoch 70/1001] [Batch 224/372] [D loss: 0.6910421848297119] [G loss: 0.6348000168800354]\n",
      "[Epoch 70/1001] [Batch 225/372] [D loss: 0.6820018291473389] [G loss: 0.7301377654075623]\n",
      "[Epoch 70/1001] [Batch 226/372] [D loss: 0.6808210611343384] [G loss: 0.6194859147071838]\n",
      "[Epoch 70/1001] [Batch 227/372] [D loss: 0.6900555491447449] [G loss: 0.7516116499900818]\n",
      "[Epoch 70/1001] [Batch 228/372] [D loss: 0.6920346617698669] [G loss: 0.6014248132705688]\n",
      "[Epoch 70/1001] [Batch 229/372] [D loss: 0.695298969745636] [G loss: 0.7214431166648865]\n",
      "[Epoch 70/1001] [Batch 230/372] [D loss: 0.6885258555412292] [G loss: 0.6040835380554199]\n",
      "[Epoch 70/1001] [Batch 231/372] [D loss: 0.6854773163795471] [G loss: 0.7644485831260681]\n",
      "[Epoch 70/1001] [Batch 232/372] [D loss: 0.6936879754066467] [G loss: 0.6224453449249268]\n",
      "[Epoch 70/1001] [Batch 233/372] [D loss: 0.6962166428565979] [G loss: 0.6738511323928833]\n",
      "[Epoch 70/1001] [Batch 234/372] [D loss: 0.6821699738502502] [G loss: 0.6990752816200256]\n",
      "[Epoch 70/1001] [Batch 235/372] [D loss: 0.6836239695549011] [G loss: 0.6465892195701599]\n",
      "[Epoch 70/1001] [Batch 236/372] [D loss: 0.6886367797851562] [G loss: 0.7184027433395386]\n",
      "[Epoch 70/1001] [Batch 237/372] [D loss: 0.6816486716270447] [G loss: 0.62689208984375]\n",
      "[Epoch 70/1001] [Batch 238/372] [D loss: 0.6903937458992004] [G loss: 0.7255638837814331]\n",
      "[Epoch 70/1001] [Batch 239/372] [D loss: 0.6810184717178345] [G loss: 0.6371886730194092]\n",
      "[Epoch 70/1001] [Batch 240/372] [D loss: 0.6838241815567017] [G loss: 0.7127813696861267]\n",
      "[Epoch 70/1001] [Batch 241/372] [D loss: 0.68314528465271] [G loss: 0.6394622921943665]\n",
      "[Epoch 70/1001] [Batch 242/372] [D loss: 0.6884781122207642] [G loss: 0.7224971055984497]\n",
      "[Epoch 70/1001] [Batch 243/372] [D loss: 0.6845365762710571] [G loss: 0.6150760054588318]\n",
      "[Epoch 70/1001] [Batch 244/372] [D loss: 0.6881849765777588] [G loss: 0.7684421539306641]\n",
      "[Epoch 70/1001] [Batch 245/372] [D loss: 0.6863152384757996] [G loss: 0.5570446252822876]\n",
      "[Epoch 70/1001] [Batch 246/372] [D loss: 0.694751501083374] [G loss: 0.8311513662338257]\n",
      "[Epoch 70/1001] [Batch 247/372] [D loss: 0.6953457593917847] [G loss: 0.5238401889801025]\n",
      "[Epoch 70/1001] [Batch 248/372] [D loss: 0.702448844909668] [G loss: 0.8677836656570435]\n",
      "[Epoch 70/1001] [Batch 249/372] [D loss: 0.7073711156845093] [G loss: 0.5072331428527832]\n",
      "[Epoch 70/1001] [Batch 250/372] [D loss: 0.7005293369293213] [G loss: 0.8563714623451233]\n",
      "[Epoch 70/1001] [Batch 251/372] [D loss: 0.7057255506515503] [G loss: 0.5674274563789368]\n",
      "[Epoch 70/1001] [Batch 252/372] [D loss: 0.6945909261703491] [G loss: 0.7505558729171753]\n",
      "[Epoch 70/1001] [Batch 253/372] [D loss: 0.687036395072937] [G loss: 0.6164368391036987]\n",
      "[Epoch 70/1001] [Batch 254/372] [D loss: 0.685515284538269] [G loss: 0.7518063187599182]\n",
      "[Epoch 70/1001] [Batch 255/372] [D loss: 0.6861423254013062] [G loss: 0.6136063933372498]\n",
      "[Epoch 70/1001] [Batch 256/372] [D loss: 0.6833089590072632] [G loss: 0.7307446599006653]\n",
      "[Epoch 70/1001] [Batch 257/372] [D loss: 0.6967385411262512] [G loss: 0.6241234540939331]\n",
      "[Epoch 70/1001] [Batch 258/372] [D loss: 0.6900177001953125] [G loss: 0.7479299306869507]\n",
      "[Epoch 70/1001] [Batch 259/372] [D loss: 0.6898067593574524] [G loss: 0.5980865955352783]\n",
      "[Epoch 70/1001] [Batch 260/372] [D loss: 0.6873859167098999] [G loss: 0.7642748355865479]\n",
      "[Epoch 70/1001] [Batch 261/372] [D loss: 0.686896562576294] [G loss: 0.6250463724136353]\n",
      "[Epoch 70/1001] [Batch 262/372] [D loss: 0.6878707408905029] [G loss: 0.7010765671730042]\n",
      "[Epoch 70/1001] [Batch 263/372] [D loss: 0.6838438510894775] [G loss: 0.6540443897247314]\n",
      "[Epoch 70/1001] [Batch 264/372] [D loss: 0.6811628341674805] [G loss: 0.6993798613548279]\n",
      "[Epoch 70/1001] [Batch 265/372] [D loss: 0.6889779567718506] [G loss: 0.662003219127655]\n",
      "[Epoch 70/1001] [Batch 266/372] [D loss: 0.6830466389656067] [G loss: 0.6927794814109802]\n",
      "[Epoch 70/1001] [Batch 267/372] [D loss: 0.6898725032806396] [G loss: 0.6572502851486206]\n",
      "[Epoch 70/1001] [Batch 268/372] [D loss: 0.6887166500091553] [G loss: 0.6680171489715576]\n",
      "[Epoch 70/1001] [Batch 269/372] [D loss: 0.6830404996871948] [G loss: 0.6810265779495239]\n",
      "[Epoch 70/1001] [Batch 270/372] [D loss: 0.6850788593292236] [G loss: 0.680402934551239]\n",
      "[Epoch 70/1001] [Batch 271/372] [D loss: 0.693140983581543] [G loss: 0.6628987789154053]\n",
      "[Epoch 70/1001] [Batch 272/372] [D loss: 0.6862905025482178] [G loss: 0.6700904965400696]\n",
      "[Epoch 70/1001] [Batch 273/372] [D loss: 0.6735024452209473] [G loss: 0.654716968536377]\n",
      "[Epoch 70/1001] [Batch 274/372] [D loss: 0.6785290837287903] [G loss: 0.7584757208824158]\n",
      "[Epoch 70/1001] [Batch 275/372] [D loss: 0.6907023191452026] [G loss: 0.5830149054527283]\n",
      "[Epoch 70/1001] [Batch 276/372] [D loss: 0.6846672296524048] [G loss: 0.8030609488487244]\n",
      "[Epoch 70/1001] [Batch 277/372] [D loss: 0.6954328417778015] [G loss: 0.5736720561981201]\n",
      "[Epoch 70/1001] [Batch 278/372] [D loss: 0.6940294504165649] [G loss: 0.7621616721153259]\n",
      "[Epoch 70/1001] [Batch 279/372] [D loss: 0.6793063879013062] [G loss: 0.5946792364120483]\n",
      "[Epoch 70/1001] [Batch 280/372] [D loss: 0.6826149225234985] [G loss: 0.7355202436447144]\n",
      "[Epoch 70/1001] [Batch 281/372] [D loss: 0.6922026872634888] [G loss: 0.6279218196868896]\n",
      "[Epoch 70/1001] [Batch 282/372] [D loss: 0.6910570859909058] [G loss: 0.7723219394683838]\n",
      "[Epoch 70/1001] [Batch 283/372] [D loss: 0.6944559216499329] [G loss: 0.5345752239227295]\n",
      "[Epoch 70/1001] [Batch 284/372] [D loss: 0.6978909969329834] [G loss: 0.8636370897293091]\n",
      "[Epoch 70/1001] [Batch 285/372] [D loss: 0.7064457535743713] [G loss: 0.5428464412689209]\n",
      "[Epoch 70/1001] [Batch 286/372] [D loss: 0.6860291957855225] [G loss: 0.7696390151977539]\n",
      "[Epoch 70/1001] [Batch 287/372] [D loss: 0.6893347501754761] [G loss: 0.6411962509155273]\n",
      "[Epoch 70/1001] [Batch 288/372] [D loss: 0.6837140917778015] [G loss: 0.6841105222702026]\n",
      "[Epoch 70/1001] [Batch 289/372] [D loss: 0.690857470035553] [G loss: 0.6860938668251038]\n",
      "[Epoch 70/1001] [Batch 290/372] [D loss: 0.6863540410995483] [G loss: 0.6512418985366821]\n",
      "[Epoch 70/1001] [Batch 291/372] [D loss: 0.6921623945236206] [G loss: 0.691927969455719]\n",
      "[Epoch 70/1001] [Batch 292/372] [D loss: 0.6843677759170532] [G loss: 0.6563431024551392]\n",
      "[Epoch 70/1001] [Batch 293/372] [D loss: 0.6852514743804932] [G loss: 0.6861799955368042]\n",
      "[Epoch 70/1001] [Batch 294/372] [D loss: 0.6833140850067139] [G loss: 0.6563763618469238]\n",
      "[Epoch 70/1001] [Batch 295/372] [D loss: 0.6805330514907837] [G loss: 0.6923876404762268]\n",
      "[Epoch 70/1001] [Batch 296/372] [D loss: 0.6840285062789917] [G loss: 0.6884794235229492]\n",
      "[Epoch 70/1001] [Batch 297/372] [D loss: 0.6840651035308838] [G loss: 0.6253828406333923]\n",
      "[Epoch 70/1001] [Batch 298/372] [D loss: 0.698419988155365] [G loss: 0.7444973587989807]\n",
      "[Epoch 70/1001] [Batch 299/372] [D loss: 0.6956132650375366] [G loss: 0.6248531937599182]\n",
      "[Epoch 70/1001] [Batch 300/372] [D loss: 0.692617654800415] [G loss: 0.710389256477356]\n",
      "[Epoch 70/1001] [Batch 301/372] [D loss: 0.6867045164108276] [G loss: 0.650871992111206]\n",
      "[Epoch 70/1001] [Batch 302/372] [D loss: 0.6767479181289673] [G loss: 0.6852641105651855]\n",
      "[Epoch 70/1001] [Batch 303/372] [D loss: 0.6914554834365845] [G loss: 0.6639435291290283]\n",
      "[Epoch 70/1001] [Batch 304/372] [D loss: 0.688689649105072] [G loss: 0.6857553720474243]\n",
      "[Epoch 70/1001] [Batch 305/372] [D loss: 0.6880796551704407] [G loss: 0.6622818112373352]\n",
      "[Epoch 70/1001] [Batch 306/372] [D loss: 0.6935797929763794] [G loss: 0.6887985467910767]\n",
      "[Epoch 70/1001] [Batch 307/372] [D loss: 0.6856998205184937] [G loss: 0.6458848714828491]\n",
      "[Epoch 70/1001] [Batch 308/372] [D loss: 0.6849831342697144] [G loss: 0.7137637734413147]\n",
      "[Epoch 70/1001] [Batch 309/372] [D loss: 0.6892105340957642] [G loss: 0.6463392376899719]\n",
      "[Epoch 70/1001] [Batch 310/372] [D loss: 0.6794859170913696] [G loss: 0.720516562461853]\n",
      "[Epoch 70/1001] [Batch 311/372] [D loss: 0.6966781616210938] [G loss: 0.6249444484710693]\n",
      "[Epoch 70/1001] [Batch 312/372] [D loss: 0.6889781951904297] [G loss: 0.7140929698944092]\n",
      "[Epoch 70/1001] [Batch 313/372] [D loss: 0.6934490203857422] [G loss: 0.6289830207824707]\n",
      "[Epoch 70/1001] [Batch 314/372] [D loss: 0.686943769454956] [G loss: 0.7114155888557434]\n",
      "[Epoch 70/1001] [Batch 315/372] [D loss: 0.6843408942222595] [G loss: 0.6305279731750488]\n",
      "[Epoch 70/1001] [Batch 316/372] [D loss: 0.6910949349403381] [G loss: 0.7454130053520203]\n",
      "[Epoch 70/1001] [Batch 317/372] [D loss: 0.6894999742507935] [G loss: 0.6118109822273254]\n",
      "[Epoch 70/1001] [Batch 318/372] [D loss: 0.6907014846801758] [G loss: 0.7187584042549133]\n",
      "[Epoch 70/1001] [Batch 319/372] [D loss: 0.6861052513122559] [G loss: 0.6182790398597717]\n",
      "[Epoch 70/1001] [Batch 320/372] [D loss: 0.6904513835906982] [G loss: 0.7476637363433838]\n",
      "[Epoch 70/1001] [Batch 321/372] [D loss: 0.6816611886024475] [G loss: 0.580236554145813]\n",
      "[Epoch 70/1001] [Batch 322/372] [D loss: 0.6927765607833862] [G loss: 0.8301174640655518]\n",
      "[Epoch 70/1001] [Batch 323/372] [D loss: 0.6926096677780151] [G loss: 0.5019186735153198]\n",
      "[Epoch 70/1001] [Batch 324/372] [D loss: 0.7118728160858154] [G loss: 0.9176285266876221]\n",
      "[Epoch 70/1001] [Batch 325/372] [D loss: 0.7134857177734375] [G loss: 0.5128710865974426]\n",
      "[Epoch 70/1001] [Batch 326/372] [D loss: 0.709600031375885] [G loss: 0.7893836498260498]\n",
      "[Epoch 70/1001] [Batch 327/372] [D loss: 0.6931859254837036] [G loss: 0.6291927695274353]\n",
      "[Epoch 70/1001] [Batch 328/372] [D loss: 0.6896251440048218] [G loss: 0.6521256566047668]\n",
      "[Epoch 70/1001] [Batch 329/372] [D loss: 0.6881783604621887] [G loss: 0.7323217988014221]\n",
      "[Epoch 70/1001] [Batch 330/372] [D loss: 0.6865126490592957] [G loss: 0.6219930648803711]\n",
      "[Epoch 70/1001] [Batch 331/372] [D loss: 0.6875172853469849] [G loss: 0.7111173868179321]\n",
      "[Epoch 70/1001] [Batch 332/372] [D loss: 0.6859334707260132] [G loss: 0.6523236632347107]\n",
      "[Epoch 70/1001] [Batch 333/372] [D loss: 0.682682454586029] [G loss: 0.6891866326332092]\n",
      "[Epoch 70/1001] [Batch 334/372] [D loss: 0.6890759468078613] [G loss: 0.6716307401657104]\n",
      "[Epoch 70/1001] [Batch 335/372] [D loss: 0.6796374320983887] [G loss: 0.6673299074172974]\n",
      "[Epoch 70/1001] [Batch 336/372] [D loss: 0.6865090131759644] [G loss: 0.6806540489196777]\n",
      "[Epoch 70/1001] [Batch 337/372] [D loss: 0.6846093535423279] [G loss: 0.6812400221824646]\n",
      "[Epoch 70/1001] [Batch 338/372] [D loss: 0.6864159107208252] [G loss: 0.6526836156845093]\n",
      "[Epoch 70/1001] [Batch 339/372] [D loss: 0.6929851770401001] [G loss: 0.7510037422180176]\n",
      "[Epoch 70/1001] [Batch 340/372] [D loss: 0.682402491569519] [G loss: 0.587424635887146]\n",
      "[Epoch 70/1001] [Batch 341/372] [D loss: 0.6847035884857178] [G loss: 0.7525272965431213]\n",
      "[Epoch 70/1001] [Batch 342/372] [D loss: 0.6941474676132202] [G loss: 0.6145192384719849]\n",
      "[Epoch 70/1001] [Batch 343/372] [D loss: 0.6868287324905396] [G loss: 0.7175959944725037]\n",
      "[Epoch 70/1001] [Batch 344/372] [D loss: 0.6842635869979858] [G loss: 0.6176961064338684]\n",
      "[Epoch 70/1001] [Batch 345/372] [D loss: 0.6826597452163696] [G loss: 0.7646360397338867]\n",
      "[Epoch 70/1001] [Batch 346/372] [D loss: 0.6918151378631592] [G loss: 0.5828925371170044]\n",
      "[Epoch 70/1001] [Batch 347/372] [D loss: 0.6893024444580078] [G loss: 0.7721182703971863]\n",
      "[Epoch 70/1001] [Batch 348/372] [D loss: 0.6817598342895508] [G loss: 0.5932233929634094]\n",
      "[Epoch 70/1001] [Batch 349/372] [D loss: 0.6819242238998413] [G loss: 0.7781676054000854]\n",
      "[Epoch 70/1001] [Batch 350/372] [D loss: 0.6859341859817505] [G loss: 0.5875848531723022]\n",
      "[Epoch 70/1001] [Batch 351/372] [D loss: 0.6858050227165222] [G loss: 0.7638666033744812]\n",
      "[Epoch 70/1001] [Batch 352/372] [D loss: 0.6911810636520386] [G loss: 0.5837135314941406]\n",
      "[Epoch 70/1001] [Batch 353/372] [D loss: 0.692484974861145] [G loss: 0.7479515075683594]\n",
      "[Epoch 70/1001] [Batch 354/372] [D loss: 0.6911269426345825] [G loss: 0.6037459969520569]\n",
      "[Epoch 70/1001] [Batch 355/372] [D loss: 0.6878919005393982] [G loss: 0.7725409269332886]\n",
      "[Epoch 70/1001] [Batch 356/372] [D loss: 0.6934921741485596] [G loss: 0.5711272358894348]\n",
      "[Epoch 70/1001] [Batch 357/372] [D loss: 0.6950713992118835] [G loss: 0.8087967038154602]\n",
      "[Epoch 70/1001] [Batch 358/372] [D loss: 0.6919425129890442] [G loss: 0.5453002452850342]\n",
      "[Epoch 70/1001] [Batch 359/372] [D loss: 0.6990218162536621] [G loss: 0.8576991558074951]\n",
      "[Epoch 70/1001] [Batch 360/372] [D loss: 0.7006218433380127] [G loss: 0.5262086987495422]\n",
      "[Epoch 70/1001] [Batch 361/372] [D loss: 0.7017718553543091] [G loss: 0.8319358229637146]\n",
      "[Epoch 70/1001] [Batch 362/372] [D loss: 0.7022425532341003] [G loss: 0.5857163667678833]\n",
      "[Epoch 70/1001] [Batch 363/372] [D loss: 0.6832598447799683] [G loss: 0.732029139995575]\n",
      "[Epoch 70/1001] [Batch 364/372] [D loss: 0.6949912905693054] [G loss: 0.6426056623458862]\n",
      "[Epoch 70/1001] [Batch 365/372] [D loss: 0.6894320249557495] [G loss: 0.6685409545898438]\n",
      "[Epoch 70/1001] [Batch 366/372] [D loss: 0.6829590797424316] [G loss: 0.7227222919464111]\n",
      "[Epoch 70/1001] [Batch 367/372] [D loss: 0.6901164054870605] [G loss: 0.6155703663825989]\n",
      "[Epoch 70/1001] [Batch 368/372] [D loss: 0.6882419586181641] [G loss: 0.7518270015716553]\n",
      "[Epoch 70/1001] [Batch 369/372] [D loss: 0.6956007480621338] [G loss: 0.5940846800804138]\n",
      "[Epoch 70/1001] [Batch 370/372] [D loss: 0.6811214089393616] [G loss: 0.7754336595535278]\n",
      "[Epoch 70/1001] [Batch 371/372] [D loss: 0.6838579177856445] [G loss: 0.6199758648872375]\n",
      "[Epoch 71/1001] [Batch 0/372] [D loss: 0.6820923089981079] [G loss: 0.725001335144043]\n",
      "[Epoch 71/1001] [Batch 1/372] [D loss: 0.6810726523399353] [G loss: 0.6149870753288269]\n",
      "[Epoch 71/1001] [Batch 2/372] [D loss: 0.682868480682373] [G loss: 0.7162268757820129]\n",
      "[Epoch 71/1001] [Batch 3/372] [D loss: 0.7010541558265686] [G loss: 0.6688496470451355]\n",
      "[Epoch 71/1001] [Batch 4/372] [D loss: 0.6846647262573242] [G loss: 0.6613368391990662]\n",
      "[Epoch 71/1001] [Batch 5/372] [D loss: 0.6871629953384399] [G loss: 0.6856324076652527]\n",
      "[Epoch 71/1001] [Batch 6/372] [D loss: 0.679069995880127] [G loss: 0.6651353240013123]\n",
      "[Epoch 71/1001] [Batch 7/372] [D loss: 0.6746370792388916] [G loss: 0.6946651339530945]\n",
      "[Epoch 71/1001] [Batch 8/372] [D loss: 0.6774018406867981] [G loss: 0.649372935295105]\n",
      "[Epoch 71/1001] [Batch 9/372] [D loss: 0.6819119453430176] [G loss: 0.7134091854095459]\n",
      "[Epoch 71/1001] [Batch 10/372] [D loss: 0.6785194873809814] [G loss: 0.6466703414916992]\n",
      "[Epoch 71/1001] [Batch 11/372] [D loss: 0.6884827017784119] [G loss: 0.6832923889160156]\n",
      "[Epoch 71/1001] [Batch 12/372] [D loss: 0.6841332316398621] [G loss: 0.6445799469947815]\n",
      "[Epoch 71/1001] [Batch 13/372] [D loss: 0.689540445804596] [G loss: 0.7691218852996826]\n",
      "[Epoch 71/1001] [Batch 14/372] [D loss: 0.6935061812400818] [G loss: 0.5374373197555542]\n",
      "[Epoch 71/1001] [Batch 15/372] [D loss: 0.6926625967025757] [G loss: 0.8727232813835144]\n",
      "[Epoch 71/1001] [Batch 16/372] [D loss: 0.6942157745361328] [G loss: 0.5194826722145081]\n",
      "[Epoch 71/1001] [Batch 17/372] [D loss: 0.7036482095718384] [G loss: 0.8606481552124023]\n",
      "[Epoch 71/1001] [Batch 18/372] [D loss: 0.6975016593933105] [G loss: 0.5699195861816406]\n",
      "[Epoch 71/1001] [Batch 19/372] [D loss: 0.6973220705986023] [G loss: 0.7333085536956787]\n",
      "[Epoch 71/1001] [Batch 20/372] [D loss: 0.6834886074066162] [G loss: 0.6675087213516235]\n",
      "[Epoch 71/1001] [Batch 21/372] [D loss: 0.6776854991912842] [G loss: 0.6493926048278809]\n",
      "[Epoch 71/1001] [Batch 22/372] [D loss: 0.6894980669021606] [G loss: 0.7273391485214233]\n",
      "[Epoch 71/1001] [Batch 23/372] [D loss: 0.6859171986579895] [G loss: 0.6182284355163574]\n",
      "[Epoch 71/1001] [Batch 24/372] [D loss: 0.6862169504165649] [G loss: 0.7223893404006958]\n",
      "[Epoch 71/1001] [Batch 25/372] [D loss: 0.6922165155410767] [G loss: 0.6459771394729614]\n",
      "[Epoch 71/1001] [Batch 26/372] [D loss: 0.6849449872970581] [G loss: 0.6852933764457703]\n",
      "[Epoch 71/1001] [Batch 27/372] [D loss: 0.6836754679679871] [G loss: 0.6860965490341187]\n",
      "[Epoch 71/1001] [Batch 28/372] [D loss: 0.6841278076171875] [G loss: 0.6525982618331909]\n",
      "[Epoch 71/1001] [Batch 29/372] [D loss: 0.6857447624206543] [G loss: 0.7075071930885315]\n",
      "[Epoch 71/1001] [Batch 30/372] [D loss: 0.6917110681533813] [G loss: 0.6485754251480103]\n",
      "[Epoch 71/1001] [Batch 31/372] [D loss: 0.6857833862304688] [G loss: 0.6979463696479797]\n",
      "[Epoch 71/1001] [Batch 32/372] [D loss: 0.686767578125] [G loss: 0.6507228016853333]\n",
      "[Epoch 71/1001] [Batch 33/372] [D loss: 0.687479555606842] [G loss: 0.7073264718055725]\n",
      "[Epoch 71/1001] [Batch 34/372] [D loss: 0.6814545392990112] [G loss: 0.6560917496681213]\n",
      "[Epoch 71/1001] [Batch 35/372] [D loss: 0.6867315769195557] [G loss: 0.6743917465209961]\n",
      "[Epoch 71/1001] [Batch 36/372] [D loss: 0.6873989105224609] [G loss: 0.6996639370918274]\n",
      "[Epoch 71/1001] [Batch 37/372] [D loss: 0.6812925338745117] [G loss: 0.659804105758667]\n",
      "[Epoch 71/1001] [Batch 38/372] [D loss: 0.6810990571975708] [G loss: 0.6897638440132141]\n",
      "[Epoch 71/1001] [Batch 39/372] [D loss: 0.6758185625076294] [G loss: 0.6546308994293213]\n",
      "[Epoch 71/1001] [Batch 40/372] [D loss: 0.6800291538238525] [G loss: 0.7089731693267822]\n",
      "[Epoch 71/1001] [Batch 41/372] [D loss: 0.6930042505264282] [G loss: 0.64598548412323]\n",
      "[Epoch 71/1001] [Batch 42/372] [D loss: 0.6772579550743103] [G loss: 0.706017792224884]\n",
      "[Epoch 71/1001] [Batch 43/372] [D loss: 0.6867587566375732] [G loss: 0.6500030159950256]\n",
      "[Epoch 71/1001] [Batch 44/372] [D loss: 0.688578724861145] [G loss: 0.6944329738616943]\n",
      "[Epoch 71/1001] [Batch 45/372] [D loss: 0.6862819194793701] [G loss: 0.6535791158676147]\n",
      "[Epoch 71/1001] [Batch 46/372] [D loss: 0.6929100155830383] [G loss: 0.6921266913414001]\n",
      "[Epoch 71/1001] [Batch 47/372] [D loss: 0.6793990135192871] [G loss: 0.6722084283828735]\n",
      "[Epoch 71/1001] [Batch 48/372] [D loss: 0.6802157163619995] [G loss: 0.6785356998443604]\n",
      "[Epoch 71/1001] [Batch 49/372] [D loss: 0.6886199712753296] [G loss: 0.6752253770828247]\n",
      "[Epoch 71/1001] [Batch 50/372] [D loss: 0.6874194145202637] [G loss: 0.6437716484069824]\n",
      "[Epoch 71/1001] [Batch 51/372] [D loss: 0.6845088005065918] [G loss: 0.6998004913330078]\n",
      "[Epoch 71/1001] [Batch 52/372] [D loss: 0.6849969625473022] [G loss: 0.6765680909156799]\n",
      "[Epoch 71/1001] [Batch 53/372] [D loss: 0.6791999340057373] [G loss: 0.6642982959747314]\n",
      "[Epoch 71/1001] [Batch 54/372] [D loss: 0.6827689409255981] [G loss: 0.6731662154197693]\n",
      "[Epoch 71/1001] [Batch 55/372] [D loss: 0.6878551244735718] [G loss: 0.6755177974700928]\n",
      "[Epoch 71/1001] [Batch 56/372] [D loss: 0.6841983199119568] [G loss: 0.6666029095649719]\n",
      "[Epoch 71/1001] [Batch 57/372] [D loss: 0.6839656829833984] [G loss: 0.6737858653068542]\n",
      "[Epoch 71/1001] [Batch 58/372] [D loss: 0.6991785168647766] [G loss: 0.6823443174362183]\n",
      "[Epoch 71/1001] [Batch 59/372] [D loss: 0.6772564649581909] [G loss: 0.6733810901641846]\n",
      "[Epoch 71/1001] [Batch 60/372] [D loss: 0.687021017074585] [G loss: 0.6949989199638367]\n",
      "[Epoch 71/1001] [Batch 61/372] [D loss: 0.6930235028266907] [G loss: 0.6385812759399414]\n",
      "[Epoch 71/1001] [Batch 62/372] [D loss: 0.6766968369483948] [G loss: 0.7055468559265137]\n",
      "[Epoch 71/1001] [Batch 63/372] [D loss: 0.6838075518608093] [G loss: 0.6495100855827332]\n",
      "[Epoch 71/1001] [Batch 64/372] [D loss: 0.6816198825836182] [G loss: 0.7005627155303955]\n",
      "[Epoch 71/1001] [Batch 65/372] [D loss: 0.6863676309585571] [G loss: 0.6555821895599365]\n",
      "[Epoch 71/1001] [Batch 66/372] [D loss: 0.6881629228591919] [G loss: 0.6714178919792175]\n",
      "[Epoch 71/1001] [Batch 67/372] [D loss: 0.6815792322158813] [G loss: 0.6857134103775024]\n",
      "[Epoch 71/1001] [Batch 68/372] [D loss: 0.6835700273513794] [G loss: 0.6655483245849609]\n",
      "[Epoch 71/1001] [Batch 69/372] [D loss: 0.6929718255996704] [G loss: 0.64662766456604]\n",
      "[Epoch 71/1001] [Batch 70/372] [D loss: 0.6889339089393616] [G loss: 0.7349840402603149]\n",
      "[Epoch 71/1001] [Batch 71/372] [D loss: 0.6899929642677307] [G loss: 0.5659269094467163]\n",
      "[Epoch 71/1001] [Batch 72/372] [D loss: 0.6892500519752502] [G loss: 0.856961727142334]\n",
      "[Epoch 71/1001] [Batch 73/372] [D loss: 0.692001223564148] [G loss: 0.49350348114967346]\n",
      "[Epoch 71/1001] [Batch 74/372] [D loss: 0.7166599035263062] [G loss: 0.9666138887405396]\n",
      "[Epoch 71/1001] [Batch 75/372] [D loss: 0.710716724395752] [G loss: 0.45125842094421387]\n",
      "[Epoch 71/1001] [Batch 76/372] [D loss: 0.7185379266738892] [G loss: 0.9192688465118408]\n",
      "[Epoch 71/1001] [Batch 77/372] [D loss: 0.7132396101951599] [G loss: 0.576650083065033]\n",
      "[Epoch 71/1001] [Batch 78/372] [D loss: 0.6973258256912231] [G loss: 0.7222108244895935]\n",
      "[Epoch 71/1001] [Batch 79/372] [D loss: 0.6805745959281921] [G loss: 0.645426869392395]\n",
      "[Epoch 71/1001] [Batch 80/372] [D loss: 0.6775735020637512] [G loss: 0.6873569488525391]\n",
      "[Epoch 71/1001] [Batch 81/372] [D loss: 0.6840592622756958] [G loss: 0.6999070048332214]\n",
      "[Epoch 71/1001] [Batch 82/372] [D loss: 0.6771863102912903] [G loss: 0.6346874833106995]\n",
      "[Epoch 71/1001] [Batch 83/372] [D loss: 0.6862671971321106] [G loss: 0.728560745716095]\n",
      "[Epoch 71/1001] [Batch 84/372] [D loss: 0.6962987780570984] [G loss: 0.5881043672561646]\n",
      "[Epoch 71/1001] [Batch 85/372] [D loss: 0.6856910586357117] [G loss: 0.8074840307235718]\n",
      "[Epoch 71/1001] [Batch 86/372] [D loss: 0.6910039186477661] [G loss: 0.5267968773841858]\n",
      "[Epoch 71/1001] [Batch 87/372] [D loss: 0.7013112902641296] [G loss: 0.8375901579856873]\n",
      "[Epoch 71/1001] [Batch 88/372] [D loss: 0.6866284608840942] [G loss: 0.5360115170478821]\n",
      "[Epoch 71/1001] [Batch 89/372] [D loss: 0.6954299211502075] [G loss: 0.812980592250824]\n",
      "[Epoch 71/1001] [Batch 90/372] [D loss: 0.7000982761383057] [G loss: 0.5444161891937256]\n",
      "[Epoch 71/1001] [Batch 91/372] [D loss: 0.6989312171936035] [G loss: 0.8330755233764648]\n",
      "[Epoch 71/1001] [Batch 92/372] [D loss: 0.6946716904640198] [G loss: 0.5575040578842163]\n",
      "[Epoch 71/1001] [Batch 93/372] [D loss: 0.6927054524421692] [G loss: 0.7984806895256042]\n",
      "[Epoch 71/1001] [Batch 94/372] [D loss: 0.6899718642234802] [G loss: 0.6103487610816956]\n",
      "[Epoch 71/1001] [Batch 95/372] [D loss: 0.68869549036026] [G loss: 0.6911599040031433]\n",
      "[Epoch 71/1001] [Batch 96/372] [D loss: 0.6785253286361694] [G loss: 0.6900038123130798]\n",
      "[Epoch 71/1001] [Batch 97/372] [D loss: 0.6851159334182739] [G loss: 0.6560487151145935]\n",
      "[Epoch 71/1001] [Batch 98/372] [D loss: 0.6925486922264099] [G loss: 0.7151672840118408]\n",
      "[Epoch 71/1001] [Batch 99/372] [D loss: 0.6781887412071228] [G loss: 0.6151554584503174]\n",
      "[Epoch 71/1001] [Batch 100/372] [D loss: 0.6840031147003174] [G loss: 0.7497367262840271]\n",
      "[Epoch 71/1001] [Batch 101/372] [D loss: 0.682839035987854] [G loss: 0.609969973564148]\n",
      "[Epoch 71/1001] [Batch 102/372] [D loss: 0.6859567165374756] [G loss: 0.7583522200584412]\n",
      "[Epoch 71/1001] [Batch 103/372] [D loss: 0.689297616481781] [G loss: 0.6058982014656067]\n",
      "[Epoch 71/1001] [Batch 104/372] [D loss: 0.6834943294525146] [G loss: 0.7457929849624634]\n",
      "[Epoch 71/1001] [Batch 105/372] [D loss: 0.6908024549484253] [G loss: 0.63361656665802]\n",
      "[Epoch 71/1001] [Batch 106/372] [D loss: 0.6943784356117249] [G loss: 0.6726204752922058]\n",
      "[Epoch 71/1001] [Batch 107/372] [D loss: 0.6843298673629761] [G loss: 0.68354731798172]\n",
      "[Epoch 71/1001] [Batch 108/372] [D loss: 0.687247633934021] [G loss: 0.652070164680481]\n",
      "[Epoch 71/1001] [Batch 109/372] [D loss: 0.6803152561187744] [G loss: 0.7051280736923218]\n",
      "[Epoch 71/1001] [Batch 110/372] [D loss: 0.6871814131736755] [G loss: 0.6918115615844727]\n",
      "[Epoch 71/1001] [Batch 111/372] [D loss: 0.6834791302680969] [G loss: 0.6381828188896179]\n",
      "[Epoch 71/1001] [Batch 112/372] [D loss: 0.6841059923171997] [G loss: 0.7342016696929932]\n",
      "[Epoch 71/1001] [Batch 113/372] [D loss: 0.6855220794677734] [G loss: 0.6454310417175293]\n",
      "[Epoch 71/1001] [Batch 114/372] [D loss: 0.6894547939300537] [G loss: 0.6789387464523315]\n",
      "[Epoch 71/1001] [Batch 115/372] [D loss: 0.6829124093055725] [G loss: 0.6857829093933105]\n",
      "[Epoch 71/1001] [Batch 116/372] [D loss: 0.6815445423126221] [G loss: 0.662765622138977]\n",
      "[Epoch 71/1001] [Batch 117/372] [D loss: 0.6763759255409241] [G loss: 0.6903694272041321]\n",
      "[Epoch 71/1001] [Batch 118/372] [D loss: 0.6882396936416626] [G loss: 0.6539776921272278]\n",
      "[Epoch 71/1001] [Batch 119/372] [D loss: 0.6778658628463745] [G loss: 0.6844273805618286]\n",
      "[Epoch 71/1001] [Batch 120/372] [D loss: 0.6895433664321899] [G loss: 0.6766312718391418]\n",
      "[Epoch 71/1001] [Batch 121/372] [D loss: 0.6785188913345337] [G loss: 0.6815598011016846]\n",
      "[Epoch 71/1001] [Batch 122/372] [D loss: 0.6863264441490173] [G loss: 0.6409693956375122]\n",
      "[Epoch 71/1001] [Batch 123/372] [D loss: 0.6866723299026489] [G loss: 0.7179306149482727]\n",
      "[Epoch 71/1001] [Batch 124/372] [D loss: 0.6885906457901001] [G loss: 0.629393458366394]\n",
      "[Epoch 71/1001] [Batch 125/372] [D loss: 0.6883730888366699] [G loss: 0.6611036658287048]\n",
      "[Epoch 71/1001] [Batch 126/372] [D loss: 0.6871917247772217] [G loss: 0.6906131505966187]\n",
      "[Epoch 71/1001] [Batch 127/372] [D loss: 0.6839283108711243] [G loss: 0.6308180093765259]\n",
      "[Epoch 71/1001] [Batch 128/372] [D loss: 0.683141827583313] [G loss: 0.7181821465492249]\n",
      "[Epoch 71/1001] [Batch 129/372] [D loss: 0.6834750175476074] [G loss: 0.614922285079956]\n",
      "[Epoch 71/1001] [Batch 130/372] [D loss: 0.6862702965736389] [G loss: 0.7603948712348938]\n",
      "[Epoch 71/1001] [Batch 131/372] [D loss: 0.6822851896286011] [G loss: 0.5609359741210938]\n",
      "[Epoch 71/1001] [Batch 132/372] [D loss: 0.6783941984176636] [G loss: 0.8709008097648621]\n",
      "[Epoch 71/1001] [Batch 133/372] [D loss: 0.6927700042724609] [G loss: 0.4850088059902191]\n",
      "[Epoch 71/1001] [Batch 134/372] [D loss: 0.7026215195655823] [G loss: 0.9314321875572205]\n",
      "[Epoch 71/1001] [Batch 135/372] [D loss: 0.7163106203079224] [G loss: 0.5015020370483398]\n",
      "[Epoch 71/1001] [Batch 136/372] [D loss: 0.6993645429611206] [G loss: 0.7819649577140808]\n",
      "[Epoch 71/1001] [Batch 137/372] [D loss: 0.6904911994934082] [G loss: 0.6424498558044434]\n",
      "[Epoch 71/1001] [Batch 138/372] [D loss: 0.689998984336853] [G loss: 0.6579684019088745]\n",
      "[Epoch 71/1001] [Batch 139/372] [D loss: 0.6907510757446289] [G loss: 0.7084805965423584]\n",
      "[Epoch 71/1001] [Batch 140/372] [D loss: 0.6845372319221497] [G loss: 0.6341668367385864]\n",
      "[Epoch 71/1001] [Batch 141/372] [D loss: 0.6810649037361145] [G loss: 0.7332678437232971]\n",
      "[Epoch 71/1001] [Batch 142/372] [D loss: 0.6850089430809021] [G loss: 0.6407038569450378]\n",
      "[Epoch 71/1001] [Batch 143/372] [D loss: 0.6837964653968811] [G loss: 0.7340661287307739]\n",
      "[Epoch 71/1001] [Batch 144/372] [D loss: 0.6846855878829956] [G loss: 0.568159282207489]\n",
      "[Epoch 71/1001] [Batch 145/372] [D loss: 0.6968666315078735] [G loss: 0.8391916155815125]\n",
      "[Epoch 71/1001] [Batch 146/372] [D loss: 0.6916850805282593] [G loss: 0.5458461046218872]\n",
      "[Epoch 71/1001] [Batch 147/372] [D loss: 0.7016087770462036] [G loss: 0.7532045245170593]\n",
      "[Epoch 71/1001] [Batch 148/372] [D loss: 0.6905601024627686] [G loss: 0.6643780469894409]\n",
      "[Epoch 71/1001] [Batch 149/372] [D loss: 0.6816046237945557] [G loss: 0.652463436126709]\n",
      "[Epoch 71/1001] [Batch 150/372] [D loss: 0.6855484843254089] [G loss: 0.6887847185134888]\n",
      "[Epoch 71/1001] [Batch 151/372] [D loss: 0.6875795125961304] [G loss: 0.6775709390640259]\n",
      "[Epoch 71/1001] [Batch 152/372] [D loss: 0.6840662956237793] [G loss: 0.6532825231552124]\n",
      "[Epoch 71/1001] [Batch 153/372] [D loss: 0.6752447485923767] [G loss: 0.7051922082901001]\n",
      "[Epoch 71/1001] [Batch 154/372] [D loss: 0.6943385004997253] [G loss: 0.6384023427963257]\n",
      "[Epoch 71/1001] [Batch 155/372] [D loss: 0.6810606718063354] [G loss: 0.7137730121612549]\n",
      "[Epoch 71/1001] [Batch 156/372] [D loss: 0.6821019053459167] [G loss: 0.6418828368186951]\n",
      "[Epoch 71/1001] [Batch 157/372] [D loss: 0.6850818395614624] [G loss: 0.6907175183296204]\n",
      "[Epoch 71/1001] [Batch 158/372] [D loss: 0.6837526559829712] [G loss: 0.67010897397995]\n",
      "[Epoch 71/1001] [Batch 159/372] [D loss: 0.6876193284988403] [G loss: 0.6916640400886536]\n",
      "[Epoch 71/1001] [Batch 160/372] [D loss: 0.6891599893569946] [G loss: 0.6460860967636108]\n",
      "[Epoch 71/1001] [Batch 161/372] [D loss: 0.6883870363235474] [G loss: 0.6931071281433105]\n",
      "[Epoch 71/1001] [Batch 162/372] [D loss: 0.6885539293289185] [G loss: 0.6379350423812866]\n",
      "[Epoch 71/1001] [Batch 163/372] [D loss: 0.6838528513908386] [G loss: 0.7128771543502808]\n",
      "[Epoch 71/1001] [Batch 164/372] [D loss: 0.6848527193069458] [G loss: 0.6130040884017944]\n",
      "[Epoch 71/1001] [Batch 165/372] [D loss: 0.6812509298324585] [G loss: 0.7582181096076965]\n",
      "[Epoch 71/1001] [Batch 166/372] [D loss: 0.6923599243164062] [G loss: 0.6095399260520935]\n",
      "[Epoch 71/1001] [Batch 167/372] [D loss: 0.6871014833450317] [G loss: 0.730679452419281]\n",
      "[Epoch 71/1001] [Batch 168/372] [D loss: 0.6890800595283508] [G loss: 0.6545877456665039]\n",
      "[Epoch 71/1001] [Batch 169/372] [D loss: 0.6858391761779785] [G loss: 0.6419074535369873]\n",
      "[Epoch 71/1001] [Batch 170/372] [D loss: 0.6733663082122803] [G loss: 0.7526293992996216]\n",
      "[Epoch 71/1001] [Batch 171/372] [D loss: 0.686829686164856] [G loss: 0.6009171009063721]\n",
      "[Epoch 71/1001] [Batch 172/372] [D loss: 0.6824408769607544] [G loss: 0.7443777918815613]\n",
      "[Epoch 71/1001] [Batch 173/372] [D loss: 0.6807653903961182] [G loss: 0.6009565591812134]\n",
      "[Epoch 71/1001] [Batch 174/372] [D loss: 0.6936283111572266] [G loss: 0.7704868912696838]\n",
      "[Epoch 71/1001] [Batch 175/372] [D loss: 0.6902304887771606] [G loss: 0.5747281312942505]\n",
      "[Epoch 71/1001] [Batch 176/372] [D loss: 0.6860433220863342] [G loss: 0.7628862261772156]\n",
      "[Epoch 71/1001] [Batch 177/372] [D loss: 0.6898829936981201] [G loss: 0.5990025997161865]\n",
      "[Epoch 71/1001] [Batch 178/372] [D loss: 0.6985499858856201] [G loss: 0.7598482966423035]\n",
      "[Epoch 71/1001] [Batch 179/372] [D loss: 0.6905584931373596] [G loss: 0.5807010531425476]\n",
      "[Epoch 71/1001] [Batch 180/372] [D loss: 0.6920362710952759] [G loss: 0.7536692023277283]\n",
      "[Epoch 71/1001] [Batch 181/372] [D loss: 0.6833614110946655] [G loss: 0.6048825979232788]\n",
      "[Epoch 71/1001] [Batch 182/372] [D loss: 0.6821669340133667] [G loss: 0.7284936904907227]\n",
      "[Epoch 71/1001] [Batch 183/372] [D loss: 0.6862627863883972] [G loss: 0.6301779747009277]\n",
      "[Epoch 71/1001] [Batch 184/372] [D loss: 0.6771589517593384] [G loss: 0.7390086054801941]\n",
      "[Epoch 71/1001] [Batch 185/372] [D loss: 0.677280068397522] [G loss: 0.6322604417800903]\n",
      "[Epoch 71/1001] [Batch 186/372] [D loss: 0.681509792804718] [G loss: 0.7390981316566467]\n",
      "[Epoch 71/1001] [Batch 187/372] [D loss: 0.688376784324646] [G loss: 0.6175788640975952]\n",
      "[Epoch 71/1001] [Batch 188/372] [D loss: 0.689121425151825] [G loss: 0.6986448168754578]\n",
      "[Epoch 71/1001] [Batch 189/372] [D loss: 0.6764518022537231] [G loss: 0.6813209652900696]\n",
      "[Epoch 71/1001] [Batch 190/372] [D loss: 0.6819964051246643] [G loss: 0.6727409958839417]\n",
      "[Epoch 71/1001] [Batch 191/372] [D loss: 0.6710993051528931] [G loss: 0.6642059087753296]\n",
      "[Epoch 71/1001] [Batch 192/372] [D loss: 0.6978846788406372] [G loss: 0.6792510747909546]\n",
      "[Epoch 71/1001] [Batch 193/372] [D loss: 0.6782212257385254] [G loss: 0.627777636051178]\n",
      "[Epoch 71/1001] [Batch 194/372] [D loss: 0.6852063536643982] [G loss: 0.737331748008728]\n",
      "[Epoch 71/1001] [Batch 195/372] [D loss: 0.6860345005989075] [G loss: 0.5850014686584473]\n",
      "[Epoch 71/1001] [Batch 196/372] [D loss: 0.6895890235900879] [G loss: 0.8398751616477966]\n",
      "[Epoch 71/1001] [Batch 197/372] [D loss: 0.707983672618866] [G loss: 0.4580068588256836]\n",
      "[Epoch 71/1001] [Batch 198/372] [D loss: 0.7184070944786072] [G loss: 0.9955540299415588]\n",
      "[Epoch 71/1001] [Batch 199/372] [D loss: 0.717991054058075] [G loss: 0.4717053472995758]\n",
      "[Epoch 71/1001] [Batch 200/372] [D loss: 0.7148532271385193] [G loss: 0.8627961874008179]\n",
      "[Epoch 71/1001] [Batch 201/372] [D loss: 0.7030975818634033] [G loss: 0.6037414073944092]\n",
      "[Epoch 71/1001] [Batch 202/372] [D loss: 0.6839901208877563] [G loss: 0.664645791053772]\n",
      "[Epoch 71/1001] [Batch 203/372] [D loss: 0.6813044548034668] [G loss: 0.7290136218070984]\n",
      "[Epoch 71/1001] [Batch 204/372] [D loss: 0.6877318620681763] [G loss: 0.6360854506492615]\n",
      "[Epoch 71/1001] [Batch 205/372] [D loss: 0.6821272373199463] [G loss: 0.6889336109161377]\n",
      "[Epoch 71/1001] [Batch 206/372] [D loss: 0.6839048266410828] [G loss: 0.6842840909957886]\n",
      "[Epoch 71/1001] [Batch 207/372] [D loss: 0.6767512559890747] [G loss: 0.6723240613937378]\n",
      "[Epoch 71/1001] [Batch 208/372] [D loss: 0.68299400806427] [G loss: 0.695866584777832]\n",
      "[Epoch 71/1001] [Batch 209/372] [D loss: 0.6833401918411255] [G loss: 0.641442596912384]\n",
      "[Epoch 71/1001] [Batch 210/372] [D loss: 0.6909234523773193] [G loss: 0.7022775411605835]\n",
      "[Epoch 71/1001] [Batch 211/372] [D loss: 0.6824498176574707] [G loss: 0.6380268335342407]\n",
      "[Epoch 71/1001] [Batch 212/372] [D loss: 0.6880500316619873] [G loss: 0.7149575352668762]\n",
      "[Epoch 71/1001] [Batch 213/372] [D loss: 0.6835401058197021] [G loss: 0.6490945816040039]\n",
      "[Epoch 71/1001] [Batch 214/372] [D loss: 0.6780421733856201] [G loss: 0.6828222274780273]\n",
      "[Epoch 71/1001] [Batch 215/372] [D loss: 0.6943759918212891] [G loss: 0.6661550998687744]\n",
      "[Epoch 71/1001] [Batch 216/372] [D loss: 0.6834187507629395] [G loss: 0.6517384052276611]\n",
      "[Epoch 71/1001] [Batch 217/372] [D loss: 0.6868163347244263] [G loss: 0.7242844700813293]\n",
      "[Epoch 71/1001] [Batch 218/372] [D loss: 0.6789254546165466] [G loss: 0.6051218509674072]\n",
      "[Epoch 71/1001] [Batch 219/372] [D loss: 0.698223352432251] [G loss: 0.7431917786598206]\n",
      "[Epoch 71/1001] [Batch 220/372] [D loss: 0.6738923788070679] [G loss: 0.6259381771087646]\n",
      "[Epoch 71/1001] [Batch 221/372] [D loss: 0.6846521496772766] [G loss: 0.7331581711769104]\n",
      "[Epoch 71/1001] [Batch 222/372] [D loss: 0.6854801774024963] [G loss: 0.6417206525802612]\n",
      "[Epoch 71/1001] [Batch 223/372] [D loss: 0.6870012283325195] [G loss: 0.6825636029243469]\n",
      "[Epoch 71/1001] [Batch 224/372] [D loss: 0.6825627088546753] [G loss: 0.6636051535606384]\n",
      "[Epoch 71/1001] [Batch 225/372] [D loss: 0.683477520942688] [G loss: 0.6880790591239929]\n",
      "[Epoch 71/1001] [Batch 226/372] [D loss: 0.6903501749038696] [G loss: 0.6561396718025208]\n",
      "[Epoch 71/1001] [Batch 227/372] [D loss: 0.6796544790267944] [G loss: 0.7106146216392517]\n",
      "[Epoch 71/1001] [Batch 228/372] [D loss: 0.6828006505966187] [G loss: 0.6269537806510925]\n",
      "[Epoch 71/1001] [Batch 229/372] [D loss: 0.6800669431686401] [G loss: 0.7411911487579346]\n",
      "[Epoch 71/1001] [Batch 230/372] [D loss: 0.6917723417282104] [G loss: 0.6009146571159363]\n",
      "[Epoch 71/1001] [Batch 231/372] [D loss: 0.688758134841919] [G loss: 0.729077160358429]\n",
      "[Epoch 71/1001] [Batch 232/372] [D loss: 0.6725233793258667] [G loss: 0.642128586769104]\n",
      "[Epoch 71/1001] [Batch 233/372] [D loss: 0.6767754554748535] [G loss: 0.7184102535247803]\n",
      "[Epoch 71/1001] [Batch 234/372] [D loss: 0.6804783940315247] [G loss: 0.6444472074508667]\n",
      "[Epoch 71/1001] [Batch 235/372] [D loss: 0.6888591647148132] [G loss: 0.6722069978713989]\n",
      "[Epoch 71/1001] [Batch 236/372] [D loss: 0.6959902048110962] [G loss: 0.6958657503128052]\n",
      "[Epoch 71/1001] [Batch 237/372] [D loss: 0.6787595748901367] [G loss: 0.6342043876647949]\n",
      "[Epoch 71/1001] [Batch 238/372] [D loss: 0.684046745300293] [G loss: 0.722808837890625]\n",
      "[Epoch 71/1001] [Batch 239/372] [D loss: 0.6776652336120605] [G loss: 0.5999389886856079]\n",
      "[Epoch 71/1001] [Batch 240/372] [D loss: 0.6887407302856445] [G loss: 0.8492218255996704]\n",
      "[Epoch 71/1001] [Batch 241/372] [D loss: 0.6875321865081787] [G loss: 0.5116199254989624]\n",
      "[Epoch 71/1001] [Batch 242/372] [D loss: 0.7010306715965271] [G loss: 0.8533141613006592]\n",
      "[Epoch 71/1001] [Batch 243/372] [D loss: 0.6968817114830017] [G loss: 0.5396872758865356]\n",
      "[Epoch 71/1001] [Batch 244/372] [D loss: 0.7060654163360596] [G loss: 0.7722713947296143]\n",
      "[Epoch 71/1001] [Batch 245/372] [D loss: 0.6889272332191467] [G loss: 0.6046475172042847]\n",
      "[Epoch 71/1001] [Batch 246/372] [D loss: 0.6876046061515808] [G loss: 0.7046592235565186]\n",
      "[Epoch 71/1001] [Batch 247/372] [D loss: 0.6867176294326782] [G loss: 0.6577898263931274]\n",
      "[Epoch 71/1001] [Batch 248/372] [D loss: 0.688888430595398] [G loss: 0.676947832107544]\n",
      "[Epoch 71/1001] [Batch 249/372] [D loss: 0.6775686144828796] [G loss: 0.6808840036392212]\n",
      "[Epoch 71/1001] [Batch 250/372] [D loss: 0.6905545592308044] [G loss: 0.6496669054031372]\n",
      "[Epoch 71/1001] [Batch 251/372] [D loss: 0.6820535659790039] [G loss: 0.7281192541122437]\n",
      "[Epoch 71/1001] [Batch 252/372] [D loss: 0.6869539618492126] [G loss: 0.6044591665267944]\n",
      "[Epoch 71/1001] [Batch 253/372] [D loss: 0.696147084236145] [G loss: 0.743956446647644]\n",
      "[Epoch 71/1001] [Batch 254/372] [D loss: 0.6906929016113281] [G loss: 0.592814028263092]\n",
      "[Epoch 71/1001] [Batch 255/372] [D loss: 0.679500937461853] [G loss: 0.7655384540557861]\n",
      "[Epoch 71/1001] [Batch 256/372] [D loss: 0.6845905780792236] [G loss: 0.6069858074188232]\n",
      "[Epoch 71/1001] [Batch 257/372] [D loss: 0.6943693161010742] [G loss: 0.7142999172210693]\n",
      "[Epoch 71/1001] [Batch 258/372] [D loss: 0.6882558465003967] [G loss: 0.6334111094474792]\n",
      "[Epoch 71/1001] [Batch 259/372] [D loss: 0.6805775165557861] [G loss: 0.7106447219848633]\n",
      "[Epoch 71/1001] [Batch 260/372] [D loss: 0.6803996562957764] [G loss: 0.6483661532402039]\n",
      "[Epoch 71/1001] [Batch 261/372] [D loss: 0.6843246221542358] [G loss: 0.6989144682884216]\n",
      "[Epoch 71/1001] [Batch 262/372] [D loss: 0.6842402219772339] [G loss: 0.6591687798500061]\n",
      "[Epoch 71/1001] [Batch 263/372] [D loss: 0.6817588210105896] [G loss: 0.6783007383346558]\n",
      "[Epoch 71/1001] [Batch 264/372] [D loss: 0.6802933812141418] [G loss: 0.706770658493042]\n",
      "[Epoch 71/1001] [Batch 265/372] [D loss: 0.6846923828125] [G loss: 0.6163159608840942]\n",
      "[Epoch 71/1001] [Batch 266/372] [D loss: 0.679341197013855] [G loss: 0.7481549382209778]\n",
      "[Epoch 71/1001] [Batch 267/372] [D loss: 0.6837785243988037] [G loss: 0.5846085548400879]\n",
      "[Epoch 71/1001] [Batch 268/372] [D loss: 0.6830753684043884] [G loss: 0.8144331574440002]\n",
      "[Epoch 71/1001] [Batch 269/372] [D loss: 0.6853219270706177] [G loss: 0.5406625866889954]\n",
      "[Epoch 71/1001] [Batch 270/372] [D loss: 0.691830039024353] [G loss: 0.8539541363716125]\n",
      "[Epoch 71/1001] [Batch 271/372] [D loss: 0.7059613466262817] [G loss: 0.4996545612812042]\n",
      "[Epoch 71/1001] [Batch 272/372] [D loss: 0.7026596665382385] [G loss: 0.9829393625259399]\n",
      "[Epoch 71/1001] [Batch 273/372] [D loss: 0.7217832803726196] [G loss: 0.4489910900592804]\n",
      "[Epoch 71/1001] [Batch 274/372] [D loss: 0.7187268733978271] [G loss: 0.8724278807640076]\n",
      "[Epoch 71/1001] [Batch 275/372] [D loss: 0.6975619792938232] [G loss: 0.6129534244537354]\n",
      "[Epoch 71/1001] [Batch 276/372] [D loss: 0.6873676776885986] [G loss: 0.6520918011665344]\n",
      "[Epoch 71/1001] [Batch 277/372] [D loss: 0.6873686909675598] [G loss: 0.737682044506073]\n",
      "[Epoch 71/1001] [Batch 278/372] [D loss: 0.6749297380447388] [G loss: 0.6423795223236084]\n",
      "[Epoch 71/1001] [Batch 279/372] [D loss: 0.6833860874176025] [G loss: 0.7056567072868347]\n",
      "[Epoch 71/1001] [Batch 280/372] [D loss: 0.6848481893539429] [G loss: 0.6579065322875977]\n",
      "[Epoch 71/1001] [Batch 281/372] [D loss: 0.6806062459945679] [G loss: 0.6814801692962646]\n",
      "[Epoch 71/1001] [Batch 282/372] [D loss: 0.6782760620117188] [G loss: 0.6829859614372253]\n",
      "[Epoch 71/1001] [Batch 283/372] [D loss: 0.6767944097518921] [G loss: 0.6513394117355347]\n",
      "[Epoch 71/1001] [Batch 284/372] [D loss: 0.6876779794692993] [G loss: 0.656825065612793]\n",
      "[Epoch 71/1001] [Batch 285/372] [D loss: 0.6788589954376221] [G loss: 0.7103527188301086]\n",
      "[Epoch 71/1001] [Batch 286/372] [D loss: 0.6778848171234131] [G loss: 0.6217908263206482]\n",
      "[Epoch 71/1001] [Batch 287/372] [D loss: 0.6915361881256104] [G loss: 0.7186304330825806]\n",
      "[Epoch 71/1001] [Batch 288/372] [D loss: 0.6793143153190613] [G loss: 0.618007481098175]\n",
      "[Epoch 71/1001] [Batch 289/372] [D loss: 0.6812301874160767] [G loss: 0.7448060512542725]\n",
      "[Epoch 71/1001] [Batch 290/372] [D loss: 0.6929469704627991] [G loss: 0.5979219675064087]\n",
      "[Epoch 71/1001] [Batch 291/372] [D loss: 0.6943222284317017] [G loss: 0.7408552169799805]\n",
      "[Epoch 71/1001] [Batch 292/372] [D loss: 0.7062619924545288] [G loss: 0.5979138612747192]\n",
      "[Epoch 71/1001] [Batch 293/372] [D loss: 0.6876713633537292] [G loss: 0.7165182828903198]\n",
      "[Epoch 71/1001] [Batch 294/372] [D loss: 0.6934627294540405] [G loss: 0.6509800553321838]\n",
      "[Epoch 71/1001] [Batch 295/372] [D loss: 0.6952074766159058] [G loss: 0.6796562075614929]\n",
      "[Epoch 71/1001] [Batch 296/372] [D loss: 0.6801416277885437] [G loss: 0.653049886226654]\n",
      "[Epoch 71/1001] [Batch 297/372] [D loss: 0.6883317232131958] [G loss: 0.7010000944137573]\n",
      "[Epoch 71/1001] [Batch 298/372] [D loss: 0.6904349327087402] [G loss: 0.6590210199356079]\n",
      "[Epoch 71/1001] [Batch 299/372] [D loss: 0.6869351863861084] [G loss: 0.6792472004890442]\n",
      "[Epoch 71/1001] [Batch 300/372] [D loss: 0.6886177062988281] [G loss: 0.6565379500389099]\n",
      "[Epoch 71/1001] [Batch 301/372] [D loss: 0.6874205470085144] [G loss: 0.6773233413696289]\n",
      "[Epoch 71/1001] [Batch 302/372] [D loss: 0.6846218109130859] [G loss: 0.6689828634262085]\n",
      "[Epoch 71/1001] [Batch 303/372] [D loss: 0.679509162902832] [G loss: 0.6797497272491455]\n",
      "[Epoch 71/1001] [Batch 304/372] [D loss: 0.6891819834709167] [G loss: 0.6632249355316162]\n",
      "[Epoch 71/1001] [Batch 305/372] [D loss: 0.6869921088218689] [G loss: 0.6924269199371338]\n",
      "[Epoch 71/1001] [Batch 306/372] [D loss: 0.6806682348251343] [G loss: 0.63796067237854]\n",
      "[Epoch 71/1001] [Batch 307/372] [D loss: 0.6804355978965759] [G loss: 0.7370997071266174]\n",
      "[Epoch 71/1001] [Batch 308/372] [D loss: 0.691827654838562] [G loss: 0.6070102453231812]\n",
      "[Epoch 71/1001] [Batch 309/372] [D loss: 0.6936146020889282] [G loss: 0.7512174248695374]\n",
      "[Epoch 71/1001] [Batch 310/372] [D loss: 0.6928892731666565] [G loss: 0.5623445510864258]\n",
      "[Epoch 71/1001] [Batch 311/372] [D loss: 0.6928166151046753] [G loss: 0.8449596166610718]\n",
      "[Epoch 71/1001] [Batch 312/372] [D loss: 0.7006714344024658] [G loss: 0.5258302092552185]\n",
      "[Epoch 71/1001] [Batch 313/372] [D loss: 0.6989810466766357] [G loss: 0.8480873107910156]\n",
      "[Epoch 71/1001] [Batch 314/372] [D loss: 0.7010684609413147] [G loss: 0.5332221388816833]\n",
      "[Epoch 71/1001] [Batch 315/372] [D loss: 0.6925946474075317] [G loss: 0.8170162439346313]\n",
      "[Epoch 71/1001] [Batch 316/372] [D loss: 0.6967976093292236] [G loss: 0.5743995308876038]\n",
      "[Epoch 71/1001] [Batch 317/372] [D loss: 0.6939390897750854] [G loss: 0.7372099757194519]\n",
      "[Epoch 71/1001] [Batch 318/372] [D loss: 0.6944332718849182] [G loss: 0.6330432891845703]\n",
      "[Epoch 71/1001] [Batch 319/372] [D loss: 0.6887879371643066] [G loss: 0.6817790865898132]\n",
      "[Epoch 71/1001] [Batch 320/372] [D loss: 0.6816521883010864] [G loss: 0.6827532052993774]\n",
      "[Epoch 71/1001] [Batch 321/372] [D loss: 0.684725284576416] [G loss: 0.6626144051551819]\n",
      "[Epoch 71/1001] [Batch 322/372] [D loss: 0.6833469271659851] [G loss: 0.6679348945617676]\n",
      "[Epoch 71/1001] [Batch 323/372] [D loss: 0.6872775554656982] [G loss: 0.709872841835022]\n",
      "[Epoch 71/1001] [Batch 324/372] [D loss: 0.6842584609985352] [G loss: 0.6259960532188416]\n",
      "[Epoch 71/1001] [Batch 325/372] [D loss: 0.6799946427345276] [G loss: 0.7134770750999451]\n",
      "[Epoch 71/1001] [Batch 326/372] [D loss: 0.6908567547798157] [G loss: 0.6104658246040344]\n",
      "[Epoch 71/1001] [Batch 327/372] [D loss: 0.6815611124038696] [G loss: 0.7285200953483582]\n",
      "[Epoch 71/1001] [Batch 328/372] [D loss: 0.7018633484840393] [G loss: 0.6784648299217224]\n",
      "[Epoch 71/1001] [Batch 329/372] [D loss: 0.6878737807273865] [G loss: 0.6294057369232178]\n",
      "[Epoch 71/1001] [Batch 330/372] [D loss: 0.6842302083969116] [G loss: 0.7237929105758667]\n",
      "[Epoch 71/1001] [Batch 331/372] [D loss: 0.6870665550231934] [G loss: 0.6177511215209961]\n",
      "[Epoch 71/1001] [Batch 332/372] [D loss: 0.6892547607421875] [G loss: 0.7233230471611023]\n",
      "[Epoch 71/1001] [Batch 333/372] [D loss: 0.6816065311431885] [G loss: 0.6366087794303894]\n",
      "[Epoch 71/1001] [Batch 334/372] [D loss: 0.6817295551300049] [G loss: 0.7218483686447144]\n",
      "[Epoch 71/1001] [Batch 335/372] [D loss: 0.6856702566146851] [G loss: 0.607338547706604]\n",
      "[Epoch 71/1001] [Batch 336/372] [D loss: 0.6972496509552002] [G loss: 0.7465111613273621]\n",
      "[Epoch 71/1001] [Batch 337/372] [D loss: 0.6964378952980042] [G loss: 0.5760837197303772]\n",
      "[Epoch 71/1001] [Batch 338/372] [D loss: 0.6863299012184143] [G loss: 0.8040722012519836]\n",
      "[Epoch 71/1001] [Batch 339/372] [D loss: 0.6932127475738525] [G loss: 0.5538634061813354]\n",
      "[Epoch 71/1001] [Batch 340/372] [D loss: 0.6998898983001709] [G loss: 0.8039852976799011]\n",
      "[Epoch 71/1001] [Batch 341/372] [D loss: 0.6947742700576782] [G loss: 0.5501240491867065]\n",
      "[Epoch 71/1001] [Batch 342/372] [D loss: 0.6973463296890259] [G loss: 0.7828103303909302]\n",
      "[Epoch 71/1001] [Batch 343/372] [D loss: 0.6828296184539795] [G loss: 0.6061059832572937]\n",
      "[Epoch 71/1001] [Batch 344/372] [D loss: 0.6900056600570679] [G loss: 0.7314682006835938]\n",
      "[Epoch 71/1001] [Batch 345/372] [D loss: 0.6797409057617188] [G loss: 0.6486780643463135]\n",
      "[Epoch 71/1001] [Batch 346/372] [D loss: 0.684796929359436] [G loss: 0.6775795221328735]\n",
      "[Epoch 71/1001] [Batch 347/372] [D loss: 0.6932599544525146] [G loss: 0.6659517288208008]\n",
      "[Epoch 71/1001] [Batch 348/372] [D loss: 0.6805936098098755] [G loss: 0.6951001286506653]\n",
      "[Epoch 71/1001] [Batch 349/372] [D loss: 0.6859556436538696] [G loss: 0.6551246643066406]\n",
      "[Epoch 71/1001] [Batch 350/372] [D loss: 0.6859420537948608] [G loss: 0.6828165650367737]\n",
      "[Epoch 71/1001] [Batch 351/372] [D loss: 0.6848994493484497] [G loss: 0.645599365234375]\n",
      "[Epoch 71/1001] [Batch 352/372] [D loss: 0.6772859692573547] [G loss: 0.7927014827728271]\n",
      "[Epoch 71/1001] [Batch 353/372] [D loss: 0.679743766784668] [G loss: 0.5434786081314087]\n",
      "[Epoch 71/1001] [Batch 354/372] [D loss: 0.6940526962280273] [G loss: 0.8360189199447632]\n",
      "[Epoch 71/1001] [Batch 355/372] [D loss: 0.6899990439414978] [G loss: 0.5367372632026672]\n",
      "[Epoch 71/1001] [Batch 356/372] [D loss: 0.7068723440170288] [G loss: 0.8735283017158508]\n",
      "[Epoch 71/1001] [Batch 357/372] [D loss: 0.7016149759292603] [G loss: 0.4998302757740021]\n",
      "[Epoch 71/1001] [Batch 358/372] [D loss: 0.70975661277771] [G loss: 0.8447969555854797]\n",
      "[Epoch 71/1001] [Batch 359/372] [D loss: 0.7041435837745667] [G loss: 0.5765337944030762]\n",
      "[Epoch 71/1001] [Batch 360/372] [D loss: 0.6907265186309814] [G loss: 0.7228770852088928]\n",
      "[Epoch 71/1001] [Batch 361/372] [D loss: 0.6860494613647461] [G loss: 0.6735633611679077]\n",
      "[Epoch 71/1001] [Batch 362/372] [D loss: 0.6834830045700073] [G loss: 0.6480891108512878]\n",
      "[Epoch 71/1001] [Batch 363/372] [D loss: 0.6756351590156555] [G loss: 0.7309678196907043]\n",
      "[Epoch 71/1001] [Batch 364/372] [D loss: 0.6933256983757019] [G loss: 0.6285699605941772]\n",
      "[Epoch 71/1001] [Batch 365/372] [D loss: 0.6845463514328003] [G loss: 0.7193445563316345]\n",
      "[Epoch 71/1001] [Batch 366/372] [D loss: 0.6826059818267822] [G loss: 0.6408905982971191]\n",
      "[Epoch 71/1001] [Batch 367/372] [D loss: 0.6796643733978271] [G loss: 0.71404629945755]\n",
      "[Epoch 71/1001] [Batch 368/372] [D loss: 0.6933311223983765] [G loss: 0.6434183716773987]\n",
      "[Epoch 71/1001] [Batch 369/372] [D loss: 0.6889613270759583] [G loss: 0.6691827774047852]\n",
      "[Epoch 71/1001] [Batch 370/372] [D loss: 0.6884827017784119] [G loss: 0.7047003507614136]\n",
      "[Epoch 71/1001] [Batch 371/372] [D loss: 0.6802875399589539] [G loss: 0.6379662156105042]\n",
      "[Epoch 72/1001] [Batch 0/372] [D loss: 0.6795971393585205] [G loss: 0.7085369825363159]\n",
      "[Epoch 72/1001] [Batch 1/372] [D loss: 0.6831678152084351] [G loss: 0.6590425372123718]\n",
      "[Epoch 72/1001] [Batch 2/372] [D loss: 0.6732708215713501] [G loss: 0.6921082735061646]\n",
      "[Epoch 72/1001] [Batch 3/372] [D loss: 0.6932871341705322] [G loss: 0.6395567059516907]\n",
      "[Epoch 72/1001] [Batch 4/372] [D loss: 0.679837167263031] [G loss: 0.6916742324829102]\n",
      "[Epoch 72/1001] [Batch 5/372] [D loss: 0.6756339073181152] [G loss: 0.6816527843475342]\n",
      "[Epoch 72/1001] [Batch 6/372] [D loss: 0.6934759616851807] [G loss: 0.6381568312644958]\n",
      "[Epoch 72/1001] [Batch 7/372] [D loss: 0.6857587099075317] [G loss: 0.7543075084686279]\n",
      "[Epoch 72/1001] [Batch 8/372] [D loss: 0.6852547526359558] [G loss: 0.5841223001480103]\n",
      "[Epoch 72/1001] [Batch 9/372] [D loss: 0.6835778951644897] [G loss: 0.8416032791137695]\n",
      "[Epoch 72/1001] [Batch 10/372] [D loss: 0.6818004846572876] [G loss: 0.5219550132751465]\n",
      "[Epoch 72/1001] [Batch 11/372] [D loss: 0.6955186128616333] [G loss: 0.9269797801971436]\n",
      "[Epoch 72/1001] [Batch 12/372] [D loss: 0.7072847485542297] [G loss: 0.5173454284667969]\n",
      "[Epoch 72/1001] [Batch 13/372] [D loss: 0.7051311731338501] [G loss: 0.7868642210960388]\n",
      "[Epoch 72/1001] [Batch 14/372] [D loss: 0.6893961429595947] [G loss: 0.6395982503890991]\n",
      "[Epoch 72/1001] [Batch 15/372] [D loss: 0.6776086091995239] [G loss: 0.6800140738487244]\n",
      "[Epoch 72/1001] [Batch 16/372] [D loss: 0.6766839027404785] [G loss: 0.7140982151031494]\n",
      "[Epoch 72/1001] [Batch 17/372] [D loss: 0.6862306594848633] [G loss: 0.6379656791687012]\n",
      "[Epoch 72/1001] [Batch 18/372] [D loss: 0.6725047826766968] [G loss: 0.7297044992446899]\n",
      "[Epoch 72/1001] [Batch 19/372] [D loss: 0.6845388412475586] [G loss: 0.6176350116729736]\n",
      "[Epoch 72/1001] [Batch 20/372] [D loss: 0.6904219388961792] [G loss: 0.6888366937637329]\n",
      "[Epoch 72/1001] [Batch 21/372] [D loss: 0.6765981912612915] [G loss: 0.6891117691993713]\n",
      "[Epoch 72/1001] [Batch 22/372] [D loss: 0.6748921871185303] [G loss: 0.6551128625869751]\n",
      "[Epoch 72/1001] [Batch 23/372] [D loss: 0.6742568016052246] [G loss: 0.7094229459762573]\n",
      "[Epoch 72/1001] [Batch 24/372] [D loss: 0.6980710029602051] [G loss: 0.5746351480484009]\n",
      "[Epoch 72/1001] [Batch 25/372] [D loss: 0.6889688968658447] [G loss: 0.8501316905021667]\n",
      "[Epoch 72/1001] [Batch 26/372] [D loss: 0.6921309232711792] [G loss: 0.48969027400016785]\n",
      "[Epoch 72/1001] [Batch 27/372] [D loss: 0.7067399024963379] [G loss: 0.9204433560371399]\n",
      "[Epoch 72/1001] [Batch 28/372] [D loss: 0.6975601315498352] [G loss: 0.5114064812660217]\n",
      "[Epoch 72/1001] [Batch 29/372] [D loss: 0.7081019878387451] [G loss: 0.8615672588348389]\n",
      "[Epoch 72/1001] [Batch 30/372] [D loss: 0.6952886581420898] [G loss: 0.6049796938896179]\n",
      "[Epoch 72/1001] [Batch 31/372] [D loss: 0.6844853162765503] [G loss: 0.7005183696746826]\n",
      "[Epoch 72/1001] [Batch 32/372] [D loss: 0.6846714019775391] [G loss: 0.6870257258415222]\n",
      "[Epoch 72/1001] [Batch 33/372] [D loss: 0.683214545249939] [G loss: 0.6312919855117798]\n",
      "[Epoch 72/1001] [Batch 34/372] [D loss: 0.6746926307678223] [G loss: 0.7223523259162903]\n",
      "[Epoch 72/1001] [Batch 35/372] [D loss: 0.6882325410842896] [G loss: 0.6239060759544373]\n",
      "[Epoch 72/1001] [Batch 36/372] [D loss: 0.6803666949272156] [G loss: 0.7180821299552917]\n",
      "[Epoch 72/1001] [Batch 37/372] [D loss: 0.6785962581634521] [G loss: 0.6296728849411011]\n",
      "[Epoch 72/1001] [Batch 38/372] [D loss: 0.6808510422706604] [G loss: 0.7018599510192871]\n",
      "[Epoch 72/1001] [Batch 39/372] [D loss: 0.6832929253578186] [G loss: 0.6349073052406311]\n",
      "[Epoch 72/1001] [Batch 40/372] [D loss: 0.6747337579727173] [G loss: 0.7383445501327515]\n",
      "[Epoch 72/1001] [Batch 41/372] [D loss: 0.6880583763122559] [G loss: 0.6146690249443054]\n",
      "[Epoch 72/1001] [Batch 42/372] [D loss: 0.6832203269004822] [G loss: 0.7247191071510315]\n",
      "[Epoch 72/1001] [Batch 43/372] [D loss: 0.6850213408470154] [G loss: 0.6388863325119019]\n",
      "[Epoch 72/1001] [Batch 44/372] [D loss: 0.6809212565422058] [G loss: 0.702506422996521]\n",
      "[Epoch 72/1001] [Batch 45/372] [D loss: 0.6895641684532166] [G loss: 0.6612052321434021]\n",
      "[Epoch 72/1001] [Batch 46/372] [D loss: 0.678014874458313] [G loss: 0.6662551164627075]\n",
      "[Epoch 72/1001] [Batch 47/372] [D loss: 0.6810362935066223] [G loss: 0.6984869241714478]\n",
      "[Epoch 72/1001] [Batch 48/372] [D loss: 0.681690514087677] [G loss: 0.6470979452133179]\n",
      "[Epoch 72/1001] [Batch 49/372] [D loss: 0.687616765499115] [G loss: 0.7104202508926392]\n",
      "[Epoch 72/1001] [Batch 50/372] [D loss: 0.6783455610275269] [G loss: 0.6279250383377075]\n",
      "[Epoch 72/1001] [Batch 51/372] [D loss: 0.6920573711395264] [G loss: 0.729122519493103]\n",
      "[Epoch 72/1001] [Batch 52/372] [D loss: 0.6882895231246948] [G loss: 0.6312440037727356]\n",
      "[Epoch 72/1001] [Batch 53/372] [D loss: 0.6770230531692505] [G loss: 0.7390573620796204]\n",
      "[Epoch 72/1001] [Batch 54/372] [D loss: 0.6830059289932251] [G loss: 0.6172391772270203]\n",
      "[Epoch 72/1001] [Batch 55/372] [D loss: 0.6806492805480957] [G loss: 0.7434802055358887]\n",
      "[Epoch 72/1001] [Batch 56/372] [D loss: 0.68637615442276] [G loss: 0.5931920409202576]\n",
      "[Epoch 72/1001] [Batch 57/372] [D loss: 0.6890256404876709] [G loss: 0.7653089165687561]\n",
      "[Epoch 72/1001] [Batch 58/372] [D loss: 0.6872690916061401] [G loss: 0.5984429717063904]\n",
      "[Epoch 72/1001] [Batch 59/372] [D loss: 0.6757282018661499] [G loss: 0.7474767565727234]\n",
      "[Epoch 72/1001] [Batch 60/372] [D loss: 0.6854896545410156] [G loss: 0.6218386888504028]\n",
      "[Epoch 72/1001] [Batch 61/372] [D loss: 0.6879044771194458] [G loss: 0.7209997773170471]\n",
      "[Epoch 72/1001] [Batch 62/372] [D loss: 0.6929619312286377] [G loss: 0.6083881855010986]\n",
      "[Epoch 72/1001] [Batch 63/372] [D loss: 0.6798374652862549] [G loss: 0.7620219588279724]\n",
      "[Epoch 72/1001] [Batch 64/372] [D loss: 0.6866821646690369] [G loss: 0.5938350558280945]\n",
      "[Epoch 72/1001] [Batch 65/372] [D loss: 0.6855710744857788] [G loss: 0.7405367493629456]\n",
      "[Epoch 72/1001] [Batch 66/372] [D loss: 0.6842066049575806] [G loss: 0.6043687462806702]\n",
      "[Epoch 72/1001] [Batch 67/372] [D loss: 0.6921983957290649] [G loss: 0.7456333041191101]\n",
      "[Epoch 72/1001] [Batch 68/372] [D loss: 0.6842911243438721] [G loss: 0.6112902760505676]\n",
      "[Epoch 72/1001] [Batch 69/372] [D loss: 0.6860551238059998] [G loss: 0.7134407758712769]\n",
      "[Epoch 72/1001] [Batch 70/372] [D loss: 0.6751323938369751] [G loss: 0.660375714302063]\n",
      "[Epoch 72/1001] [Batch 71/372] [D loss: 0.6897746324539185] [G loss: 0.6902894973754883]\n",
      "[Epoch 72/1001] [Batch 72/372] [D loss: 0.6857020854949951] [G loss: 0.6420041918754578]\n",
      "[Epoch 72/1001] [Batch 73/372] [D loss: 0.6890842914581299] [G loss: 0.7077871561050415]\n",
      "[Epoch 72/1001] [Batch 74/372] [D loss: 0.6815372705459595] [G loss: 0.6208524107933044]\n",
      "[Epoch 72/1001] [Batch 75/372] [D loss: 0.6886047124862671] [G loss: 0.7836601734161377]\n",
      "[Epoch 72/1001] [Batch 76/372] [D loss: 0.695836067199707] [G loss: 0.5408477187156677]\n",
      "[Epoch 72/1001] [Batch 77/372] [D loss: 0.6925597190856934] [G loss: 0.8206086158752441]\n",
      "[Epoch 72/1001] [Batch 78/372] [D loss: 0.6979070901870728] [G loss: 0.5622538328170776]\n",
      "[Epoch 72/1001] [Batch 79/372] [D loss: 0.6887977123260498] [G loss: 0.7680810689926147]\n",
      "[Epoch 72/1001] [Batch 80/372] [D loss: 0.68668532371521] [G loss: 0.6256744861602783]\n",
      "[Epoch 72/1001] [Batch 81/372] [D loss: 0.6846346855163574] [G loss: 0.7136318683624268]\n",
      "[Epoch 72/1001] [Batch 82/372] [D loss: 0.6884134411811829] [G loss: 0.6449562311172485]\n",
      "[Epoch 72/1001] [Batch 83/372] [D loss: 0.6810993552207947] [G loss: 0.6839291453361511]\n",
      "[Epoch 72/1001] [Batch 84/372] [D loss: 0.6797305941581726] [G loss: 0.6755809187889099]\n",
      "[Epoch 72/1001] [Batch 85/372] [D loss: 0.6797851324081421] [G loss: 0.6656768321990967]\n",
      "[Epoch 72/1001] [Batch 86/372] [D loss: 0.6951597929000854] [G loss: 0.6862691044807434]\n",
      "[Epoch 72/1001] [Batch 87/372] [D loss: 0.6881893873214722] [G loss: 0.6304424405097961]\n",
      "[Epoch 72/1001] [Batch 88/372] [D loss: 0.6854971051216125] [G loss: 0.7207342982292175]\n",
      "[Epoch 72/1001] [Batch 89/372] [D loss: 0.6805793046951294] [G loss: 0.5912396907806396]\n",
      "[Epoch 72/1001] [Batch 90/372] [D loss: 0.6887260675430298] [G loss: 0.7767080664634705]\n",
      "[Epoch 72/1001] [Batch 91/372] [D loss: 0.7006338238716125] [G loss: 0.5504282116889954]\n",
      "[Epoch 72/1001] [Batch 92/372] [D loss: 0.6880385279655457] [G loss: 0.8417708873748779]\n",
      "[Epoch 72/1001] [Batch 93/372] [D loss: 0.6972646713256836] [G loss: 0.5053870677947998]\n",
      "[Epoch 72/1001] [Batch 94/372] [D loss: 0.696583092212677] [G loss: 0.8866830468177795]\n",
      "[Epoch 72/1001] [Batch 95/372] [D loss: 0.6914832592010498] [G loss: 0.5278369784355164]\n",
      "[Epoch 72/1001] [Batch 96/372] [D loss: 0.6848395466804504] [G loss: 0.8352664709091187]\n",
      "[Epoch 72/1001] [Batch 97/372] [D loss: 0.6908286809921265] [G loss: 0.5514376163482666]\n",
      "[Epoch 72/1001] [Batch 98/372] [D loss: 0.693509578704834] [G loss: 0.8402979969978333]\n",
      "[Epoch 72/1001] [Batch 99/372] [D loss: 0.6875146627426147] [G loss: 0.5604339838027954]\n",
      "[Epoch 72/1001] [Batch 100/372] [D loss: 0.6913734674453735] [G loss: 0.7756186127662659]\n",
      "[Epoch 72/1001] [Batch 101/372] [D loss: 0.6864022016525269] [G loss: 0.6309825778007507]\n",
      "[Epoch 72/1001] [Batch 102/372] [D loss: 0.6930137872695923] [G loss: 0.6805015802383423]\n",
      "[Epoch 72/1001] [Batch 103/372] [D loss: 0.6871634721755981] [G loss: 0.6625211834907532]\n",
      "[Epoch 72/1001] [Batch 104/372] [D loss: 0.6787642240524292] [G loss: 0.68782639503479]\n",
      "[Epoch 72/1001] [Batch 105/372] [D loss: 0.6886705160140991] [G loss: 0.6597883105278015]\n",
      "[Epoch 72/1001] [Batch 106/372] [D loss: 0.6888272762298584] [G loss: 0.6800745725631714]\n",
      "[Epoch 72/1001] [Batch 107/372] [D loss: 0.6825954914093018] [G loss: 0.6426949501037598]\n",
      "[Epoch 72/1001] [Batch 108/372] [D loss: 0.6960893869400024] [G loss: 0.683978259563446]\n",
      "[Epoch 72/1001] [Batch 109/372] [D loss: 0.685616135597229] [G loss: 0.6550667881965637]\n",
      "[Epoch 72/1001] [Batch 110/372] [D loss: 0.6816026568412781] [G loss: 0.6489366888999939]\n",
      "[Epoch 72/1001] [Batch 111/372] [D loss: 0.6916369199752808] [G loss: 0.7075884938240051]\n",
      "[Epoch 72/1001] [Batch 112/372] [D loss: 0.6890393495559692] [G loss: 0.6212119460105896]\n",
      "[Epoch 72/1001] [Batch 113/372] [D loss: 0.6856195330619812] [G loss: 0.7357540726661682]\n",
      "[Epoch 72/1001] [Batch 114/372] [D loss: 0.6927064657211304] [G loss: 0.5991718769073486]\n",
      "[Epoch 72/1001] [Batch 115/372] [D loss: 0.6853526830673218] [G loss: 0.7503016591072083]\n",
      "[Epoch 72/1001] [Batch 116/372] [D loss: 0.6882437467575073] [G loss: 0.5974853038787842]\n",
      "[Epoch 72/1001] [Batch 117/372] [D loss: 0.6873072385787964] [G loss: 0.7286074161529541]\n",
      "[Epoch 72/1001] [Batch 118/372] [D loss: 0.6793647408485413] [G loss: 0.6619279384613037]\n",
      "[Epoch 72/1001] [Batch 119/372] [D loss: 0.6809672117233276] [G loss: 0.7107964158058167]\n",
      "[Epoch 72/1001] [Batch 120/372] [D loss: 0.6777051687240601] [G loss: 0.6187642216682434]\n",
      "[Epoch 72/1001] [Batch 121/372] [D loss: 0.6844759583473206] [G loss: 0.7504895925521851]\n",
      "[Epoch 72/1001] [Batch 122/372] [D loss: 0.6758968234062195] [G loss: 0.5910626649856567]\n",
      "[Epoch 72/1001] [Batch 123/372] [D loss: 0.6894099116325378] [G loss: 0.8070265650749207]\n",
      "[Epoch 72/1001] [Batch 124/372] [D loss: 0.7026099562644958] [G loss: 0.5443419218063354]\n",
      "[Epoch 72/1001] [Batch 125/372] [D loss: 0.6945529580116272] [G loss: 0.82330721616745]\n",
      "[Epoch 72/1001] [Batch 126/372] [D loss: 0.695652186870575] [G loss: 0.5441687107086182]\n",
      "[Epoch 72/1001] [Batch 127/372] [D loss: 0.6989080905914307] [G loss: 0.7837318778038025]\n",
      "[Epoch 72/1001] [Batch 128/372] [D loss: 0.6816602945327759] [G loss: 0.6235756874084473]\n",
      "[Epoch 72/1001] [Batch 129/372] [D loss: 0.6832664012908936] [G loss: 0.6641982197761536]\n",
      "[Epoch 72/1001] [Batch 130/372] [D loss: 0.6854431629180908] [G loss: 0.7147423624992371]\n",
      "[Epoch 72/1001] [Batch 131/372] [D loss: 0.6809885501861572] [G loss: 0.6255037784576416]\n",
      "[Epoch 72/1001] [Batch 132/372] [D loss: 0.6848627328872681] [G loss: 0.7479787468910217]\n",
      "[Epoch 72/1001] [Batch 133/372] [D loss: 0.6824366450309753] [G loss: 0.6060916185379028]\n",
      "[Epoch 72/1001] [Batch 134/372] [D loss: 0.691429078578949] [G loss: 0.7445921301841736]\n",
      "[Epoch 72/1001] [Batch 135/372] [D loss: 0.6847167015075684] [G loss: 0.6110023856163025]\n",
      "[Epoch 72/1001] [Batch 136/372] [D loss: 0.6835883855819702] [G loss: 0.7231802940368652]\n",
      "[Epoch 72/1001] [Batch 137/372] [D loss: 0.6825277209281921] [G loss: 0.6262326240539551]\n",
      "[Epoch 72/1001] [Batch 138/372] [D loss: 0.6756398677825928] [G loss: 0.7143859267234802]\n",
      "[Epoch 72/1001] [Batch 139/372] [D loss: 0.6792751550674438] [G loss: 0.6667789220809937]\n",
      "[Epoch 72/1001] [Batch 140/372] [D loss: 0.6869795322418213] [G loss: 0.6218217015266418]\n",
      "[Epoch 72/1001] [Batch 141/372] [D loss: 0.6873124837875366] [G loss: 0.7538852691650391]\n",
      "[Epoch 72/1001] [Batch 142/372] [D loss: 0.6867227554321289] [G loss: 0.5637655258178711]\n",
      "[Epoch 72/1001] [Batch 143/372] [D loss: 0.6840764284133911] [G loss: 0.7907431125640869]\n",
      "[Epoch 72/1001] [Batch 144/372] [D loss: 0.6826416254043579] [G loss: 0.5892174243927002]\n",
      "[Epoch 72/1001] [Batch 145/372] [D loss: 0.6794530153274536] [G loss: 0.7486414313316345]\n",
      "[Epoch 72/1001] [Batch 146/372] [D loss: 0.6877313852310181] [G loss: 0.5969007015228271]\n",
      "[Epoch 72/1001] [Batch 147/372] [D loss: 0.6925347447395325] [G loss: 0.7514217495918274]\n",
      "[Epoch 72/1001] [Batch 148/372] [D loss: 0.6816511154174805] [G loss: 0.6193459033966064]\n",
      "[Epoch 72/1001] [Batch 149/372] [D loss: 0.6904957294464111] [G loss: 0.7048692107200623]\n",
      "[Epoch 72/1001] [Batch 150/372] [D loss: 0.6786945462226868] [G loss: 0.6761089563369751]\n",
      "[Epoch 72/1001] [Batch 151/372] [D loss: 0.6817913055419922] [G loss: 0.6454265713691711]\n",
      "[Epoch 72/1001] [Batch 152/372] [D loss: 0.6935935616493225] [G loss: 0.6740466356277466]\n",
      "[Epoch 72/1001] [Batch 153/372] [D loss: 0.677925705909729] [G loss: 0.7135116457939148]\n",
      "[Epoch 72/1001] [Batch 154/372] [D loss: 0.6783244609832764] [G loss: 0.6091651916503906]\n",
      "[Epoch 72/1001] [Batch 155/372] [D loss: 0.6906129121780396] [G loss: 0.7606011629104614]\n",
      "[Epoch 72/1001] [Batch 156/372] [D loss: 0.6769806146621704] [G loss: 0.6241432428359985]\n",
      "[Epoch 72/1001] [Batch 157/372] [D loss: 0.6867611408233643] [G loss: 0.6871034502983093]\n",
      "[Epoch 72/1001] [Batch 158/372] [D loss: 0.6885441541671753] [G loss: 0.6658559441566467]\n",
      "[Epoch 72/1001] [Batch 159/372] [D loss: 0.6830935478210449] [G loss: 0.6582201719284058]\n",
      "[Epoch 72/1001] [Batch 160/372] [D loss: 0.6793840527534485] [G loss: 0.6882701516151428]\n",
      "[Epoch 72/1001] [Batch 161/372] [D loss: 0.6821492910385132] [G loss: 0.6984613537788391]\n",
      "[Epoch 72/1001] [Batch 162/372] [D loss: 0.6818852424621582] [G loss: 0.6156218647956848]\n",
      "[Epoch 72/1001] [Batch 163/372] [D loss: 0.6769677400588989] [G loss: 0.7438506484031677]\n",
      "[Epoch 72/1001] [Batch 164/372] [D loss: 0.6893984079360962] [G loss: 0.6043290495872498]\n",
      "[Epoch 72/1001] [Batch 165/372] [D loss: 0.693626880645752] [G loss: 0.7465581297874451]\n",
      "[Epoch 72/1001] [Batch 166/372] [D loss: 0.6871781349182129] [G loss: 0.6065472364425659]\n",
      "[Epoch 72/1001] [Batch 167/372] [D loss: 0.6892093420028687] [G loss: 0.7387046813964844]\n",
      "[Epoch 72/1001] [Batch 168/372] [D loss: 0.6822752952575684] [G loss: 0.6226838231086731]\n",
      "[Epoch 72/1001] [Batch 169/372] [D loss: 0.6909685134887695] [G loss: 0.722102165222168]\n",
      "[Epoch 72/1001] [Batch 170/372] [D loss: 0.6855638027191162] [G loss: 0.6244400143623352]\n",
      "[Epoch 72/1001] [Batch 171/372] [D loss: 0.687529444694519] [G loss: 0.7294542789459229]\n",
      "[Epoch 72/1001] [Batch 172/372] [D loss: 0.67661452293396] [G loss: 0.6016192436218262]\n",
      "[Epoch 72/1001] [Batch 173/372] [D loss: 0.6830949187278748] [G loss: 0.8143348693847656]\n",
      "[Epoch 72/1001] [Batch 174/372] [D loss: 0.6860944032669067] [G loss: 0.5085643529891968]\n",
      "[Epoch 72/1001] [Batch 175/372] [D loss: 0.7054884433746338] [G loss: 0.8784148097038269]\n",
      "[Epoch 72/1001] [Batch 176/372] [D loss: 0.6957662105560303] [G loss: 0.48362988233566284]\n",
      "[Epoch 72/1001] [Batch 177/372] [D loss: 0.7067021131515503] [G loss: 0.8882989883422852]\n",
      "[Epoch 72/1001] [Batch 178/372] [D loss: 0.6994425058364868] [G loss: 0.5274908542633057]\n",
      "[Epoch 72/1001] [Batch 179/372] [D loss: 0.6950438022613525] [G loss: 0.7861368060112]\n",
      "[Epoch 72/1001] [Batch 180/372] [D loss: 0.6893624067306519] [G loss: 0.604840874671936]\n",
      "[Epoch 72/1001] [Batch 181/372] [D loss: 0.6914628148078918] [G loss: 0.7088538408279419]\n",
      "[Epoch 72/1001] [Batch 182/372] [D loss: 0.6892846822738647] [G loss: 0.6437127590179443]\n",
      "[Epoch 72/1001] [Batch 183/372] [D loss: 0.6933234333992004] [G loss: 0.6921754479408264]\n",
      "[Epoch 72/1001] [Batch 184/372] [D loss: 0.696213960647583] [G loss: 0.6475070714950562]\n",
      "[Epoch 72/1001] [Batch 185/372] [D loss: 0.6903030276298523] [G loss: 0.6692902445793152]\n",
      "[Epoch 72/1001] [Batch 186/372] [D loss: 0.679101824760437] [G loss: 0.6761073470115662]\n",
      "[Epoch 72/1001] [Batch 187/372] [D loss: 0.6838861703872681] [G loss: 0.655030369758606]\n",
      "[Epoch 72/1001] [Batch 188/372] [D loss: 0.6785063743591309] [G loss: 0.6861835718154907]\n",
      "[Epoch 72/1001] [Batch 189/372] [D loss: 0.6944602727890015] [G loss: 0.6820602416992188]\n",
      "[Epoch 72/1001] [Batch 190/372] [D loss: 0.684494137763977] [G loss: 0.6314144134521484]\n",
      "[Epoch 72/1001] [Batch 191/372] [D loss: 0.6835935115814209] [G loss: 0.7345978021621704]\n",
      "[Epoch 72/1001] [Batch 192/372] [D loss: 0.6934533715248108] [G loss: 0.6074852347373962]\n",
      "[Epoch 72/1001] [Batch 193/372] [D loss: 0.6854496002197266] [G loss: 0.7045047879219055]\n",
      "[Epoch 72/1001] [Batch 194/372] [D loss: 0.682181715965271] [G loss: 0.6492437124252319]\n",
      "[Epoch 72/1001] [Batch 195/372] [D loss: 0.6875638961791992] [G loss: 0.7316833138465881]\n",
      "[Epoch 72/1001] [Batch 196/372] [D loss: 0.6860725283622742] [G loss: 0.6026644706726074]\n",
      "[Epoch 72/1001] [Batch 197/372] [D loss: 0.6899406909942627] [G loss: 0.764046311378479]\n",
      "[Epoch 72/1001] [Batch 198/372] [D loss: 0.6976812481880188] [G loss: 0.5692847967147827]\n",
      "[Epoch 72/1001] [Batch 199/372] [D loss: 0.6891130208969116] [G loss: 0.7997951507568359]\n",
      "[Epoch 72/1001] [Batch 200/372] [D loss: 0.6872546672821045] [G loss: 0.5492529273033142]\n",
      "[Epoch 72/1001] [Batch 201/372] [D loss: 0.6984940767288208] [G loss: 0.8343631029129028]\n",
      "[Epoch 72/1001] [Batch 202/372] [D loss: 0.6966421604156494] [G loss: 0.5217570662498474]\n",
      "[Epoch 72/1001] [Batch 203/372] [D loss: 0.6945217847824097] [G loss: 0.8574235439300537]\n",
      "[Epoch 72/1001] [Batch 204/372] [D loss: 0.706171989440918] [G loss: 0.5227997899055481]\n",
      "[Epoch 72/1001] [Batch 205/372] [D loss: 0.6912561655044556] [G loss: 0.8083784580230713]\n",
      "[Epoch 72/1001] [Batch 206/372] [D loss: 0.694011926651001] [G loss: 0.5975267887115479]\n",
      "[Epoch 72/1001] [Batch 207/372] [D loss: 0.6891340613365173] [G loss: 0.694156289100647]\n",
      "[Epoch 72/1001] [Batch 208/372] [D loss: 0.6881052255630493] [G loss: 0.6892297267913818]\n",
      "[Epoch 72/1001] [Batch 209/372] [D loss: 0.6941351890563965] [G loss: 0.6428931951522827]\n",
      "[Epoch 72/1001] [Batch 210/372] [D loss: 0.6852934956550598] [G loss: 0.7146713137626648]\n",
      "[Epoch 72/1001] [Batch 211/372] [D loss: 0.6901823282241821] [G loss: 0.6353200078010559]\n",
      "[Epoch 72/1001] [Batch 212/372] [D loss: 0.6894373893737793] [G loss: 0.6812474131584167]\n",
      "[Epoch 72/1001] [Batch 213/372] [D loss: 0.6810595989227295] [G loss: 0.6785756945610046]\n",
      "[Epoch 72/1001] [Batch 214/372] [D loss: 0.679857611656189] [G loss: 0.6686978936195374]\n",
      "[Epoch 72/1001] [Batch 215/372] [D loss: 0.6874301433563232] [G loss: 0.701095700263977]\n",
      "[Epoch 72/1001] [Batch 216/372] [D loss: 0.6769746541976929] [G loss: 0.6239213943481445]\n",
      "[Epoch 72/1001] [Batch 217/372] [D loss: 0.6911718845367432] [G loss: 0.7342904210090637]\n",
      "[Epoch 72/1001] [Batch 218/372] [D loss: 0.6926029324531555] [G loss: 0.5967994332313538]\n",
      "[Epoch 72/1001] [Batch 219/372] [D loss: 0.693144679069519] [G loss: 0.7491159439086914]\n",
      "[Epoch 72/1001] [Batch 220/372] [D loss: 0.6882579326629639] [G loss: 0.5933719873428345]\n",
      "[Epoch 72/1001] [Batch 221/372] [D loss: 0.6874890327453613] [G loss: 0.7403103113174438]\n",
      "[Epoch 72/1001] [Batch 222/372] [D loss: 0.691067099571228] [G loss: 0.6231745481491089]\n",
      "[Epoch 72/1001] [Batch 223/372] [D loss: 0.68171226978302] [G loss: 0.7417067289352417]\n",
      "[Epoch 72/1001] [Batch 224/372] [D loss: 0.6867890357971191] [G loss: 0.6021332740783691]\n",
      "[Epoch 72/1001] [Batch 225/372] [D loss: 0.6994569301605225] [G loss: 0.7513758540153503]\n",
      "[Epoch 72/1001] [Batch 226/372] [D loss: 0.6940516233444214] [G loss: 0.5902385711669922]\n",
      "[Epoch 72/1001] [Batch 227/372] [D loss: 0.6926534175872803] [G loss: 0.7297170162200928]\n",
      "[Epoch 72/1001] [Batch 228/372] [D loss: 0.6878305077552795] [G loss: 0.6466435790061951]\n",
      "[Epoch 72/1001] [Batch 229/372] [D loss: 0.6885353326797485] [G loss: 0.6547073125839233]\n",
      "[Epoch 72/1001] [Batch 230/372] [D loss: 0.6885524988174438] [G loss: 0.7074059844017029]\n",
      "[Epoch 72/1001] [Batch 231/372] [D loss: 0.6857316493988037] [G loss: 0.6398957967758179]\n",
      "[Epoch 72/1001] [Batch 232/372] [D loss: 0.680452287197113] [G loss: 0.7209383845329285]\n",
      "[Epoch 72/1001] [Batch 233/372] [D loss: 0.6977248191833496] [G loss: 0.5985374450683594]\n",
      "[Epoch 72/1001] [Batch 234/372] [D loss: 0.6939181089401245] [G loss: 0.7873135805130005]\n",
      "[Epoch 72/1001] [Batch 235/372] [D loss: 0.6895524859428406] [G loss: 0.5856062173843384]\n",
      "[Epoch 72/1001] [Batch 236/372] [D loss: 0.6924179792404175] [G loss: 0.7464796900749207]\n",
      "[Epoch 72/1001] [Batch 237/372] [D loss: 0.6889737844467163] [G loss: 0.6120591163635254]\n",
      "[Epoch 72/1001] [Batch 238/372] [D loss: 0.6952368021011353] [G loss: 0.7183505892753601]\n",
      "[Epoch 72/1001] [Batch 239/372] [D loss: 0.6887779831886292] [G loss: 0.6283727884292603]\n",
      "[Epoch 72/1001] [Batch 240/372] [D loss: 0.6924006342887878] [G loss: 0.7290549278259277]\n",
      "[Epoch 72/1001] [Batch 241/372] [D loss: 0.688105046749115] [G loss: 0.6075097322463989]\n",
      "[Epoch 72/1001] [Batch 242/372] [D loss: 0.6869097948074341] [G loss: 0.7795272469520569]\n",
      "[Epoch 72/1001] [Batch 243/372] [D loss: 0.6935992240905762] [G loss: 0.5723483562469482]\n",
      "[Epoch 72/1001] [Batch 244/372] [D loss: 0.689845085144043] [G loss: 0.7665897011756897]\n",
      "[Epoch 72/1001] [Batch 245/372] [D loss: 0.6948292255401611] [G loss: 0.5704191327095032]\n",
      "[Epoch 72/1001] [Batch 246/372] [D loss: 0.6880807280540466] [G loss: 0.8035088181495667]\n",
      "[Epoch 72/1001] [Batch 247/372] [D loss: 0.6938918828964233] [G loss: 0.5845995545387268]\n",
      "[Epoch 72/1001] [Batch 248/372] [D loss: 0.6829160451889038] [G loss: 0.7397888898849487]\n",
      "[Epoch 72/1001] [Batch 249/372] [D loss: 0.6875259280204773] [G loss: 0.642357349395752]\n",
      "[Epoch 72/1001] [Batch 250/372] [D loss: 0.6817415952682495] [G loss: 0.678247332572937]\n",
      "[Epoch 72/1001] [Batch 251/372] [D loss: 0.6871161460876465] [G loss: 0.7061285972595215]\n",
      "[Epoch 72/1001] [Batch 252/372] [D loss: 0.687603235244751] [G loss: 0.6433361768722534]\n",
      "[Epoch 72/1001] [Batch 253/372] [D loss: 0.6864159107208252] [G loss: 0.6968737840652466]\n",
      "[Epoch 72/1001] [Batch 254/372] [D loss: 0.6771271228790283] [G loss: 0.6551780700683594]\n",
      "[Epoch 72/1001] [Batch 255/372] [D loss: 0.6836878657341003] [G loss: 0.6927652359008789]\n",
      "[Epoch 72/1001] [Batch 256/372] [D loss: 0.6899758577346802] [G loss: 0.6555410623550415]\n",
      "[Epoch 72/1001] [Batch 257/372] [D loss: 0.6807026863098145] [G loss: 0.701519787311554]\n",
      "[Epoch 72/1001] [Batch 258/372] [D loss: 0.6828951835632324] [G loss: 0.6381111145019531]\n",
      "[Epoch 72/1001] [Batch 259/372] [D loss: 0.6904550790786743] [G loss: 0.6915256381034851]\n",
      "[Epoch 72/1001] [Batch 260/372] [D loss: 0.6806881427764893] [G loss: 0.6476012468338013]\n",
      "[Epoch 72/1001] [Batch 261/372] [D loss: 0.6829313635826111] [G loss: 0.7252087593078613]\n",
      "[Epoch 72/1001] [Batch 262/372] [D loss: 0.6764745712280273] [G loss: 0.5984159111976624]\n",
      "[Epoch 72/1001] [Batch 263/372] [D loss: 0.6913349628448486] [G loss: 0.7891347408294678]\n",
      "[Epoch 72/1001] [Batch 264/372] [D loss: 0.6967589855194092] [G loss: 0.5263266563415527]\n",
      "[Epoch 72/1001] [Batch 265/372] [D loss: 0.7084205746650696] [G loss: 0.9407771229743958]\n",
      "[Epoch 72/1001] [Batch 266/372] [D loss: 0.7194311618804932] [G loss: 0.4861305058002472]\n",
      "[Epoch 72/1001] [Batch 267/372] [D loss: 0.715224027633667] [G loss: 0.8227070569992065]\n",
      "[Epoch 72/1001] [Batch 268/372] [D loss: 0.704927921295166] [G loss: 0.6041285991668701]\n",
      "[Epoch 72/1001] [Batch 269/372] [D loss: 0.6847043037414551] [G loss: 0.6762123703956604]\n",
      "[Epoch 72/1001] [Batch 270/372] [D loss: 0.6764736175537109] [G loss: 0.7129849791526794]\n",
      "[Epoch 72/1001] [Batch 271/372] [D loss: 0.6861899495124817] [G loss: 0.6461011171340942]\n",
      "[Epoch 72/1001] [Batch 272/372] [D loss: 0.6831924915313721] [G loss: 0.6667393445968628]\n",
      "[Epoch 72/1001] [Batch 273/372] [D loss: 0.684289813041687] [G loss: 0.674419105052948]\n",
      "[Epoch 72/1001] [Batch 274/372] [D loss: 0.6822491884231567] [G loss: 0.6748092174530029]\n",
      "[Epoch 72/1001] [Batch 275/372] [D loss: 0.6877197623252869] [G loss: 0.6679210662841797]\n",
      "[Epoch 72/1001] [Batch 276/372] [D loss: 0.6821262836456299] [G loss: 0.6999542117118835]\n",
      "[Epoch 72/1001] [Batch 277/372] [D loss: 0.674501895904541] [G loss: 0.6378237009048462]\n",
      "[Epoch 72/1001] [Batch 278/372] [D loss: 0.691864013671875] [G loss: 0.7471691370010376]\n",
      "[Epoch 72/1001] [Batch 279/372] [D loss: 0.6952422857284546] [G loss: 0.6081514358520508]\n",
      "[Epoch 72/1001] [Batch 280/372] [D loss: 0.6946024298667908] [G loss: 0.7000799179077148]\n",
      "[Epoch 72/1001] [Batch 281/372] [D loss: 0.6927294731140137] [G loss: 0.6554964184761047]\n",
      "[Epoch 72/1001] [Batch 282/372] [D loss: 0.6845364570617676] [G loss: 0.6910377740859985]\n",
      "[Epoch 72/1001] [Batch 283/372] [D loss: 0.6834594011306763] [G loss: 0.6681379675865173]\n",
      "[Epoch 72/1001] [Batch 284/372] [D loss: 0.6810458898544312] [G loss: 0.6908665895462036]\n",
      "[Epoch 72/1001] [Batch 285/372] [D loss: 0.6861554384231567] [G loss: 0.6484288573265076]\n",
      "[Epoch 72/1001] [Batch 286/372] [D loss: 0.684536874294281] [G loss: 0.6705120801925659]\n",
      "[Epoch 72/1001] [Batch 287/372] [D loss: 0.689175009727478] [G loss: 0.6953025460243225]\n",
      "[Epoch 72/1001] [Batch 288/372] [D loss: 0.6752686500549316] [G loss: 0.6330716609954834]\n",
      "[Epoch 72/1001] [Batch 289/372] [D loss: 0.6890247464179993] [G loss: 0.7075854539871216]\n",
      "[Epoch 72/1001] [Batch 290/372] [D loss: 0.687768280506134] [G loss: 0.6771146655082703]\n",
      "[Epoch 72/1001] [Batch 291/372] [D loss: 0.6930294036865234] [G loss: 0.6512988805770874]\n",
      "[Epoch 72/1001] [Batch 292/372] [D loss: 0.6791425943374634] [G loss: 0.7152144908905029]\n",
      "[Epoch 72/1001] [Batch 293/372] [D loss: 0.6810070872306824] [G loss: 0.6152165532112122]\n",
      "[Epoch 72/1001] [Batch 294/372] [D loss: 0.6865761876106262] [G loss: 0.7144519090652466]\n",
      "[Epoch 72/1001] [Batch 295/372] [D loss: 0.6832057237625122] [G loss: 0.6411198973655701]\n",
      "[Epoch 72/1001] [Batch 296/372] [D loss: 0.6835510730743408] [G loss: 0.7102060914039612]\n",
      "[Epoch 72/1001] [Batch 297/372] [D loss: 0.6841537952423096] [G loss: 0.6173771023750305]\n",
      "[Epoch 72/1001] [Batch 298/372] [D loss: 0.6878665685653687] [G loss: 0.7532246708869934]\n",
      "[Epoch 72/1001] [Batch 299/372] [D loss: 0.686348557472229] [G loss: 0.6262955665588379]\n",
      "[Epoch 72/1001] [Batch 300/372] [D loss: 0.6898339986801147] [G loss: 0.6952407956123352]\n",
      "[Epoch 72/1001] [Batch 301/372] [D loss: 0.6834951639175415] [G loss: 0.631069540977478]\n",
      "[Epoch 72/1001] [Batch 302/372] [D loss: 0.6899590492248535] [G loss: 0.7245168089866638]\n",
      "[Epoch 72/1001] [Batch 303/372] [D loss: 0.6801321506500244] [G loss: 0.6623914837837219]\n",
      "[Epoch 72/1001] [Batch 304/372] [D loss: 0.6843363642692566] [G loss: 0.6556643843650818]\n",
      "[Epoch 72/1001] [Batch 305/372] [D loss: 0.6737418174743652] [G loss: 0.6938977837562561]\n",
      "[Epoch 72/1001] [Batch 306/372] [D loss: 0.6794333457946777] [G loss: 0.6995406150817871]\n",
      "[Epoch 72/1001] [Batch 307/372] [D loss: 0.6799230575561523] [G loss: 0.6508728265762329]\n",
      "[Epoch 72/1001] [Batch 308/372] [D loss: 0.6767510175704956] [G loss: 0.6855354905128479]\n",
      "[Epoch 72/1001] [Batch 309/372] [D loss: 0.6833456754684448] [G loss: 0.6864499449729919]\n",
      "[Epoch 72/1001] [Batch 310/372] [D loss: 0.6799536943435669] [G loss: 0.659554660320282]\n",
      "[Epoch 72/1001] [Batch 311/372] [D loss: 0.67755126953125] [G loss: 0.6828306317329407]\n",
      "[Epoch 72/1001] [Batch 312/372] [D loss: 0.6840074062347412] [G loss: 0.7035452127456665]\n",
      "[Epoch 72/1001] [Batch 313/372] [D loss: 0.685211181640625] [G loss: 0.624692440032959]\n",
      "[Epoch 72/1001] [Batch 314/372] [D loss: 0.6828142404556274] [G loss: 0.7813491821289062]\n",
      "[Epoch 72/1001] [Batch 315/372] [D loss: 0.6907801032066345] [G loss: 0.5355094075202942]\n",
      "[Epoch 72/1001] [Batch 316/372] [D loss: 0.6949673891067505] [G loss: 0.844823956489563]\n",
      "[Epoch 72/1001] [Batch 317/372] [D loss: 0.7020500302314758] [G loss: 0.4923360347747803]\n",
      "[Epoch 72/1001] [Batch 318/372] [D loss: 0.700715184211731] [G loss: 0.9346047639846802]\n",
      "[Epoch 72/1001] [Batch 319/372] [D loss: 0.7089887857437134] [G loss: 0.4605548679828644]\n",
      "[Epoch 72/1001] [Batch 320/372] [D loss: 0.7097160816192627] [G loss: 0.9874975681304932]\n",
      "[Epoch 72/1001] [Batch 321/372] [D loss: 0.7202358841896057] [G loss: 0.470602422952652]\n",
      "[Epoch 72/1001] [Batch 322/372] [D loss: 0.7173000574111938] [G loss: 0.7994456887245178]\n",
      "[Epoch 72/1001] [Batch 323/372] [D loss: 0.6926208734512329] [G loss: 0.6474811434745789]\n",
      "[Epoch 72/1001] [Batch 324/372] [D loss: 0.6868433952331543] [G loss: 0.6628658175468445]\n",
      "[Epoch 72/1001] [Batch 325/372] [D loss: 0.6932641267776489] [G loss: 0.6775718927383423]\n",
      "[Epoch 72/1001] [Batch 326/372] [D loss: 0.6852505803108215] [G loss: 0.6790468096733093]\n",
      "[Epoch 72/1001] [Batch 327/372] [D loss: 0.6831254363059998] [G loss: 0.650702714920044]\n",
      "[Epoch 72/1001] [Batch 328/372] [D loss: 0.6784987449645996] [G loss: 0.6868588924407959]\n",
      "[Epoch 72/1001] [Batch 329/372] [D loss: 0.6898049116134644] [G loss: 0.6988129615783691]\n",
      "[Epoch 72/1001] [Batch 330/372] [D loss: 0.6854727268218994] [G loss: 0.6530174612998962]\n",
      "[Epoch 72/1001] [Batch 331/372] [D loss: 0.6916949152946472] [G loss: 0.6825621128082275]\n",
      "[Epoch 72/1001] [Batch 332/372] [D loss: 0.6859089732170105] [G loss: 0.672258198261261]\n",
      "[Epoch 72/1001] [Batch 333/372] [D loss: 0.6804417371749878] [G loss: 0.6502335667610168]\n",
      "[Epoch 72/1001] [Batch 334/372] [D loss: 0.6835952997207642] [G loss: 0.6863291263580322]\n",
      "[Epoch 72/1001] [Batch 335/372] [D loss: 0.6892507076263428] [G loss: 0.637883186340332]\n",
      "[Epoch 72/1001] [Batch 336/372] [D loss: 0.6869919300079346] [G loss: 0.751873254776001]\n",
      "[Epoch 72/1001] [Batch 337/372] [D loss: 0.6888453364372253] [G loss: 0.5961154699325562]\n",
      "[Epoch 72/1001] [Batch 338/372] [D loss: 0.6890349984169006] [G loss: 0.734339714050293]\n",
      "[Epoch 72/1001] [Batch 339/372] [D loss: 0.6857569217681885] [G loss: 0.6020484566688538]\n",
      "[Epoch 72/1001] [Batch 340/372] [D loss: 0.6782727241516113] [G loss: 0.7532970905303955]\n",
      "[Epoch 72/1001] [Batch 341/372] [D loss: 0.6887844800949097] [G loss: 0.6177501678466797]\n",
      "[Epoch 72/1001] [Batch 342/372] [D loss: 0.6739888191223145] [G loss: 0.7571885585784912]\n",
      "[Epoch 72/1001] [Batch 343/372] [D loss: 0.6824474334716797] [G loss: 0.5934094786643982]\n",
      "[Epoch 72/1001] [Batch 344/372] [D loss: 0.686194658279419] [G loss: 0.8089251518249512]\n",
      "[Epoch 72/1001] [Batch 345/372] [D loss: 0.6954009532928467] [G loss: 0.5462610125541687]\n",
      "[Epoch 72/1001] [Batch 346/372] [D loss: 0.704666256904602] [G loss: 0.8204925656318665]\n",
      "[Epoch 72/1001] [Batch 347/372] [D loss: 0.6863558292388916] [G loss: 0.583354115486145]\n",
      "[Epoch 72/1001] [Batch 348/372] [D loss: 0.6913217306137085] [G loss: 0.7247861623764038]\n",
      "[Epoch 72/1001] [Batch 349/372] [D loss: 0.6862682700157166] [G loss: 0.6562843322753906]\n",
      "[Epoch 72/1001] [Batch 350/372] [D loss: 0.6895884275436401] [G loss: 0.651848554611206]\n",
      "[Epoch 72/1001] [Batch 351/372] [D loss: 0.6859986782073975] [G loss: 0.7172137498855591]\n",
      "[Epoch 72/1001] [Batch 352/372] [D loss: 0.6934887170791626] [G loss: 0.619032621383667]\n",
      "[Epoch 72/1001] [Batch 353/372] [D loss: 0.674767017364502] [G loss: 0.7321619391441345]\n",
      "[Epoch 72/1001] [Batch 354/372] [D loss: 0.6870725750923157] [G loss: 0.6094297170639038]\n",
      "[Epoch 72/1001] [Batch 355/372] [D loss: 0.686760663986206] [G loss: 0.762943685054779]\n",
      "[Epoch 72/1001] [Batch 356/372] [D loss: 0.6940586566925049] [G loss: 0.5738369822502136]\n",
      "[Epoch 72/1001] [Batch 357/372] [D loss: 0.6856712102890015] [G loss: 0.7766478657722473]\n",
      "[Epoch 72/1001] [Batch 358/372] [D loss: 0.6925346851348877] [G loss: 0.5652859807014465]\n",
      "[Epoch 72/1001] [Batch 359/372] [D loss: 0.6876606941223145] [G loss: 0.8519193530082703]\n",
      "[Epoch 72/1001] [Batch 360/372] [D loss: 0.6912428140640259] [G loss: 0.5287268757820129]\n",
      "[Epoch 72/1001] [Batch 361/372] [D loss: 0.696758508682251] [G loss: 0.8328867554664612]\n",
      "[Epoch 72/1001] [Batch 362/372] [D loss: 0.7007001638412476] [G loss: 0.5561361312866211]\n",
      "[Epoch 72/1001] [Batch 363/372] [D loss: 0.6904937028884888] [G loss: 0.7785990238189697]\n",
      "[Epoch 72/1001] [Batch 364/372] [D loss: 0.6855405569076538] [G loss: 0.6345459818840027]\n",
      "[Epoch 72/1001] [Batch 365/372] [D loss: 0.6813746094703674] [G loss: 0.6798168420791626]\n",
      "[Epoch 72/1001] [Batch 366/372] [D loss: 0.6843966841697693] [G loss: 0.7020291090011597]\n",
      "[Epoch 72/1001] [Batch 367/372] [D loss: 0.685824990272522] [G loss: 0.6501035690307617]\n",
      "[Epoch 72/1001] [Batch 368/372] [D loss: 0.6844643354415894] [G loss: 0.7035094499588013]\n",
      "[Epoch 72/1001] [Batch 369/372] [D loss: 0.6828993558883667] [G loss: 0.6505416035652161]\n",
      "[Epoch 72/1001] [Batch 370/372] [D loss: 0.6829861402511597] [G loss: 0.6996363997459412]\n",
      "[Epoch 72/1001] [Batch 371/372] [D loss: 0.6810646653175354] [G loss: 0.6302551031112671]\n",
      "[Epoch 73/1001] [Batch 0/372] [D loss: 0.6892905831336975] [G loss: 0.720384418964386]\n",
      "[Epoch 73/1001] [Batch 1/372] [D loss: 0.6793817281723022] [G loss: 0.6409403085708618]\n",
      "[Epoch 73/1001] [Batch 2/372] [D loss: 0.6814587712287903] [G loss: 0.7253726720809937]\n",
      "[Epoch 73/1001] [Batch 3/372] [D loss: 0.6933568716049194] [G loss: 0.6291097402572632]\n",
      "[Epoch 73/1001] [Batch 4/372] [D loss: 0.6867825984954834] [G loss: 0.7103917598724365]\n",
      "[Epoch 73/1001] [Batch 5/372] [D loss: 0.6825563907623291] [G loss: 0.6427353024482727]\n",
      "[Epoch 73/1001] [Batch 6/372] [D loss: 0.6747885942459106] [G loss: 0.7101345658302307]\n",
      "[Epoch 73/1001] [Batch 7/372] [D loss: 0.6854448318481445] [G loss: 0.6716153621673584]\n",
      "[Epoch 73/1001] [Batch 8/372] [D loss: 0.6781353950500488] [G loss: 0.6764862537384033]\n",
      "[Epoch 73/1001] [Batch 9/372] [D loss: 0.678631603717804] [G loss: 0.6921507716178894]\n",
      "[Epoch 73/1001] [Batch 10/372] [D loss: 0.6865197420120239] [G loss: 0.645264744758606]\n",
      "[Epoch 73/1001] [Batch 11/372] [D loss: 0.6867700815200806] [G loss: 0.6937050223350525]\n",
      "[Epoch 73/1001] [Batch 12/372] [D loss: 0.6859408617019653] [G loss: 0.6645199060440063]\n",
      "[Epoch 73/1001] [Batch 13/372] [D loss: 0.6833197474479675] [G loss: 0.6861974596977234]\n",
      "[Epoch 73/1001] [Batch 14/372] [D loss: 0.6861296892166138] [G loss: 0.6543555855751038]\n",
      "[Epoch 73/1001] [Batch 15/372] [D loss: 0.6848611831665039] [G loss: 0.6884078979492188]\n",
      "[Epoch 73/1001] [Batch 16/372] [D loss: 0.6805726289749146] [G loss: 0.6703040599822998]\n",
      "[Epoch 73/1001] [Batch 17/372] [D loss: 0.6761034727096558] [G loss: 0.693448543548584]\n",
      "[Epoch 73/1001] [Batch 18/372] [D loss: 0.6901990175247192] [G loss: 0.6464914679527283]\n",
      "[Epoch 73/1001] [Batch 19/372] [D loss: 0.6765979528427124] [G loss: 0.7222990393638611]\n",
      "[Epoch 73/1001] [Batch 20/372] [D loss: 0.6830045580863953] [G loss: 0.6423295736312866]\n",
      "[Epoch 73/1001] [Batch 21/372] [D loss: 0.6790720224380493] [G loss: 0.7047695517539978]\n",
      "[Epoch 73/1001] [Batch 22/372] [D loss: 0.6808583736419678] [G loss: 0.6601325273513794]\n",
      "[Epoch 73/1001] [Batch 23/372] [D loss: 0.675345242023468] [G loss: 0.6800283789634705]\n",
      "[Epoch 73/1001] [Batch 24/372] [D loss: 0.6762311458587646] [G loss: 0.6852869987487793]\n",
      "[Epoch 73/1001] [Batch 25/372] [D loss: 0.6823683977127075] [G loss: 0.6808267831802368]\n",
      "[Epoch 73/1001] [Batch 26/372] [D loss: 0.6809527277946472] [G loss: 0.6400793790817261]\n",
      "[Epoch 73/1001] [Batch 27/372] [D loss: 0.6818907260894775] [G loss: 0.7591323852539062]\n",
      "[Epoch 73/1001] [Batch 28/372] [D loss: 0.680073082447052] [G loss: 0.5803515315055847]\n",
      "[Epoch 73/1001] [Batch 29/372] [D loss: 0.6916118860244751] [G loss: 0.8009136319160461]\n",
      "[Epoch 73/1001] [Batch 30/372] [D loss: 0.6999010443687439] [G loss: 0.5638131499290466]\n",
      "[Epoch 73/1001] [Batch 31/372] [D loss: 0.6783873438835144] [G loss: 0.7989281415939331]\n",
      "[Epoch 73/1001] [Batch 32/372] [D loss: 0.6838458776473999] [G loss: 0.5726956129074097]\n",
      "[Epoch 73/1001] [Batch 33/372] [D loss: 0.6854761242866516] [G loss: 0.7760365009307861]\n",
      "[Epoch 73/1001] [Batch 34/372] [D loss: 0.6824712753295898] [G loss: 0.629677414894104]\n",
      "[Epoch 73/1001] [Batch 35/372] [D loss: 0.6609099507331848] [G loss: 0.6898179054260254]\n",
      "[Epoch 73/1001] [Batch 36/372] [D loss: 0.6852564811706543] [G loss: 0.7096631526947021]\n",
      "[Epoch 73/1001] [Batch 37/372] [D loss: 0.6753191947937012] [G loss: 0.6191896796226501]\n",
      "[Epoch 73/1001] [Batch 38/372] [D loss: 0.6922147274017334] [G loss: 0.7261629104614258]\n",
      "[Epoch 73/1001] [Batch 39/372] [D loss: 0.6911100745201111] [G loss: 0.6126079559326172]\n",
      "[Epoch 73/1001] [Batch 40/372] [D loss: 0.682327151298523] [G loss: 0.7365281581878662]\n",
      "[Epoch 73/1001] [Batch 41/372] [D loss: 0.685829758644104] [G loss: 0.5662334561347961]\n",
      "[Epoch 73/1001] [Batch 42/372] [D loss: 0.7048255205154419] [G loss: 0.8839797377586365]\n",
      "[Epoch 73/1001] [Batch 43/372] [D loss: 0.6937750577926636] [G loss: 0.48559775948524475]\n",
      "[Epoch 73/1001] [Batch 44/372] [D loss: 0.7128913402557373] [G loss: 0.8922767639160156]\n",
      "[Epoch 73/1001] [Batch 45/372] [D loss: 0.7001698017120361] [G loss: 0.5386638045310974]\n",
      "[Epoch 73/1001] [Batch 46/372] [D loss: 0.6982309818267822] [G loss: 0.7862128615379333]\n",
      "[Epoch 73/1001] [Batch 47/372] [D loss: 0.6930549740791321] [G loss: 0.6155871152877808]\n",
      "[Epoch 73/1001] [Batch 48/372] [D loss: 0.6845703125] [G loss: 0.6957179307937622]\n",
      "[Epoch 73/1001] [Batch 49/372] [D loss: 0.6900148987770081] [G loss: 0.6543630957603455]\n",
      "[Epoch 73/1001] [Batch 50/372] [D loss: 0.6771301031112671] [G loss: 0.699392557144165]\n",
      "[Epoch 73/1001] [Batch 51/372] [D loss: 0.6748219728469849] [G loss: 0.657332181930542]\n",
      "[Epoch 73/1001] [Batch 52/372] [D loss: 0.6895575523376465] [G loss: 0.7245694994926453]\n",
      "[Epoch 73/1001] [Batch 53/372] [D loss: 0.6802049279212952] [G loss: 0.6189126372337341]\n",
      "[Epoch 73/1001] [Batch 54/372] [D loss: 0.6888090372085571] [G loss: 0.7039033770561218]\n",
      "[Epoch 73/1001] [Batch 55/372] [D loss: 0.6933545470237732] [G loss: 0.6214346885681152]\n",
      "[Epoch 73/1001] [Batch 56/372] [D loss: 0.6781082153320312] [G loss: 0.7251704931259155]\n",
      "[Epoch 73/1001] [Batch 57/372] [D loss: 0.6887521743774414] [G loss: 0.6242563724517822]\n",
      "[Epoch 73/1001] [Batch 58/372] [D loss: 0.6780763864517212] [G loss: 0.7413087487220764]\n",
      "[Epoch 73/1001] [Batch 59/372] [D loss: 0.6796051263809204] [G loss: 0.591495156288147]\n",
      "[Epoch 73/1001] [Batch 60/372] [D loss: 0.6809253096580505] [G loss: 0.8107573986053467]\n",
      "[Epoch 73/1001] [Batch 61/372] [D loss: 0.6900988221168518] [G loss: 0.533108651638031]\n",
      "[Epoch 73/1001] [Batch 62/372] [D loss: 0.6932915449142456] [G loss: 0.8681406378746033]\n",
      "[Epoch 73/1001] [Batch 63/372] [D loss: 0.6947411298751831] [G loss: 0.528851330280304]\n",
      "[Epoch 73/1001] [Batch 64/372] [D loss: 0.6935882568359375] [G loss: 0.8045341968536377]\n",
      "[Epoch 73/1001] [Batch 65/372] [D loss: 0.6869162321090698] [G loss: 0.6016913056373596]\n",
      "[Epoch 73/1001] [Batch 66/372] [D loss: 0.6916865706443787] [G loss: 0.7009328603744507]\n",
      "[Epoch 73/1001] [Batch 67/372] [D loss: 0.6781841516494751] [G loss: 0.6649224162101746]\n",
      "[Epoch 73/1001] [Batch 68/372] [D loss: 0.6849463582038879] [G loss: 0.67673659324646]\n",
      "[Epoch 73/1001] [Batch 69/372] [D loss: 0.679469108581543] [G loss: 0.672850489616394]\n",
      "[Epoch 73/1001] [Batch 70/372] [D loss: 0.6785967946052551] [G loss: 0.6475524306297302]\n",
      "[Epoch 73/1001] [Batch 71/372] [D loss: 0.6781812906265259] [G loss: 0.7095150947570801]\n",
      "[Epoch 73/1001] [Batch 72/372] [D loss: 0.6743704080581665] [G loss: 0.6755169034004211]\n",
      "[Epoch 73/1001] [Batch 73/372] [D loss: 0.6794068813323975] [G loss: 0.6518386602401733]\n",
      "[Epoch 73/1001] [Batch 74/372] [D loss: 0.6828896999359131] [G loss: 0.706917405128479]\n",
      "[Epoch 73/1001] [Batch 75/372] [D loss: 0.6794959306716919] [G loss: 0.6350777745246887]\n",
      "[Epoch 73/1001] [Batch 76/372] [D loss: 0.6820866465568542] [G loss: 0.6951251029968262]\n",
      "[Epoch 73/1001] [Batch 77/372] [D loss: 0.6783422827720642] [G loss: 0.6560395956039429]\n",
      "[Epoch 73/1001] [Batch 78/372] [D loss: 0.6830059289932251] [G loss: 0.7047737836837769]\n",
      "[Epoch 73/1001] [Batch 79/372] [D loss: 0.6825180053710938] [G loss: 0.6113772988319397]\n",
      "[Epoch 73/1001] [Batch 80/372] [D loss: 0.6802085638046265] [G loss: 0.7424373030662537]\n",
      "[Epoch 73/1001] [Batch 81/372] [D loss: 0.6858642101287842] [G loss: 0.6338208317756653]\n",
      "[Epoch 73/1001] [Batch 82/372] [D loss: 0.6836534738540649] [G loss: 0.6663241386413574]\n",
      "[Epoch 73/1001] [Batch 83/372] [D loss: 0.6832369565963745] [G loss: 0.6612143516540527]\n",
      "[Epoch 73/1001] [Batch 84/372] [D loss: 0.682043194770813] [G loss: 0.6859962940216064]\n",
      "[Epoch 73/1001] [Batch 85/372] [D loss: 0.6782348155975342] [G loss: 0.6792547702789307]\n",
      "[Epoch 73/1001] [Batch 86/372] [D loss: 0.6814609169960022] [G loss: 0.667783796787262]\n",
      "[Epoch 73/1001] [Batch 87/372] [D loss: 0.6916193962097168] [G loss: 0.6605606079101562]\n",
      "[Epoch 73/1001] [Batch 88/372] [D loss: 0.6718106865882874] [G loss: 0.6882609724998474]\n",
      "[Epoch 73/1001] [Batch 89/372] [D loss: 0.6831427812576294] [G loss: 0.6659438610076904]\n",
      "[Epoch 73/1001] [Batch 90/372] [D loss: 0.6929247379302979] [G loss: 0.67230623960495]\n",
      "[Epoch 73/1001] [Batch 91/372] [D loss: 0.687252402305603] [G loss: 0.667249858379364]\n",
      "[Epoch 73/1001] [Batch 92/372] [D loss: 0.6733205318450928] [G loss: 0.6876853704452515]\n",
      "[Epoch 73/1001] [Batch 93/372] [D loss: 0.6803796291351318] [G loss: 0.6675108671188354]\n",
      "[Epoch 73/1001] [Batch 94/372] [D loss: 0.6855787634849548] [G loss: 0.7014285326004028]\n",
      "[Epoch 73/1001] [Batch 95/372] [D loss: 0.682144284248352] [G loss: 0.6727123856544495]\n",
      "[Epoch 73/1001] [Batch 96/372] [D loss: 0.684992790222168] [G loss: 0.6164976358413696]\n",
      "[Epoch 73/1001] [Batch 97/372] [D loss: 0.6846581697463989] [G loss: 0.7386091351509094]\n",
      "[Epoch 73/1001] [Batch 98/372] [D loss: 0.6814771890640259] [G loss: 0.6158808469772339]\n",
      "[Epoch 73/1001] [Batch 99/372] [D loss: 0.691051721572876] [G loss: 0.7009267807006836]\n",
      "[Epoch 73/1001] [Batch 100/372] [D loss: 0.6828551292419434] [G loss: 0.6078299283981323]\n",
      "[Epoch 73/1001] [Batch 101/372] [D loss: 0.6862277984619141] [G loss: 0.7828486561775208]\n",
      "[Epoch 73/1001] [Batch 102/372] [D loss: 0.671245813369751] [G loss: 0.537367045879364]\n",
      "[Epoch 73/1001] [Batch 103/372] [D loss: 0.6876732110977173] [G loss: 0.9199419021606445]\n",
      "[Epoch 73/1001] [Batch 104/372] [D loss: 0.7007914781570435] [G loss: 0.4500402808189392]\n",
      "[Epoch 73/1001] [Batch 105/372] [D loss: 0.7220158576965332] [G loss: 1.0243397951126099]\n",
      "[Epoch 73/1001] [Batch 106/372] [D loss: 0.7363154888153076] [G loss: 0.43763959407806396]\n",
      "[Epoch 73/1001] [Batch 107/372] [D loss: 0.7224097847938538] [G loss: 0.8377421498298645]\n",
      "[Epoch 73/1001] [Batch 108/372] [D loss: 0.7029455900192261] [G loss: 0.6177859902381897]\n",
      "[Epoch 73/1001] [Batch 109/372] [D loss: 0.6885817050933838] [G loss: 0.6652824878692627]\n",
      "[Epoch 73/1001] [Batch 110/372] [D loss: 0.6774935126304626] [G loss: 0.7236494421958923]\n",
      "[Epoch 73/1001] [Batch 111/372] [D loss: 0.6769708395004272] [G loss: 0.6061206459999084]\n",
      "[Epoch 73/1001] [Batch 112/372] [D loss: 0.6942132711410522] [G loss: 0.766602635383606]\n",
      "[Epoch 73/1001] [Batch 113/372] [D loss: 0.683030366897583] [G loss: 0.5975862145423889]\n",
      "[Epoch 73/1001] [Batch 114/372] [D loss: 0.692168653011322] [G loss: 0.7422265410423279]\n",
      "[Epoch 73/1001] [Batch 115/372] [D loss: 0.6795637607574463] [G loss: 0.6303182244300842]\n",
      "[Epoch 73/1001] [Batch 116/372] [D loss: 0.6831478476524353] [G loss: 0.7438465356826782]\n",
      "[Epoch 73/1001] [Batch 117/372] [D loss: 0.6796749234199524] [G loss: 0.6040897965431213]\n",
      "[Epoch 73/1001] [Batch 118/372] [D loss: 0.6870409250259399] [G loss: 0.7538496851921082]\n",
      "[Epoch 73/1001] [Batch 119/372] [D loss: 0.6941311359405518] [G loss: 0.5738868117332458]\n",
      "[Epoch 73/1001] [Batch 120/372] [D loss: 0.6897804141044617] [G loss: 0.7762473821640015]\n",
      "[Epoch 73/1001] [Batch 121/372] [D loss: 0.6952618360519409] [G loss: 0.6023106575012207]\n",
      "[Epoch 73/1001] [Batch 122/372] [D loss: 0.6847957372665405] [G loss: 0.6969938278198242]\n",
      "[Epoch 73/1001] [Batch 123/372] [D loss: 0.6811628341674805] [G loss: 0.6753257513046265]\n",
      "[Epoch 73/1001] [Batch 124/372] [D loss: 0.6833131313323975] [G loss: 0.6535950899124146]\n",
      "[Epoch 73/1001] [Batch 125/372] [D loss: 0.6849024295806885] [G loss: 0.6822043061256409]\n",
      "[Epoch 73/1001] [Batch 126/372] [D loss: 0.6812152862548828] [G loss: 0.6883564591407776]\n",
      "[Epoch 73/1001] [Batch 127/372] [D loss: 0.6844562292098999] [G loss: 0.6314212679862976]\n",
      "[Epoch 73/1001] [Batch 128/372] [D loss: 0.6776947975158691] [G loss: 0.6981719136238098]\n",
      "[Epoch 73/1001] [Batch 129/372] [D loss: 0.6828031539916992] [G loss: 0.6648455262184143]\n",
      "[Epoch 73/1001] [Batch 130/372] [D loss: 0.6834235191345215] [G loss: 0.7200472950935364]\n",
      "[Epoch 73/1001] [Batch 131/372] [D loss: 0.678587794303894] [G loss: 0.6129494309425354]\n",
      "[Epoch 73/1001] [Batch 132/372] [D loss: 0.6770455837249756] [G loss: 0.773453414440155]\n",
      "[Epoch 73/1001] [Batch 133/372] [D loss: 0.6862161159515381] [G loss: 0.551225483417511]\n",
      "[Epoch 73/1001] [Batch 134/372] [D loss: 0.6971288919448853] [G loss: 0.8074805736541748]\n",
      "[Epoch 73/1001] [Batch 135/372] [D loss: 0.6933858394622803] [G loss: 0.5402944684028625]\n",
      "[Epoch 73/1001] [Batch 136/372] [D loss: 0.6937075257301331] [G loss: 0.8331665396690369]\n",
      "[Epoch 73/1001] [Batch 137/372] [D loss: 0.700448751449585] [G loss: 0.5445432662963867]\n",
      "[Epoch 73/1001] [Batch 138/372] [D loss: 0.6961652040481567] [G loss: 0.7549977898597717]\n",
      "[Epoch 73/1001] [Batch 139/372] [D loss: 0.6867499351501465] [G loss: 0.622731626033783]\n",
      "[Epoch 73/1001] [Batch 140/372] [D loss: 0.6859211921691895] [G loss: 0.6921793222427368]\n",
      "[Epoch 73/1001] [Batch 141/372] [D loss: 0.6755834817886353] [G loss: 0.6663835644721985]\n",
      "[Epoch 73/1001] [Batch 142/372] [D loss: 0.6860746145248413] [G loss: 0.7136135101318359]\n",
      "[Epoch 73/1001] [Batch 143/372] [D loss: 0.6717585325241089] [G loss: 0.6431996822357178]\n",
      "[Epoch 73/1001] [Batch 144/372] [D loss: 0.6799317002296448] [G loss: 0.7314602732658386]\n",
      "[Epoch 73/1001] [Batch 145/372] [D loss: 0.6785217523574829] [G loss: 0.6299928426742554]\n",
      "[Epoch 73/1001] [Batch 146/372] [D loss: 0.6885638236999512] [G loss: 0.6840817332267761]\n",
      "[Epoch 73/1001] [Batch 147/372] [D loss: 0.6805658340454102] [G loss: 0.7013064622879028]\n",
      "[Epoch 73/1001] [Batch 148/372] [D loss: 0.6784927845001221] [G loss: 0.6616448163986206]\n",
      "[Epoch 73/1001] [Batch 149/372] [D loss: 0.6849162578582764] [G loss: 0.6775833964347839]\n",
      "[Epoch 73/1001] [Batch 150/372] [D loss: 0.6865284442901611] [G loss: 0.6588981747627258]\n",
      "[Epoch 73/1001] [Batch 151/372] [D loss: 0.6894722580909729] [G loss: 0.6727405190467834]\n",
      "[Epoch 73/1001] [Batch 152/372] [D loss: 0.683356523513794] [G loss: 0.6775737404823303]\n",
      "[Epoch 73/1001] [Batch 153/372] [D loss: 0.6873800158500671] [G loss: 0.6735167503356934]\n",
      "[Epoch 73/1001] [Batch 154/372] [D loss: 0.6803054809570312] [G loss: 0.6504284739494324]\n",
      "[Epoch 73/1001] [Batch 155/372] [D loss: 0.6897468566894531] [G loss: 0.7394829392433167]\n",
      "[Epoch 73/1001] [Batch 156/372] [D loss: 0.6786549091339111] [G loss: 0.5999854803085327]\n",
      "[Epoch 73/1001] [Batch 157/372] [D loss: 0.6929589509963989] [G loss: 0.7339352965354919]\n",
      "[Epoch 73/1001] [Batch 158/372] [D loss: 0.6814416646957397] [G loss: 0.6359152793884277]\n",
      "[Epoch 73/1001] [Batch 159/372] [D loss: 0.6960514783859253] [G loss: 0.6892409324645996]\n",
      "[Epoch 73/1001] [Batch 160/372] [D loss: 0.6920236349105835] [G loss: 0.6758859753608704]\n",
      "[Epoch 73/1001] [Batch 161/372] [D loss: 0.68576580286026] [G loss: 0.6385632157325745]\n",
      "[Epoch 73/1001] [Batch 162/372] [D loss: 0.6938061118125916] [G loss: 0.7161074280738831]\n",
      "[Epoch 73/1001] [Batch 163/372] [D loss: 0.6767957806587219] [G loss: 0.6093276739120483]\n",
      "[Epoch 73/1001] [Batch 164/372] [D loss: 0.6896141767501831] [G loss: 0.7698901891708374]\n",
      "[Epoch 73/1001] [Batch 165/372] [D loss: 0.6792453527450562] [G loss: 0.5780738592147827]\n",
      "[Epoch 73/1001] [Batch 166/372] [D loss: 0.6905995011329651] [G loss: 0.7665532827377319]\n",
      "[Epoch 73/1001] [Batch 167/372] [D loss: 0.6933455467224121] [G loss: 0.5878428220748901]\n",
      "[Epoch 73/1001] [Batch 168/372] [D loss: 0.6901341676712036] [G loss: 0.7585412263870239]\n",
      "[Epoch 73/1001] [Batch 169/372] [D loss: 0.6859762668609619] [G loss: 0.576113224029541]\n",
      "[Epoch 73/1001] [Batch 170/372] [D loss: 0.6906250715255737] [G loss: 0.8287134170532227]\n",
      "[Epoch 73/1001] [Batch 171/372] [D loss: 0.69242262840271] [G loss: 0.5252358913421631]\n",
      "[Epoch 73/1001] [Batch 172/372] [D loss: 0.694229245185852] [G loss: 0.8788284063339233]\n",
      "[Epoch 73/1001] [Batch 173/372] [D loss: 0.692522406578064] [G loss: 0.5144290924072266]\n",
      "[Epoch 73/1001] [Batch 174/372] [D loss: 0.7021100521087646] [G loss: 0.8559339642524719]\n",
      "[Epoch 73/1001] [Batch 175/372] [D loss: 0.6928409934043884] [G loss: 0.5340529084205627]\n",
      "[Epoch 73/1001] [Batch 176/372] [D loss: 0.6955076456069946] [G loss: 0.7899666428565979]\n",
      "[Epoch 73/1001] [Batch 177/372] [D loss: 0.688201367855072] [G loss: 0.6116718649864197]\n",
      "[Epoch 73/1001] [Batch 178/372] [D loss: 0.6822768449783325] [G loss: 0.7105699777603149]\n",
      "[Epoch 73/1001] [Batch 179/372] [D loss: 0.6931969523429871] [G loss: 0.6555430889129639]\n",
      "[Epoch 73/1001] [Batch 180/372] [D loss: 0.6832481622695923] [G loss: 0.6802850961685181]\n",
      "[Epoch 73/1001] [Batch 181/372] [D loss: 0.6829559206962585] [G loss: 0.6525331139564514]\n",
      "[Epoch 73/1001] [Batch 182/372] [D loss: 0.6821244955062866] [G loss: 0.7059371471405029]\n",
      "[Epoch 73/1001] [Batch 183/372] [D loss: 0.6742571592330933] [G loss: 0.6572303771972656]\n",
      "[Epoch 73/1001] [Batch 184/372] [D loss: 0.6847583055496216] [G loss: 0.7026036977767944]\n",
      "[Epoch 73/1001] [Batch 185/372] [D loss: 0.6768964529037476] [G loss: 0.6342583894729614]\n",
      "[Epoch 73/1001] [Batch 186/372] [D loss: 0.678740382194519] [G loss: 0.7140430808067322]\n",
      "[Epoch 73/1001] [Batch 187/372] [D loss: 0.6830028295516968] [G loss: 0.6292017102241516]\n",
      "[Epoch 73/1001] [Batch 188/372] [D loss: 0.6784095764160156] [G loss: 0.7278552055358887]\n",
      "[Epoch 73/1001] [Batch 189/372] [D loss: 0.6857532262802124] [G loss: 0.5628679394721985]\n",
      "[Epoch 73/1001] [Batch 190/372] [D loss: 0.6941355466842651] [G loss: 0.9016123414039612]\n",
      "[Epoch 73/1001] [Batch 191/372] [D loss: 0.7062368392944336] [G loss: 0.4705965220928192]\n",
      "[Epoch 73/1001] [Batch 192/372] [D loss: 0.7069623470306396] [G loss: 0.957070529460907]\n",
      "[Epoch 73/1001] [Batch 193/372] [D loss: 0.7244532108306885] [G loss: 0.5036280155181885]\n",
      "[Epoch 73/1001] [Batch 194/372] [D loss: 0.7039139270782471] [G loss: 0.7702466249465942]\n",
      "[Epoch 73/1001] [Batch 195/372] [D loss: 0.6885120868682861] [G loss: 0.6647315621376038]\n",
      "[Epoch 73/1001] [Batch 196/372] [D loss: 0.6771228313446045] [G loss: 0.6734421253204346]\n",
      "[Epoch 73/1001] [Batch 197/372] [D loss: 0.679381787776947] [G loss: 0.6893036365509033]\n",
      "[Epoch 73/1001] [Batch 198/372] [D loss: 0.6774075031280518] [G loss: 0.6325956583023071]\n",
      "[Epoch 73/1001] [Batch 199/372] [D loss: 0.6785283088684082] [G loss: 0.748898446559906]\n",
      "[Epoch 73/1001] [Batch 200/372] [D loss: 0.698479950428009] [G loss: 0.6180108785629272]\n",
      "[Epoch 73/1001] [Batch 201/372] [D loss: 0.6783955097198486] [G loss: 0.7085623741149902]\n",
      "[Epoch 73/1001] [Batch 202/372] [D loss: 0.678636908531189] [G loss: 0.6341668963432312]\n",
      "[Epoch 73/1001] [Batch 203/372] [D loss: 0.6864945888519287] [G loss: 0.6739737391471863]\n",
      "[Epoch 73/1001] [Batch 204/372] [D loss: 0.6774387955665588] [G loss: 0.680251955986023]\n",
      "[Epoch 73/1001] [Batch 205/372] [D loss: 0.680753231048584] [G loss: 0.6742438077926636]\n",
      "[Epoch 73/1001] [Batch 206/372] [D loss: 0.6903907656669617] [G loss: 0.6960713267326355]\n",
      "[Epoch 73/1001] [Batch 207/372] [D loss: 0.6819853186607361] [G loss: 0.6449217796325684]\n",
      "[Epoch 73/1001] [Batch 208/372] [D loss: 0.6820021271705627] [G loss: 0.7045413851737976]\n",
      "[Epoch 73/1001] [Batch 209/372] [D loss: 0.6945369243621826] [G loss: 0.6187768578529358]\n",
      "[Epoch 73/1001] [Batch 210/372] [D loss: 0.6884019374847412] [G loss: 0.7116540670394897]\n",
      "[Epoch 73/1001] [Batch 211/372] [D loss: 0.686735987663269] [G loss: 0.6477574110031128]\n",
      "[Epoch 73/1001] [Batch 212/372] [D loss: 0.6842955350875854] [G loss: 0.6997646689414978]\n",
      "[Epoch 73/1001] [Batch 213/372] [D loss: 0.6943533420562744] [G loss: 0.624271810054779]\n",
      "[Epoch 73/1001] [Batch 214/372] [D loss: 0.6788963675498962] [G loss: 0.755511999130249]\n",
      "[Epoch 73/1001] [Batch 215/372] [D loss: 0.6837248206138611] [G loss: 0.5980169773101807]\n",
      "[Epoch 73/1001] [Batch 216/372] [D loss: 0.6977059841156006] [G loss: 0.7206932902336121]\n",
      "[Epoch 73/1001] [Batch 217/372] [D loss: 0.6849535703659058] [G loss: 0.6377246379852295]\n",
      "[Epoch 73/1001] [Batch 218/372] [D loss: 0.6863222122192383] [G loss: 0.7024943828582764]\n",
      "[Epoch 73/1001] [Batch 219/372] [D loss: 0.6767444014549255] [G loss: 0.6593193411827087]\n",
      "[Epoch 73/1001] [Batch 220/372] [D loss: 0.6800271272659302] [G loss: 0.6839089393615723]\n",
      "[Epoch 73/1001] [Batch 221/372] [D loss: 0.6759905219078064] [G loss: 0.6648538708686829]\n",
      "[Epoch 73/1001] [Batch 222/372] [D loss: 0.6844793558120728] [G loss: 0.660420298576355]\n",
      "[Epoch 73/1001] [Batch 223/372] [D loss: 0.6860947608947754] [G loss: 0.6969361305236816]\n",
      "[Epoch 73/1001] [Batch 224/372] [D loss: 0.6780373454093933] [G loss: 0.6434729099273682]\n",
      "[Epoch 73/1001] [Batch 225/372] [D loss: 0.6886748671531677] [G loss: 0.7261626124382019]\n",
      "[Epoch 73/1001] [Batch 226/372] [D loss: 0.6865730881690979] [G loss: 0.6679998636245728]\n",
      "[Epoch 73/1001] [Batch 227/372] [D loss: 0.6692607402801514] [G loss: 0.6521608829498291]\n",
      "[Epoch 73/1001] [Batch 228/372] [D loss: 0.687477707862854] [G loss: 0.7027655243873596]\n",
      "[Epoch 73/1001] [Batch 229/372] [D loss: 0.6882745027542114] [G loss: 0.6527414917945862]\n",
      "[Epoch 73/1001] [Batch 230/372] [D loss: 0.6829522252082825] [G loss: 0.6521708369255066]\n",
      "[Epoch 73/1001] [Batch 231/372] [D loss: 0.6809617280960083] [G loss: 0.6838452816009521]\n",
      "[Epoch 73/1001] [Batch 232/372] [D loss: 0.6722164154052734] [G loss: 0.6859259009361267]\n",
      "[Epoch 73/1001] [Batch 233/372] [D loss: 0.6857348084449768] [G loss: 0.6760179996490479]\n",
      "[Epoch 73/1001] [Batch 234/372] [D loss: 0.6797349452972412] [G loss: 0.6472287774085999]\n",
      "[Epoch 73/1001] [Batch 235/372] [D loss: 0.7011909484863281] [G loss: 0.7137844562530518]\n",
      "[Epoch 73/1001] [Batch 236/372] [D loss: 0.6854421496391296] [G loss: 0.623282790184021]\n",
      "[Epoch 73/1001] [Batch 237/372] [D loss: 0.6925491094589233] [G loss: 0.7479226589202881]\n",
      "[Epoch 73/1001] [Batch 238/372] [D loss: 0.6932435035705566] [G loss: 0.5956538915634155]\n",
      "[Epoch 73/1001] [Batch 239/372] [D loss: 0.6929967403411865] [G loss: 0.752862811088562]\n",
      "[Epoch 73/1001] [Batch 240/372] [D loss: 0.6899408102035522] [G loss: 0.5861797332763672]\n",
      "[Epoch 73/1001] [Batch 241/372] [D loss: 0.6885585784912109] [G loss: 0.7904483675956726]\n",
      "[Epoch 73/1001] [Batch 242/372] [D loss: 0.6962588429450989] [G loss: 0.5348769426345825]\n",
      "[Epoch 73/1001] [Batch 243/372] [D loss: 0.7022234201431274] [G loss: 0.8393599987030029]\n",
      "[Epoch 73/1001] [Batch 244/372] [D loss: 0.6968764066696167] [G loss: 0.5555724501609802]\n",
      "[Epoch 73/1001] [Batch 245/372] [D loss: 0.6883689761161804] [G loss: 0.754953145980835]\n",
      "[Epoch 73/1001] [Batch 246/372] [D loss: 0.6865230202674866] [G loss: 0.6513211727142334]\n",
      "[Epoch 73/1001] [Batch 247/372] [D loss: 0.6887347102165222] [G loss: 0.6694625616073608]\n",
      "[Epoch 73/1001] [Batch 248/372] [D loss: 0.6768125891685486] [G loss: 0.6847859621047974]\n",
      "[Epoch 73/1001] [Batch 249/372] [D loss: 0.6863707304000854] [G loss: 0.6783320307731628]\n",
      "[Epoch 73/1001] [Batch 250/372] [D loss: 0.6796068549156189] [G loss: 0.6501250863075256]\n",
      "[Epoch 73/1001] [Batch 251/372] [D loss: 0.6881134510040283] [G loss: 0.6786351203918457]\n",
      "[Epoch 73/1001] [Batch 252/372] [D loss: 0.684893012046814] [G loss: 0.6941944360733032]\n",
      "[Epoch 73/1001] [Batch 253/372] [D loss: 0.6911383271217346] [G loss: 0.5996649861335754]\n",
      "[Epoch 73/1001] [Batch 254/372] [D loss: 0.6883657574653625] [G loss: 0.772077739238739]\n",
      "[Epoch 73/1001] [Batch 255/372] [D loss: 0.689557671546936] [G loss: 0.5652942657470703]\n",
      "[Epoch 73/1001] [Batch 256/372] [D loss: 0.6847819089889526] [G loss: 0.8074316382408142]\n",
      "[Epoch 73/1001] [Batch 257/372] [D loss: 0.6882294416427612] [G loss: 0.5554240942001343]\n",
      "[Epoch 73/1001] [Batch 258/372] [D loss: 0.695499062538147] [G loss: 0.8136993646621704]\n",
      "[Epoch 73/1001] [Batch 259/372] [D loss: 0.6832486391067505] [G loss: 0.5403012037277222]\n",
      "[Epoch 73/1001] [Batch 260/372] [D loss: 0.6920003890991211] [G loss: 0.8489935994148254]\n",
      "[Epoch 73/1001] [Batch 261/372] [D loss: 0.6927927136421204] [G loss: 0.5380138158798218]\n",
      "[Epoch 73/1001] [Batch 262/372] [D loss: 0.7003231048583984] [G loss: 0.782139778137207]\n",
      "[Epoch 73/1001] [Batch 263/372] [D loss: 0.6914168000221252] [G loss: 0.5932831764221191]\n",
      "[Epoch 73/1001] [Batch 264/372] [D loss: 0.6856944561004639] [G loss: 0.7678041458129883]\n",
      "[Epoch 73/1001] [Batch 265/372] [D loss: 0.6847622990608215] [G loss: 0.637891411781311]\n",
      "[Epoch 73/1001] [Batch 266/372] [D loss: 0.6892912983894348] [G loss: 0.6565499305725098]\n",
      "[Epoch 73/1001] [Batch 267/372] [D loss: 0.6855000257492065] [G loss: 0.7013182640075684]\n",
      "[Epoch 73/1001] [Batch 268/372] [D loss: 0.680424690246582] [G loss: 0.6192054152488708]\n",
      "[Epoch 73/1001] [Batch 269/372] [D loss: 0.6776525378227234] [G loss: 0.7331039905548096]\n",
      "[Epoch 73/1001] [Batch 270/372] [D loss: 0.690238893032074] [G loss: 0.6273319125175476]\n",
      "[Epoch 73/1001] [Batch 271/372] [D loss: 0.679898738861084] [G loss: 0.7097429037094116]\n",
      "[Epoch 73/1001] [Batch 272/372] [D loss: 0.6848920583724976] [G loss: 0.6377085447311401]\n",
      "[Epoch 73/1001] [Batch 273/372] [D loss: 0.6998449563980103] [G loss: 0.7338483333587646]\n",
      "[Epoch 73/1001] [Batch 274/372] [D loss: 0.6882253885269165] [G loss: 0.5946800708770752]\n",
      "[Epoch 73/1001] [Batch 275/372] [D loss: 0.6930846571922302] [G loss: 0.7504914402961731]\n",
      "[Epoch 73/1001] [Batch 276/372] [D loss: 0.6811827421188354] [G loss: 0.6166490316390991]\n",
      "[Epoch 73/1001] [Batch 277/372] [D loss: 0.6848149299621582] [G loss: 0.7382581233978271]\n",
      "[Epoch 73/1001] [Batch 278/372] [D loss: 0.6888924837112427] [G loss: 0.6112333536148071]\n",
      "[Epoch 73/1001] [Batch 279/372] [D loss: 0.6791074872016907] [G loss: 0.7462537288665771]\n",
      "[Epoch 73/1001] [Batch 280/372] [D loss: 0.6949166059494019] [G loss: 0.5748566389083862]\n",
      "[Epoch 73/1001] [Batch 281/372] [D loss: 0.6847487688064575] [G loss: 0.834984302520752]\n",
      "[Epoch 73/1001] [Batch 282/372] [D loss: 0.6818729043006897] [G loss: 0.526125967502594]\n",
      "[Epoch 73/1001] [Batch 283/372] [D loss: 0.6983001828193665] [G loss: 0.8257360458374023]\n",
      "[Epoch 73/1001] [Batch 284/372] [D loss: 0.690142035484314] [G loss: 0.5225620269775391]\n",
      "[Epoch 73/1001] [Batch 285/372] [D loss: 0.7047738432884216] [G loss: 0.8984453678131104]\n",
      "[Epoch 73/1001] [Batch 286/372] [D loss: 0.7072199583053589] [G loss: 0.49351638555526733]\n",
      "[Epoch 73/1001] [Batch 287/372] [D loss: 0.7063771486282349] [G loss: 0.8433893322944641]\n",
      "[Epoch 73/1001] [Batch 288/372] [D loss: 0.7078704833984375] [G loss: 0.5664524435997009]\n",
      "[Epoch 73/1001] [Batch 289/372] [D loss: 0.6830266714096069] [G loss: 0.7250158786773682]\n",
      "[Epoch 73/1001] [Batch 290/372] [D loss: 0.6872261762619019] [G loss: 0.6567163467407227]\n",
      "[Epoch 73/1001] [Batch 291/372] [D loss: 0.6772942543029785] [G loss: 0.686071515083313]\n",
      "[Epoch 73/1001] [Batch 292/372] [D loss: 0.6830041408538818] [G loss: 0.655453622341156]\n",
      "[Epoch 73/1001] [Batch 293/372] [D loss: 0.6865498423576355] [G loss: 0.6903988718986511]\n",
      "[Epoch 73/1001] [Batch 294/372] [D loss: 0.6745193600654602] [G loss: 0.6671289205551147]\n",
      "[Epoch 73/1001] [Batch 295/372] [D loss: 0.6884748935699463] [G loss: 0.6710797548294067]\n",
      "[Epoch 73/1001] [Batch 296/372] [D loss: 0.6874362230300903] [G loss: 0.6850684285163879]\n",
      "[Epoch 73/1001] [Batch 297/372] [D loss: 0.6836472749710083] [G loss: 0.6583138704299927]\n",
      "[Epoch 73/1001] [Batch 298/372] [D loss: 0.6812597513198853] [G loss: 0.7061352729797363]\n",
      "[Epoch 73/1001] [Batch 299/372] [D loss: 0.6867990493774414] [G loss: 0.63429194688797]\n",
      "[Epoch 73/1001] [Batch 300/372] [D loss: 0.689281702041626] [G loss: 0.6984056830406189]\n",
      "[Epoch 73/1001] [Batch 301/372] [D loss: 0.6797206997871399] [G loss: 0.6609444618225098]\n",
      "[Epoch 73/1001] [Batch 302/372] [D loss: 0.6790533065795898] [G loss: 0.6789501905441284]\n",
      "[Epoch 73/1001] [Batch 303/372] [D loss: 0.6848238706588745] [G loss: 0.6728938817977905]\n",
      "[Epoch 73/1001] [Batch 304/372] [D loss: 0.6836702227592468] [G loss: 0.6989249587059021]\n",
      "[Epoch 73/1001] [Batch 305/372] [D loss: 0.6877586245536804] [G loss: 0.6263076066970825]\n",
      "[Epoch 73/1001] [Batch 306/372] [D loss: 0.6773531436920166] [G loss: 0.7475033402442932]\n",
      "[Epoch 73/1001] [Batch 307/372] [D loss: 0.6801027059555054] [G loss: 0.5864368677139282]\n",
      "[Epoch 73/1001] [Batch 308/372] [D loss: 0.6950698494911194] [G loss: 0.727789044380188]\n",
      "[Epoch 73/1001] [Batch 309/372] [D loss: 0.6819536685943604] [G loss: 0.6413301825523376]\n",
      "[Epoch 73/1001] [Batch 310/372] [D loss: 0.6832419633865356] [G loss: 0.7070465087890625]\n",
      "[Epoch 73/1001] [Batch 311/372] [D loss: 0.6864370107650757] [G loss: 0.5962297320365906]\n",
      "[Epoch 73/1001] [Batch 312/372] [D loss: 0.6859803795814514] [G loss: 0.8451611399650574]\n",
      "[Epoch 73/1001] [Batch 313/372] [D loss: 0.7021418809890747] [G loss: 0.4878292679786682]\n",
      "[Epoch 73/1001] [Batch 314/372] [D loss: 0.711627721786499] [G loss: 0.9071940183639526]\n",
      "[Epoch 73/1001] [Batch 315/372] [D loss: 0.7084294557571411] [G loss: 0.5124756097793579]\n",
      "[Epoch 73/1001] [Batch 316/372] [D loss: 0.7037814855575562] [G loss: 0.8138624429702759]\n",
      "[Epoch 73/1001] [Batch 317/372] [D loss: 0.6886700391769409] [G loss: 0.6181062459945679]\n",
      "[Epoch 73/1001] [Batch 318/372] [D loss: 0.6927657723426819] [G loss: 0.7029603719711304]\n",
      "[Epoch 73/1001] [Batch 319/372] [D loss: 0.68045973777771] [G loss: 0.6500241160392761]\n",
      "[Epoch 73/1001] [Batch 320/372] [D loss: 0.6905542612075806] [G loss: 0.6845245361328125]\n",
      "[Epoch 73/1001] [Batch 321/372] [D loss: 0.6865932941436768] [G loss: 0.6727113127708435]\n",
      "[Epoch 73/1001] [Batch 322/372] [D loss: 0.6679227948188782] [G loss: 0.6646810173988342]\n",
      "[Epoch 73/1001] [Batch 323/372] [D loss: 0.6832518577575684] [G loss: 0.7064588665962219]\n",
      "[Epoch 73/1001] [Batch 324/372] [D loss: 0.6853963136672974] [G loss: 0.6405594348907471]\n",
      "[Epoch 73/1001] [Batch 325/372] [D loss: 0.6834389567375183] [G loss: 0.687820315361023]\n",
      "[Epoch 73/1001] [Batch 326/372] [D loss: 0.6847565770149231] [G loss: 0.6771447062492371]\n",
      "[Epoch 73/1001] [Batch 327/372] [D loss: 0.6786289215087891] [G loss: 0.6239938735961914]\n",
      "[Epoch 73/1001] [Batch 328/372] [D loss: 0.6903915405273438] [G loss: 0.7619578242301941]\n",
      "[Epoch 73/1001] [Batch 329/372] [D loss: 0.6900819540023804] [G loss: 0.609080970287323]\n",
      "[Epoch 73/1001] [Batch 330/372] [D loss: 0.6930863857269287] [G loss: 0.7181689739227295]\n",
      "[Epoch 73/1001] [Batch 331/372] [D loss: 0.6803656220436096] [G loss: 0.6946092247962952]\n",
      "[Epoch 73/1001] [Batch 332/372] [D loss: 0.6894343495368958] [G loss: 0.6144382953643799]\n",
      "[Epoch 73/1001] [Batch 333/372] [D loss: 0.6940760612487793] [G loss: 0.7241547703742981]\n",
      "[Epoch 73/1001] [Batch 334/372] [D loss: 0.6897386312484741] [G loss: 0.6443438529968262]\n",
      "[Epoch 73/1001] [Batch 335/372] [D loss: 0.6863179802894592] [G loss: 0.678737461566925]\n",
      "[Epoch 73/1001] [Batch 336/372] [D loss: 0.685254693031311] [G loss: 0.6723694205284119]\n",
      "[Epoch 73/1001] [Batch 337/372] [D loss: 0.6874990463256836] [G loss: 0.6833323836326599]\n",
      "[Epoch 73/1001] [Batch 338/372] [D loss: 0.6789888739585876] [G loss: 0.6729526519775391]\n",
      "[Epoch 73/1001] [Batch 339/372] [D loss: 0.6792150735855103] [G loss: 0.6785081624984741]\n",
      "[Epoch 73/1001] [Batch 340/372] [D loss: 0.688218355178833] [G loss: 0.6713290214538574]\n",
      "[Epoch 73/1001] [Batch 341/372] [D loss: 0.6751421689987183] [G loss: 0.6737726330757141]\n",
      "[Epoch 73/1001] [Batch 342/372] [D loss: 0.6820070743560791] [G loss: 0.6847612857818604]\n",
      "[Epoch 73/1001] [Batch 343/372] [D loss: 0.6801475882530212] [G loss: 0.6735008358955383]\n",
      "[Epoch 73/1001] [Batch 344/372] [D loss: 0.6790345907211304] [G loss: 0.6824873685836792]\n",
      "[Epoch 73/1001] [Batch 345/372] [D loss: 0.685489296913147] [G loss: 0.6754342317581177]\n",
      "[Epoch 73/1001] [Batch 346/372] [D loss: 0.6890082359313965] [G loss: 0.6542799472808838]\n",
      "[Epoch 73/1001] [Batch 347/372] [D loss: 0.6852631568908691] [G loss: 0.6807618141174316]\n",
      "[Epoch 73/1001] [Batch 348/372] [D loss: 0.686617374420166] [G loss: 0.6803077459335327]\n",
      "[Epoch 73/1001] [Batch 349/372] [D loss: 0.6819613575935364] [G loss: 0.653063952922821]\n",
      "[Epoch 73/1001] [Batch 350/372] [D loss: 0.6962249875068665] [G loss: 0.6922025680541992]\n",
      "[Epoch 73/1001] [Batch 351/372] [D loss: 0.6613050699234009] [G loss: 0.6852499842643738]\n",
      "[Epoch 73/1001] [Batch 352/372] [D loss: 0.6796712875366211] [G loss: 0.676507294178009]\n",
      "[Epoch 73/1001] [Batch 353/372] [D loss: 0.689280092716217] [G loss: 0.6680882573127747]\n",
      "[Epoch 73/1001] [Batch 354/372] [D loss: 0.6756992340087891] [G loss: 0.6697818040847778]\n",
      "[Epoch 73/1001] [Batch 355/372] [D loss: 0.6911025047302246] [G loss: 0.6867660880088806]\n",
      "[Epoch 73/1001] [Batch 356/372] [D loss: 0.678240954875946] [G loss: 0.6577448844909668]\n",
      "[Epoch 73/1001] [Batch 357/372] [D loss: 0.6870895624160767] [G loss: 0.6793828010559082]\n",
      "[Epoch 73/1001] [Batch 358/372] [D loss: 0.684039831161499] [G loss: 0.6965466141700745]\n",
      "[Epoch 73/1001] [Batch 359/372] [D loss: 0.6714455485343933] [G loss: 0.673507571220398]\n",
      "[Epoch 73/1001] [Batch 360/372] [D loss: 0.6849086284637451] [G loss: 0.6690923571586609]\n",
      "[Epoch 73/1001] [Batch 361/372] [D loss: 0.6860128045082092] [G loss: 0.6539509296417236]\n",
      "[Epoch 73/1001] [Batch 362/372] [D loss: 0.6848037242889404] [G loss: 0.6596720218658447]\n",
      "[Epoch 73/1001] [Batch 363/372] [D loss: 0.6868746280670166] [G loss: 0.6989765763282776]\n",
      "[Epoch 73/1001] [Batch 364/372] [D loss: 0.6986134052276611] [G loss: 0.6214354038238525]\n",
      "[Epoch 73/1001] [Batch 365/372] [D loss: 0.6877930164337158] [G loss: 0.7235492467880249]\n",
      "[Epoch 73/1001] [Batch 366/372] [D loss: 0.6901404857635498] [G loss: 0.6080344915390015]\n",
      "[Epoch 73/1001] [Batch 367/372] [D loss: 0.690880298614502] [G loss: 0.7615658640861511]\n",
      "[Epoch 73/1001] [Batch 368/372] [D loss: 0.694952130317688] [G loss: 0.5066044926643372]\n",
      "[Epoch 73/1001] [Batch 369/372] [D loss: 0.7050908207893372] [G loss: 0.9599896669387817]\n",
      "[Epoch 73/1001] [Batch 370/372] [D loss: 0.7097117304801941] [G loss: 0.43679872155189514]\n",
      "[Epoch 73/1001] [Batch 371/372] [D loss: 0.7353916168212891] [G loss: 0.979069709777832]\n",
      "[Epoch 74/1001] [Batch 0/372] [D loss: 0.7167600989341736] [G loss: 0.49585461616516113]\n",
      "[Epoch 74/1001] [Batch 1/372] [D loss: 0.6959844827651978] [G loss: 0.7728080749511719]\n",
      "[Epoch 74/1001] [Batch 2/372] [D loss: 0.6789602041244507] [G loss: 0.6621670722961426]\n",
      "[Epoch 74/1001] [Batch 3/372] [D loss: 0.691956639289856] [G loss: 0.6487172842025757]\n",
      "[Epoch 74/1001] [Batch 4/372] [D loss: 0.6825985908508301] [G loss: 0.7229079604148865]\n",
      "[Epoch 74/1001] [Batch 5/372] [D loss: 0.6835656762123108] [G loss: 0.6338907480239868]\n",
      "[Epoch 74/1001] [Batch 6/372] [D loss: 0.6743223667144775] [G loss: 0.7104985117912292]\n",
      "[Epoch 74/1001] [Batch 7/372] [D loss: 0.6860398650169373] [G loss: 0.6303005218505859]\n",
      "[Epoch 74/1001] [Batch 8/372] [D loss: 0.6758853197097778] [G loss: 0.732211172580719]\n",
      "[Epoch 74/1001] [Batch 9/372] [D loss: 0.6820200681686401] [G loss: 0.6597938537597656]\n",
      "[Epoch 74/1001] [Batch 10/372] [D loss: 0.6815261840820312] [G loss: 0.6326360702514648]\n",
      "[Epoch 74/1001] [Batch 11/372] [D loss: 0.6889322996139526] [G loss: 0.7709512114524841]\n",
      "[Epoch 74/1001] [Batch 12/372] [D loss: 0.6866565942764282] [G loss: 0.5737073421478271]\n",
      "[Epoch 74/1001] [Batch 13/372] [D loss: 0.6895432472229004] [G loss: 0.7464625835418701]\n",
      "[Epoch 74/1001] [Batch 14/372] [D loss: 0.6871175765991211] [G loss: 0.6359803676605225]\n",
      "[Epoch 74/1001] [Batch 15/372] [D loss: 0.6835236549377441] [G loss: 0.6805204749107361]\n",
      "[Epoch 74/1001] [Batch 16/372] [D loss: 0.6866381764411926] [G loss: 0.6627807021141052]\n",
      "[Epoch 74/1001] [Batch 17/372] [D loss: 0.6859303712844849] [G loss: 0.6726243495941162]\n",
      "[Epoch 74/1001] [Batch 18/372] [D loss: 0.6768522262573242] [G loss: 0.6851312518119812]\n",
      "[Epoch 74/1001] [Batch 19/372] [D loss: 0.6897133588790894] [G loss: 0.6850490570068359]\n",
      "[Epoch 74/1001] [Batch 20/372] [D loss: 0.6818093061447144] [G loss: 0.6103594899177551]\n",
      "[Epoch 74/1001] [Batch 21/372] [D loss: 0.6912159323692322] [G loss: 0.7468041777610779]\n",
      "[Epoch 74/1001] [Batch 22/372] [D loss: 0.6904526948928833] [G loss: 0.622884213924408]\n",
      "[Epoch 74/1001] [Batch 23/372] [D loss: 0.6818825006484985] [G loss: 0.6794205904006958]\n",
      "[Epoch 74/1001] [Batch 24/372] [D loss: 0.6817077994346619] [G loss: 0.7128556370735168]\n",
      "[Epoch 74/1001] [Batch 25/372] [D loss: 0.6856123208999634] [G loss: 0.6145060658454895]\n",
      "[Epoch 74/1001] [Batch 26/372] [D loss: 0.6788115501403809] [G loss: 0.7445769309997559]\n",
      "[Epoch 74/1001] [Batch 27/372] [D loss: 0.6851494312286377] [G loss: 0.6041776537895203]\n",
      "[Epoch 74/1001] [Batch 28/372] [D loss: 0.6743374466896057] [G loss: 0.7324673533439636]\n",
      "[Epoch 74/1001] [Batch 29/372] [D loss: 0.6742191910743713] [G loss: 0.6646910905838013]\n",
      "[Epoch 74/1001] [Batch 30/372] [D loss: 0.6897568106651306] [G loss: 0.6577820777893066]\n",
      "[Epoch 74/1001] [Batch 31/372] [D loss: 0.6858009099960327] [G loss: 0.7306678891181946]\n",
      "[Epoch 74/1001] [Batch 32/372] [D loss: 0.6918073892593384] [G loss: 0.586370587348938]\n",
      "[Epoch 74/1001] [Batch 33/372] [D loss: 0.6884245872497559] [G loss: 0.8048620223999023]\n",
      "[Epoch 74/1001] [Batch 34/372] [D loss: 0.6834558248519897] [G loss: 0.5222055315971375]\n",
      "[Epoch 74/1001] [Batch 35/372] [D loss: 0.7059475779533386] [G loss: 0.8476444482803345]\n",
      "[Epoch 74/1001] [Batch 36/372] [D loss: 0.7036586999893188] [G loss: 0.5369213819503784]\n",
      "[Epoch 74/1001] [Batch 37/372] [D loss: 0.700580358505249] [G loss: 0.7630820870399475]\n",
      "[Epoch 74/1001] [Batch 38/372] [D loss: 0.6877921223640442] [G loss: 0.6401686072349548]\n",
      "[Epoch 74/1001] [Batch 39/372] [D loss: 0.6816734075546265] [G loss: 0.6655989289283752]\n",
      "[Epoch 74/1001] [Batch 40/372] [D loss: 0.6700255870819092] [G loss: 0.697607696056366]\n",
      "[Epoch 74/1001] [Batch 41/372] [D loss: 0.6871989965438843] [G loss: 0.651469349861145]\n",
      "[Epoch 74/1001] [Batch 42/372] [D loss: 0.6879106760025024] [G loss: 0.680496871471405]\n",
      "[Epoch 74/1001] [Batch 43/372] [D loss: 0.6726504564285278] [G loss: 0.6684893369674683]\n",
      "[Epoch 74/1001] [Batch 44/372] [D loss: 0.6802949905395508] [G loss: 0.6690044403076172]\n",
      "[Epoch 74/1001] [Batch 45/372] [D loss: 0.6781282424926758] [G loss: 0.6969919204711914]\n",
      "[Epoch 74/1001] [Batch 46/372] [D loss: 0.6781814098358154] [G loss: 0.6452499628067017]\n",
      "[Epoch 74/1001] [Batch 47/372] [D loss: 0.6838096380233765] [G loss: 0.6461817026138306]\n",
      "[Epoch 74/1001] [Batch 48/372] [D loss: 0.6745078563690186] [G loss: 0.7308141589164734]\n",
      "[Epoch 74/1001] [Batch 49/372] [D loss: 0.6733201742172241] [G loss: 0.5906850099563599]\n",
      "[Epoch 74/1001] [Batch 50/372] [D loss: 0.6794288158416748] [G loss: 0.8183438777923584]\n",
      "[Epoch 74/1001] [Batch 51/372] [D loss: 0.680195152759552] [G loss: 0.5096195936203003]\n",
      "[Epoch 74/1001] [Batch 52/372] [D loss: 0.7008477449417114] [G loss: 0.9127081036567688]\n",
      "[Epoch 74/1001] [Batch 53/372] [D loss: 0.705618143081665] [G loss: 0.47807812690734863]\n",
      "[Epoch 74/1001] [Batch 54/372] [D loss: 0.7014056444168091] [G loss: 0.8937037587165833]\n",
      "[Epoch 74/1001] [Batch 55/372] [D loss: 0.698570728302002] [G loss: 0.5481105446815491]\n",
      "[Epoch 74/1001] [Batch 56/372] [D loss: 0.6880612373352051] [G loss: 0.7764484882354736]\n",
      "[Epoch 74/1001] [Batch 57/372] [D loss: 0.6890737414360046] [G loss: 0.5960739850997925]\n",
      "[Epoch 74/1001] [Batch 58/372] [D loss: 0.6918243765830994] [G loss: 0.7845338582992554]\n",
      "[Epoch 74/1001] [Batch 59/372] [D loss: 0.678471565246582] [G loss: 0.5643759369850159]\n",
      "[Epoch 74/1001] [Batch 60/372] [D loss: 0.6842546463012695] [G loss: 0.7959957122802734]\n",
      "[Epoch 74/1001] [Batch 61/372] [D loss: 0.6834685206413269] [G loss: 0.6294441223144531]\n",
      "[Epoch 74/1001] [Batch 62/372] [D loss: 0.6895140409469604] [G loss: 0.6862260103225708]\n",
      "[Epoch 74/1001] [Batch 63/372] [D loss: 0.6815041303634644] [G loss: 0.6585732698440552]\n",
      "[Epoch 74/1001] [Batch 64/372] [D loss: 0.6832340955734253] [G loss: 0.7051140069961548]\n",
      "[Epoch 74/1001] [Batch 65/372] [D loss: 0.6749695539474487] [G loss: 0.6347013711929321]\n",
      "[Epoch 74/1001] [Batch 66/372] [D loss: 0.68421471118927] [G loss: 0.6994482278823853]\n",
      "[Epoch 74/1001] [Batch 67/372] [D loss: 0.6916149854660034] [G loss: 0.6470394134521484]\n",
      "[Epoch 74/1001] [Batch 68/372] [D loss: 0.6826502084732056] [G loss: 0.7016161680221558]\n",
      "[Epoch 74/1001] [Batch 69/372] [D loss: 0.6851823329925537] [G loss: 0.6198892593383789]\n",
      "[Epoch 74/1001] [Batch 70/372] [D loss: 0.6932984590530396] [G loss: 0.7202506065368652]\n",
      "[Epoch 74/1001] [Batch 71/372] [D loss: 0.6805861592292786] [G loss: 0.6461819410324097]\n",
      "[Epoch 74/1001] [Batch 72/372] [D loss: 0.6824579238891602] [G loss: 0.650935173034668]\n",
      "[Epoch 74/1001] [Batch 73/372] [D loss: 0.6842714548110962] [G loss: 0.734546422958374]\n",
      "[Epoch 74/1001] [Batch 74/372] [D loss: 0.6813268661499023] [G loss: 0.6163468956947327]\n",
      "[Epoch 74/1001] [Batch 75/372] [D loss: 0.6864757537841797] [G loss: 0.7221290469169617]\n",
      "[Epoch 74/1001] [Batch 76/372] [D loss: 0.6829788088798523] [G loss: 0.6037201285362244]\n",
      "[Epoch 74/1001] [Batch 77/372] [D loss: 0.6918287873268127] [G loss: 0.7400000095367432]\n",
      "[Epoch 74/1001] [Batch 78/372] [D loss: 0.6923352479934692] [G loss: 0.6021316647529602]\n",
      "[Epoch 74/1001] [Batch 79/372] [D loss: 0.6896325349807739] [G loss: 0.7423680424690247]\n",
      "[Epoch 74/1001] [Batch 80/372] [D loss: 0.6899080872535706] [G loss: 0.6200886368751526]\n",
      "[Epoch 74/1001] [Batch 81/372] [D loss: 0.6861864328384399] [G loss: 0.7088363766670227]\n",
      "[Epoch 74/1001] [Batch 82/372] [D loss: 0.6765991449356079] [G loss: 0.6522210240364075]\n",
      "[Epoch 74/1001] [Batch 83/372] [D loss: 0.6767071485519409] [G loss: 0.6772955060005188]\n",
      "[Epoch 74/1001] [Batch 84/372] [D loss: 0.6807128190994263] [G loss: 0.6608295440673828]\n",
      "[Epoch 74/1001] [Batch 85/372] [D loss: 0.6854325532913208] [G loss: 0.6749227046966553]\n",
      "[Epoch 74/1001] [Batch 86/372] [D loss: 0.6760483980178833] [G loss: 0.7102468013763428]\n",
      "[Epoch 74/1001] [Batch 87/372] [D loss: 0.6908029317855835] [G loss: 0.6066392660140991]\n",
      "[Epoch 74/1001] [Batch 88/372] [D loss: 0.6805758476257324] [G loss: 0.7494708299636841]\n",
      "[Epoch 74/1001] [Batch 89/372] [D loss: 0.687064528465271] [G loss: 0.6120821237564087]\n",
      "[Epoch 74/1001] [Batch 90/372] [D loss: 0.6722320318222046] [G loss: 0.7056487202644348]\n",
      "[Epoch 74/1001] [Batch 91/372] [D loss: 0.6836217641830444] [G loss: 0.6277827620506287]\n",
      "[Epoch 74/1001] [Batch 92/372] [D loss: 0.6727395057678223] [G loss: 0.7232338190078735]\n",
      "[Epoch 74/1001] [Batch 93/372] [D loss: 0.6790630221366882] [G loss: 0.6458350419998169]\n",
      "[Epoch 74/1001] [Batch 94/372] [D loss: 0.6867837905883789] [G loss: 0.7018336057662964]\n",
      "[Epoch 74/1001] [Batch 95/372] [D loss: 0.6750514507293701] [G loss: 0.619699239730835]\n",
      "[Epoch 74/1001] [Batch 96/372] [D loss: 0.6820278167724609] [G loss: 0.707607626914978]\n",
      "[Epoch 74/1001] [Batch 97/372] [D loss: 0.6859394311904907] [G loss: 0.6359102129936218]\n",
      "[Epoch 74/1001] [Batch 98/372] [D loss: 0.6854080557823181] [G loss: 0.7323602437973022]\n",
      "[Epoch 74/1001] [Batch 99/372] [D loss: 0.6726673245429993] [G loss: 0.5786546468734741]\n",
      "[Epoch 74/1001] [Batch 100/372] [D loss: 0.6888320446014404] [G loss: 0.766896665096283]\n",
      "[Epoch 74/1001] [Batch 101/372] [D loss: 0.6899298429489136] [G loss: 0.5817031264305115]\n",
      "[Epoch 74/1001] [Batch 102/372] [D loss: 0.6890990734100342] [G loss: 0.7959116101264954]\n",
      "[Epoch 74/1001] [Batch 103/372] [D loss: 0.685550332069397] [G loss: 0.5819449424743652]\n",
      "[Epoch 74/1001] [Batch 104/372] [D loss: 0.6809857487678528] [G loss: 0.7538824081420898]\n",
      "[Epoch 74/1001] [Batch 105/372] [D loss: 0.6761534214019775] [G loss: 0.5934539437294006]\n",
      "[Epoch 74/1001] [Batch 106/372] [D loss: 0.6805822849273682] [G loss: 0.7614808678627014]\n",
      "[Epoch 74/1001] [Batch 107/372] [D loss: 0.6896680593490601] [G loss: 0.561759352684021]\n",
      "[Epoch 74/1001] [Batch 108/372] [D loss: 0.6872255802154541] [G loss: 0.8699363470077515]\n",
      "[Epoch 74/1001] [Batch 109/372] [D loss: 0.6985726356506348] [G loss: 0.48499971628189087]\n",
      "[Epoch 74/1001] [Batch 110/372] [D loss: 0.6930971741676331] [G loss: 0.9061346650123596]\n",
      "[Epoch 74/1001] [Batch 111/372] [D loss: 0.7092949151992798] [G loss: 0.5034171938896179]\n",
      "[Epoch 74/1001] [Batch 112/372] [D loss: 0.6915443539619446] [G loss: 0.8723801970481873]\n",
      "[Epoch 74/1001] [Batch 113/372] [D loss: 0.6924186944961548] [G loss: 0.5677165389060974]\n",
      "[Epoch 74/1001] [Batch 114/372] [D loss: 0.6909024715423584] [G loss: 0.7276644706726074]\n",
      "[Epoch 74/1001] [Batch 115/372] [D loss: 0.6861873865127563] [G loss: 0.6417299509048462]\n",
      "[Epoch 74/1001] [Batch 116/372] [D loss: 0.6742684841156006] [G loss: 0.6657171249389648]\n",
      "[Epoch 74/1001] [Batch 117/372] [D loss: 0.6840418577194214] [G loss: 0.7064412832260132]\n",
      "[Epoch 74/1001] [Batch 118/372] [D loss: 0.6814619302749634] [G loss: 0.6185462474822998]\n",
      "[Epoch 74/1001] [Batch 119/372] [D loss: 0.6991035342216492] [G loss: 0.7436414361000061]\n",
      "[Epoch 74/1001] [Batch 120/372] [D loss: 0.6885544061660767] [G loss: 0.5897191762924194]\n",
      "[Epoch 74/1001] [Batch 121/372] [D loss: 0.6762118935585022] [G loss: 0.7493320107460022]\n",
      "[Epoch 74/1001] [Batch 122/372] [D loss: 0.676934003829956] [G loss: 0.5972819924354553]\n",
      "[Epoch 74/1001] [Batch 123/372] [D loss: 0.6831345558166504] [G loss: 0.818395733833313]\n",
      "[Epoch 74/1001] [Batch 124/372] [D loss: 0.6963099241256714] [G loss: 0.5507675409317017]\n",
      "[Epoch 74/1001] [Batch 125/372] [D loss: 0.685413122177124] [G loss: 0.8221167922019958]\n",
      "[Epoch 74/1001] [Batch 126/372] [D loss: 0.6747815012931824] [G loss: 0.5767562985420227]\n",
      "[Epoch 74/1001] [Batch 127/372] [D loss: 0.6927200555801392] [G loss: 0.7542678117752075]\n",
      "[Epoch 74/1001] [Batch 128/372] [D loss: 0.6930502653121948] [G loss: 0.6259081959724426]\n",
      "[Epoch 74/1001] [Batch 129/372] [D loss: 0.6842296123504639] [G loss: 0.6927202939987183]\n",
      "[Epoch 74/1001] [Batch 130/372] [D loss: 0.6876234412193298] [G loss: 0.6662660837173462]\n",
      "[Epoch 74/1001] [Batch 131/372] [D loss: 0.6721897125244141] [G loss: 0.6544855833053589]\n",
      "[Epoch 74/1001] [Batch 132/372] [D loss: 0.6842565536499023] [G loss: 0.6759174466133118]\n",
      "[Epoch 74/1001] [Batch 133/372] [D loss: 0.6865290999412537] [G loss: 0.6850979924201965]\n",
      "[Epoch 74/1001] [Batch 134/372] [D loss: 0.6736173629760742] [G loss: 0.6262164115905762]\n",
      "[Epoch 74/1001] [Batch 135/372] [D loss: 0.6767739057540894] [G loss: 0.7451176643371582]\n",
      "[Epoch 74/1001] [Batch 136/372] [D loss: 0.6759930849075317] [G loss: 0.5940419435501099]\n",
      "[Epoch 74/1001] [Batch 137/372] [D loss: 0.688327968120575] [G loss: 0.7809835076332092]\n",
      "[Epoch 74/1001] [Batch 138/372] [D loss: 0.6758778095245361] [G loss: 0.5479501485824585]\n",
      "[Epoch 74/1001] [Batch 139/372] [D loss: 0.6850994825363159] [G loss: 0.8450897336006165]\n",
      "[Epoch 74/1001] [Batch 140/372] [D loss: 0.6965619325637817] [G loss: 0.4889642894268036]\n",
      "[Epoch 74/1001] [Batch 141/372] [D loss: 0.7017397880554199] [G loss: 0.9023466110229492]\n",
      "[Epoch 74/1001] [Batch 142/372] [D loss: 0.6981441378593445] [G loss: 0.5297880172729492]\n",
      "[Epoch 74/1001] [Batch 143/372] [D loss: 0.6919267177581787] [G loss: 0.803864598274231]\n",
      "[Epoch 74/1001] [Batch 144/372] [D loss: 0.6804375648498535] [G loss: 0.6086809039115906]\n",
      "[Epoch 74/1001] [Batch 145/372] [D loss: 0.6738313436508179] [G loss: 0.7133145928382874]\n",
      "[Epoch 74/1001] [Batch 146/372] [D loss: 0.6862305402755737] [G loss: 0.6547032594680786]\n",
      "[Epoch 74/1001] [Batch 147/372] [D loss: 0.6730253100395203] [G loss: 0.680576741695404]\n",
      "[Epoch 74/1001] [Batch 148/372] [D loss: 0.6850540637969971] [G loss: 0.721632719039917]\n",
      "[Epoch 74/1001] [Batch 149/372] [D loss: 0.6808995008468628] [G loss: 0.594409704208374]\n",
      "[Epoch 74/1001] [Batch 150/372] [D loss: 0.6904141902923584] [G loss: 0.7596155405044556]\n",
      "[Epoch 74/1001] [Batch 151/372] [D loss: 0.6970498561859131] [G loss: 0.5761022567749023]\n",
      "[Epoch 74/1001] [Batch 152/372] [D loss: 0.6919766068458557] [G loss: 0.8177763223648071]\n",
      "[Epoch 74/1001] [Batch 153/372] [D loss: 0.686689555644989] [G loss: 0.5519402027130127]\n",
      "[Epoch 74/1001] [Batch 154/372] [D loss: 0.6904796361923218] [G loss: 0.7908177971839905]\n",
      "[Epoch 74/1001] [Batch 155/372] [D loss: 0.677280068397522] [G loss: 0.6244803667068481]\n",
      "[Epoch 74/1001] [Batch 156/372] [D loss: 0.6842502951622009] [G loss: 0.6741946935653687]\n",
      "[Epoch 74/1001] [Batch 157/372] [D loss: 0.6830266714096069] [G loss: 0.6878282427787781]\n",
      "[Epoch 74/1001] [Batch 158/372] [D loss: 0.6828143000602722] [G loss: 0.660121738910675]\n",
      "[Epoch 74/1001] [Batch 159/372] [D loss: 0.6851271390914917] [G loss: 0.6619521975517273]\n",
      "[Epoch 74/1001] [Batch 160/372] [D loss: 0.6770128011703491] [G loss: 0.6943170428276062]\n",
      "[Epoch 74/1001] [Batch 161/372] [D loss: 0.6869907379150391] [G loss: 0.6602264046669006]\n",
      "[Epoch 74/1001] [Batch 162/372] [D loss: 0.6861914992332458] [G loss: 0.6780446767807007]\n",
      "[Epoch 74/1001] [Batch 163/372] [D loss: 0.6777026653289795] [G loss: 0.6928145885467529]\n",
      "[Epoch 74/1001] [Batch 164/372] [D loss: 0.6843740940093994] [G loss: 0.6154464483261108]\n",
      "[Epoch 74/1001] [Batch 165/372] [D loss: 0.6834189295768738] [G loss: 0.7236301898956299]\n",
      "[Epoch 74/1001] [Batch 166/372] [D loss: 0.6886463165283203] [G loss: 0.6354538202285767]\n",
      "[Epoch 74/1001] [Batch 167/372] [D loss: 0.685052752494812] [G loss: 0.7193611860275269]\n",
      "[Epoch 74/1001] [Batch 168/372] [D loss: 0.6785191297531128] [G loss: 0.6193514466285706]\n",
      "[Epoch 74/1001] [Batch 169/372] [D loss: 0.6816987991333008] [G loss: 0.7080270051956177]\n",
      "[Epoch 74/1001] [Batch 170/372] [D loss: 0.68485426902771] [G loss: 0.6217050552368164]\n",
      "[Epoch 74/1001] [Batch 171/372] [D loss: 0.68406081199646] [G loss: 0.7651992440223694]\n",
      "[Epoch 74/1001] [Batch 172/372] [D loss: 0.6899206042289734] [G loss: 0.5835379362106323]\n",
      "[Epoch 74/1001] [Batch 173/372] [D loss: 0.6898074150085449] [G loss: 0.7442609667778015]\n",
      "[Epoch 74/1001] [Batch 174/372] [D loss: 0.6827480792999268] [G loss: 0.5891445279121399]\n",
      "[Epoch 74/1001] [Batch 175/372] [D loss: 0.6855043172836304] [G loss: 0.7843092679977417]\n",
      "[Epoch 74/1001] [Batch 176/372] [D loss: 0.6949486136436462] [G loss: 0.574906587600708]\n",
      "[Epoch 74/1001] [Batch 177/372] [D loss: 0.6972916126251221] [G loss: 0.7257390022277832]\n",
      "[Epoch 74/1001] [Batch 178/372] [D loss: 0.6910741925239563] [G loss: 0.6178798675537109]\n",
      "[Epoch 74/1001] [Batch 179/372] [D loss: 0.6831526756286621] [G loss: 0.7196770906448364]\n",
      "[Epoch 74/1001] [Batch 180/372] [D loss: 0.680902361869812] [G loss: 0.6103177666664124]\n",
      "[Epoch 74/1001] [Batch 181/372] [D loss: 0.6938133239746094] [G loss: 0.7020910382270813]\n",
      "[Epoch 74/1001] [Batch 182/372] [D loss: 0.6802566051483154] [G loss: 0.6424502730369568]\n",
      "[Epoch 74/1001] [Batch 183/372] [D loss: 0.6888367533683777] [G loss: 0.6900322437286377]\n",
      "[Epoch 74/1001] [Batch 184/372] [D loss: 0.6824209690093994] [G loss: 0.6560567617416382]\n",
      "[Epoch 74/1001] [Batch 185/372] [D loss: 0.695792555809021] [G loss: 0.6831809878349304]\n",
      "[Epoch 74/1001] [Batch 186/372] [D loss: 0.6758201122283936] [G loss: 0.6455467343330383]\n",
      "[Epoch 74/1001] [Batch 187/372] [D loss: 0.6823177337646484] [G loss: 0.7282106876373291]\n",
      "[Epoch 74/1001] [Batch 188/372] [D loss: 0.6900315284729004] [G loss: 0.6140591502189636]\n",
      "[Epoch 74/1001] [Batch 189/372] [D loss: 0.6884171962738037] [G loss: 0.7582980990409851]\n",
      "[Epoch 74/1001] [Batch 190/372] [D loss: 0.6857739090919495] [G loss: 0.5761144161224365]\n",
      "[Epoch 74/1001] [Batch 191/372] [D loss: 0.6902380585670471] [G loss: 0.7882154583930969]\n",
      "[Epoch 74/1001] [Batch 192/372] [D loss: 0.6927095055580139] [G loss: 0.5772891640663147]\n",
      "[Epoch 74/1001] [Batch 193/372] [D loss: 0.6941699981689453] [G loss: 0.7412634491920471]\n",
      "[Epoch 74/1001] [Batch 194/372] [D loss: 0.6842726469039917] [G loss: 0.6105974316596985]\n",
      "[Epoch 74/1001] [Batch 195/372] [D loss: 0.6879608631134033] [G loss: 0.7289420366287231]\n",
      "[Epoch 74/1001] [Batch 196/372] [D loss: 0.6915388107299805] [G loss: 0.6154601573944092]\n",
      "[Epoch 74/1001] [Batch 197/372] [D loss: 0.6761332154273987] [G loss: 0.7372299432754517]\n",
      "[Epoch 74/1001] [Batch 198/372] [D loss: 0.687963604927063] [G loss: 0.6052765846252441]\n",
      "[Epoch 74/1001] [Batch 199/372] [D loss: 0.6822022795677185] [G loss: 0.7248491048812866]\n",
      "[Epoch 74/1001] [Batch 200/372] [D loss: 0.6933327913284302] [G loss: 0.6277992725372314]\n",
      "[Epoch 74/1001] [Batch 201/372] [D loss: 0.6849005222320557] [G loss: 0.740688681602478]\n",
      "[Epoch 74/1001] [Batch 202/372] [D loss: 0.6813652515411377] [G loss: 0.6058529615402222]\n",
      "[Epoch 74/1001] [Batch 203/372] [D loss: 0.6830285787582397] [G loss: 0.7596580386161804]\n",
      "[Epoch 74/1001] [Batch 204/372] [D loss: 0.681706428527832] [G loss: 0.5707975029945374]\n",
      "[Epoch 74/1001] [Batch 205/372] [D loss: 0.6788987517356873] [G loss: 0.7682727575302124]\n",
      "[Epoch 74/1001] [Batch 206/372] [D loss: 0.6851091384887695] [G loss: 0.6163398623466492]\n",
      "[Epoch 74/1001] [Batch 207/372] [D loss: 0.6805490255355835] [G loss: 0.7362236976623535]\n",
      "[Epoch 74/1001] [Batch 208/372] [D loss: 0.6823266744613647] [G loss: 0.600994884967804]\n",
      "[Epoch 74/1001] [Batch 209/372] [D loss: 0.6894795894622803] [G loss: 0.7225974202156067]\n",
      "[Epoch 74/1001] [Batch 210/372] [D loss: 0.6828346252441406] [G loss: 0.6210492849349976]\n",
      "[Epoch 74/1001] [Batch 211/372] [D loss: 0.6760232448577881] [G loss: 0.7218330502510071]\n",
      "[Epoch 74/1001] [Batch 212/372] [D loss: 0.6879245042800903] [G loss: 0.6122260689735413]\n",
      "[Epoch 74/1001] [Batch 213/372] [D loss: 0.6886566877365112] [G loss: 0.717278778553009]\n",
      "[Epoch 74/1001] [Batch 214/372] [D loss: 0.6892188787460327] [G loss: 0.6166220903396606]\n",
      "[Epoch 74/1001] [Batch 215/372] [D loss: 0.6781243085861206] [G loss: 0.7397975325584412]\n",
      "[Epoch 74/1001] [Batch 216/372] [D loss: 0.6969733238220215] [G loss: 0.5427694916725159]\n",
      "[Epoch 74/1001] [Batch 217/372] [D loss: 0.6979024410247803] [G loss: 0.8989280462265015]\n",
      "[Epoch 74/1001] [Batch 218/372] [D loss: 0.709649920463562] [G loss: 0.477033406496048]\n",
      "[Epoch 74/1001] [Batch 219/372] [D loss: 0.7100307941436768] [G loss: 0.8599319458007812]\n",
      "[Epoch 74/1001] [Batch 220/372] [D loss: 0.7056789398193359] [G loss: 0.5540123581886292]\n",
      "[Epoch 74/1001] [Batch 221/372] [D loss: 0.6915574669837952] [G loss: 0.7578979730606079]\n",
      "[Epoch 74/1001] [Batch 222/372] [D loss: 0.6835981607437134] [G loss: 0.6281635761260986]\n",
      "[Epoch 74/1001] [Batch 223/372] [D loss: 0.6901606321334839] [G loss: 0.6760295629501343]\n",
      "[Epoch 74/1001] [Batch 224/372] [D loss: 0.6841356754302979] [G loss: 0.6950126886367798]\n",
      "[Epoch 74/1001] [Batch 225/372] [D loss: 0.6812146902084351] [G loss: 0.650641918182373]\n",
      "[Epoch 74/1001] [Batch 226/372] [D loss: 0.6763661503791809] [G loss: 0.6952652931213379]\n",
      "[Epoch 74/1001] [Batch 227/372] [D loss: 0.679831862449646] [G loss: 0.6575291156768799]\n",
      "[Epoch 74/1001] [Batch 228/372] [D loss: 0.6781748533248901] [G loss: 0.6987555027008057]\n",
      "[Epoch 74/1001] [Batch 229/372] [D loss: 0.6808539628982544] [G loss: 0.6414256691932678]\n",
      "[Epoch 74/1001] [Batch 230/372] [D loss: 0.678507387638092] [G loss: 0.7233745455741882]\n",
      "[Epoch 74/1001] [Batch 231/372] [D loss: 0.6771928668022156] [G loss: 0.5997307300567627]\n",
      "[Epoch 74/1001] [Batch 232/372] [D loss: 0.6984195709228516] [G loss: 0.7822980880737305]\n",
      "[Epoch 74/1001] [Batch 233/372] [D loss: 0.6795495748519897] [G loss: 0.619845449924469]\n",
      "[Epoch 74/1001] [Batch 234/372] [D loss: 0.6898972988128662] [G loss: 0.6790475845336914]\n",
      "[Epoch 74/1001] [Batch 235/372] [D loss: 0.683020830154419] [G loss: 0.7147146463394165]\n",
      "[Epoch 74/1001] [Batch 236/372] [D loss: 0.6843804121017456] [G loss: 0.6217496991157532]\n",
      "[Epoch 74/1001] [Batch 237/372] [D loss: 0.6807793378829956] [G loss: 0.7014234066009521]\n",
      "[Epoch 74/1001] [Batch 238/372] [D loss: 0.6886322498321533] [G loss: 0.641331672668457]\n",
      "[Epoch 74/1001] [Batch 239/372] [D loss: 0.6936636567115784] [G loss: 0.7497035264968872]\n",
      "[Epoch 74/1001] [Batch 240/372] [D loss: 0.686100959777832] [G loss: 0.5821748971939087]\n",
      "[Epoch 74/1001] [Batch 241/372] [D loss: 0.6857236623764038] [G loss: 0.73638916015625]\n",
      "[Epoch 74/1001] [Batch 242/372] [D loss: 0.6795840859413147] [G loss: 0.6088337898254395]\n",
      "[Epoch 74/1001] [Batch 243/372] [D loss: 0.6915934681892395] [G loss: 0.7455465793609619]\n",
      "[Epoch 74/1001] [Batch 244/372] [D loss: 0.7004439830780029] [G loss: 0.6044682264328003]\n",
      "[Epoch 74/1001] [Batch 245/372] [D loss: 0.6914114952087402] [G loss: 0.72685706615448]\n",
      "[Epoch 74/1001] [Batch 246/372] [D loss: 0.6842087507247925] [G loss: 0.6338694095611572]\n",
      "[Epoch 74/1001] [Batch 247/372] [D loss: 0.6881797313690186] [G loss: 0.6823524236679077]\n",
      "[Epoch 74/1001] [Batch 248/372] [D loss: 0.6817225217819214] [G loss: 0.7214565873146057]\n",
      "[Epoch 74/1001] [Batch 249/372] [D loss: 0.6779483556747437] [G loss: 0.6170802712440491]\n",
      "[Epoch 74/1001] [Batch 250/372] [D loss: 0.6906816959381104] [G loss: 0.7564652562141418]\n",
      "[Epoch 74/1001] [Batch 251/372] [D loss: 0.6791800260543823] [G loss: 0.6109497547149658]\n",
      "[Epoch 74/1001] [Batch 252/372] [D loss: 0.6916569471359253] [G loss: 0.7367338538169861]\n",
      "[Epoch 74/1001] [Batch 253/372] [D loss: 0.6905502080917358] [G loss: 0.5800005197525024]\n",
      "[Epoch 74/1001] [Batch 254/372] [D loss: 0.6976625323295593] [G loss: 0.7960724234580994]\n",
      "[Epoch 74/1001] [Batch 255/372] [D loss: 0.7027477025985718] [G loss: 0.5381714105606079]\n",
      "[Epoch 74/1001] [Batch 256/372] [D loss: 0.704959511756897] [G loss: 0.787567675113678]\n",
      "[Epoch 74/1001] [Batch 257/372] [D loss: 0.695688009262085] [G loss: 0.5768424272537231]\n",
      "[Epoch 74/1001] [Batch 258/372] [D loss: 0.6886583566665649] [G loss: 0.7636879682540894]\n",
      "[Epoch 74/1001] [Batch 259/372] [D loss: 0.6870435476303101] [G loss: 0.6122627854347229]\n",
      "[Epoch 74/1001] [Batch 260/372] [D loss: 0.6906046271324158] [G loss: 0.7451013922691345]\n",
      "[Epoch 74/1001] [Batch 261/372] [D loss: 0.6958252787590027] [G loss: 0.5925111770629883]\n",
      "[Epoch 74/1001] [Batch 262/372] [D loss: 0.6834110021591187] [G loss: 0.7437765598297119]\n",
      "[Epoch 74/1001] [Batch 263/372] [D loss: 0.686877429485321] [G loss: 0.6106405258178711]\n",
      "[Epoch 74/1001] [Batch 264/372] [D loss: 0.6888630390167236] [G loss: 0.7550393342971802]\n",
      "[Epoch 74/1001] [Batch 265/372] [D loss: 0.6828151941299438] [G loss: 0.5852756500244141]\n",
      "[Epoch 74/1001] [Batch 266/372] [D loss: 0.6886709928512573] [G loss: 0.753684937953949]\n",
      "[Epoch 74/1001] [Batch 267/372] [D loss: 0.6835005879402161] [G loss: 0.6158351898193359]\n",
      "[Epoch 74/1001] [Batch 268/372] [D loss: 0.6968535780906677] [G loss: 0.7156957387924194]\n",
      "[Epoch 74/1001] [Batch 269/372] [D loss: 0.6895511150360107] [G loss: 0.6037801504135132]\n",
      "[Epoch 74/1001] [Batch 270/372] [D loss: 0.6801797151565552] [G loss: 0.8040871620178223]\n",
      "[Epoch 74/1001] [Batch 271/372] [D loss: 0.684810996055603] [G loss: 0.5243470072746277]\n",
      "[Epoch 74/1001] [Batch 272/372] [D loss: 0.6951964497566223] [G loss: 0.929399847984314]\n",
      "[Epoch 74/1001] [Batch 273/372] [D loss: 0.6930006146430969] [G loss: 0.45590758323669434]\n",
      "[Epoch 74/1001] [Batch 274/372] [D loss: 0.7074823379516602] [G loss: 0.9481531381607056]\n",
      "[Epoch 74/1001] [Batch 275/372] [D loss: 0.7117823362350464] [G loss: 0.5181757807731628]\n",
      "[Epoch 74/1001] [Batch 276/372] [D loss: 0.7020623683929443] [G loss: 0.7673279643058777]\n",
      "[Epoch 74/1001] [Batch 277/372] [D loss: 0.6957638263702393] [G loss: 0.6301855444908142]\n",
      "[Epoch 74/1001] [Batch 278/372] [D loss: 0.6722137331962585] [G loss: 0.6725603342056274]\n",
      "[Epoch 74/1001] [Batch 279/372] [D loss: 0.6816527843475342] [G loss: 0.7094947695732117]\n",
      "[Epoch 74/1001] [Batch 280/372] [D loss: 0.6808122396469116] [G loss: 0.6229122877120972]\n",
      "[Epoch 74/1001] [Batch 281/372] [D loss: 0.6898664236068726] [G loss: 0.7388843297958374]\n",
      "[Epoch 74/1001] [Batch 282/372] [D loss: 0.687538743019104] [G loss: 0.6149095892906189]\n",
      "[Epoch 74/1001] [Batch 283/372] [D loss: 0.6887753009796143] [G loss: 0.7244234085083008]\n",
      "[Epoch 74/1001] [Batch 284/372] [D loss: 0.6813535690307617] [G loss: 0.6596757173538208]\n",
      "[Epoch 74/1001] [Batch 285/372] [D loss: 0.6746609210968018] [G loss: 0.6660654544830322]\n",
      "[Epoch 74/1001] [Batch 286/372] [D loss: 0.6797755360603333] [G loss: 0.6856253147125244]\n",
      "[Epoch 74/1001] [Batch 287/372] [D loss: 0.7002203464508057] [G loss: 0.6632779836654663]\n",
      "[Epoch 74/1001] [Batch 288/372] [D loss: 0.6882846355438232] [G loss: 0.6783528327941895]\n",
      "[Epoch 74/1001] [Batch 289/372] [D loss: 0.6955299377441406] [G loss: 0.6464136838912964]\n",
      "[Epoch 74/1001] [Batch 290/372] [D loss: 0.6832451224327087] [G loss: 0.6886418461799622]\n",
      "[Epoch 74/1001] [Batch 291/372] [D loss: 0.685073733329773] [G loss: 0.6733537912368774]\n",
      "[Epoch 74/1001] [Batch 292/372] [D loss: 0.6867525577545166] [G loss: 0.6466010212898254]\n",
      "[Epoch 74/1001] [Batch 293/372] [D loss: 0.6953202486038208] [G loss: 0.6912778615951538]\n",
      "[Epoch 74/1001] [Batch 294/372] [D loss: 0.6788976192474365] [G loss: 0.6368919610977173]\n",
      "[Epoch 74/1001] [Batch 295/372] [D loss: 0.6826480627059937] [G loss: 0.6922322511672974]\n",
      "[Epoch 74/1001] [Batch 296/372] [D loss: 0.672653079032898] [G loss: 0.6626882553100586]\n",
      "[Epoch 74/1001] [Batch 297/372] [D loss: 0.6865158081054688] [G loss: 0.76845383644104]\n",
      "[Epoch 74/1001] [Batch 298/372] [D loss: 0.6886028051376343] [G loss: 0.5710735321044922]\n",
      "[Epoch 74/1001] [Batch 299/372] [D loss: 0.6920271515846252] [G loss: 0.7728614807128906]\n",
      "[Epoch 74/1001] [Batch 300/372] [D loss: 0.6908712387084961] [G loss: 0.5939748287200928]\n",
      "[Epoch 74/1001] [Batch 301/372] [D loss: 0.6867678165435791] [G loss: 0.7319234609603882]\n",
      "[Epoch 74/1001] [Batch 302/372] [D loss: 0.6844006776809692] [G loss: 0.6497406363487244]\n",
      "[Epoch 74/1001] [Batch 303/372] [D loss: 0.6829558610916138] [G loss: 0.6623179316520691]\n",
      "[Epoch 74/1001] [Batch 304/372] [D loss: 0.6768038868904114] [G loss: 0.6989849805831909]\n",
      "[Epoch 74/1001] [Batch 305/372] [D loss: 0.6839345693588257] [G loss: 0.6565605998039246]\n",
      "[Epoch 74/1001] [Batch 306/372] [D loss: 0.6847853660583496] [G loss: 0.6870802640914917]\n",
      "[Epoch 74/1001] [Batch 307/372] [D loss: 0.6808520555496216] [G loss: 0.6488265991210938]\n",
      "[Epoch 74/1001] [Batch 308/372] [D loss: 0.6902879476547241] [G loss: 0.7006927728652954]\n",
      "[Epoch 74/1001] [Batch 309/372] [D loss: 0.6747379302978516] [G loss: 0.6195318102836609]\n",
      "[Epoch 74/1001] [Batch 310/372] [D loss: 0.6879421472549438] [G loss: 0.7706050276756287]\n",
      "[Epoch 74/1001] [Batch 311/372] [D loss: 0.684991717338562] [G loss: 0.5905142426490784]\n",
      "[Epoch 74/1001] [Batch 312/372] [D loss: 0.6856472492218018] [G loss: 0.7433927059173584]\n",
      "[Epoch 74/1001] [Batch 313/372] [D loss: 0.6932425498962402] [G loss: 0.6637334823608398]\n",
      "[Epoch 74/1001] [Batch 314/372] [D loss: 0.6917232275009155] [G loss: 0.6479259133338928]\n",
      "[Epoch 74/1001] [Batch 315/372] [D loss: 0.6794738173484802] [G loss: 0.7259992957115173]\n",
      "[Epoch 74/1001] [Batch 316/372] [D loss: 0.6725577116012573] [G loss: 0.6295523047447205]\n",
      "[Epoch 74/1001] [Batch 317/372] [D loss: 0.687275767326355] [G loss: 0.710028350353241]\n",
      "[Epoch 74/1001] [Batch 318/372] [D loss: 0.6697456240653992] [G loss: 0.6576429009437561]\n",
      "[Epoch 74/1001] [Batch 319/372] [D loss: 0.6724714040756226] [G loss: 0.7132779359817505]\n",
      "[Epoch 74/1001] [Batch 320/372] [D loss: 0.688197135925293] [G loss: 0.6124154925346375]\n",
      "[Epoch 74/1001] [Batch 321/372] [D loss: 0.6773821115493774] [G loss: 0.8384506702423096]\n",
      "[Epoch 74/1001] [Batch 322/372] [D loss: 0.6877917647361755] [G loss: 0.4928511679172516]\n",
      "[Epoch 74/1001] [Batch 323/372] [D loss: 0.7062340974807739] [G loss: 0.8747105598449707]\n",
      "[Epoch 74/1001] [Batch 324/372] [D loss: 0.7008823752403259] [G loss: 0.5308336019515991]\n",
      "[Epoch 74/1001] [Batch 325/372] [D loss: 0.7075715065002441] [G loss: 0.7759498953819275]\n",
      "[Epoch 74/1001] [Batch 326/372] [D loss: 0.6882859468460083] [G loss: 0.6135246753692627]\n",
      "[Epoch 74/1001] [Batch 327/372] [D loss: 0.6794840097427368] [G loss: 0.7114846706390381]\n",
      "[Epoch 74/1001] [Batch 328/372] [D loss: 0.6897914409637451] [G loss: 0.6414684057235718]\n",
      "[Epoch 74/1001] [Batch 329/372] [D loss: 0.6826806664466858] [G loss: 0.712177574634552]\n",
      "[Epoch 74/1001] [Batch 330/372] [D loss: 0.6963921785354614] [G loss: 0.6316723823547363]\n",
      "[Epoch 74/1001] [Batch 331/372] [D loss: 0.6803873777389526] [G loss: 0.7410293221473694]\n",
      "[Epoch 74/1001] [Batch 332/372] [D loss: 0.6868759393692017] [G loss: 0.6637614965438843]\n",
      "[Epoch 74/1001] [Batch 333/372] [D loss: 0.6834006309509277] [G loss: 0.657791018486023]\n",
      "[Epoch 74/1001] [Batch 334/372] [D loss: 0.6938146948814392] [G loss: 0.6901382803916931]\n",
      "[Epoch 74/1001] [Batch 335/372] [D loss: 0.6764392852783203] [G loss: 0.7093070149421692]\n",
      "[Epoch 74/1001] [Batch 336/372] [D loss: 0.687432050704956] [G loss: 0.6059115529060364]\n",
      "[Epoch 74/1001] [Batch 337/372] [D loss: 0.6899838447570801] [G loss: 0.7334728837013245]\n",
      "[Epoch 74/1001] [Batch 338/372] [D loss: 0.6801879405975342] [G loss: 0.6267392635345459]\n",
      "[Epoch 74/1001] [Batch 339/372] [D loss: 0.6883649230003357] [G loss: 0.6895575523376465]\n",
      "[Epoch 74/1001] [Batch 340/372] [D loss: 0.6859132051467896] [G loss: 0.6783426403999329]\n",
      "[Epoch 74/1001] [Batch 341/372] [D loss: 0.68095862865448] [G loss: 0.6530396342277527]\n",
      "[Epoch 74/1001] [Batch 342/372] [D loss: 0.6900302171707153] [G loss: 0.6660135984420776]\n",
      "[Epoch 74/1001] [Batch 343/372] [D loss: 0.6688840389251709] [G loss: 0.6925954818725586]\n",
      "[Epoch 74/1001] [Batch 344/372] [D loss: 0.6732066869735718] [G loss: 0.6449639797210693]\n",
      "[Epoch 74/1001] [Batch 345/372] [D loss: 0.6755238771438599] [G loss: 0.7247592210769653]\n",
      "[Epoch 74/1001] [Batch 346/372] [D loss: 0.6781193017959595] [G loss: 0.6223816871643066]\n",
      "[Epoch 74/1001] [Batch 347/372] [D loss: 0.6780695915222168] [G loss: 0.8219538331031799]\n",
      "[Epoch 74/1001] [Batch 348/372] [D loss: 0.6919894218444824] [G loss: 0.46267494559288025]\n",
      "[Epoch 74/1001] [Batch 349/372] [D loss: 0.7230655550956726] [G loss: 1.026815414428711]\n",
      "[Epoch 74/1001] [Batch 350/372] [D loss: 0.7451274991035461] [G loss: 0.4188387393951416]\n",
      "[Epoch 74/1001] [Batch 351/372] [D loss: 0.7354161739349365] [G loss: 0.866083562374115]\n",
      "[Epoch 74/1001] [Batch 352/372] [D loss: 0.7039111852645874] [G loss: 0.5981528162956238]\n",
      "[Epoch 74/1001] [Batch 353/372] [D loss: 0.6847074031829834] [G loss: 0.655666172504425]\n",
      "[Epoch 74/1001] [Batch 354/372] [D loss: 0.6828697919845581] [G loss: 0.7411170601844788]\n",
      "[Epoch 74/1001] [Batch 355/372] [D loss: 0.6770526170730591] [G loss: 0.5963551998138428]\n",
      "[Epoch 74/1001] [Batch 356/372] [D loss: 0.6858543157577515] [G loss: 0.7260746955871582]\n",
      "[Epoch 74/1001] [Batch 357/372] [D loss: 0.6690621376037598] [G loss: 0.6677354574203491]\n",
      "[Epoch 74/1001] [Batch 358/372] [D loss: 0.6843241453170776] [G loss: 0.6648335456848145]\n",
      "[Epoch 74/1001] [Batch 359/372] [D loss: 0.6941226720809937] [G loss: 0.6887403726577759]\n",
      "[Epoch 74/1001] [Batch 360/372] [D loss: 0.6860071420669556] [G loss: 0.6210794448852539]\n",
      "[Epoch 74/1001] [Batch 361/372] [D loss: 0.6878246068954468] [G loss: 0.7213146686553955]\n",
      "[Epoch 74/1001] [Batch 362/372] [D loss: 0.6917792558670044] [G loss: 0.6177465915679932]\n",
      "[Epoch 74/1001] [Batch 363/372] [D loss: 0.6814896464347839] [G loss: 0.7131067514419556]\n",
      "[Epoch 74/1001] [Batch 364/372] [D loss: 0.6836394667625427] [G loss: 0.6550600528717041]\n",
      "[Epoch 74/1001] [Batch 365/372] [D loss: 0.6895002126693726] [G loss: 0.6532466411590576]\n",
      "[Epoch 74/1001] [Batch 366/372] [D loss: 0.6900342106819153] [G loss: 0.715260922908783]\n",
      "[Epoch 74/1001] [Batch 367/372] [D loss: 0.6800309419631958] [G loss: 0.6252838969230652]\n",
      "[Epoch 74/1001] [Batch 368/372] [D loss: 0.678172767162323] [G loss: 0.7148779630661011]\n",
      "[Epoch 74/1001] [Batch 369/372] [D loss: 0.6851367950439453] [G loss: 0.6259565353393555]\n",
      "[Epoch 74/1001] [Batch 370/372] [D loss: 0.6895119547843933] [G loss: 0.7266542911529541]\n",
      "[Epoch 74/1001] [Batch 371/372] [D loss: 0.6735883951187134] [G loss: 0.6261152029037476]\n",
      "[Epoch 75/1001] [Batch 0/372] [D loss: 0.6764525175094604] [G loss: 0.7425261735916138]\n",
      "[Epoch 75/1001] [Batch 1/372] [D loss: 0.6873746514320374] [G loss: 0.6309937834739685]\n",
      "[Epoch 75/1001] [Batch 2/372] [D loss: 0.6743128895759583] [G loss: 0.7086078524589539]\n",
      "[Epoch 75/1001] [Batch 3/372] [D loss: 0.6812739968299866] [G loss: 0.6697825193405151]\n",
      "[Epoch 75/1001] [Batch 4/372] [D loss: 0.6805521249771118] [G loss: 0.6831990480422974]\n",
      "[Epoch 75/1001] [Batch 5/372] [D loss: 0.6721199154853821] [G loss: 0.6588835716247559]\n",
      "[Epoch 75/1001] [Batch 6/372] [D loss: 0.683337926864624] [G loss: 0.6771668195724487]\n",
      "[Epoch 75/1001] [Batch 7/372] [D loss: 0.6754486560821533] [G loss: 0.6952972412109375]\n",
      "[Epoch 75/1001] [Batch 8/372] [D loss: 0.6813653707504272] [G loss: 0.6325926184654236]\n",
      "[Epoch 75/1001] [Batch 9/372] [D loss: 0.6763347387313843] [G loss: 0.7054290771484375]\n",
      "[Epoch 75/1001] [Batch 10/372] [D loss: 0.6835565567016602] [G loss: 0.6892242431640625]\n",
      "[Epoch 75/1001] [Batch 11/372] [D loss: 0.6938209533691406] [G loss: 0.5921992063522339]\n",
      "[Epoch 75/1001] [Batch 12/372] [D loss: 0.677014946937561] [G loss: 0.7742026448249817]\n",
      "[Epoch 75/1001] [Batch 13/372] [D loss: 0.6724754571914673] [G loss: 0.5748733878135681]\n",
      "[Epoch 75/1001] [Batch 14/372] [D loss: 0.6802622079849243] [G loss: 0.755243718624115]\n",
      "[Epoch 75/1001] [Batch 15/372] [D loss: 0.6667510867118835] [G loss: 0.5659921169281006]\n",
      "[Epoch 75/1001] [Batch 16/372] [D loss: 0.6882059574127197] [G loss: 0.94505774974823]\n",
      "[Epoch 75/1001] [Batch 17/372] [D loss: 0.7001572251319885] [G loss: 0.41374140977859497]\n",
      "[Epoch 75/1001] [Batch 18/372] [D loss: 0.730613112449646] [G loss: 1.1074142456054688]\n",
      "[Epoch 75/1001] [Batch 19/372] [D loss: 0.7363077402114868] [G loss: 0.4733884632587433]\n",
      "[Epoch 75/1001] [Batch 20/372] [D loss: 0.7018636465072632] [G loss: 0.7368959188461304]\n",
      "[Epoch 75/1001] [Batch 21/372] [D loss: 0.6799386739730835] [G loss: 0.7158412933349609]\n",
      "[Epoch 75/1001] [Batch 22/372] [D loss: 0.6908318996429443] [G loss: 0.6020761728286743]\n",
      "[Epoch 75/1001] [Batch 23/372] [D loss: 0.683617889881134] [G loss: 0.7025598287582397]\n",
      "[Epoch 75/1001] [Batch 24/372] [D loss: 0.6816474199295044] [G loss: 0.6524618268013]\n",
      "[Epoch 75/1001] [Batch 25/372] [D loss: 0.6910650730133057] [G loss: 0.7216087579727173]\n",
      "[Epoch 75/1001] [Batch 26/372] [D loss: 0.6736133098602295] [G loss: 0.6443265676498413]\n",
      "[Epoch 75/1001] [Batch 27/372] [D loss: 0.6799603700637817] [G loss: 0.6713719964027405]\n",
      "[Epoch 75/1001] [Batch 28/372] [D loss: 0.6692880392074585] [G loss: 0.7227535843849182]\n",
      "[Epoch 75/1001] [Batch 29/372] [D loss: 0.6791692972183228] [G loss: 0.6120347380638123]\n",
      "[Epoch 75/1001] [Batch 30/372] [D loss: 0.6781400442123413] [G loss: 0.7256869077682495]\n",
      "[Epoch 75/1001] [Batch 31/372] [D loss: 0.6954643726348877] [G loss: 0.6551848649978638]\n",
      "[Epoch 75/1001] [Batch 32/372] [D loss: 0.6790781617164612] [G loss: 0.6491783857345581]\n",
      "[Epoch 75/1001] [Batch 33/372] [D loss: 0.6896346807479858] [G loss: 0.7226184606552124]\n",
      "[Epoch 75/1001] [Batch 34/372] [D loss: 0.6878268718719482] [G loss: 0.6324057579040527]\n",
      "[Epoch 75/1001] [Batch 35/372] [D loss: 0.6815444827079773] [G loss: 0.6993705034255981]\n",
      "[Epoch 75/1001] [Batch 36/372] [D loss: 0.6887429356575012] [G loss: 0.6393788456916809]\n",
      "[Epoch 75/1001] [Batch 37/372] [D loss: 0.6878984570503235] [G loss: 0.7113372683525085]\n",
      "[Epoch 75/1001] [Batch 38/372] [D loss: 0.6921697854995728] [G loss: 0.6292020678520203]\n",
      "[Epoch 75/1001] [Batch 39/372] [D loss: 0.6923731565475464] [G loss: 0.701539158821106]\n",
      "[Epoch 75/1001] [Batch 40/372] [D loss: 0.6741368770599365] [G loss: 0.6451526880264282]\n",
      "[Epoch 75/1001] [Batch 41/372] [D loss: 0.6870900392532349] [G loss: 0.7000105977058411]\n",
      "[Epoch 75/1001] [Batch 42/372] [D loss: 0.679021954536438] [G loss: 0.6597473621368408]\n",
      "[Epoch 75/1001] [Batch 43/372] [D loss: 0.6838517189025879] [G loss: 0.6657840609550476]\n",
      "[Epoch 75/1001] [Batch 44/372] [D loss: 0.6728155612945557] [G loss: 0.7164025902748108]\n",
      "[Epoch 75/1001] [Batch 45/372] [D loss: 0.678697943687439] [G loss: 0.6082568168640137]\n",
      "[Epoch 75/1001] [Batch 46/372] [D loss: 0.6795200109481812] [G loss: 0.7572945356369019]\n",
      "[Epoch 75/1001] [Batch 47/372] [D loss: 0.6680941581726074] [G loss: 0.6019427180290222]\n",
      "[Epoch 75/1001] [Batch 48/372] [D loss: 0.6807006597518921] [G loss: 0.7364228963851929]\n",
      "[Epoch 75/1001] [Batch 49/372] [D loss: 0.6777652502059937] [G loss: 0.6469922661781311]\n",
      "[Epoch 75/1001] [Batch 50/372] [D loss: 0.687207818031311] [G loss: 0.6763988733291626]\n",
      "[Epoch 75/1001] [Batch 51/372] [D loss: 0.6823657155036926] [G loss: 0.6492471694946289]\n",
      "[Epoch 75/1001] [Batch 52/372] [D loss: 0.6748068332672119] [G loss: 0.6963735818862915]\n",
      "[Epoch 75/1001] [Batch 53/372] [D loss: 0.6733602285385132] [G loss: 0.6339267492294312]\n",
      "[Epoch 75/1001] [Batch 54/372] [D loss: 0.6900629997253418] [G loss: 0.6741542220115662]\n",
      "[Epoch 75/1001] [Batch 55/372] [D loss: 0.6759650707244873] [G loss: 0.7182486653327942]\n",
      "[Epoch 75/1001] [Batch 56/372] [D loss: 0.687342643737793] [G loss: 0.6273804306983948]\n",
      "[Epoch 75/1001] [Batch 57/372] [D loss: 0.6919968128204346] [G loss: 0.7330666780471802]\n",
      "[Epoch 75/1001] [Batch 58/372] [D loss: 0.6934062838554382] [G loss: 0.6242727041244507]\n",
      "[Epoch 75/1001] [Batch 59/372] [D loss: 0.6859081983566284] [G loss: 0.6930125951766968]\n",
      "[Epoch 75/1001] [Batch 60/372] [D loss: 0.6849095821380615] [G loss: 0.6338502764701843]\n",
      "[Epoch 75/1001] [Batch 61/372] [D loss: 0.6878594160079956] [G loss: 0.6904919147491455]\n",
      "[Epoch 75/1001] [Batch 62/372] [D loss: 0.6787269115447998] [G loss: 0.6912715435028076]\n",
      "[Epoch 75/1001] [Batch 63/372] [D loss: 0.6847214102745056] [G loss: 0.6520674824714661]\n",
      "[Epoch 75/1001] [Batch 64/372] [D loss: 0.6866117119789124] [G loss: 0.6831393241882324]\n",
      "[Epoch 75/1001] [Batch 65/372] [D loss: 0.6842977404594421] [G loss: 0.6594449877738953]\n",
      "[Epoch 75/1001] [Batch 66/372] [D loss: 0.686872124671936] [G loss: 0.6731805205345154]\n",
      "[Epoch 75/1001] [Batch 67/372] [D loss: 0.6810052990913391] [G loss: 0.6861051321029663]\n",
      "[Epoch 75/1001] [Batch 68/372] [D loss: 0.6938595771789551] [G loss: 0.6549901962280273]\n",
      "[Epoch 75/1001] [Batch 69/372] [D loss: 0.6778229475021362] [G loss: 0.6530314683914185]\n",
      "[Epoch 75/1001] [Batch 70/372] [D loss: 0.6858315467834473] [G loss: 0.6947041153907776]\n",
      "[Epoch 75/1001] [Batch 71/372] [D loss: 0.6624768972396851] [G loss: 0.6544277667999268]\n",
      "[Epoch 75/1001] [Batch 72/372] [D loss: 0.6867992877960205] [G loss: 0.6782647371292114]\n",
      "[Epoch 75/1001] [Batch 73/372] [D loss: 0.6770857572555542] [G loss: 0.6846364140510559]\n",
      "[Epoch 75/1001] [Batch 74/372] [D loss: 0.6941192150115967] [G loss: 0.6740682721138]\n",
      "[Epoch 75/1001] [Batch 75/372] [D loss: 0.679004430770874] [G loss: 0.6752917170524597]\n",
      "[Epoch 75/1001] [Batch 76/372] [D loss: 0.6800305843353271] [G loss: 0.6292052865028381]\n",
      "[Epoch 75/1001] [Batch 77/372] [D loss: 0.6810646057128906] [G loss: 0.7704816460609436]\n",
      "[Epoch 75/1001] [Batch 78/372] [D loss: 0.6893357634544373] [G loss: 0.5445127487182617]\n",
      "[Epoch 75/1001] [Batch 79/372] [D loss: 0.6903994083404541] [G loss: 0.8653124570846558]\n",
      "[Epoch 75/1001] [Batch 80/372] [D loss: 0.6990108489990234] [G loss: 0.5118820667266846]\n",
      "[Epoch 75/1001] [Batch 81/372] [D loss: 0.696975827217102] [G loss: 0.8872299194335938]\n",
      "[Epoch 75/1001] [Batch 82/372] [D loss: 0.7101598978042603] [G loss: 0.5262556076049805]\n",
      "[Epoch 75/1001] [Batch 83/372] [D loss: 0.698535680770874] [G loss: 0.77504563331604]\n",
      "[Epoch 75/1001] [Batch 84/372] [D loss: 0.6859984993934631] [G loss: 0.648867130279541]\n",
      "[Epoch 75/1001] [Batch 85/372] [D loss: 0.6717146635055542] [G loss: 0.6643749475479126]\n",
      "[Epoch 75/1001] [Batch 86/372] [D loss: 0.6817398071289062] [G loss: 0.733389139175415]\n",
      "[Epoch 75/1001] [Batch 87/372] [D loss: 0.688395082950592] [G loss: 0.6019248366355896]\n",
      "[Epoch 75/1001] [Batch 88/372] [D loss: 0.671462893486023] [G loss: 0.7221958637237549]\n",
      "[Epoch 75/1001] [Batch 89/372] [D loss: 0.6731926798820496] [G loss: 0.6236717700958252]\n",
      "[Epoch 75/1001] [Batch 90/372] [D loss: 0.6745216250419617] [G loss: 0.7828935384750366]\n",
      "[Epoch 75/1001] [Batch 91/372] [D loss: 0.6947509050369263] [G loss: 0.5640162229537964]\n",
      "[Epoch 75/1001] [Batch 92/372] [D loss: 0.687178909778595] [G loss: 0.8437830209732056]\n",
      "[Epoch 75/1001] [Batch 93/372] [D loss: 0.6951832175254822] [G loss: 0.5283305644989014]\n",
      "[Epoch 75/1001] [Batch 94/372] [D loss: 0.6927137970924377] [G loss: 0.8569502830505371]\n",
      "[Epoch 75/1001] [Batch 95/372] [D loss: 0.6979647874832153] [G loss: 0.5352049469947815]\n",
      "[Epoch 75/1001] [Batch 96/372] [D loss: 0.6881802082061768] [G loss: 0.7632717490196228]\n",
      "[Epoch 75/1001] [Batch 97/372] [D loss: 0.6943857669830322] [G loss: 0.6376025676727295]\n",
      "[Epoch 75/1001] [Batch 98/372] [D loss: 0.6903761625289917] [G loss: 0.6588043570518494]\n",
      "[Epoch 75/1001] [Batch 99/372] [D loss: 0.6778111457824707] [G loss: 0.7313405871391296]\n",
      "[Epoch 75/1001] [Batch 100/372] [D loss: 0.6833193302154541] [G loss: 0.6259822249412537]\n",
      "[Epoch 75/1001] [Batch 101/372] [D loss: 0.6807809472084045] [G loss: 0.7277641892433167]\n",
      "[Epoch 75/1001] [Batch 102/372] [D loss: 0.6832106113433838] [G loss: 0.6381580829620361]\n",
      "[Epoch 75/1001] [Batch 103/372] [D loss: 0.6866027116775513] [G loss: 0.6797228455543518]\n",
      "[Epoch 75/1001] [Batch 104/372] [D loss: 0.6904876232147217] [G loss: 0.6688222885131836]\n",
      "[Epoch 75/1001] [Batch 105/372] [D loss: 0.6798142194747925] [G loss: 0.6871113777160645]\n",
      "[Epoch 75/1001] [Batch 106/372] [D loss: 0.6721372604370117] [G loss: 0.642852783203125]\n",
      "[Epoch 75/1001] [Batch 107/372] [D loss: 0.6934847235679626] [G loss: 0.7471432089805603]\n",
      "[Epoch 75/1001] [Batch 108/372] [D loss: 0.6853668093681335] [G loss: 0.6144986152648926]\n",
      "[Epoch 75/1001] [Batch 109/372] [D loss: 0.6784636974334717] [G loss: 0.707848310470581]\n",
      "[Epoch 75/1001] [Batch 110/372] [D loss: 0.6781460046768188] [G loss: 0.6902743577957153]\n",
      "[Epoch 75/1001] [Batch 111/372] [D loss: 0.6889554858207703] [G loss: 0.6293491125106812]\n",
      "[Epoch 75/1001] [Batch 112/372] [D loss: 0.6840240955352783] [G loss: 0.7153480052947998]\n",
      "[Epoch 75/1001] [Batch 113/372] [D loss: 0.6808551549911499] [G loss: 0.6261436939239502]\n",
      "[Epoch 75/1001] [Batch 114/372] [D loss: 0.675412654876709] [G loss: 0.7286868095397949]\n",
      "[Epoch 75/1001] [Batch 115/372] [D loss: 0.6887794733047485] [G loss: 0.6064866185188293]\n",
      "[Epoch 75/1001] [Batch 116/372] [D loss: 0.6886041164398193] [G loss: 0.7304592132568359]\n",
      "[Epoch 75/1001] [Batch 117/372] [D loss: 0.6833682060241699] [G loss: 0.6398924589157104]\n",
      "[Epoch 75/1001] [Batch 118/372] [D loss: 0.6754742860794067] [G loss: 0.7111710906028748]\n",
      "[Epoch 75/1001] [Batch 119/372] [D loss: 0.6811307668685913] [G loss: 0.6384686827659607]\n",
      "[Epoch 75/1001] [Batch 120/372] [D loss: 0.675676703453064] [G loss: 0.7433029413223267]\n",
      "[Epoch 75/1001] [Batch 121/372] [D loss: 0.6701014041900635] [G loss: 0.6382288932800293]\n",
      "[Epoch 75/1001] [Batch 122/372] [D loss: 0.6731138229370117] [G loss: 0.6942545771598816]\n",
      "[Epoch 75/1001] [Batch 123/372] [D loss: 0.683542013168335] [G loss: 0.6839781999588013]\n",
      "[Epoch 75/1001] [Batch 124/372] [D loss: 0.6818952560424805] [G loss: 0.621244490146637]\n",
      "[Epoch 75/1001] [Batch 125/372] [D loss: 0.6900637745857239] [G loss: 0.7594627737998962]\n",
      "[Epoch 75/1001] [Batch 126/372] [D loss: 0.6943906545639038] [G loss: 0.536941409111023]\n",
      "[Epoch 75/1001] [Batch 127/372] [D loss: 0.7070648670196533] [G loss: 0.9398282766342163]\n",
      "[Epoch 75/1001] [Batch 128/372] [D loss: 0.7121245861053467] [G loss: 0.4629509747028351]\n",
      "[Epoch 75/1001] [Batch 129/372] [D loss: 0.7055571675300598] [G loss: 0.8910548686981201]\n",
      "[Epoch 75/1001] [Batch 130/372] [D loss: 0.694143533706665] [G loss: 0.6187859773635864]\n",
      "[Epoch 75/1001] [Batch 131/372] [D loss: 0.6862573623657227] [G loss: 0.6797449588775635]\n",
      "[Epoch 75/1001] [Batch 132/372] [D loss: 0.6911814212799072] [G loss: 0.6797199249267578]\n",
      "[Epoch 75/1001] [Batch 133/372] [D loss: 0.6812620162963867] [G loss: 0.6682360172271729]\n",
      "[Epoch 75/1001] [Batch 134/372] [D loss: 0.6890513896942139] [G loss: 0.6742538809776306]\n",
      "[Epoch 75/1001] [Batch 135/372] [D loss: 0.6750360727310181] [G loss: 0.6834491491317749]\n",
      "[Epoch 75/1001] [Batch 136/372] [D loss: 0.6678663492202759] [G loss: 0.7137098908424377]\n",
      "[Epoch 75/1001] [Batch 137/372] [D loss: 0.7012714147567749] [G loss: 0.6291620135307312]\n",
      "[Epoch 75/1001] [Batch 138/372] [D loss: 0.6761606335639954] [G loss: 0.7248733639717102]\n",
      "[Epoch 75/1001] [Batch 139/372] [D loss: 0.6862621307373047] [G loss: 0.6330134272575378]\n",
      "[Epoch 75/1001] [Batch 140/372] [D loss: 0.6847457885742188] [G loss: 0.67882239818573]\n",
      "[Epoch 75/1001] [Batch 141/372] [D loss: 0.6861389875411987] [G loss: 0.7017402052879333]\n",
      "[Epoch 75/1001] [Batch 142/372] [D loss: 0.6785333156585693] [G loss: 0.626377522945404]\n",
      "[Epoch 75/1001] [Batch 143/372] [D loss: 0.6868975758552551] [G loss: 0.7127031683921814]\n",
      "[Epoch 75/1001] [Batch 144/372] [D loss: 0.6860987544059753] [G loss: 0.663841962814331]\n",
      "[Epoch 75/1001] [Batch 145/372] [D loss: 0.6818128824234009] [G loss: 0.6879751682281494]\n",
      "[Epoch 75/1001] [Batch 146/372] [D loss: 0.6833536624908447] [G loss: 0.631522536277771]\n",
      "[Epoch 75/1001] [Batch 147/372] [D loss: 0.6699433326721191] [G loss: 0.7150447964668274]\n",
      "[Epoch 75/1001] [Batch 148/372] [D loss: 0.6968222856521606] [G loss: 0.6486780047416687]\n",
      "[Epoch 75/1001] [Batch 149/372] [D loss: 0.6717798113822937] [G loss: 0.6936095952987671]\n",
      "[Epoch 75/1001] [Batch 150/372] [D loss: 0.6733234524726868] [G loss: 0.6290963888168335]\n",
      "[Epoch 75/1001] [Batch 151/372] [D loss: 0.681908369064331] [G loss: 0.7543303966522217]\n",
      "[Epoch 75/1001] [Batch 152/372] [D loss: 0.6985384225845337] [G loss: 0.5733333826065063]\n",
      "[Epoch 75/1001] [Batch 153/372] [D loss: 0.6783312559127808] [G loss: 0.7772735357284546]\n",
      "[Epoch 75/1001] [Batch 154/372] [D loss: 0.6869082450866699] [G loss: 0.5763672590255737]\n",
      "[Epoch 75/1001] [Batch 155/372] [D loss: 0.6909924745559692] [G loss: 0.7803003191947937]\n",
      "[Epoch 75/1001] [Batch 156/372] [D loss: 0.6893256902694702] [G loss: 0.5612053871154785]\n",
      "[Epoch 75/1001] [Batch 157/372] [D loss: 0.6844395399093628] [G loss: 0.8019719123840332]\n",
      "[Epoch 75/1001] [Batch 158/372] [D loss: 0.6977150440216064] [G loss: 0.5462299585342407]\n",
      "[Epoch 75/1001] [Batch 159/372] [D loss: 0.6891440749168396] [G loss: 0.8050630688667297]\n",
      "[Epoch 75/1001] [Batch 160/372] [D loss: 0.6906569004058838] [G loss: 0.5717969536781311]\n",
      "[Epoch 75/1001] [Batch 161/372] [D loss: 0.6821786165237427] [G loss: 0.8147507905960083]\n",
      "[Epoch 75/1001] [Batch 162/372] [D loss: 0.675250232219696] [G loss: 0.5974669456481934]\n",
      "[Epoch 75/1001] [Batch 163/372] [D loss: 0.6861773729324341] [G loss: 0.6836240291595459]\n",
      "[Epoch 75/1001] [Batch 164/372] [D loss: 0.6904553174972534] [G loss: 0.7076818346977234]\n",
      "[Epoch 75/1001] [Batch 165/372] [D loss: 0.6797710657119751] [G loss: 0.6318062543869019]\n",
      "[Epoch 75/1001] [Batch 166/372] [D loss: 0.6794770359992981] [G loss: 0.720005989074707]\n",
      "[Epoch 75/1001] [Batch 167/372] [D loss: 0.6962007284164429] [G loss: 0.6626065969467163]\n",
      "[Epoch 75/1001] [Batch 168/372] [D loss: 0.6865613460540771] [G loss: 0.6579582095146179]\n",
      "[Epoch 75/1001] [Batch 169/372] [D loss: 0.6800742745399475] [G loss: 0.7165471315383911]\n",
      "[Epoch 75/1001] [Batch 170/372] [D loss: 0.6895393133163452] [G loss: 0.6269480586051941]\n",
      "[Epoch 75/1001] [Batch 171/372] [D loss: 0.6948403120040894] [G loss: 0.6801092028617859]\n",
      "[Epoch 75/1001] [Batch 172/372] [D loss: 0.6767499446868896] [G loss: 0.6333906650543213]\n",
      "[Epoch 75/1001] [Batch 173/372] [D loss: 0.6796590089797974] [G loss: 0.7181063890457153]\n",
      "[Epoch 75/1001] [Batch 174/372] [D loss: 0.6851190328598022] [G loss: 0.6219717860221863]\n",
      "[Epoch 75/1001] [Batch 175/372] [D loss: 0.6844742298126221] [G loss: 0.7751832008361816]\n",
      "[Epoch 75/1001] [Batch 176/372] [D loss: 0.6779282093048096] [G loss: 0.5606014132499695]\n",
      "[Epoch 75/1001] [Batch 177/372] [D loss: 0.6837000846862793] [G loss: 0.8010132908821106]\n",
      "[Epoch 75/1001] [Batch 178/372] [D loss: 0.6915019750595093] [G loss: 0.5817079544067383]\n",
      "[Epoch 75/1001] [Batch 179/372] [D loss: 0.6840782761573792] [G loss: 0.7384434938430786]\n",
      "[Epoch 75/1001] [Batch 180/372] [D loss: 0.6816742420196533] [G loss: 0.6275084018707275]\n",
      "[Epoch 75/1001] [Batch 181/372] [D loss: 0.6855026483535767] [G loss: 0.69952791929245]\n",
      "[Epoch 75/1001] [Batch 182/372] [D loss: 0.6847776174545288] [G loss: 0.6445976495742798]\n",
      "[Epoch 75/1001] [Batch 183/372] [D loss: 0.6858341693878174] [G loss: 0.6870337724685669]\n",
      "[Epoch 75/1001] [Batch 184/372] [D loss: 0.68345046043396] [G loss: 0.671073853969574]\n",
      "[Epoch 75/1001] [Batch 185/372] [D loss: 0.6862181425094604] [G loss: 0.6591962575912476]\n",
      "[Epoch 75/1001] [Batch 186/372] [D loss: 0.6877615451812744] [G loss: 0.687741219997406]\n",
      "[Epoch 75/1001] [Batch 187/372] [D loss: 0.6793696880340576] [G loss: 0.6521046161651611]\n",
      "[Epoch 75/1001] [Batch 188/372] [D loss: 0.6833429932594299] [G loss: 0.6674149632453918]\n",
      "[Epoch 75/1001] [Batch 189/372] [D loss: 0.6759371757507324] [G loss: 0.6682636141777039]\n",
      "[Epoch 75/1001] [Batch 190/372] [D loss: 0.6752521991729736] [G loss: 0.6870613694190979]\n",
      "[Epoch 75/1001] [Batch 191/372] [D loss: 0.690819263458252] [G loss: 0.6586610674858093]\n",
      "[Epoch 75/1001] [Batch 192/372] [D loss: 0.6752229928970337] [G loss: 0.6564474701881409]\n",
      "[Epoch 75/1001] [Batch 193/372] [D loss: 0.6812300086021423] [G loss: 0.7035918235778809]\n",
      "[Epoch 75/1001] [Batch 194/372] [D loss: 0.6823835372924805] [G loss: 0.6075423955917358]\n",
      "[Epoch 75/1001] [Batch 195/372] [D loss: 0.686394214630127] [G loss: 0.7768701314926147]\n",
      "[Epoch 75/1001] [Batch 196/372] [D loss: 0.682100236415863] [G loss: 0.5566816926002502]\n",
      "[Epoch 75/1001] [Batch 197/372] [D loss: 0.6891809105873108] [G loss: 0.8567671775817871]\n",
      "[Epoch 75/1001] [Batch 198/372] [D loss: 0.7009150385856628] [G loss: 0.49716025590896606]\n",
      "[Epoch 75/1001] [Batch 199/372] [D loss: 0.6927776336669922] [G loss: 0.8853040933609009]\n",
      "[Epoch 75/1001] [Batch 200/372] [D loss: 0.695067822933197] [G loss: 0.525490939617157]\n",
      "[Epoch 75/1001] [Batch 201/372] [D loss: 0.6959593892097473] [G loss: 0.8519834280014038]\n",
      "[Epoch 75/1001] [Batch 202/372] [D loss: 0.6947159767150879] [G loss: 0.546151876449585]\n",
      "[Epoch 75/1001] [Batch 203/372] [D loss: 0.7022051811218262] [G loss: 0.7958483695983887]\n",
      "[Epoch 75/1001] [Batch 204/372] [D loss: 0.6961385011672974] [G loss: 0.5836365222930908]\n",
      "[Epoch 75/1001] [Batch 205/372] [D loss: 0.6876826286315918] [G loss: 0.7537856101989746]\n",
      "[Epoch 75/1001] [Batch 206/372] [D loss: 0.6887075901031494] [G loss: 0.6310563683509827]\n",
      "[Epoch 75/1001] [Batch 207/372] [D loss: 0.6892291903495789] [G loss: 0.690199613571167]\n",
      "[Epoch 75/1001] [Batch 208/372] [D loss: 0.6823868751525879] [G loss: 0.6769716739654541]\n",
      "[Epoch 75/1001] [Batch 209/372] [D loss: 0.6816046833992004] [G loss: 0.6608891487121582]\n",
      "[Epoch 75/1001] [Batch 210/372] [D loss: 0.6912987232208252] [G loss: 0.6600986123085022]\n",
      "[Epoch 75/1001] [Batch 211/372] [D loss: 0.6809432506561279] [G loss: 0.6667641401290894]\n",
      "[Epoch 75/1001] [Batch 212/372] [D loss: 0.6856341361999512] [G loss: 0.6572685837745667]\n",
      "[Epoch 75/1001] [Batch 213/372] [D loss: 0.6804835796356201] [G loss: 0.6822676658630371]\n",
      "[Epoch 75/1001] [Batch 214/372] [D loss: 0.6819375157356262] [G loss: 0.653255045413971]\n",
      "[Epoch 75/1001] [Batch 215/372] [D loss: 0.666153073310852] [G loss: 0.7154370546340942]\n",
      "[Epoch 75/1001] [Batch 216/372] [D loss: 0.6854566335678101] [G loss: 0.6281085014343262]\n",
      "[Epoch 75/1001] [Batch 217/372] [D loss: 0.6854926347732544] [G loss: 0.7191823124885559]\n",
      "[Epoch 75/1001] [Batch 218/372] [D loss: 0.6883476972579956] [G loss: 0.6043455600738525]\n",
      "[Epoch 75/1001] [Batch 219/372] [D loss: 0.6842319965362549] [G loss: 0.7599679231643677]\n",
      "[Epoch 75/1001] [Batch 220/372] [D loss: 0.6988440155982971] [G loss: 0.5707752704620361]\n",
      "[Epoch 75/1001] [Batch 221/372] [D loss: 0.6945862174034119] [G loss: 0.7747827172279358]\n",
      "[Epoch 75/1001] [Batch 222/372] [D loss: 0.6792043447494507] [G loss: 0.5964778661727905]\n",
      "[Epoch 75/1001] [Batch 223/372] [D loss: 0.6858309507369995] [G loss: 0.7423503994941711]\n",
      "[Epoch 75/1001] [Batch 224/372] [D loss: 0.6868826150894165] [G loss: 0.6064610481262207]\n",
      "[Epoch 75/1001] [Batch 225/372] [D loss: 0.681131899356842] [G loss: 0.7114816904067993]\n",
      "[Epoch 75/1001] [Batch 226/372] [D loss: 0.6799665689468384] [G loss: 0.645340085029602]\n",
      "[Epoch 75/1001] [Batch 227/372] [D loss: 0.6785529851913452] [G loss: 0.675159752368927]\n",
      "[Epoch 75/1001] [Batch 228/372] [D loss: 0.678592324256897] [G loss: 0.721553385257721]\n",
      "[Epoch 75/1001] [Batch 229/372] [D loss: 0.6943295001983643] [G loss: 0.6067197322845459]\n",
      "[Epoch 75/1001] [Batch 230/372] [D loss: 0.6712299585342407] [G loss: 0.7442581057548523]\n",
      "[Epoch 75/1001] [Batch 231/372] [D loss: 0.6832873225212097] [G loss: 0.5619183778762817]\n",
      "[Epoch 75/1001] [Batch 232/372] [D loss: 0.7004551291465759] [G loss: 0.8228892087936401]\n",
      "[Epoch 75/1001] [Batch 233/372] [D loss: 0.6914622783660889] [G loss: 0.5223655104637146]\n",
      "[Epoch 75/1001] [Batch 234/372] [D loss: 0.6971307992935181] [G loss: 0.8358683586120605]\n",
      "[Epoch 75/1001] [Batch 235/372] [D loss: 0.6911070346832275] [G loss: 0.5288137197494507]\n",
      "[Epoch 75/1001] [Batch 236/372] [D loss: 0.6894711256027222] [G loss: 0.7969055771827698]\n",
      "[Epoch 75/1001] [Batch 237/372] [D loss: 0.6852361559867859] [G loss: 0.6304748058319092]\n",
      "[Epoch 75/1001] [Batch 238/372] [D loss: 0.6708179712295532] [G loss: 0.682988703250885]\n",
      "[Epoch 75/1001] [Batch 239/372] [D loss: 0.6717137694358826] [G loss: 0.6717285513877869]\n",
      "[Epoch 75/1001] [Batch 240/372] [D loss: 0.6846209764480591] [G loss: 0.6655348539352417]\n",
      "[Epoch 75/1001] [Batch 241/372] [D loss: 0.682330846786499] [G loss: 0.6466062664985657]\n",
      "[Epoch 75/1001] [Batch 242/372] [D loss: 0.6849814653396606] [G loss: 0.6914272308349609]\n",
      "[Epoch 75/1001] [Batch 243/372] [D loss: 0.6844764947891235] [G loss: 0.6415888071060181]\n",
      "[Epoch 75/1001] [Batch 244/372] [D loss: 0.6877585649490356] [G loss: 0.7078774571418762]\n",
      "[Epoch 75/1001] [Batch 245/372] [D loss: 0.6825426816940308] [G loss: 0.6221949458122253]\n",
      "[Epoch 75/1001] [Batch 246/372] [D loss: 0.6842550039291382] [G loss: 0.7281988859176636]\n",
      "[Epoch 75/1001] [Batch 247/372] [D loss: 0.6809451580047607] [G loss: 0.6047019958496094]\n",
      "[Epoch 75/1001] [Batch 248/372] [D loss: 0.6766927242279053] [G loss: 0.7633963227272034]\n",
      "[Epoch 75/1001] [Batch 249/372] [D loss: 0.6964697241783142] [G loss: 0.572454571723938]\n",
      "[Epoch 75/1001] [Batch 250/372] [D loss: 0.6949843168258667] [G loss: 0.7731913924217224]\n",
      "[Epoch 75/1001] [Batch 251/372] [D loss: 0.6861650943756104] [G loss: 0.557625412940979]\n",
      "[Epoch 75/1001] [Batch 252/372] [D loss: 0.68306565284729] [G loss: 0.8617384433746338]\n",
      "[Epoch 75/1001] [Batch 253/372] [D loss: 0.6997111439704895] [G loss: 0.4810662865638733]\n",
      "[Epoch 75/1001] [Batch 254/372] [D loss: 0.7125376462936401] [G loss: 0.8533304929733276]\n",
      "[Epoch 75/1001] [Batch 255/372] [D loss: 0.695976734161377] [G loss: 0.5652438402175903]\n",
      "[Epoch 75/1001] [Batch 256/372] [D loss: 0.6829841136932373] [G loss: 0.7444285750389099]\n",
      "[Epoch 75/1001] [Batch 257/372] [D loss: 0.6901044249534607] [G loss: 0.6432716846466064]\n",
      "[Epoch 75/1001] [Batch 258/372] [D loss: 0.6696815490722656] [G loss: 0.6861680746078491]\n",
      "[Epoch 75/1001] [Batch 259/372] [D loss: 0.6849298477172852] [G loss: 0.6728581190109253]\n",
      "[Epoch 75/1001] [Batch 260/372] [D loss: 0.6877832412719727] [G loss: 0.6381447315216064]\n",
      "[Epoch 75/1001] [Batch 261/372] [D loss: 0.6832497119903564] [G loss: 0.7100546360015869]\n",
      "[Epoch 75/1001] [Batch 262/372] [D loss: 0.6774416565895081] [G loss: 0.6261698603630066]\n",
      "[Epoch 75/1001] [Batch 263/372] [D loss: 0.6708256602287292] [G loss: 0.7153949737548828]\n",
      "[Epoch 75/1001] [Batch 264/372] [D loss: 0.6782729625701904] [G loss: 0.6513229608535767]\n",
      "[Epoch 75/1001] [Batch 265/372] [D loss: 0.670732855796814] [G loss: 0.7158729434013367]\n",
      "[Epoch 75/1001] [Batch 266/372] [D loss: 0.696870744228363] [G loss: 0.6421693563461304]\n",
      "[Epoch 75/1001] [Batch 267/372] [D loss: 0.6748483180999756] [G loss: 0.6602014303207397]\n",
      "[Epoch 75/1001] [Batch 268/372] [D loss: 0.6920684576034546] [G loss: 0.7108232975006104]\n",
      "[Epoch 75/1001] [Batch 269/372] [D loss: 0.682054877281189] [G loss: 0.5960063934326172]\n",
      "[Epoch 75/1001] [Batch 270/372] [D loss: 0.6952505111694336] [G loss: 0.8229460716247559]\n",
      "[Epoch 75/1001] [Batch 271/372] [D loss: 0.6894212961196899] [G loss: 0.5363526940345764]\n",
      "[Epoch 75/1001] [Batch 272/372] [D loss: 0.6791127920150757] [G loss: 0.8022724390029907]\n",
      "[Epoch 75/1001] [Batch 273/372] [D loss: 0.704483151435852] [G loss: 0.5879330635070801]\n",
      "[Epoch 75/1001] [Batch 274/372] [D loss: 0.6874370574951172] [G loss: 0.7539010047912598]\n",
      "[Epoch 75/1001] [Batch 275/372] [D loss: 0.6944894790649414] [G loss: 0.5924151539802551]\n",
      "[Epoch 75/1001] [Batch 276/372] [D loss: 0.6820459365844727] [G loss: 0.7753658294677734]\n",
      "[Epoch 75/1001] [Batch 277/372] [D loss: 0.6825679540634155] [G loss: 0.5816379189491272]\n",
      "[Epoch 75/1001] [Batch 278/372] [D loss: 0.6838768720626831] [G loss: 0.7297578454017639]\n",
      "[Epoch 75/1001] [Batch 279/372] [D loss: 0.6986377835273743] [G loss: 0.611648440361023]\n",
      "[Epoch 75/1001] [Batch 280/372] [D loss: 0.6901782751083374] [G loss: 0.713715136051178]\n",
      "[Epoch 75/1001] [Batch 281/372] [D loss: 0.689633846282959] [G loss: 0.6430673599243164]\n",
      "[Epoch 75/1001] [Batch 282/372] [D loss: 0.6913657188415527] [G loss: 0.6856869459152222]\n",
      "[Epoch 75/1001] [Batch 283/372] [D loss: 0.6850758194923401] [G loss: 0.6539984941482544]\n",
      "[Epoch 75/1001] [Batch 284/372] [D loss: 0.6827971935272217] [G loss: 0.7153393030166626]\n",
      "[Epoch 75/1001] [Batch 285/372] [D loss: 0.6895602345466614] [G loss: 0.6096935868263245]\n",
      "[Epoch 75/1001] [Batch 286/372] [D loss: 0.6915038824081421] [G loss: 0.7728472948074341]\n",
      "[Epoch 75/1001] [Batch 287/372] [D loss: 0.6930054426193237] [G loss: 0.5454806089401245]\n",
      "[Epoch 75/1001] [Batch 288/372] [D loss: 0.7089088559150696] [G loss: 0.8158637285232544]\n",
      "[Epoch 75/1001] [Batch 289/372] [D loss: 0.6801502704620361] [G loss: 0.5287172794342041]\n",
      "[Epoch 75/1001] [Batch 290/372] [D loss: 0.7027193903923035] [G loss: 0.8690380454063416]\n",
      "[Epoch 75/1001] [Batch 291/372] [D loss: 0.698755145072937] [G loss: 0.5112650990486145]\n",
      "[Epoch 75/1001] [Batch 292/372] [D loss: 0.6985094547271729] [G loss: 0.8582099080085754]\n",
      "[Epoch 75/1001] [Batch 293/372] [D loss: 0.7036572694778442] [G loss: 0.5343523621559143]\n",
      "[Epoch 75/1001] [Batch 294/372] [D loss: 0.696757435798645] [G loss: 0.7954050898551941]\n",
      "[Epoch 75/1001] [Batch 295/372] [D loss: 0.6824266910552979] [G loss: 0.6102803349494934]\n",
      "[Epoch 75/1001] [Batch 296/372] [D loss: 0.68414306640625] [G loss: 0.7104904055595398]\n",
      "[Epoch 75/1001] [Batch 297/372] [D loss: 0.6899625062942505] [G loss: 0.6344767808914185]\n",
      "[Epoch 75/1001] [Batch 298/372] [D loss: 0.6843971610069275] [G loss: 0.7092397212982178]\n",
      "[Epoch 75/1001] [Batch 299/372] [D loss: 0.6729550361633301] [G loss: 0.6466017365455627]\n",
      "[Epoch 75/1001] [Batch 300/372] [D loss: 0.6884375810623169] [G loss: 0.6793960332870483]\n",
      "[Epoch 75/1001] [Batch 301/372] [D loss: 0.6790704727172852] [G loss: 0.6728796362876892]\n",
      "[Epoch 75/1001] [Batch 302/372] [D loss: 0.6809347867965698] [G loss: 0.6701524257659912]\n",
      "[Epoch 75/1001] [Batch 303/372] [D loss: 0.6752692461013794] [G loss: 0.6820430159568787]\n",
      "[Epoch 75/1001] [Batch 304/372] [D loss: 0.6857843995094299] [G loss: 0.6745477914810181]\n",
      "[Epoch 75/1001] [Batch 305/372] [D loss: 0.6900666952133179] [G loss: 0.6588754057884216]\n",
      "[Epoch 75/1001] [Batch 306/372] [D loss: 0.6784305572509766] [G loss: 0.6583559513092041]\n",
      "[Epoch 75/1001] [Batch 307/372] [D loss: 0.6745325326919556] [G loss: 0.7278498411178589]\n",
      "[Epoch 75/1001] [Batch 308/372] [D loss: 0.6776182651519775] [G loss: 0.6131235361099243]\n",
      "[Epoch 75/1001] [Batch 309/372] [D loss: 0.6804283857345581] [G loss: 0.7727463245391846]\n",
      "[Epoch 75/1001] [Batch 310/372] [D loss: 0.6903829574584961] [G loss: 0.5553178191184998]\n",
      "[Epoch 75/1001] [Batch 311/372] [D loss: 0.674664318561554] [G loss: 0.8623862862586975]\n",
      "[Epoch 75/1001] [Batch 312/372] [D loss: 0.6970106363296509] [G loss: 0.49762552976608276]\n",
      "[Epoch 75/1001] [Batch 313/372] [D loss: 0.6983693838119507] [G loss: 0.8855049014091492]\n",
      "[Epoch 75/1001] [Batch 314/372] [D loss: 0.7159889936447144] [G loss: 0.5386616587638855]\n",
      "[Epoch 75/1001] [Batch 315/372] [D loss: 0.6970205903053284] [G loss: 0.7747464776039124]\n",
      "[Epoch 75/1001] [Batch 316/372] [D loss: 0.6966173052787781] [G loss: 0.6057515144348145]\n",
      "[Epoch 75/1001] [Batch 317/372] [D loss: 0.6786589622497559] [G loss: 0.7017557621002197]\n",
      "[Epoch 75/1001] [Batch 318/372] [D loss: 0.682839035987854] [G loss: 0.693929135799408]\n",
      "[Epoch 75/1001] [Batch 319/372] [D loss: 0.687093198299408] [G loss: 0.6157954931259155]\n",
      "[Epoch 75/1001] [Batch 320/372] [D loss: 0.6767468452453613] [G loss: 0.7772020697593689]\n",
      "[Epoch 75/1001] [Batch 321/372] [D loss: 0.6926511526107788] [G loss: 0.603244423866272]\n",
      "[Epoch 75/1001] [Batch 322/372] [D loss: 0.6828323602676392] [G loss: 0.6751761436462402]\n",
      "[Epoch 75/1001] [Batch 323/372] [D loss: 0.6809393167495728] [G loss: 0.718363344669342]\n",
      "[Epoch 75/1001] [Batch 324/372] [D loss: 0.6860631704330444] [G loss: 0.6179175972938538]\n",
      "[Epoch 75/1001] [Batch 325/372] [D loss: 0.6908541321754456] [G loss: 0.7144039869308472]\n",
      "[Epoch 75/1001] [Batch 326/372] [D loss: 0.6907674074172974] [G loss: 0.660100519657135]\n",
      "[Epoch 75/1001] [Batch 327/372] [D loss: 0.6865918636322021] [G loss: 0.6740719079971313]\n",
      "[Epoch 75/1001] [Batch 328/372] [D loss: 0.6815253496170044] [G loss: 0.672978401184082]\n",
      "[Epoch 75/1001] [Batch 329/372] [D loss: 0.6788623332977295] [G loss: 0.6811771392822266]\n",
      "[Epoch 75/1001] [Batch 330/372] [D loss: 0.6843036413192749] [G loss: 0.6551365852355957]\n",
      "[Epoch 75/1001] [Batch 331/372] [D loss: 0.6846848726272583] [G loss: 0.7101618647575378]\n",
      "[Epoch 75/1001] [Batch 332/372] [D loss: 0.6870326995849609] [G loss: 0.6256223917007446]\n",
      "[Epoch 75/1001] [Batch 333/372] [D loss: 0.6835103631019592] [G loss: 0.7178831696510315]\n",
      "[Epoch 75/1001] [Batch 334/372] [D loss: 0.6855335235595703] [G loss: 0.6398329138755798]\n",
      "[Epoch 75/1001] [Batch 335/372] [D loss: 0.6931242346763611] [G loss: 0.6606200337409973]\n",
      "[Epoch 75/1001] [Batch 336/372] [D loss: 0.6882646083831787] [G loss: 0.7261768579483032]\n",
      "[Epoch 75/1001] [Batch 337/372] [D loss: 0.685741126537323] [G loss: 0.6078944802284241]\n",
      "[Epoch 75/1001] [Batch 338/372] [D loss: 0.6799148321151733] [G loss: 0.7320215106010437]\n",
      "[Epoch 75/1001] [Batch 339/372] [D loss: 0.6840217709541321] [G loss: 0.6165284514427185]\n",
      "[Epoch 75/1001] [Batch 340/372] [D loss: 0.6860879063606262] [G loss: 0.7043308615684509]\n",
      "[Epoch 75/1001] [Batch 341/372] [D loss: 0.6828006505966187] [G loss: 0.64574134349823]\n",
      "[Epoch 75/1001] [Batch 342/372] [D loss: 0.6729981303215027] [G loss: 0.7235034704208374]\n",
      "[Epoch 75/1001] [Batch 343/372] [D loss: 0.6814630031585693] [G loss: 0.6209139227867126]\n",
      "[Epoch 75/1001] [Batch 344/372] [D loss: 0.6873166561126709] [G loss: 0.7593894600868225]\n",
      "[Epoch 75/1001] [Batch 345/372] [D loss: 0.685384213924408] [G loss: 0.5831289291381836]\n",
      "[Epoch 75/1001] [Batch 346/372] [D loss: 0.6902316808700562] [G loss: 0.7575293779373169]\n",
      "[Epoch 75/1001] [Batch 347/372] [D loss: 0.6825834512710571] [G loss: 0.5744490623474121]\n",
      "[Epoch 75/1001] [Batch 348/372] [D loss: 0.6802408695220947] [G loss: 0.8048646450042725]\n",
      "[Epoch 75/1001] [Batch 349/372] [D loss: 0.6946389079093933] [G loss: 0.5711960196495056]\n",
      "[Epoch 75/1001] [Batch 350/372] [D loss: 0.6907724142074585] [G loss: 0.7768694758415222]\n",
      "[Epoch 75/1001] [Batch 351/372] [D loss: 0.6831005811691284] [G loss: 0.6136801838874817]\n",
      "[Epoch 75/1001] [Batch 352/372] [D loss: 0.6823551654815674] [G loss: 0.694088339805603]\n",
      "[Epoch 75/1001] [Batch 353/372] [D loss: 0.6902362108230591] [G loss: 0.6509138941764832]\n",
      "[Epoch 75/1001] [Batch 354/372] [D loss: 0.6865963339805603] [G loss: 0.6590944528579712]\n",
      "[Epoch 75/1001] [Batch 355/372] [D loss: 0.6764224171638489] [G loss: 0.7023118138313293]\n",
      "[Epoch 75/1001] [Batch 356/372] [D loss: 0.6794606447219849] [G loss: 0.6684296131134033]\n",
      "[Epoch 75/1001] [Batch 357/372] [D loss: 0.6940481662750244] [G loss: 0.6760731935501099]\n",
      "[Epoch 75/1001] [Batch 358/372] [D loss: 0.6749187111854553] [G loss: 0.6712347269058228]\n",
      "[Epoch 75/1001] [Batch 359/372] [D loss: 0.6832708120346069] [G loss: 0.694482684135437]\n",
      "[Epoch 75/1001] [Batch 360/372] [D loss: 0.6776844263076782] [G loss: 0.6061781644821167]\n",
      "[Epoch 75/1001] [Batch 361/372] [D loss: 0.6907722353935242] [G loss: 0.7939580082893372]\n",
      "[Epoch 75/1001] [Batch 362/372] [D loss: 0.7039446234703064] [G loss: 0.5051155090332031]\n",
      "[Epoch 75/1001] [Batch 363/372] [D loss: 0.7093255519866943] [G loss: 0.8931580781936646]\n",
      "[Epoch 75/1001] [Batch 364/372] [D loss: 0.7044739723205566] [G loss: 0.5133172869682312]\n",
      "[Epoch 75/1001] [Batch 365/372] [D loss: 0.7031612396240234] [G loss: 0.7818690538406372]\n",
      "[Epoch 75/1001] [Batch 366/372] [D loss: 0.6859086751937866] [G loss: 0.6442601680755615]\n",
      "[Epoch 75/1001] [Batch 367/372] [D loss: 0.6865547895431519] [G loss: 0.6738572120666504]\n",
      "[Epoch 75/1001] [Batch 368/372] [D loss: 0.6796810626983643] [G loss: 0.6834340691566467]\n",
      "[Epoch 75/1001] [Batch 369/372] [D loss: 0.6781962513923645] [G loss: 0.6657808423042297]\n",
      "[Epoch 75/1001] [Batch 370/372] [D loss: 0.6744914054870605] [G loss: 0.6834884881973267]\n",
      "[Epoch 75/1001] [Batch 371/372] [D loss: 0.6907906532287598] [G loss: 0.674919843673706]\n",
      "[Epoch 76/1001] [Batch 0/372] [D loss: 0.682763934135437] [G loss: 0.6533030271530151]\n",
      "[Epoch 76/1001] [Batch 1/372] [D loss: 0.6856704950332642] [G loss: 0.7084007263183594]\n",
      "[Epoch 76/1001] [Batch 2/372] [D loss: 0.6769906878471375] [G loss: 0.6480808258056641]\n",
      "[Epoch 76/1001] [Batch 3/372] [D loss: 0.6688759326934814] [G loss: 0.687926709651947]\n",
      "[Epoch 76/1001] [Batch 4/372] [D loss: 0.6895703077316284] [G loss: 0.6657586693763733]\n",
      "[Epoch 76/1001] [Batch 5/372] [D loss: 0.6758655905723572] [G loss: 0.6785634756088257]\n",
      "[Epoch 76/1001] [Batch 6/372] [D loss: 0.6705799102783203] [G loss: 0.683590292930603]\n",
      "[Epoch 76/1001] [Batch 7/372] [D loss: 0.6868938207626343] [G loss: 0.6726269721984863]\n",
      "[Epoch 76/1001] [Batch 8/372] [D loss: 0.6789631843566895] [G loss: 0.6855055093765259]\n",
      "[Epoch 76/1001] [Batch 9/372] [D loss: 0.6788603067398071] [G loss: 0.6179797649383545]\n",
      "[Epoch 76/1001] [Batch 10/372] [D loss: 0.6750407218933105] [G loss: 0.7289323806762695]\n",
      "[Epoch 76/1001] [Batch 11/372] [D loss: 0.670397162437439] [G loss: 0.6132809519767761]\n",
      "[Epoch 76/1001] [Batch 12/372] [D loss: 0.6859399080276489] [G loss: 0.8034094572067261]\n",
      "[Epoch 76/1001] [Batch 13/372] [D loss: 0.6792223453521729] [G loss: 0.512695848941803]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Discriminator loss for real and fake images\u001b[39;00m\n\u001b[1;32m     67\u001b[0m real_loss \u001b[38;5;241m=\u001b[39m adversarial_loss(discriminator(real_imgs, labels), real_labels)\n\u001b[0;32m---> 68\u001b[0m fake_loss \u001b[38;5;241m=\u001b[39m adversarial_loss(\u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_imgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m, fake_labels)\n\u001b[1;32m     69\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m (real_loss \u001b[38;5;241m+\u001b[39m fake_loss) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Backpropagation and optimization\u001b[39;00m\n",
      "File \u001b[0;32m~/Bachelor_work/bachelor_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Bachelor_work/bachelor_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[35], line 71\u001b[0m, in \u001b[0;36mDiscriminator_conv.forward\u001b[0;34m(self, x, labels)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, labels):\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Forward pass through the convolutional layers\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# Flatten the output\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m256\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m8\u001b[39m)\n",
      "File \u001b[0;32m~/Bachelor_work/bachelor_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Bachelor_work/bachelor_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Bachelor_work/bachelor_env/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Bachelor_work/bachelor_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Bachelor_work/bachelor_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Bachelor_work/bachelor_env/lib/python3.10/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Bachelor_work/bachelor_env/lib/python3.10/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Conv GAN training\n",
    "\n",
    "epochs = 1001  # Number of training epochs\n",
    "start_epoch = 50\n",
    "\n",
    "# Initialize the Generator and Discriminator for cGAN\n",
    "latent_dim = 100  # Dimensionality of the latent noise vector\n",
    "embedding_dim = 100  # Dimensionality of the class embedding\n",
    "img_shape = (1, 64, 64)  # Grayscale images (1 channel, 64x64)\n",
    "\n",
    "# generator = Generator_conv(latent_dim, embedding_dim, num_classes).cuda()\n",
    "# discriminator = Discriminator_conv(embedding_dim, num_classes).cuda()\n",
    "\n",
    "generator = Generator_conv(latent_dim, embedding_dim, num_classes_GAN)\n",
    "discriminator = Discriminator_conv(embedding_dim, num_classes_GAN)\n",
    "\n",
    "# Check if the model is already trained\n",
    "\n",
    "if os.path.exists(f\"/mnt/d/Bachelor_work/data_for_model/conv_GAN_train_generated/generator_conv_epoch_{start_epoch}.pth\"):\n",
    "    generator.load_state_dict(torch.load(f\"/mnt/d/Bachelor_work/data_for_model/conv_GAN_train_generated/generator_conv_epoch_{start_epoch}.pth\"))\n",
    "    discriminator.load_state_dict(torch.load(f\"/mnt/d/Bachelor_work/data_for_model/conv_GAN_train_generated/discriminator_conv_epoch_{start_epoch}.pth\"))\n",
    "\n",
    "    print(f\"Loaded generator and discriminator from epoch {start_epoch}\")\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    for i, (imgs, labels) in enumerate(dataloader_GAN):\n",
    "        # Transfer real images and labels to GPU\n",
    "\n",
    "        # real_imgs = imgs.cuda()\n",
    "        # labels = labels.cuda()  # Class labels for conditional GAN\n",
    "\n",
    "        real_imgs = imgs\n",
    "\n",
    "        # print(\"Real_imgs: \", real_imgs.shape)\n",
    "\n",
    "        # Labels for real and fake images (real: 1, fake: 0)\n",
    "\n",
    "        # real_labels = torch.ones((imgs.size(0), 1)).cuda()\n",
    "        # fake_labels = torch.zeros((imgs.size(0), 1)).cuda()\n",
    "\n",
    "        real_labels = torch.ones((imgs.size(0), 1))\n",
    "        fake_labels = torch.zeros((imgs.size(0), 1))\n",
    "\n",
    "        # ======================\n",
    "        # Train Discriminator\n",
    "        # ======================\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Sample random noise and generate class labels\n",
    "        \n",
    "        # z = torch.randn(imgs.size(0), latent_dim).cuda()\n",
    "\n",
    "        z = torch.randn(imgs.size(0), latent_dim)\n",
    "\n",
    "        # Generate fake images based on noise and class labels\n",
    "        fake_imgs = generator(z, labels)\n",
    "\n",
    "        # Discriminator loss for real and fake images\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs, labels), real_labels)\n",
    "        fake_loss = adversarial_loss(discriminator(fake_imgs.detach(), labels), fake_labels)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # ======================\n",
    "        # Train Generator\n",
    "        # ======================\n",
    "        for _ in range(5):\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Generate fake images based on noise and class labels\n",
    "            z = torch.randn(imgs.size(0), latent_dim) \n",
    "            fake_imgs = generator(z, labels)\n",
    "\n",
    "            # Try to fool the discriminator with generated images\n",
    "            g_loss = adversarial_loss(discriminator(fake_imgs, labels), real_labels)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch}/{epochs}] [Batch {i}/{len(dataloader_GAN)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
    "\n",
    "        # Save generated images at regular intervals\n",
    "        if i % 300 == 0:\n",
    "            save_image(fake_imgs.data[:25], f\"/mnt/d/Bachelor_work/data_for_model/conv_GAN_train_generated/{epoch}_{i}.png\", nrow=5, normalize=True)\n",
    "\n",
    "    # Save the model checkpoints\n",
    "    if epoch % 5 == 0 and epoch != start_epoch:\n",
    "        torch.save(generator.state_dict(), f\"/mnt/d/Bachelor_work/data_for_model/conv_GAN_train_generated/generator_conv_epoch_{epoch}.pth\")\n",
    "        torch.save(discriminator.state_dict(), f\"/mnt/d/Bachelor_work/data_for_model/conv_GAN_train_generated/discriminator_conv_epoch_{epoch}.pth\")\n",
    "\n",
    "        # Save last epoch number\n",
    "        with open(\"/mnt/d/Bachelor_work/data_for_model/conv_GAN_train_generated/last_epoch.txt\", \"w\") as file:\n",
    "            file.write(str(epoch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big CGAN Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/1001] [Batch 0/372] [D loss: 0.6970921754837036] [G loss: 0.6138484477996826\n",
      "[Epoch 0/1001] [Batch 1/372] [D loss: 0.6480926275253296] [G loss: 0.1846485137939453\n",
      "[Epoch 0/1001] [Batch 2/372] [D loss: 1.147436261177063] [G loss: 0.21116706728935242\n",
      "[Epoch 0/1001] [Batch 3/372] [D loss: 1.022820234298706] [G loss: 0.4221837520599365\n",
      "[Epoch 0/1001] [Batch 4/372] [D loss: 0.785101056098938] [G loss: 0.5847344398498535\n",
      "[Epoch 0/1001] [Batch 5/372] [D loss: 0.7029513716697693] [G loss: 0.6433526277542114\n",
      "[Epoch 0/1001] [Batch 6/372] [D loss: 0.6833410263061523] [G loss: 0.6476348638534546\n",
      "[Epoch 0/1001] [Batch 7/372] [D loss: 0.6830207109451294] [G loss: 0.620567798614502\n",
      "[Epoch 0/1001] [Batch 8/372] [D loss: 0.7089893221855164] [G loss: 0.4644010365009308\n",
      "[Epoch 0/1001] [Batch 9/372] [D loss: 0.8894323110580444] [G loss: 0.4340360164642334\n",
      "[Epoch 0/1001] [Batch 10/372] [D loss: 0.8578359484672546] [G loss: 0.5222459435462952\n",
      "[Epoch 0/1001] [Batch 11/372] [D loss: 0.7638813257217407] [G loss: 0.5940170288085938\n",
      "[Epoch 0/1001] [Batch 12/372] [D loss: 0.6976538300514221] [G loss: 0.5949752330780029\n",
      "[Epoch 0/1001] [Batch 13/372] [D loss: 0.6900701522827148] [G loss: 0.5870733261108398\n",
      "[Epoch 0/1001] [Batch 14/372] [D loss: 0.6811143755912781] [G loss: 0.6263256072998047\n",
      "[Epoch 0/1001] [Batch 15/372] [D loss: 0.6847999095916748] [G loss: 0.6830641627311707\n",
      "[Epoch 0/1001] [Batch 16/372] [D loss: 0.6824941635131836] [G loss: 0.6726289987564087\n",
      "[Epoch 0/1001] [Batch 17/372] [D loss: 0.6910549402236938] [G loss: 0.6608990430831909\n",
      "[Epoch 0/1001] [Batch 18/372] [D loss: 0.7368688583374023] [G loss: 0.6392032504081726\n",
      "[Epoch 0/1001] [Batch 19/372] [D loss: 0.7454656362533569] [G loss: 0.6599239110946655\n",
      "[Epoch 0/1001] [Batch 20/372] [D loss: 0.7325584888458252] [G loss: 0.6810591816902161\n",
      "[Epoch 0/1001] [Batch 21/372] [D loss: 0.7262091636657715] [G loss: 0.6867332458496094\n",
      "[Epoch 0/1001] [Batch 22/372] [D loss: 0.7146726846694946] [G loss: 0.6918644905090332\n",
      "[Epoch 0/1001] [Batch 23/372] [D loss: 0.7061270475387573] [G loss: 0.6650046110153198\n",
      "[Epoch 0/1001] [Batch 24/372] [D loss: 0.7115996479988098] [G loss: 0.6633578538894653\n",
      "[Epoch 0/1001] [Batch 25/372] [D loss: 0.7111787796020508] [G loss: 0.6736201643943787\n",
      "[Epoch 0/1001] [Batch 26/372] [D loss: 0.7103022933006287] [G loss: 0.674178421497345\n",
      "[Epoch 0/1001] [Batch 27/372] [D loss: 0.7021864652633667] [G loss: 0.6794459819793701\n",
      "[Epoch 0/1001] [Batch 28/372] [D loss: 0.697441577911377] [G loss: 0.6812651753425598\n",
      "[Epoch 0/1001] [Batch 29/372] [D loss: 0.6883580684661865] [G loss: 0.6920574903488159\n",
      "[Epoch 0/1001] [Batch 30/372] [D loss: 0.6874938011169434] [G loss: 0.6853322386741638\n",
      "[Epoch 0/1001] [Batch 31/372] [D loss: 0.6904944181442261] [G loss: 0.6746848225593567\n",
      "[Epoch 0/1001] [Batch 32/372] [D loss: 0.69757080078125] [G loss: 0.6691465377807617\n",
      "[Epoch 0/1001] [Batch 33/372] [D loss: 0.7034038305282593] [G loss: 0.6680806279182434\n",
      "[Epoch 0/1001] [Batch 34/372] [D loss: 0.7039996385574341] [G loss: 0.6722995042800903\n",
      "[Epoch 0/1001] [Batch 35/372] [D loss: 0.7057803869247437] [G loss: 0.6641728281974792\n",
      "[Epoch 0/1001] [Batch 36/372] [D loss: 0.7081444263458252] [G loss: 0.6523594260215759\n",
      "[Epoch 0/1001] [Batch 37/372] [D loss: 0.7048062086105347] [G loss: 0.6324791312217712\n",
      "[Epoch 0/1001] [Batch 38/372] [D loss: 0.7040419578552246] [G loss: 0.6247972249984741\n",
      "[Epoch 0/1001] [Batch 39/372] [D loss: 0.7097573280334473] [G loss: 0.6538880467414856\n",
      "[Epoch 0/1001] [Batch 40/372] [D loss: 0.7071110010147095] [G loss: 0.6892839670181274\n",
      "[Epoch 0/1001] [Batch 41/372] [D loss: 0.7077908515930176] [G loss: 0.7059134840965271\n",
      "[Epoch 0/1001] [Batch 42/372] [D loss: 0.7084264755249023] [G loss: 0.7056519389152527\n",
      "[Epoch 0/1001] [Batch 43/372] [D loss: 0.7116088271141052] [G loss: 0.6934992074966431\n",
      "[Epoch 0/1001] [Batch 44/372] [D loss: 0.7062169313430786] [G loss: 0.6778972148895264\n",
      "[Epoch 0/1001] [Batch 45/372] [D loss: 0.7005292773246765] [G loss: 0.6602170467376709\n",
      "[Epoch 0/1001] [Batch 46/372] [D loss: 0.7001152634620667] [G loss: 0.6555917263031006\n",
      "[Epoch 0/1001] [Batch 47/372] [D loss: 0.7020437717437744] [G loss: 0.6818115711212158\n",
      "[Epoch 0/1001] [Batch 48/372] [D loss: 0.7005211114883423] [G loss: 0.7092748880386353\n",
      "[Epoch 0/1001] [Batch 49/372] [D loss: 0.7001767158508301] [G loss: 0.7236127853393555\n",
      "[Epoch 0/1001] [Batch 50/372] [D loss: 0.7003605365753174] [G loss: 0.7225592136383057\n",
      "[Epoch 0/1001] [Batch 51/372] [D loss: 0.701974630355835] [G loss: 0.7079552412033081\n",
      "[Epoch 0/1001] [Batch 52/372] [D loss: 0.7016196250915527] [G loss: 0.691946268081665\n",
      "[Epoch 0/1001] [Batch 53/372] [D loss: 0.6989952325820923] [G loss: 0.6810144186019897\n",
      "[Epoch 0/1001] [Batch 54/372] [D loss: 0.6976261734962463] [G loss: 0.6823492050170898\n",
      "[Epoch 0/1001] [Batch 55/372] [D loss: 0.6981765031814575] [G loss: 0.6933408379554749\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, epochs):\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (imgs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader_GAN):\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;66;03m# Transfer real images and labels to GPU\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m         \u001b[38;5;66;03m# real_imgs = imgs.cuda()\u001b[39;00m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;66;03m# labels = labels.cuda()  # Class labels for conditional GAN\u001b[39;00m\n\u001b[1;32m     40\u001b[0m         real_imgs \u001b[38;5;241m=\u001b[39m imgs\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;66;03m# print(\"Real_imgs: \", real_imgs.shape)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m         \u001b[38;5;66;03m# Labels for real and fake images (real: 1, fake: 0)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m         \u001b[38;5;66;03m# real_labels = torch.ones((imgs.size(0), 1)).cuda()\u001b[39;00m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;66;03m# fake_labels = torch.zeros((imgs.size(0), 1)).cuda()\u001b[39;00m\n",
      "File \u001b[0;32m~/Bachelor_work/bachelor_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Bachelor_work/bachelor_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/Bachelor_work/bachelor_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1293\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1293\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1294\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Bachelor_work/bachelor_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/usr/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Big GAN training\n",
    "\n",
    "epochs = 1001  # Number of training epochs\n",
    "start_epoch = 0\n",
    "\n",
    "# Initialize the Generator and Discriminator for cGAN\n",
    "latent_dim = 100  # Dimensionality of the latent noise vector\n",
    "embedding_dim = 100  # Dimensionality of the class embedding\n",
    "img_shape = (1, 64, 64)  # Grayscale images (1 channel, 64x64)\n",
    "\n",
    "# generator = Generator_big(latent_dim, img_shape, num_classes).cuda()\n",
    "# discriminator = Discriminator_big(img_shape, num_classes).cuda()\n",
    "\n",
    "generator = Generator_big(latent_dim, img_shape, num_classes_GAN)\n",
    "discriminator = Discriminator_big(img_shape, num_classes_GAN)\n",
    "\n",
    "# Check if the model is already trained\n",
    "\n",
    "if os.path.exists(f\"/mnt/d/Bachelor_work/data_for_model/big_GAN_train_generated/generator_big_epoch_{start_epoch}.pth\"):\n",
    "    generator.load_state_dict(torch.load(f\"/mnt/d/Bachelor_work/data_for_model/big_GAN_train_generated/generator_big_epoch_{start_epoch}.pth\"))\n",
    "    discriminator.load_state_dict(torch.load(f\"/mnt/d/Bachelor_work/data_for_model/big_GAN_train_generated/discriminator_big_epoch_{start_epoch}.pth\"))\n",
    "\n",
    "    print(f\"Loaded generator and discriminator from epoch {start_epoch}\")\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    for i, (imgs, labels) in enumerate(dataloader_GAN):\n",
    "        # Transfer real images and labels to GPU\n",
    "\n",
    "        # real_imgs = imgs.cuda()\n",
    "        # labels = labels.cuda()  # Class labels for conditional GAN\n",
    "\n",
    "        real_imgs = imgs\n",
    "\n",
    "        # print(\"Real_imgs: \", real_imgs.shape)\n",
    "\n",
    "        # Labels for real and fake images (real: 1, fake: 0)\n",
    "\n",
    "        # real_labels = torch.ones((imgs.size(0), 1)).cuda()\n",
    "        # fake_labels = torch.zeros((imgs.size(0), 1)).cuda()\n",
    "\n",
    "        real_labels = torch.ones((imgs.size(0), 1))\n",
    "        fake_labels = torch.zeros((imgs.size(0), 1))\n",
    "\n",
    "        # ======================\n",
    "        # Train Discriminator\n",
    "        # ======================\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Sample random noise and generate class labels\n",
    "        \n",
    "        # z = torch.randn(imgs.size(0), latent_dim).cuda()\n",
    "\n",
    "        z = torch.randn(imgs.size(0), latent_dim)\n",
    "\n",
    "        # Generate fake images based on noise and class labels\n",
    "        fake_imgs = generator(z, labels)\n",
    "\n",
    "        # Discriminator loss for real and fake images\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs, labels), real_labels)\n",
    "        fake_loss = adversarial_loss(discriminator(fake_imgs.detach(), labels), fake_labels)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # ======================\n",
    "        # Train Generator\n",
    "        # ======================\n",
    "        for _ in range(5):\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Generate fake images based on noise and class labels\n",
    "            z = torch.randn(imgs.size(0), latent_dim) \n",
    "            fake_imgs = generator(z, labels)\n",
    "\n",
    "            # Try to fool the discriminator with generated images\n",
    "            g_loss = adversarial_loss(discriminator(fake_imgs, labels), real_labels)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch}/{epochs}] [Batch {i}/{len(dataloader_GAN)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
    "\n",
    "        # Save generated images at regular intervals\n",
    "        if i % 300 == 0:\n",
    "            save_image(fake_imgs.data[:25], f\"/mnt/d/Bachelor_work/data_for_model/big_GAN_train_generated/{epoch}_{i}.png\", nrow=5, normalize=True)\n",
    "\n",
    "    # Save the model checkpoints\n",
    "    if epoch % 10 == 0 and epoch != start_epoch:\n",
    "        torch.save(generator.state_dict(), f\"/mnt/d/Bachelor_work/data_for_model/big_GAN_train_generated/generator_big_epoch_{epoch}.pth\")\n",
    "        torch.save(discriminator.state_dict(), f\"/mnt/d/Bachelor_work/data_for_model/big_GAN_train_generated/discriminator_big_epoch_{epoch}.pth\")\n",
    "\n",
    "        # Save last epoch number\n",
    "        with open(\"/mnt/d/Bachelor_work/data_for_model/big_GAN_train_generated/last_epoch.txt\", \"w\") as file:\n",
    "            file.write(str(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test generation with CGANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_367754/3089575798.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  generator.load_state_dict(torch.load(f\"/mnt/d/Bachelor_work/data_for_model/GAN_train_generated/generator_epoch_{last_epoch}.pth\"))\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 100  # Dimensionality of the latent noise vector\n",
    "embedding_dim = 100  # Dimensionality of the class embedding\n",
    "img_shape = (1, 64, 64)  # Grayscale images (1 channel, 64x64)\n",
    "\n",
    "# Initialize the models (make sure the architecture is the same)\n",
    "generator = Generator(latent_dim, img_shape, num_classes_GAN)\n",
    "\n",
    "last_epoch = 420\n",
    "\n",
    "# Load the saved model parameters\n",
    "generator.load_state_dict(torch.load(f\"/mnt/d/Bachelor_work/data_for_model/GAN_train_generated/generator_epoch_{last_epoch}.pth\"))\n",
    "\n",
    "# Move models to GPU (if available)\n",
    "# generator.cuda()\n",
    "\n",
    "# Load the trained generator model\n",
    "generator.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Define the latent noise vector and the class label (e.g., class label U+4C61)\n",
    "latent_dim = 100  # Dimensionality of the latent noise vector\n",
    "\n",
    "# class_label = torch.tensor([0]).cuda()\n",
    "\n",
    "сlass_label = torch.tensor([0])\n",
    "\n",
    "# Generate random noise vector\n",
    "\n",
    "# z = torch.randn(1, latent_dim).cuda()\n",
    "\n",
    "z = torch.randn(1, latent_dim)\n",
    "\n",
    "# Generate the image for the chosen class\n",
    "with torch.no_grad():\n",
    "    generated_img = generator(z, сlass_label)\n",
    "\n",
    "# Save the generated image\n",
    "save_image(generated_img[0], f\"/mnt/d/Bachelor_work/data_for_model/testing_GAN/{last_epoch}_U+4C61.png\", normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each directory in group '15-29' use generative adversarial network to reach 30 images\n",
    "\n",
    "for unicode_char in kkanji_images_classes_by_amount['15-29']:\n",
    "    # Define the directory\n",
    "    char_dir = os.path.join(main_kkanji_dir, unicode_char)\n",
    "    \n",
    "    # List all files in the directory\n",
    "    files = os.listdir(char_dir)\n",
    "    \n",
    "    # Check if there are less than 30 images\n",
    "    if len(files) < 30:\n",
    "        # Load the random image\n",
    "        rand_number = np.random.randint(0, len(files))\n",
    "        image = Image.open(os.path.join(char_dir, files[rand_number]))\n",
    "        \n",
    "        # Apply generative adversarial network to generate the images\n",
    "        for i in range(30 - len(files)):\n",
    "            a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L\n",
      "L\n"
     ]
    }
   ],
   "source": [
    "test_img_0 = Image.open('/mnt/d/Bachelor_work/data_for_model/kkanji_for_GAN/U+4C61/e6eeb23552e1a21c.png')\n",
    "test_img_1 = Image.open('/mnt/d/Bachelor_work/data_for_model/kkanji_for_GAN/U+4C61/gen_aug1_0.png')\n",
    "\n",
    "# Check which img is grayscale\n",
    "print(test_img_0.mode)\n",
    "print(test_img_1.mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kanji for CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy all ''1-14' and '15-29' and '30-99' images from kkanji2_with_gen directory to the kkanji_for_VAE directory \n",
    "\n",
    "for unicode_char in kkanji_images_classes_by_amount['1-14'] + kkanji_images_classes_by_amount['15-29'] + kkanji_images_classes_by_amount['30-99']:\n",
    "    # Define the directory\n",
    "    char_dir = os.path.join(generative_kkanji_dir, unicode_char)\n",
    "\n",
    "    # Create the directory in kkanji_for_GAN\n",
    "    os.makedirs(os.path.join(kkanji_for_VAE_dir, unicode_char), exist_ok=True)\n",
    "    \n",
    "    # List all files in the directory\n",
    "    files = os.listdir(char_dir)\n",
    "    \n",
    "    # Copy all files to the kkanji_for_GAN directory\n",
    "    for file in files:\n",
    "        os.system(f\"cp {os.path.join(char_dir, file)} {os.path.join(kkanji_for_VAE_dir, unicode_char)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# original CVAE encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder for cVAE\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, img_shape, latent_dim, num_classes, embedding_dim=100):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.img_shape = img_shape\n",
    "\n",
    "        # Calculate the flattened image size\n",
    "        img_flattened_size = int(torch.prod(torch.tensor(img_shape)))\n",
    "\n",
    "        self.fc1 = nn.Linear(img_flattened_size + embedding_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256, latent_dim)\n",
    "\n",
    "        # Embedding for class labels\n",
    "        self.label_embedding = nn.Embedding(num_classes, embedding_dim)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        # Embed labels and concatenate with input images\n",
    "        label_embeds = self.label_embedding(labels)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.cat([x, label_embeds], dim=1)  # Concatenate image and label embeddings\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original CVAE decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder for cVAE\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, img_shape, num_classes, embedding_dim=100):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Calculate the flattened image size\n",
    "        img_flattened_size = int(torch.prod(torch.tensor(img_shape)))\n",
    "\n",
    "        self.fc1 = nn.Linear(latent_dim + embedding_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.fc3 = nn.Linear(512, img_flattened_size)\n",
    "        self.img_shape = img_shape\n",
    "\n",
    "        # Embedding for class labels\n",
    "        self.label_embedding = nn.Embedding(num_classes, embedding_dim)\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        label_embeds = self.label_embedding(labels)\n",
    "        z = torch.cat([z, label_embeds], dim=1)  # Concatenate latent vector and label embeddings\n",
    "        h = F.relu(self.fc1(z))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        img = torch.tanh(self.fc3(h))  # Outputs images in the range (-1, 1)\n",
    "        img = img.view(img.size(0), *self.img_shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orig CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Conditional Variational Autoencoder (cVAE)\n",
    "\n",
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, img_shape, latent_dim, num_classes):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = Encoder(img_shape, latent_dim, num_classes)\n",
    "        self.decoder = Decoder(latent_dim, img_shape, num_classes)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        mu, logvar = self.encoder(x, labels)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed_img = self.decoder(z, labels)\n",
    "        return reconstructed_img, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv CVAE Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder for CVAE with convolutional layers\n",
    "\n",
    "class Encoder_conv(nn.Module):\n",
    "    def __init__(self, img_shape, latent_dim, num_classes, embedding_dim=100):\n",
    "        super(Encoder_conv, self).__init__()\n",
    "        self.img_shape = img_shape\n",
    "\n",
    "        # Calculate the flattened image size\n",
    "        img_flattened_size = int(torch.prod(torch.tensor(img_shape)))\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8 + embedding_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256, latent_dim)\n",
    "\n",
    "        # Embedding for class labels\n",
    "        self.label_embedding = nn.Embedding(num_classes, embedding_dim)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        # Forward pass through the convolutional layers\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        # Flatten the output\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        \n",
    "        # Embed labels and concatenate with input images\n",
    "        label_embeds = self.label_embedding(labels)\n",
    "        x = torch.cat([x, label_embeds], dim=1)  # Concatenate image and label embeddings\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv CVAE Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder for CVAE with convolutional layers\n",
    "\n",
    "class Decoder_conv(nn.Module):\n",
    "    def __init__(self, latent_dim, img_shape, num_classes, embedding_dim=100):\n",
    "        super(Decoder_conv, self).__init__()\n",
    "\n",
    "        # Calculate the flattened image size\n",
    "        img_flattened_size = int(torch.prod(torch.tensor(img_shape)))\n",
    "\n",
    "        self.fc1 = nn.Linear(latent_dim + embedding_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.fc3 = nn.Linear(512, img_flattened_size)\n",
    "        self.img_shape = img_shape\n",
    "\n",
    "        # Embedding for class labels\n",
    "        self.label_embedding = nn.Embedding(num_classes, embedding_dim)\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        label_embeds = self.label_embedding(labels)\n",
    "        z = torch.cat([z, label_embeds], dim=1)  # Concatenate latent vector and label embeddings\n",
    "        h = F.relu(self.fc1(z))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        img = torch.tanh(self.fc3(h))  # Outputs images in the range (-1, 1)\n",
    "        img = img.view(img.size(0), *self.img_shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Conditional Variational Autoencoder (cVAE) with convolutional layers\n",
    "\n",
    "class ConditionalVAE_conv(nn.Module):\n",
    "    def __init__(self, img_shape, latent_dim, num_classes):\n",
    "        super(ConditionalVAE_conv, self).__init__()\n",
    "        self.encoder = Encoder_conv(img_shape, latent_dim, num_classes)\n",
    "        self.decoder = Decoder_conv(latent_dim, img_shape, num_classes)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        mu, logvar = self.encoder(x, labels)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed_img = self.decoder(z, labels)\n",
    "        return reconstructed_img, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big CVAE Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder for big cVAE\n",
    "\n",
    "class Encoder_big(nn.Module):\n",
    "    def __init__(self, img_shape, latent_dim, num_classes, embedding_dim=100):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.img_shape = img_shape\n",
    "\n",
    "        # Calculate the flattened image size\n",
    "        img_flattened_size = int(torch.prod(torch.tensor(img_shape)))\n",
    "\n",
    "        self.fc1 = nn.Linear(img_flattened_size + embedding_dim, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256, latent_dim)\n",
    "\n",
    "        # Embedding for class labels\n",
    "        self.label_embedding = nn.Embedding(num_classes, embedding_dim)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        # Embed labels and concatenate with input images\n",
    "        label_embeds = self.label_embedding(labels)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.cat([x, label_embeds], dim=1)  # Concatenate image and label embeddings\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        h = F.relu(self.fc3(h))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big CVAE Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder for big cVAE\n",
    "\n",
    "class Decoder_big(nn.Module):\n",
    "    def __init__(self, latent_dim, img_shape, num_classes, embedding_dim=100):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Calculate the flattened image size\n",
    "        img_flattened_size = int(torch.prod(torch.tensor(img_shape)))\n",
    "\n",
    "        self.fc1 = nn.Linear(latent_dim + embedding_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.fc3 = nn.Linear(512, 1024)\n",
    "        self.fc4 = nn.Linear(1024, img_flattened_size)\n",
    "        self.img_shape = img_shape\n",
    "\n",
    "        # Embedding for class labels\n",
    "        self.label_embedding = nn.Embedding(num_classes, embedding_dim)\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        label_embeds = self.label_embedding(labels)\n",
    "        z = torch.cat([z, label_embeds], dim=1)  # Concatenate latent vector and label embeddings\n",
    "        h = F.relu(self.fc1(z))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        h = F.relu(self.fc3(h))\n",
    "        img = torch.tanh(self.fc4(h))  # Outputs images in the range (-1, 1)\n",
    "        img = img.view(img.size(0), *self.img_shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the big Conditional Variational Autoencoder (cVAE)\n",
    "\n",
    "class ConditionalVAE_big(nn.Module):\n",
    "    def __init__(self, img_shape, latent_dim, num_classes):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = Encoder_big(img_shape, latent_dim, num_classes)\n",
    "        self.decoder = Decoder_big(latent_dim, img_shape, num_classes)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        mu, logvar = self.encoder(x, labels)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed_img = self.decoder(z, labels)\n",
    "        return reconstructed_img, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "dataloader_VAE = DataLoader(\n",
    "    datasets.ImageFolder(\n",
    "        kkanji_for_VAE_dir,  # Your dataset folder\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.Resize(64),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])  # Normalize to (-1, 1)\n",
    "        ])\n",
    "    ),\n",
    "    batch_size=128, shuffle=True, num_workers=4\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orig CVAE training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "latent_dim = 100\n",
    "img_shape = (1, 64, 64)  # Grayscale 64x64 images\n",
    "epochs = 1001\n",
    "start_epoch = 0\n",
    "\n",
    "# Initialize CVAE model\n",
    "\n",
    "# cvae = ConditionalVAE(img_shape, latent_dim, num_classes_VAE).cuda()\n",
    "\n",
    "cvae = ConditionalVAE(img_shape, latent_dim, num_classes_VAE)\n",
    "\n",
    "# Download trained model\n",
    "if os.path.exists(f\"/mnt/d/Bachelor_work/data_for_model/VAE_train_generated/cvae_epoch_{start_epoch}.pth\"):\n",
    "    cvae.load_state_dict(torch.load(f\"/mnt/d/Bachelor_work/data_for_model/VAE_train_generated/cvae_epoch_{start_epoch}.pth\"))\n",
    "\n",
    "    print(\"Loaded cvae model from epoch \", start_epoch) \n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(cvae.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "cvae.train()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    train_loss = 0\n",
    "    for i, (imgs, labels) in enumerate(dataloader_VAE):\n",
    "\n",
    "        # imgs = imgs.cuda()\n",
    "        # labels = labels.cuda()\n",
    "\n",
    "        # Forward pass\n",
    "        recon_imgs, mu, logvar = cvae(imgs, labels)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function_VAE(recon_imgs, imgs, mu, logvar)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save the reconstructed images at regular intervals\n",
    "        if i % 300 == 0:\n",
    "            save_image(recon_imgs.data[:25], f\"/mnt/d/Bachelor_work/data_for_model/VAE_train_generated/reconstructed_img_{epoch}_{i}.png\", nrow=5, normalize=True)\n",
    "\n",
    "    print(f\"[Epoch {epoch}/{epochs}] [Loss: {train_loss / len(dataloader_VAE.dataset)}]\")\n",
    "\n",
    "    # Save the model every 10 epochs\n",
    "    if epoch % 10 == 0 and epoch != start_epoch:\n",
    "        torch.save(cvae.state_dict(), f\"/mnt/d/Bachelor_work/data_for_model/VAE_train_generated/cvae_epoch_{start_epoch}.pth\")\n",
    "\n",
    "        # Save last epoch number\n",
    "        with open(\"/mnt/d/Bachelor_work/data_for_model/VAE_train_generated/last_epoch.txt\", \"w\") as file:\n",
    "            file.write(str(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv CVAE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "latent_dim = 100\n",
    "img_shape = (1, 64, 64)  # Grayscale 64x64 images\n",
    "epochs = 1001\n",
    "start_epoch = 0\n",
    "\n",
    "# Initialize conv CVAE model\n",
    "\n",
    "# cvae_conv = ConditionalVAE_conv(img_shape, latent_dim, num_classes_VAE).cuda()\n",
    "\n",
    "cvae_conv = ConditionalVAE_conv(img_shape, latent_dim, num_classes_VAE)\n",
    "\n",
    "# Download trained model\n",
    "\n",
    "if os.path.exists(f\"/mnt/d/Bachelor_work/data_for_model/conv_VAE_train_generated/cvae_conv_epoch_{start_epoch}.pth\"):\n",
    "    cvae_conv.load_state_dict(torch.load(f\"/mnt/d/Bachelor_work/data_for_model/conv_VAE_train_generated/cvae_conv_epoch_{start_epoch}.pth\"))\n",
    "\n",
    "    print(\"Loaded cvae_conv model from epoch \", start_epoch)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(cvae_conv.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "cvae_conv.train()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    train_loss = 0\n",
    "    for i, (imgs, labels) in enumerate(dataloader_VAE):\n",
    "\n",
    "        # imgs = imgs.cuda()\n",
    "        # labels = labels.cuda()\n",
    "\n",
    "        # Forward pass\n",
    "        recon_imgs, mu, logvar = cvae_conv(imgs, labels)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function_VAE(recon_imgs, imgs, mu, logvar)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save the reconstructed images at regular intervals\n",
    "        if i % 300 == 0:\n",
    "            save_image(recon_imgs.data[:25], f\"/mnt/d/Bachelor_work/data_for_model/conv_VAE_train_generated/reconstructed_img_conv_{epoch}_{i}.png\", nrow=5, normalize=True)\n",
    "\n",
    "    print(f\"[Epoch {epoch}/{epochs}] [Loss: {train_loss / len(dataloader_VAE.dataset)}]\")\n",
    "\n",
    "    # Save the model every 10 epochs\n",
    "    if epoch % 5 == 0 and epoch != start_epoch:\n",
    "        torch.save(cvae_conv.state_dict(), f\"/mnt/d/Bachelor_work/data_for_model/conv_VAE_train_generated/cvae_conv_epoch_{start_epoch}.pth\")\n",
    "\n",
    "        # Save last epoch number\n",
    "        with open(\"/mnt/d/Bachelor_work/data_for_model/conv_VAE_train_generated/last_epoch.txt\", \"w\") as file:\n",
    "            file.write(str(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big CVAE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "latent_dim = 100\n",
    "img_shape = (1, 64, 64)  # Grayscale 64x64 images\n",
    "epochs = 1001\n",
    "start_epoch = 0\n",
    "\n",
    "# Initialize big CVAE model\n",
    "\n",
    "# cvae_big = ConditionalVAE_big(img_shape, latent_dim, num_classes_VAE).cuda()\n",
    "\n",
    "cvae_big = ConditionalVAE_big(img_shape, latent_dim, num_classes_VAE)\n",
    "\n",
    "# Download trained model\n",
    "\n",
    "if os.path.exists(f\"/mnt/d/Bachelor_work/data_for_model/big_VAE_train_generated/cvae_big_epoch_{start_epoch}.pth\"):\n",
    "    cvae_big.load_state_dict(torch.load(f\"/mnt/d/Bachelor_work/data_for_model/big_VAE_train_generated/cvae_big_epoch_{start_epoch}.pth\"))\n",
    "\n",
    "    print(\"Loaded cvae_big model from epoch \", start_epoch)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(cvae_big.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "cvae_big.train()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    train_loss = 0\n",
    "    for i, (imgs, labels) in enumerate(dataloader_VAE):\n",
    "\n",
    "        # imgs = imgs.cuda()\n",
    "        # labels = labels.cuda()\n",
    "\n",
    "        # Forward pass\n",
    "        recon_imgs, mu, logvar = cvae_big(imgs, labels)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function_VAE(recon_imgs, imgs, mu, logvar)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save the reconstructed images at regular intervals\n",
    "        if i % 300 == 0:\n",
    "            save_image(recon_imgs.data[:25], f\"/mnt/d/Bachelor_work/data_for_model/big_VAE_train_generated/reconstructed_img_big_{epoch}_{i}.png\", nrow=5, normalize=True)\n",
    "\n",
    "    print(f\"[Epoch {epoch}/{epochs}] [Loss: {train_loss / len(dataloader_VAE.dataset)}]\")\n",
    "\n",
    "    # Save the model every 10 epochs\n",
    "    if epoch % 10 == 0 and epoch != start_epoch:\n",
    "        torch.save(cvae_big.state_dict(), f\"/mnt/d/Bachelor_work/data_for_model/big_VAE_train_generated/cvae_big_epoch_{start_epoch}.pth\")\n",
    "\n",
    "        # Save last epoch number\n",
    "        with open(\"/mnt/d/Bachelor_work/data_for_model/big_VAE_train_generated/last_epoch.txt\", \"w\") as file:\n",
    "            file.write(str(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test CVAE Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model (if needed)\n",
    "cvae.load_state_dict(torch.load(\"cvae_epoch_100.pth\"))\n",
    "cvae.eval()\n",
    "\n",
    "# Choose a specific class (e.g., class 42)\n",
    "class_label = 42\n",
    "\n",
    "# class_label_tensor = torch.tensor([class_label]).cuda()\n",
    "\n",
    "class_label_tensor = torch.tensor([class_label])\n",
    "\n",
    "# Sample random latent vector\n",
    "\n",
    "# z = torch.randn(1, latent_dim).cuda()\n",
    "\n",
    "z = torch.randn(1, latent_dim).cuda()\n",
    "\n",
    "# Generate an image for the specific class\n",
    "with torch.no_grad():\n",
    "    generated_img = cvae.decoder(z, class_label_tensor)\n",
    "\n",
    "# Save the generated image\n",
    "save_image(generated_img[0], \"generated_img.png\", normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data for aug2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy all ''1-14' and '15-29' and '30-99' and '100-299' images from kkanji2_with_gen directory to the kkanji_for_aug2 directory  \n",
    "\n",
    "for unicode_char in kkanji_images_classes_by_amount['1-14'] + kkanji_images_classes_by_amount['15-29'] + kkanji_images_classes_by_amount['30-99'] + kkanji_images_classes_by_amount['100-299']:\n",
    "    # Define the directory\n",
    "    char_dir = os.path.join(generative_kkanji_dir, unicode_char)\n",
    "\n",
    "    # Create the directory in kkanji_for_GAN\n",
    "    os.makedirs(os.path.join(kkanji_for_aug2_dir, unicode_char), exist_ok=True)\n",
    "    \n",
    "    # List all files in the directory\n",
    "    files = os.listdir(char_dir)\n",
    "    \n",
    "    # Copy all files to the kkanji_for_GAN directory\n",
    "    for file in files:\n",
    "        os.system(f\"cp {os.path.join(char_dir, file)} {os.path.join(kkanji_for_aug2_dir, unicode_char)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aug2 using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use old augmentation pipeline to reach 300 images for each class\n",
    "\n",
    "for unicode_char in kkanji_images_classes_by_amount['1-14'] + kkanji_images_classes_by_amount['15-29'] + kkanji_images_classes_by_amount['30-99'] + kkanji_images_classes_by_amount['100-299']:\n",
    "    # Define the directory\n",
    "    char_dir = os.path.join(kkanji_for_aug2_dir, unicode_char)\n",
    "    \n",
    "    # List all files in the directory\n",
    "    files = os.listdir(char_dir)\n",
    "    \n",
    "    # Check if there are less than 300 images\n",
    "    if len(files) < 300:\n",
    "        # Apply augmentation pipeline\n",
    "        for i in range(300 - len(files)):\n",
    "            # Load the random image\n",
    "            rand_number = np.random.randint(0, len(files))\n",
    "            image = Image.open(os.path.join(char_dir, files[rand_number]))\n",
    "\n",
    "            augmented_image = augmentation_pipeline(image)\n",
    "\n",
    "            save_dir = os.path.join(generative_kkanji_dir, unicode_char)\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            \n",
    "            # Save the augmented image\n",
    "            augmented_image.save(os.path.join(save_dir, f\"gen_aug2_{i}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full balance with aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy files that are less than 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy all ''1-14' and '15-29' and '30-99' and '100-299' images from main_kkanji_dir directory to the kkanji_aug_only directory\n",
    "\n",
    "for unicode_char in kkanji_images_classes_by_amount['1-14'] + kkanji_images_classes_by_amount['15-29'] + kkanji_images_classes_by_amount['30-99'] + kkanji_images_classes_by_amount['100-299']:\n",
    "    # Define the directory\n",
    "    char_dir = os.path.join(main_kkanji_dir, unicode_char)\n",
    "\n",
    "    # Create the directory in kkanji_aug_only\n",
    "    os.makedirs(os.path.join(kkanji_aug_only_dir, unicode_char), exist_ok=True)\n",
    "    \n",
    "    # List all files in the directory\n",
    "    files = os.listdir(char_dir)\n",
    "    \n",
    "    # Copy all files to the kkanji_aug_only directory\n",
    "    for file in files:\n",
    "        os.system(f\"cp {os.path.join(char_dir, file)} {os.path.join(kkanji_aug_only_dir, unicode_char)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use old augmentation pipeline to reach 300 images for each class that is lower than 300\n",
    "\n",
    "for unicode_char in kkanji_images_classes_by_amount['1-14'] + kkanji_images_classes_by_amount['15-29'] + kkanji_images_classes_by_amount['30-99'] + kkanji_images_classes_by_amount['100-299']:\n",
    "    # Define the directory\n",
    "    char_dir = os.path.join(main_kkanji_dir, unicode_char)\n",
    "    \n",
    "    # List all files in the directory\n",
    "    files = os.listdir(char_dir)\n",
    "    \n",
    "    # Check if there are less than 300 images\n",
    "    if len(files) < 300:\n",
    "        # Apply augmentation pipeline\n",
    "        for i in range(300 - len(files)):\n",
    "            # Load the random image\n",
    "            rand_number = np.random.randint(0, len(files))\n",
    "            image = Image.open(os.path.join(char_dir, files[rand_number]))\n",
    "\n",
    "            augmented_image = augmentation_pipeline(image)\n",
    "\n",
    "            save_dir = os.path.join(kkanji_aug_only_dir, unicode_char)\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            \n",
    "            # Save the augmented image\n",
    "            augmented_image.save(os.path.join(save_dir, f\"gen_aug_only_{i}.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random sampling from classes more than 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for classes 300+ choose random 300 images\n",
    "\n",
    "for unicode_char in kkanji_images_classes_by_amount['300+']:\n",
    "    # Define the directory\n",
    "    char_dir = os.path.join(main_kkanji_dir, unicode_char)\n",
    "    \n",
    "    # List all files in the directory\n",
    "    files = os.listdir(char_dir)\n",
    "    \n",
    "    # Choose 300 random images that does not repeat \n",
    "    random_files = np.random.choice(files, 300, replace=False)\n",
    "\n",
    "    # Create the directory in kkanji_aug_only\n",
    "    os.makedirs(os.path.join(kkanji_aug_only_dir, unicode_char), exist_ok=True)\n",
    "\n",
    "    # Copy the random files to the kkanji_aug_only directory\n",
    "    for file in random_files:\n",
    "        os.system(f\"cp {os.path.join(char_dir, file)} {os.path.join(kkanji_aug_only_dir, unicode_char)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
